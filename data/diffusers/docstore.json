[
  [
    "0",
    {
      "pageContent": "def pytest_addoption(parser):\n    from diffusers.utils.testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)",
      "metadata": {
        "source": "examples/conftest.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "1",
    {
      "pageContent": "def pytest_terminal_summary(terminalreporter):\n    from diffusers.utils.testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)",
      "metadata": {
        "source": "examples/conftest.py",
        "range": {
          "start": { "row": 39, "column": 0 },
          "end": { "row": 39, "column": 0 }
        }
      }
    }
  ],
  [
    "2",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\", type=str, default=None, required=True, help=\"A folder containing the training data.\"\n    )\n    parser.add_argument(\n        \"--placeholder_token\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A token to use as a placeholder for the concept.\",\n    )\n    parser.add_argument(\n        \"--initializer_token\", type=str, default=None, required=True, help=\"A token to use as initializer word.\"\n    )\n    parser.add_argument(\"--learnable_property\", type=str, default=\"object\", help=\"Choose between 'object' and 'style'\")\n    parser.add_argument(\"--repeats\", type=int, default=100, help=\"How many times to repeat the training data.\")\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"text-inversion-model\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n  ",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "3",
    {
      "pageContent": "class TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 255, "column": 0 },
          "end": { "row": 255, "column": 0 }
        }
      }
    }
  ],
  [
    "4",
    {
      "pageContent": "def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 256, "column": 4 },
          "end": { "row": 256, "column": 4 }
        }
      }
    }
  ],
  [
    "5",
    {
      "pageContent": "def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            (\n                h,\n                w,\n            ) = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 298, "column": 4 },
          "end": { "row": 298, "column": 4 }
        }
      }
    }
  ],
  [
    "6",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 341, "column": 0 },
          "end": { "row": 341, "column": 0 }
        }
      }
    }
  ],
  [
    "7",
    {
      "pageContent": "def resize_token_embeddings(model, new_num_tokens, initializer_token_id, placeholder_token_id, rng):\n    if model.config.vocab_size == new_num_tokens or new_num_tokens is None:\n        return\n    model.config.vocab_size = new_num_tokens\n\n    params = model.params\n    old_embeddings = params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"]\n    old_num_tokens, emb_dim = old_embeddings.shape\n\n    initializer = jax.nn.initializers.normal()\n\n    new_embeddings = initializer(rng, (new_num_tokens, emb_dim))\n    new_embeddings = new_embeddings.at[:old_num_tokens].set(old_embeddings)\n    new_embeddings = new_embeddings.at[placeholder_token_id].set(new_embeddings[initializer_token_id])\n    params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"] = new_embeddings\n\n    model.params = params\n    return model",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 351, "column": 0 },
          "end": { "row": 351, "column": 0 }
        }
      }
    }
  ],
  [
    "8",
    {
      "pageContent": "def main():\n    args = parse_args()\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    if jax.process_index() == 0:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            create_repo(repo_name, exist_ok=True, token=args.hub_token)\n            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    # Setup logging, we only want one process per machine to log things on the screen.\n    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n    if jax.process_index() == 0:\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n\n    # Load the tokenizer and add the placeholder token as a additional special token\n    if args.tokenizer_name:\n        tokenizer ",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 375, "column": 0 },
          "end": { "row": 375, "column": 0 }
        }
      }
    }
  ],
  [
    "9",
    {
      "pageContent": "def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n\n        batch = {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n        batch = {k: v.numpy() for k, v in batch.items()}\n\n        return batch",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 465, "column": 4 },
          "end": { "row": 465, "column": 4 }
        }
      }
    }
  ],
  [
    "10",
    {
      "pageContent": "def create_mask(params, label_fn):\n        def _map(params, mask, label_fn):\n            for k in params:\n                if label_fn(k):\n                    mask[k] = \"token_embedding\"\n                else:\n                    if isinstance(params[k], dict):\n                        mask[k] = {}\n                        _map(params[k], mask[k], label_fn)\n                    else:\n                        mask[k] = \"zero\"\n\n        mask = {}\n        _map(params, mask, label_fn)\n        return mask",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 493, "column": 4 },
          "end": { "row": 493, "column": 4 }
        }
      }
    }
  ],
  [
    "11",
    {
      "pageContent": "def zero_grads():\n        # from https://github.com/deepmind/optax/issues/159#issuecomment-896459491\n        def init_fn(_):\n            return ()\n\n        def update_fn(updates, state, params=None):\n            return jax.tree_util.tree_map(jnp.zeros_like, updates), ()\n\n        return optax.GradientTransformation(init_fn, update_fn)",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 509, "column": 4 },
          "end": { "row": 509, "column": 4 }
        }
      }
    }
  ],
  [
    "12",
    {
      "pageContent": "def train_step(state, vae_params, unet_params, batch, train_rng):\n        dropout_rng, sample_rng, new_train_rng = jax.random.split(train_rng, 3)\n\n        def compute_loss(params):\n            vae_outputs = vae.apply(\n                {\"params\": vae_params}, batch[\"pixel_values\"], deterministic=True, method=vae.encode\n            )\n            latents = vae_outputs.latent_dist.sample(sample_rng)\n            # (NHWC) -> (NCHW)\n            latents = jnp.transpose(latents, (0, 3, 1, 2))\n            latents = latents * vae.config.scaling_factor\n\n            noise_rng, timestep_rng = jax.random.split(sample_rng)\n            noise = jax.random.normal(noise_rng, latents.shape)\n            bsz = latents.shape[0]\n            timesteps = jax.random.randint(\n                timestep_rng,\n                (bsz,),\n                0,\n                noise_scheduler.config.num_train_timesteps,\n            )\n            noisy_latents = noise_scheduler.add_noise(noise_scheduler_state, latents, noise, timesteps)\n            encoder_hidden_states = state.apply_fn(\n                batch[\"input_ids\"], params=params, dropout_rng=dropout_rng, train=True\n            )[0]\n            # Predict the noise residual and compute loss\n            model_pred = unet.apply(\n                {\"params\": unet_params}, noisy_latents, timesteps, encoder_hidden_states, train=False\n            ).sample\n\n            # Get the target for loss depending on the prediction type\n            if noise_scheduler.config.prediction_type == \"epsilon\":\n                target = noise\n            elif noise_scheduler.config.predict",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 536, "column": 4 },
          "end": { "row": 536, "column": 4 }
        }
      }
    }
  ],
  [
    "13",
    {
      "pageContent": "def log_validation(text_encoder, tokenizer, unet, vae, args, accelerator, weight_dtype, epoch):\n    logger.info(\n        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n        f\" {args.validation_prompt}.\"\n    )\n    # create pipeline (note: unet and vae are loaded again in float32)\n    pipeline = DiffusionPipeline.from_pretrained(\n        args.pretrained_model_name_or_path,\n        text_encoder=accelerator.unwrap_model(text_encoder),\n        tokenizer=tokenizer,\n        unet=unet,\n        vae=vae,\n        revision=args.revision,\n        torch_dtype=weight_dtype,\n    )\n    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n    pipeline = pipeline.to(accelerator.device)\n    pipeline.set_progress_bar_config(disable=True)\n\n    # run inference\n    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)\n    images = []\n    for _ in range(args.num_validation_images):\n        with torch.autocast(\"cuda\"):\n            image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\n        images.append(image)\n\n    for tracker in accelerator.trackers:\n        if tracker.name == \"tensorboard\":\n            np_images = np.stack([np.asarray(img) for img in images])\n            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\n        if tracker.name == \"wandb\":\n            tracker.log(\n                {\n                    \"validation\": [\n                        wandb.Image(image, caption=f\"{i}: {args",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 85, "column": 0 },
          "end": { "row": 85, "column": 0 }
        }
      }
    }
  ],
  [
    "14",
    {
      "pageContent": "def save_progress(text_encoder, placeholder_token_id, accelerator, args, save_path):\n    logger.info(\"Saving embeddings\")\n    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n    learned_embeds_dict = {args.placeholder_token: learned_embeds.detach().cpu()}\n    torch.save(learned_embeds_dict, save_path)",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 129, "column": 0 },
          "end": { "row": 129, "column": 0 }
        }
      }
    }
  ],
  [
    "15",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Save learned_embeds.bin every X updates steps.\",\n    )\n    parser.add_argument(\n        \"--only_save_embeds\",\n        action=\"store_true\",\n        default=False,\n        help=\"Save only the embeddings for the new concept.\",\n    )\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\", type=str, default=None, required=True, help=\"A folder containing the training data.\"\n    )\n    parser.add_argument(\n        \"--placeholder_token\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A token to use as a placeholder for the concept.\",\n    )\n    parser.add_argument(\n        \"--initializer_token\", type=str, default=None, required=True, help=\"A token to use as initializer word.\"\n    )\n    parser.add_argument(\"--learnable_property\", type=str, default=\"object\", h",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 136, "column": 0 },
          "end": { "row": 136, "column": 0 }
        }
      }
    }
  ],
  [
    "16",
    {
      "pageContent": "class TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 435, "column": 0 },
          "end": { "row": 435, "column": 0 }
        }
      }
    }
  ],
  [
    "17",
    {
      "pageContent": "def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 436, "column": 4 },
          "end": { "row": 436, "column": 4 }
        }
      }
    }
  ],
  [
    "18",
    {
      "pageContent": "def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            (\n                h,\n                w,\n            ) = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 478, "column": 4 },
          "end": { "row": 478, "column": 4 }
        }
      }
    }
  ],
  [
    "19",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 521, "column": 0 },
          "end": { "row": 521, "column": 0 }
        }
      }
    }
  ],
  [
    "20",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n ",
      "metadata": {
        "source": "examples/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 531, "column": 0 },
          "end": { "row": 531, "column": 0 }
        }
      }
    }
  ],
  [
    "21",
    {
      "pageContent": "def prepare_mask_and_masked_image(image, mask):\n    image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    mask = np.array(mask.convert(\"L\"))\n    mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask)\n\n    masked_image = image * (mask < 0.5)\n\n    return mask, masked_image",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "22",
    {
      "pageContent": "def random_mask(im_shape, ratio=1, mask_full_image=False):\n    mask = Image.new(\"L\", im_shape, 0)\n    draw = ImageDraw.Draw(mask)\n    size = (random.randint(0, int(im_shape[0] * ratio)), random.randint(0, int(im_shape[1] * ratio)))\n    # use this to always mask the whole image\n    if mask_full_image:\n        size = (int(im_shape[0] * ratio), int(im_shape[1] * ratio))\n    limits = (im_shape[0] - size[0] // 2, im_shape[1] - size[1] // 2)\n    center = (random.randint(size[0] // 2, limits[0]), random.randint(size[1] // 2, limits[1]))\n    draw_type = random.randint(0, 1)\n    if draw_type == 0 or mask_full_image:\n        draw.rectangle(\n            (center[0] - size[0] // 2, center[1] - size[1] // 2, center[0] + size[0] // 2, center[1] + size[1] // 2),\n            fill=255,\n        )\n    else:\n        draw.ellipse(\n            (center[0] - size[0] // 2, center[1] - size[1] // 2, center[0] + size[0] // 2, center[1] + size[1] // 2),\n            fill=255,\n        )\n\n    return mask",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 58, "column": 0 },
          "end": { "row": 58, "column": 0 }
        }
      }
    }
  ],
  [
    "23",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--with_prior_preservation\",\n        default=False,\n        action=\"store_true\",\n        help=\"Flag to add prior preservation loss.\",\n    )\n    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")\n    parser.add_argument(\n        \"--num_class_i",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 82, "column": 0 },
          "end": { "row": 82, "column": 0 }
        }
      }
    }
  ],
  [
    "24",
    {
      "pageContent": "class DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms_resize_and_crop = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n  ",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 297, "column": 0 },
          "end": { "row": 297, "column": 0 }
        }
      }
    }
  ],
  [
    "25",
    {
      "pageContent": "def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms_resize_and_crop = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n            ]\n        )\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.ToTe",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 303, "column": 4 },
          "end": { "row": 303, "column": 4 }
        }
      }
    }
  ],
  [
    "26",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        instance_image = self.image_transforms_resize_and_crop(instance_image)\n\n        example[\"PIL_images\"] = instance_image\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            self.instance_prompt,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            class_image = self.image_transforms_resize_and_crop(class_image)\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_PIL_images\"] = class_image\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                padding=\"do_not_pad\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids\n\n        return example",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 353, "column": 4 },
          "end": { "row": 353, "column": 4 }
        }
      }
    }
  ],
  [
    "27",
    {
      "pageContent": "class PromptDataset(Dataset):\n    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n\n    def __init__(self, prompt, num_samples):\n        self.prompt = prompt\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 387, "column": 0 },
          "end": { "row": 387, "column": 0 }
        }
      }
    }
  ],
  [
    "28",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 397, "column": 4 },
          "end": { "row": 397, "column": 4 }
        }
      }
    }
  ],
  [
    "29",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 404, "column": 0 },
          "end": { "row": 404, "column": 0 }
        }
      }
    }
  ],
  [
    "30",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=\"tensorboard\",\n        logging_dir=logging_dir,\n        accelerator_project_config=accelerator_project_config,\n    )\n\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n        raise ValueError(\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n        )\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    if args.with_prior_preservation:\n        class_images_dir = Path(args.class_data_dir)\n        if not class_images_dir.exists():\n            class_images_dir.mkdir(parents=True)\n        cur_class_images = len(list(class_images_dir.iterdir()))\n\n        if cur_class_images < args.num_class_images:\n            torch_dtype = torch.float16 if accelerator.device",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 414, "column": 0 },
          "end": { "row": 414, "column": 0 }
        }
      }
    }
  ],
  [
    "31",
    {
      "pageContent": "def collate_fn(examples):\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n        pixel_values = [example[\"instance_images\"] for example in examples]\n\n        # Concat class and instance examples for prior preservation.\n        # We do this to avoid doing two forward passes.\n        if args.with_prior_preservation:\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n            pixel_values += [example[\"class_images\"] for example in examples]\n            pior_pil = [example[\"class_PIL_images\"] for example in examples]\n\n        masks = []\n        masked_images = []\n        for example in examples:\n            pil_image = example[\"PIL_images\"]\n            # generate a random mask\n            mask = random_mask(pil_image.size, 1, False)\n            # prepare mask and masked image\n            mask, masked_image = prepare_mask_and_masked_image(pil_image, mask)\n\n            masks.append(mask)\n            masked_images.append(masked_image)\n\n        if args.with_prior_preservation:\n            for pil_image in pior_pil:\n                # generate a random mask\n                mask = random_mask(pil_image.size, 1, False)\n                # prepare mask and masked image\n                mask, masked_image = prepare_mask_and_masked_image(pil_image, mask)\n\n                masks.append(mask)\n                masked_images.append(masked_image)\n\n        pixel_values = torch.stack(pixel_values)\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n        input_ids = tokenizer.pad({\"input_ids\": input_ids}",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py",
        "range": {
          "start": { "row": 564, "column": 4 },
          "end": { "row": 564, "column": 4 }
        }
      }
    }
  ],
  [
    "32",
    {
      "pageContent": "def prepare_mask_and_masked_image(image, mask):\n    image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    mask = np.array(mask.convert(\"L\"))\n    mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask)\n\n    masked_image = image * (mask < 0.5)\n\n    return mask, masked_image",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "33",
    {
      "pageContent": "def random_mask(im_shape, ratio=1, mask_full_image=False):\n    mask = Image.new(\"L\", im_shape, 0)\n    draw = ImageDraw.Draw(mask)\n    size = (random.randint(0, int(im_shape[0] * ratio)), random.randint(0, int(im_shape[1] * ratio)))\n    # use this to always mask the whole image\n    if mask_full_image:\n        size = (int(im_shape[0] * ratio), int(im_shape[1] * ratio))\n    limits = (im_shape[0] - size[0] // 2, im_shape[1] - size[1] // 2)\n    center = (random.randint(size[0] // 2, limits[0]), random.randint(size[1] // 2, limits[1]))\n    draw_type = random.randint(0, 1)\n    if draw_type == 0 or mask_full_image:\n        draw.rectangle(\n            (center[0] - size[0] // 2, center[1] - size[1] // 2, center[0] + size[0] // 2, center[1] + size[1] // 2),\n            fill=255,\n        )\n    else:\n        draw.ellipse(\n            (center[0] - size[0] // 2, center[1] - size[1] // 2, center[0] + size[0] // 2, center[1] + size[1] // 2),\n            fill=255,\n        )\n\n    return mask",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "34",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--with_prior_preservation\",\n        default=False,\n        action=\"store_true\",\n        help=\"Flag to add prior preservation loss.\",\n    )\n    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")\n    parser.add_argument(\n        \"--num_class_i",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 78, "column": 0 },
          "end": { "row": 78, "column": 0 }
        }
      }
    }
  ],
  [
    "35",
    {
      "pageContent": "class DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms_resize_and_crop = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n  ",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 296, "column": 0 },
          "end": { "row": 296, "column": 0 }
        }
      }
    }
  ],
  [
    "36",
    {
      "pageContent": "def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms_resize_and_crop = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n            ]\n        )\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.ToTe",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 302, "column": 4 },
          "end": { "row": 302, "column": 4 }
        }
      }
    }
  ],
  [
    "37",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        instance_image = self.image_transforms_resize_and_crop(instance_image)\n\n        example[\"PIL_images\"] = instance_image\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            self.instance_prompt,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            class_image = self.image_transforms_resize_and_crop(class_image)\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_PIL_images\"] = class_image\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                padding=\"do_not_pad\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids\n\n        return example",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 352, "column": 4 },
          "end": { "row": 352, "column": 4 }
        }
      }
    }
  ],
  [
    "38",
    {
      "pageContent": "class PromptDataset(Dataset):\n    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n\n    def __init__(self, prompt, num_samples):\n        self.prompt = prompt\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 386, "column": 0 },
          "end": { "row": 386, "column": 0 }
        }
      }
    }
  ],
  [
    "39",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 396, "column": 4 },
          "end": { "row": 396, "column": 4 }
        }
      }
    }
  ],
  [
    "40",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 403, "column": 0 },
          "end": { "row": 403, "column": 0 }
        }
      }
    }
  ],
  [
    "41",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=\"tensorboard\",\n        logging_dir=logging_dir,\n        accelerator_project_config=accelerator_project_config,\n    )\n\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n        raise ValueError(\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n        )\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    if args.with_prior_preservation:\n        class_images_dir = Path(args.class_data_dir)\n        if not class_images_dir.exists():\n            class_images_dir.mkdir(parents=True)\n        cur_class_images = len(list(class_images_dir.iterdir()))\n\n        if cur_class_images < args.num_class_images:\n            torch_dtype = torch.float16 if accelerator.device",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 413, "column": 0 },
          "end": { "row": 413, "column": 0 }
        }
      }
    }
  ],
  [
    "42",
    {
      "pageContent": "def collate_fn(examples):\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n        pixel_values = [example[\"instance_images\"] for example in examples]\n\n        # Concat class and instance examples for prior preservation.\n        # We do this to avoid doing two forward passes.\n        if args.with_prior_preservation:\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n            pixel_values += [example[\"class_images\"] for example in examples]\n            pior_pil = [example[\"class_PIL_images\"] for example in examples]\n\n        masks = []\n        masked_images = []\n        for example in examples:\n            pil_image = example[\"PIL_images\"]\n            # generate a random mask\n            mask = random_mask(pil_image.size, 1, False)\n            # prepare mask and masked image\n            mask, masked_image = prepare_mask_and_masked_image(pil_image, mask)\n\n            masks.append(mask)\n            masked_images.append(masked_image)\n\n        if args.with_prior_preservation:\n            for pil_image in pior_pil:\n                # generate a random mask\n                mask = random_mask(pil_image.size, 1, False)\n                # prepare mask and masked image\n                mask, masked_image = prepare_mask_and_masked_image(pil_image, mask)\n\n                masks.append(mask)\n                masked_images.append(masked_image)\n\n        pixel_values = torch.stack(pixel_values)\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n        input_ids = tokenizer.pad({\"input_ids\": input_ids}",
      "metadata": {
        "source": "examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora.py",
        "range": {
          "start": { "row": 610, "column": 4 },
          "end": { "row": 610, "column": 4 }
        }
      }
    }
  ],
  [
    "43",
    {
      "pageContent": "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=args.revision,\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"RobertaSeriesModelWithTransformation\":\n        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n\n        return RobertaSeriesModelWithTransformation\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "44",
    {
      "pageContent": "def parse_args(input_args=None):\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=\"a photo of sks dog\",\n        required=False,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--with_prior_preservation\",\n        default=False",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 53, "column": 0 },
          "end": { "row": 53, "column": 0 }
        }
      }
    }
  ],
  [
    "45",
    {
      "pageContent": "class DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                tr",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 250, "column": 0 },
          "end": { "row": 250, "column": 0 }
        }
      }
    }
  ],
  [
    "46",
    {
      "pageContent": "def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5], [0.5]),\n            ]\n        )",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 256, "column": 4 },
          "end": { "row": 256, "column": 4 }
        }
      }
    }
  ],
  [
    "47",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            self.instance_prompt,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                padding=\"do_not_pad\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids\n\n        return example",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 301, "column": 4 },
          "end": { "row": 301, "column": 4 }
        }
      }
    }
  ],
  [
    "48",
    {
      "pageContent": "class PromptDataset(Dataset):\n    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n\n    def __init__(self, prompt, num_samples):\n        self.prompt = prompt\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 329, "column": 0 },
          "end": { "row": 329, "column": 0 }
        }
      }
    }
  ],
  [
    "49",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 339, "column": 4 },
          "end": { "row": 339, "column": 4 }
        }
      }
    }
  ],
  [
    "50",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 346, "column": 0 },
          "end": { "row": 346, "column": 0 }
        }
      }
    }
  ],
  [
    "51",
    {
      "pageContent": "def gemini_zero_dpp(model: torch.nn.Module, placememt_policy: str = \"auto\"):\n    from colossalai.nn.parallel import GeminiDDP\n\n    model = GeminiDDP(\n        model, device=get_current_device(), placement_policy=placememt_policy, pin_memory=True, search_range_mb=64\n    )\n    return model",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 357, "column": 0 },
          "end": { "row": 357, "column": 0 }
        }
      }
    }
  ],
  [
    "52",
    {
      "pageContent": "def main(args):\n    if args.seed is None:\n        colossalai.launch_from_torch(config={})\n    else:\n        colossalai.launch_from_torch(config={}, seed=args.seed)\n\n    local_rank = gpc.get_local_rank(ParallelMode.DATA)\n    world_size = gpc.get_world_size(ParallelMode.DATA)\n\n    if args.with_prior_preservation:\n        class_images_dir = Path(args.class_data_dir)\n        if not class_images_dir.exists():\n            class_images_dir.mkdir(parents=True)\n        cur_class_images = len(list(class_images_dir.iterdir()))\n\n        if cur_class_images < args.num_class_images:\n            torch_dtype = torch.float16 if get_current_device() == \"cuda\" else torch.float32\n            pipeline = DiffusionPipeline.from_pretrained(\n                args.pretrained_model_name_or_path,\n                torch_dtype=torch_dtype,\n                safety_checker=None,\n                revision=args.revision,\n            )\n            pipeline.set_progress_bar_config(disable=True)\n\n            num_new_images = args.num_class_images - cur_class_images\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n\n            pipeline.to(get_current_device())\n\n            for example in tqdm(\n                sample_dataloader,\n                desc=\"Generating class images\",\n                disable=not local_rank == 0,\n            ):\n                images = pipeline(example[\"prompt\"]).images\n\n    ",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 366, "column": 0 },
          "end": { "row": 366, "column": 0 }
        }
      }
    }
  ],
  [
    "53",
    {
      "pageContent": "def collate_fn(examples):\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n        pixel_values = [example[\"instance_images\"] for example in examples]\n\n        # Concat class and instance examples for prior preservation.\n        # We do this to avoid doing two forward passes.\n        if args.with_prior_preservation:\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n            pixel_values += [example[\"class_images\"] for example in examples]\n\n        pixel_values = torch.stack(pixel_values)\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n        input_ids = tokenizer.pad(\n            {\"input_ids\": input_ids},\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids\n\n        batch = {\n            \"input_ids\": input_ids,\n            \"pixel_values\": pixel_values,\n        }\n        return batch",
      "metadata": {
        "source": "examples/research_projects/colossalai/train_dreambooth_colossalai.py",
        "range": {
          "start": { "row": 504, "column": 4 },
          "end": { "row": 504, "column": 4 }
        }
      }
    }
  ],
  [
    "54",
    {
      "pageContent": "def save_model_card(repo_name, images=None, base_model=str, dataset_name=str, repo_folder=None):\n    img_str = \"\"\n    for i, image in enumerate(images):\n        image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n        img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n\n    yaml = f\"\"\"\n---\nlicense: creativeml-openrail-m\nbase_model: {base_model}\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- diffusers\n- lora\ninference: true\n---\n    \"\"\"\n    model_card = f\"\"\"\n# LoRA text2image fine-tuning - {repo_name}\nThese are LoRA adaption weights for {base_model}. The weights were fine-tuned on the {dataset_name} dataset. You can find some example images in the following. \\n\n{img_str}\n\"\"\"\n    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n        f.write(yaml + model_card)",
      "metadata": {
        "source": "examples/research_projects/lora/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "55",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that  Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n   ",
      "metadata": {
        "source": "examples/research_projects/lora/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 85, "column": 0 },
          "end": { "row": 85, "column": 0 }
        }
      }
    }
  ],
  [
    "56",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/lora/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 388, "column": 0 },
          "end": { "row": 388, "column": 0 }
        }
      }
    }
  ],
  [
    "57",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_",
      "metadata": {
        "source": "examples/research_projects/lora/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 403, "column": 0 },
          "end": { "row": 403, "column": 0 }
        }
      }
    }
  ],
  [
    "58",
    {
      "pageContent": "def tokenize_captions(examples, is_train=True):\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n            elif isinstance(caption, (list, np.ndarray)):\n                # take a random caption if there are multiple\n                captions.append(random.choice(caption) if is_train else caption[0])\n            else:\n                raise ValueError(\n                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n                )\n        inputs = tokenizer(\n            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return inputs.input_ids",
      "metadata": {
        "source": "examples/research_projects/lora/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 658, "column": 4 },
          "end": { "row": 658, "column": 4 }
        }
      }
    }
  ],
  [
    "59",
    {
      "pageContent": "def preprocess_train(examples):\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples)\n        return examples",
      "metadata": {
        "source": "examples/research_projects/lora/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 686, "column": 4 },
          "end": { "row": 686, "column": 4 }
        }
      }
    }
  ],
  [
    "60",
    {
      "pageContent": "def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}",
      "metadata": {
        "source": "examples/research_projects/lora/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 698, "column": 4 },
          "end": { "row": 698, "column": 4 }
        }
      }
    }
  ],
  [
    "61",
    {
      "pageContent": "class MultiTokenCLIPTokenizer(CLIPTokenizer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.token_map = {}\n\n    def try_adding_tokens(self, placeholder_token, *args, **kwargs):\n        num_added_tokens = super().add_tokens(placeholder_token, *args, **kwargs)\n        if num_added_tokens == 0:\n            raise ValueError(\n                f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n                \" `placeholder_token` that is not already in the tokenizer.\"\n            )\n\n    def add_placeholder_tokens(self, placeholder_token, *args, num_vec_per_token=1, **kwargs):\n        output = []\n        if num_vec_per_token == 1:\n            self.try_adding_tokens(placeholder_token, *args, **kwargs)\n            output.append(placeholder_token)\n        else:\n            output = []\n            for i in range(num_vec_per_token):\n                ith_token = placeholder_token + f\"_{i}\"\n                self.try_adding_tokens(ith_token, *args, **kwargs)\n                output.append(ith_token)\n        # handle cases where there is a new placeholder token that contains the current placeholder token but is larger\n        for token in self.token_map:\n            if token in placeholder_token:\n                raise ValueError(\n                    f\"The tokenizer already has placeholder token {token} that can get confused with\"\n                    f\" {placeholder_token}keep placeholder tokens independent\"\n                )\n        self.token_map[placeholder_token] = output\n\n    def replace_placeholder_to",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/multi_token_clip.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "62",
    {
      "pageContent": "def try_adding_tokens(self, placeholder_token, *args, **kwargs):\n        num_added_tokens = super().add_tokens(placeholder_token, *args, **kwargs)\n        if num_added_tokens == 0:\n            raise ValueError(\n                f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n                \" `placeholder_token` that is not already in the tokenizer.\"\n            )",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/multi_token_clip.py",
        "range": {
          "start": { "row": 36, "column": 4 },
          "end": { "row": 36, "column": 4 }
        }
      }
    }
  ],
  [
    "63",
    {
      "pageContent": "def add_placeholder_tokens(self, placeholder_token, *args, num_vec_per_token=1, **kwargs):\n        output = []\n        if num_vec_per_token == 1:\n            self.try_adding_tokens(placeholder_token, *args, **kwargs)\n            output.append(placeholder_token)\n        else:\n            output = []\n            for i in range(num_vec_per_token):\n                ith_token = placeholder_token + f\"_{i}\"\n                self.try_adding_tokens(ith_token, *args, **kwargs)\n                output.append(ith_token)\n        # handle cases where there is a new placeholder token that contains the current placeholder token but is larger\n        for token in self.token_map:\n            if token in placeholder_token:\n                raise ValueError(\n                    f\"The tokenizer already has placeholder token {token} that can get confused with\"\n                    f\" {placeholder_token}keep placeholder tokens independent\"\n                )\n        self.token_map[placeholder_token] = output",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/multi_token_clip.py",
        "range": {
          "start": { "row": 44, "column": 4 },
          "end": { "row": 44, "column": 4 }
        }
      }
    }
  ],
  [
    "64",
    {
      "pageContent": "def replace_placeholder_tokens_in_text(self, text, vector_shuffle=False, prop_tokens_to_load=1.0):\n        \"\"\"\n        Here, we replace the placeholder tokens in text recorded in token_map so that the text_encoder\n        can encode them\n        vector_shuffle was inspired by https://github.com/rinongal/textual_inversion/pull/119\n        where shuffling tokens were found to force the model to learn the concepts more descriptively.\n        \"\"\"\n        if isinstance(text, list):\n            output = []\n            for i in range(len(text)):\n                output.append(self.replace_placeholder_tokens_in_text(text[i], vector_shuffle=vector_shuffle))\n            return output\n        for placeholder_token in self.token_map:\n            if placeholder_token in text:\n                tokens = self.token_map[placeholder_token]\n                tokens = tokens[: 1 + int(len(tokens) * prop_tokens_to_load)]\n                if vector_shuffle:\n                    tokens = copy.copy(tokens)\n                    random.shuffle(tokens)\n                text = text.replace(placeholder_token, \" \".join(tokens))\n        return text",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/multi_token_clip.py",
        "range": {
          "start": { "row": 64, "column": 4 },
          "end": { "row": 64, "column": 4 }
        }
      }
    }
  ],
  [
    "65",
    {
      "pageContent": "def __call__(self, text, *args, vector_shuffle=False, prop_tokens_to_load=1.0, **kwargs):\n        return super().__call__(\n            self.replace_placeholder_tokens_in_text(\n                text, vector_shuffle=vector_shuffle, prop_tokens_to_load=prop_tokens_to_load\n            ),\n            *args,\n            **kwargs,\n        )",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/multi_token_clip.py",
        "range": {
          "start": { "row": 86, "column": 4 },
          "end": { "row": 86, "column": 4 }
        }
      }
    }
  ],
  [
    "66",
    {
      "pageContent": "def encode(self, text, *args, vector_shuffle=False, prop_tokens_to_load=1.0, **kwargs):\n        return super().encode(\n            self.replace_placeholder_tokens_in_text(\n                text, vector_shuffle=vector_shuffle, prop_tokens_to_load=prop_tokens_to_load\n            ),\n            *args,\n            **kwargs,\n        )",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/multi_token_clip.py",
        "range": {
          "start": { "row": 95, "column": 4 },
          "end": { "row": 95, "column": 4 }
        }
      }
    }
  ],
  [
    "67",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\", type=str, default=None, required=True, help=\"A folder containing the training data.\"\n    )\n    parser.add_argument(\n        \"--placeholder_token\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A token to use as a placeholder for the concept.\",\n    )\n    parser.add_argument(\n        \"--initializer_token\", type=str, default=None, required=True, help=\"A token to use as initializer word.\"\n    )\n    parser.add_argument(\"--learnable_property\", type=str, default=\"object\", help=\"Choose between 'object' and 'style'\")\n    parser.add_argument(\"--repeats\", type=int, default=100, help=\"How many times to repeat the training data.\")\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"text-inversion-model\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n  ",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "68",
    {
      "pageContent": "class TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 242, "column": 0 },
          "end": { "row": 242, "column": 0 }
        }
      }
    }
  ],
  [
    "69",
    {
      "pageContent": "def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 243, "column": 4 },
          "end": { "row": 243, "column": 4 }
        }
      }
    }
  ],
  [
    "70",
    {
      "pageContent": "def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            (\n                h,\n                w,\n            ) = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "71",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 328, "column": 0 },
          "end": { "row": 328, "column": 0 }
        }
      }
    }
  ],
  [
    "72",
    {
      "pageContent": "def resize_token_embeddings(model, new_num_tokens, initializer_token_id, placeholder_token_id, rng):\n    if model.config.vocab_size == new_num_tokens or new_num_tokens is None:\n        return\n    model.config.vocab_size = new_num_tokens\n\n    params = model.params\n    old_embeddings = params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"]\n    old_num_tokens, emb_dim = old_embeddings.shape\n\n    initializer = jax.nn.initializers.normal()\n\n    new_embeddings = initializer(rng, (new_num_tokens, emb_dim))\n    new_embeddings = new_embeddings.at[:old_num_tokens].set(old_embeddings)\n    new_embeddings = new_embeddings.at[placeholder_token_id].set(new_embeddings[initializer_token_id])\n    params[\"text_model\"][\"embeddings\"][\"token_embedding\"][\"embedding\"] = new_embeddings\n\n    model.params = params\n    return model",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 338, "column": 0 },
          "end": { "row": 338, "column": 0 }
        }
      }
    }
  ],
  [
    "73",
    {
      "pageContent": "def main():\n    args = parse_args()\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    if jax.process_index() == 0:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            create_repo(repo_name, exist_ok=True, token=args.hub_token)\n            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    # Setup logging, we only want one process per machine to log things on the screen.\n    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n    if jax.process_index() == 0:\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n\n    # Load the tokenizer and add the placeholder token as a additional special token\n    if args.tokenizer_name:\n        tokenizer ",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 362, "column": 0 },
          "end": { "row": 362, "column": 0 }
        }
      }
    }
  ],
  [
    "74",
    {
      "pageContent": "def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n\n        batch = {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n        batch = {k: v.numpy() for k, v in batch.items()}\n\n        return batch",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 446, "column": 4 },
          "end": { "row": 446, "column": 4 }
        }
      }
    }
  ],
  [
    "75",
    {
      "pageContent": "def create_mask(params, label_fn):\n        def _map(params, mask, label_fn):\n            for k in params:\n                if label_fn(k):\n                    mask[k] = \"token_embedding\"\n                else:\n                    if isinstance(params[k], dict):\n                        mask[k] = {}\n                        _map(params[k], mask[k], label_fn)\n                    else:\n                        mask[k] = \"zero\"\n\n        mask = {}\n        _map(params, mask, label_fn)\n        return mask",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 474, "column": 4 },
          "end": { "row": 474, "column": 4 }
        }
      }
    }
  ],
  [
    "76",
    {
      "pageContent": "def zero_grads():\n        # from https://github.com/deepmind/optax/issues/159#issuecomment-896459491\n        def init_fn(_):\n            return ()\n\n        def update_fn(updates, state, params=None):\n            return jax.tree_util.tree_map(jnp.zeros_like, updates), ()\n\n        return optax.GradientTransformation(init_fn, update_fn)",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 490, "column": 4 },
          "end": { "row": 490, "column": 4 }
        }
      }
    }
  ],
  [
    "77",
    {
      "pageContent": "def train_step(state, vae_params, unet_params, batch, train_rng):\n        dropout_rng, sample_rng, new_train_rng = jax.random.split(train_rng, 3)\n\n        def compute_loss(params):\n            vae_outputs = vae.apply(\n                {\"params\": vae_params}, batch[\"pixel_values\"], deterministic=True, method=vae.encode\n            )\n            latents = vae_outputs.latent_dist.sample(sample_rng)\n            # (NHWC) -> (NCHW)\n            latents = jnp.transpose(latents, (0, 3, 1, 2))\n            latents = latents * vae.config.scaling_factor\n\n            noise_rng, timestep_rng = jax.random.split(sample_rng)\n            noise = jax.random.normal(noise_rng, latents.shape)\n            bsz = latents.shape[0]\n            timesteps = jax.random.randint(\n                timestep_rng,\n                (bsz,),\n                0,\n                noise_scheduler.config.num_train_timesteps,\n            )\n            noisy_latents = noise_scheduler.add_noise(noise_scheduler_state, latents, noise, timesteps)\n            encoder_hidden_states = state.apply_fn(\n                batch[\"input_ids\"], params=params, dropout_rng=dropout_rng, train=True\n            )[0]\n            # Predict the noise residual and compute loss\n            model_pred = unet.apply(\n                {\"params\": unet_params}, noisy_latents, timesteps, encoder_hidden_states, train=False\n            ).sample\n\n            # Get the target for loss depending on the prediction type\n            if noise_scheduler.config.prediction_type == \"epsilon\":\n                target = noise\n            elif noise_scheduler.config.predict",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion_flax.py",
        "range": {
          "start": { "row": 517, "column": 4 },
          "end": { "row": 517, "column": 4 }
        }
      }
    }
  ],
  [
    "78",
    {
      "pageContent": "def add_tokens(tokenizer, text_encoder, placeholder_token, num_vec_per_token=1, initializer_token=None):\n    \"\"\"\n    Add tokens to the tokenizer and set the initial value of token embeddings\n    \"\"\"\n    tokenizer.add_placeholder_tokens(placeholder_token, num_vec_per_token=num_vec_per_token)\n    text_encoder.resize_token_embeddings(len(tokenizer))\n    token_embeds = text_encoder.get_input_embeddings().weight.data\n    placeholder_token_ids = tokenizer.encode(placeholder_token, add_special_tokens=False)\n    if initializer_token:\n        token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n        for i, placeholder_token_id in enumerate(placeholder_token_ids):\n            token_embeds[placeholder_token_id] = token_embeds[token_ids[i * len(token_ids) // num_vec_per_token]]\n    else:\n        for i, placeholder_token_id in enumerate(placeholder_token_ids):\n            token_embeds[placeholder_token_id] = torch.randn_like(token_embeds[placeholder_token_id])\n    return placeholder_token",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 82, "column": 0 },
          "end": { "row": 82, "column": 0 }
        }
      }
    }
  ],
  [
    "79",
    {
      "pageContent": "def save_progress(tokenizer, text_encoder, accelerator, save_path):\n    for placeholder_token in tokenizer.token_map:\n        placeholder_token_ids = tokenizer.encode(placeholder_token, add_special_tokens=False)\n        learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_ids]\n        if len(placeholder_token_ids) == 1:\n            learned_embeds = learned_embeds[None]\n        learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n        torch.save(learned_embeds_dict, save_path)",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 100, "column": 0 },
          "end": { "row": 100, "column": 0 }
        }
      }
    }
  ],
  [
    "80",
    {
      "pageContent": "def load_multitoken_tokenizer(tokenizer, text_encoder, learned_embeds_dict):\n    for placeholder_token in learned_embeds_dict:\n        placeholder_embeds = learned_embeds_dict[placeholder_token]\n        num_vec_per_token = placeholder_embeds.shape[0]\n        placeholder_embeds = placeholder_embeds.to(dtype=text_encoder.dtype)\n        add_tokens(tokenizer, text_encoder, placeholder_token, num_vec_per_token=num_vec_per_token)\n        placeholder_token_ids = tokenizer.encode(placeholder_token, add_special_tokens=False)\n        token_embeds = text_encoder.get_input_embeddings().weight.data\n        for i, placeholder_token_id in enumerate(placeholder_token_ids):\n            token_embeds[placeholder_token_id] = placeholder_embeds[i]",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 110, "column": 0 },
          "end": { "row": 110, "column": 0 }
        }
      }
    }
  ],
  [
    "81",
    {
      "pageContent": "def load_multitoken_tokenizer_from_automatic(tokenizer, text_encoder, automatic_dict, placeholder_token):\n    \"\"\"\n    Automatic1111's tokens have format\n    {'string_to_token': {'*': 265}, 'string_to_param': {'*': tensor([[ 0.0833,  0.0030,  0.0057,  ..., -0.0264, -0.0616, -0.0529],\n        [ 0.0058, -0.0190, -0.0584,  ..., -0.0025, -0.0945, -0.0490],\n        [ 0.0916,  0.0025,  0.0365,  ..., -0.0685, -0.0124,  0.0728],\n        [ 0.0812, -0.0199, -0.0100,  ..., -0.0581, -0.0780,  0.0254]],\n       requires_grad=True)}, 'name': 'FloralMarble-400', 'step': 399, 'sd_checkpoint': '4bdfc29c', 'sd_checkpoint_name': 'SD2.1-768'}\n    \"\"\"\n    learned_embeds_dict = {}\n    learned_embeds_dict[placeholder_token] = automatic_dict[\"string_to_param\"][\"*\"]\n    load_multitoken_tokenizer(tokenizer, text_encoder, learned_embeds_dict)",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 122, "column": 0 },
          "end": { "row": 122, "column": 0 }
        }
      }
    }
  ],
  [
    "82",
    {
      "pageContent": "def get_mask(tokenizer, accelerator):\n    # Get the mask of the weights that won't change\n    mask = torch.ones(len(tokenizer)).to(accelerator.device, dtype=torch.bool)\n    for placeholder_token in tokenizer.token_map:\n        placeholder_token_ids = tokenizer.encode(placeholder_token, add_special_tokens=False)\n        for i in range(len(placeholder_token_ids)):\n            mask = mask & (torch.arange(len(tokenizer)) != placeholder_token_ids[i]).to(accelerator.device)\n    return mask",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 136, "column": 0 },
          "end": { "row": 136, "column": 0 }
        }
      }
    }
  ],
  [
    "83",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--progressive_tokens_max_steps\",\n        type=int,\n        default=2000,\n        help=\"The number of steps until all tokens will be used.\",\n    )\n    parser.add_argument(\n        \"--progressive_tokens\",\n        action=\"store_true\",\n        help=\"Progressively train the tokens. For example, first train for 1 token, then 2 tokens and so on.\",\n    )\n    parser.add_argument(\"--vector_shuffle\", action=\"store_true\", help=\"Shuffling tokens durint training\")\n    parser.add_argument(\n        \"--num_vec_per_token\",\n        type=int,\n        default=1,\n        help=(\n            \"The number of vectors used to represent the placeholder token. The higher the number, the better the\"\n            \" result at the cost of editability. This can be fixed by prompt editing.\"\n        ),\n    )\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Save learned_embeds.bin every X updates steps.\",\n    )\n    parser.add_argument(\n        \"--only_save_embeds\",\n        action=\"store_true\",\n        default=False,\n        help=\"Save only the embeddings for the new concept.\",\n    )\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        ",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 146, "column": 0 },
          "end": { "row": 146, "column": 0 }
        }
      }
    }
  ],
  [
    "84",
    {
      "pageContent": "class TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n        vector_shuffle=False,\n        progressive_tokens=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n        self.vector_shuffle = vector_shuffle\n        self.progressive_tokens = progressive_tokens\n        self.prop_tokens_to_load = 0\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return se",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 456, "column": 0 },
          "end": { "row": 456, "column": 0 }
        }
      }
    }
  ],
  [
    "85",
    {
      "pageContent": "def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n        vector_shuffle=False,\n        progressive_tokens=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n        self.vector_shuffle = vector_shuffle\n        self.progressive_tokens = progressive_tokens\n        self.prop_tokens_to_load = 0\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 457, "column": 4 },
          "end": { "row": 457, "column": 4 }
        }
      }
    }
  ],
  [
    "86",
    {
      "pageContent": "def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer.encode(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n            vector_shuffle=self.vector_shuffle,\n            prop_tokens_to_load=self.prop_tokens_to_load if self.progressive_tokens else 1.0,\n        )[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            (\n                h,\n                w,\n            ) = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 504, "column": 4 },
          "end": { "row": 504, "column": 4 }
        }
      }
    }
  ],
  [
    "87",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 549, "column": 0 },
          "end": { "row": 549, "column": 0 }
        }
      }
    }
  ],
  [
    "88",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token",
      "metadata": {
        "source": "examples/research_projects/mulit_token_textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 559, "column": 0 },
          "end": { "row": 559, "column": 0 }
        }
      }
    }
  ],
  [
    "89",
    {
      "pageContent": "def save_progress(text_encoder, placeholder_token_id, accelerator, args, save_path):\n    logger.info(\"Saving embeddings\")\n    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n    learned_embeds_dict = {args.placeholder_token: learned_embeds.detach().cpu()}\n    torch.save(learned_embeds_dict, save_path)",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 83, "column": 0 },
          "end": { "row": 83, "column": 0 }
        }
      }
    }
  ],
  [
    "90",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Save learned_embeds.bin every X updates steps.\",\n    )\n    parser.add_argument(\n        \"--only_save_embeds\",\n        action=\"store_true\",\n        default=False,\n        help=\"Save only the embeddings for the new concept.\",\n    )\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\", type=str, default=None, required=True, help=\"A folder containing the training data.\"\n    )\n    parser.add_argument(\n        \"--placeholder_token\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A token to use as a placeholder for the concept.\",\n    )\n    parser.add_argument(\n        \"--initializer_token\", type=str, default=None, required=True, help=\"A token to use as initializer word.\"\n    )\n    parser.add_argument(\"--learnable_property\", type=str, default=\"object\", h",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 90, "column": 0 },
          "end": { "row": 90, "column": 0 }
        }
      }
    }
  ],
  [
    "91",
    {
      "pageContent": "class TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 379, "column": 0 },
          "end": { "row": 379, "column": 0 }
        }
      }
    }
  ],
  [
    "92",
    {
      "pageContent": "def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 380, "column": 4 },
          "end": { "row": 380, "column": 4 }
        }
      }
    }
  ],
  [
    "93",
    {
      "pageContent": "def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            (\n                h,\n                w,\n            ) = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 422, "column": 4 },
          "end": { "row": 422, "column": 4 }
        }
      }
    }
  ],
  [
    "94",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 465, "column": 0 },
          "end": { "row": 465, "column": 0 }
        }
      }
    }
  ],
  [
    "95",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/textual_inversion/textual_inversion.py",
        "range": {
          "start": { "row": 475, "column": 0 },
          "end": { "row": 475, "column": 0 }
        }
      }
    }
  ],
  [
    "96",
    {
      "pageContent": "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    if not isinstance(arr, torch.Tensor):\n        arr = torch.from_numpy(arr)\n    res = arr[timesteps].float().to(timesteps.device)\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res.expand(broadcast_shape)",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "97",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that HF Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"ddpm-model-64\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--overwrite_output_dir\", action=\"store_true\")\n    parser.add_argument(\n        \"--cache_dir\",\n        type=str,\n        default=None,\n        help=\"The directory where the downloaded models and d",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 50, "column": 0 },
          "end": { "row": 50, "column": 0 }
        }
      }
    }
  ],
  [
    "98",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 265, "column": 0 },
          "end": { "row": 265, "column": 0 }
        }
      }
    }
  ],
  [
    "99",
    {
      "pageContent": "def main(args):\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.logger,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    if args.logger == \"tensorboard\":\n        if not is_tensorboard_available():\n            raise ImportError(\"Make sure to install tensorboard if you want to use it for logging during training.\")\n\n    elif args.logger == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_r",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 275, "column": 0 },
          "end": { "row": 275, "column": 0 }
        }
      }
    }
  ],
  [
    "100",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that  Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n   ",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 53, "column": 0 },
          "end": { "row": 53, "column": 0 }
        }
      }
    }
  ],
  [
    "101",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 315, "column": 0 },
          "end": { "row": 315, "column": 0 }
        }
      }
    }
  ],
  [
    "102",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        accelerator_project_config=accelerator_project_config,\n    )\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            create",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 330, "column": 0 },
          "end": { "row": 330, "column": 0 }
        }
      }
    }
  ],
  [
    "103",
    {
      "pageContent": "def tokenize_captions(examples, is_train=True):\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n            elif isinstance(caption, (list, np.ndarray)):\n                # take a random caption if there are multiple\n                captions.append(random.choice(caption) if is_train else caption[0])\n            else:\n                raise ValueError(\n                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n                )\n        inputs = tokenizer(\n            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return inputs.input_ids",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 495, "column": 4 },
          "end": { "row": 495, "column": 4 }
        }
      }
    }
  ],
  [
    "104",
    {
      "pageContent": "def preprocess_train(examples):\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples)\n        return examples",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 523, "column": 4 },
          "end": { "row": 523, "column": 4 }
        }
      }
    }
  ],
  [
    "105",
    {
      "pageContent": "def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}",
      "metadata": {
        "source": "examples/research_projects/onnxruntime/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 535, "column": 4 },
          "end": { "row": 535, "column": 4 }
        }
      }
    }
  ],
  [
    "106",
    {
      "pageContent": "def save_progress(text_encoder, placeholder_token_id, accelerator, args, save_path):\n    logger.info(\"Saving embeddings\")\n    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n    learned_embeds_dict = {args.placeholder_token: learned_embeds.detach().cpu()}\n    torch.save(learned_embeds_dict, save_path)",
      "metadata": {
        "source": "examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py",
        "range": {
          "start": { "row": 59, "column": 0 },
          "end": { "row": 59, "column": 0 }
        }
      }
    }
  ],
  [
    "107",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Save learned_embeds.bin every X updates steps.\",\n    )\n    parser.add_argument(\n        \"--only_save_embeds\",\n        action=\"store_true\",\n        default=False,\n        help=\"Save only the embeddings for the new concept.\",\n    )\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\", type=str, default=None, required=True, help=\"A folder containing the training data.\"\n    )\n    parser.add_argument(\n        \"--placeholder_token\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A token to use as a placeholder for the concept.\",\n    )\n    parser.add_argument(\n        \"--initializer_token\", type=str, default=None, required=True, help=\"A token to use as initializer word.\"\n    )\n    parser.add_argument(\"--learnable_property\", type=str, default=\"object\", h",
      "metadata": {
        "source": "examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py",
        "range": {
          "start": { "row": 66, "column": 0 },
          "end": { "row": 66, "column": 0 }
        }
      }
    }
  ],
  [
    "108",
    {
      "pageContent": "class TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert",
      "metadata": {
        "source": "examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py",
        "range": {
          "start": { "row": 272, "column": 0 },
          "end": { "row": 272, "column": 0 }
        }
      }
    }
  ],
  [
    "109",
    {
      "pageContent": "def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL_INTERPOLATION[\"linear\"],\n            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)",
      "metadata": {
        "source": "examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py",
        "range": {
          "start": { "row": 273, "column": 4 },
          "end": { "row": 273, "column": 4 }
        }
      }
    }
  ],
  [
    "110",
    {
      "pageContent": "def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            (\n                h,\n                w,\n            ) = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example",
      "metadata": {
        "source": "examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py",
        "range": {
          "start": { "row": 315, "column": 4 },
          "end": { "row": 315, "column": 4 }
        }
      }
    }
  ],
  [
    "111",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py",
        "range": {
          "start": { "row": 358, "column": 0 },
          "end": { "row": 358, "column": 0 }
        }
      }
    }
  ],
  [
    "112",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=\"tensorboard\",\n        logging_dir=logging_dir,\n    )\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            create_repo(repo_name, exist_ok=True, token=args.hub_token)\n            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Load the tokenizer and add the placeholder token as a additional special token\n    if args.tokenizer_name:\n        tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\n    elif args.pretrained_model_name_or_path:\n        tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder",
      "metadata": {
        "source": "examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py",
        "range": {
          "start": { "row": 373, "column": 0 },
          "end": { "row": 373, "column": 0 }
        }
      }
    }
  ],
  [
    "113",
    {
      "pageContent": "def image_grid(imgs, rows, cols):\n    assert len(imgs) == rows * cols\n\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n    grid_w, grid_h = grid.size\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid",
      "metadata": {
        "source": "examples/research_projects/intel_opts/inference_bf16.py",
        "range": {
          "start": { "row": 7, "column": 0 },
          "end": { "row": 7, "column": 0 }
        }
      }
    }
  ],
  [
    "114",
    {
      "pageContent": "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=revision,\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"RobertaSeriesModelWithTransformation\":\n        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n\n        return RobertaSeriesModelWithTransformation\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "115",
    {
      "pageContent": "def parse_args(input_args=None):\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--with_prior_preservation\",\n        default=False,\n        action=",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 58, "column": 0 },
          "end": { "row": 58, "column": 0 }
        }
      }
    }
  ],
  [
    "116",
    {
      "pageContent": "class DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = []\n        self.instance_images_path = []\n        self.num_instance_images = []\n        self.instance_prompt = []\n        self.class_data_root = []\n        self.class_images_path = []\n        self.num_class_images = []\n        self.class_prompt = []\n        self._length = 0\n\n        for i in range(len(instance_data_root)):\n            self.instance_data_root.append(Path(instance_data_root[i]))\n            if not self.instance_data_root[i].exists():\n                raise ValueError(\"Instance images root doesn't exists.\")\n\n            self.instance_images_path.append(list(Path(instance_data_root[i]).iterdir()))\n            self.num_instance_images.append(len(self.instance_images_path[i]))\n            self.instance_prompt.append(instance_prompt[i])\n            self._length += self.num_instance_images[i]\n\n            if class_data_root is not None:\n                self.class_data_root.append(Path(class_data_root[i]))\n                self.class_data_root[i].mkdir(parents=True, exist_ok=True)\n                self.class_images_path.app",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 325, "column": 0 },
          "end": { "row": 325, "column": 0 }
        }
      }
    }
  ],
  [
    "117",
    {
      "pageContent": "def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = []\n        self.instance_images_path = []\n        self.num_instance_images = []\n        self.instance_prompt = []\n        self.class_data_root = []\n        self.class_images_path = []\n        self.num_class_images = []\n        self.class_prompt = []\n        self._length = 0\n\n        for i in range(len(instance_data_root)):\n            self.instance_data_root.append(Path(instance_data_root[i]))\n            if not self.instance_data_root[i].exists():\n                raise ValueError(\"Instance images root doesn't exists.\")\n\n            self.instance_images_path.append(list(Path(instance_data_root[i]).iterdir()))\n            self.num_instance_images.append(len(self.instance_images_path[i]))\n            self.instance_prompt.append(instance_prompt[i])\n            self._length += self.num_instance_images[i]\n\n            if class_data_root is not None:\n                self.class_data_root.append(Path(class_data_root[i]))\n                self.class_data_root[i].mkdir(parents=True, exist_ok=True)\n                self.class_images_path.append(list(self.class_data_root[i].iterdir()))\n                self.num_class_images.append(len(self.class_images_path))\n                if self.num_class_images[i] > self.num_instance_images[i]:\n                   ",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 331, "column": 4 },
          "end": { "row": 331, "column": 4 }
        }
      }
    }
  ],
  [
    "118",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        for i in range(len(self.instance_images_path)):\n            instance_image = Image.open(self.instance_images_path[i][index % self.num_instance_images[i]])\n            if not instance_image.mode == \"RGB\":\n                instance_image = instance_image.convert(\"RGB\")\n            example[f\"instance_images_{i}\"] = self.image_transforms(instance_image)\n            example[f\"instance_prompt_ids_{i}\"] = self.tokenizer(\n                self.instance_prompt[i],\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            ).input_ids\n\n        if self.class_data_root:\n            for i in range(len(self.class_data_root)):\n                class_image = Image.open(self.class_images_path[i][index % self.num_class_images[i]])\n                if not class_image.mode == \"RGB\":\n                    class_image = class_image.convert(\"RGB\")\n                example[f\"class_images_{i}\"] = self.image_transforms(class_image)\n                example[f\"class_prompt_ids_{i}\"] = self.tokenizer(\n                    self.class_prompt[i],\n                    truncation=True,\n                    padding=\"max_length\",\n                    max_length=self.tokenizer.model_max_length,\n                    return_tensors=\"pt\",\n                ).input_ids\n\n        return example",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 389, "column": 4 },
          "end": { "row": 389, "column": 4 }
        }
      }
    }
  ],
  [
    "119",
    {
      "pageContent": "def collate_fn(num_instances, examples, with_prior_preservation=False):\n    input_ids = []\n    pixel_values = []\n\n    for i in range(num_instances):\n        input_ids += [example[f\"instance_prompt_ids_{i}\"] for example in examples]\n        pixel_values += [example[f\"instance_images_{i}\"] for example in examples]\n\n    # Concat class and instance examples for prior preservation.\n    # We do this to avoid doing two forward passes.\n    if with_prior_preservation:\n        for i in range(num_instances):\n            input_ids += [example[f\"class_prompt_ids_{i}\"] for example in examples]\n            pixel_values += [example[f\"class_images_{i}\"] for example in examples]\n\n    pixel_values = torch.stack(pixel_values)\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n    input_ids = torch.cat(input_ids, dim=0)\n\n    batch = {\n        \"input_ids\": input_ids,\n        \"pixel_values\": pixel_values,\n    }\n    return batch",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 421, "column": 0 },
          "end": { "row": 421, "column": 0 }
        }
      }
    }
  ],
  [
    "120",
    {
      "pageContent": "class PromptDataset(Dataset):\n    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n\n    def __init__(self, prompt, num_samples):\n        self.prompt = prompt\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 448, "column": 0 },
          "end": { "row": 448, "column": 0 }
        }
      }
    }
  ],
  [
    "121",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 458, "column": 4 },
          "end": { "row": 458, "column": 4 }
        }
      }
    }
  ],
  [
    "122",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 465, "column": 0 },
          "end": { "row": 465, "column": 0 }
        }
      }
    }
  ],
  [
    "123",
    {
      "pageContent": "def main(args):\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n        raise ValueError(\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n        )\n\n    # Parse instance and class inputs, and double check that lengths match\n    instance_data_dir = args.instance_data_dir.split(\",\")\n    instance_prompt = args.instance_prompt.split(\",\")\n    assert all(\n        x == len(instance_data_dir) for x in [len(instance_data_dir), len(instance_prompt)]\n    ), \"Instance data dir and prompt inputs are not of the same length.\"\n\n    if args.with_prior_preservation:\n        class_data_dir = args.class_data_dir.",
      "metadata": {
        "source": "examples/research_projects/multi_subject_dreambooth/train_multi_subject_dreambooth.py",
        "range": {
          "start": { "row": 475, "column": 0 },
          "end": { "row": 475, "column": 0 }
        }
      }
    }
  ],
  [
    "124",
    {
      "pageContent": "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    if not isinstance(arr, torch.Tensor):\n        arr = torch.from_numpy(arr)\n    res = arr[timesteps].float().to(timesteps.device)\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res.expand(broadcast_shape)",
      "metadata": {
        "source": "examples/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "125",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that HF Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--model_config_name_or_path\",\n        type=str,\n        default=None,\n        help=\"The config of the UNet model to train, leave as None to use standard DDPM configuration.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"ddpm-model-64\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    p",
      "metadata": {
        "source": "examples/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 53, "column": 0 },
          "end": { "row": 53, "column": 0 }
        }
      }
    }
  ],
  [
    "126",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 277, "column": 0 },
          "end": { "row": 277, "column": 0 }
        }
      }
    }
  ],
  [
    "127",
    {
      "pageContent": "def main(args):\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.logger,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    if args.logger == \"tensorboard\":\n        if not is_tensorboard_available():\n            raise ImportError(\"Make sure to install tensorboard if you want to use it for logging during training.\")\n\n    elif args.logger == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # `accelerate` 0.16.0 will have better support for customized saving\n    if version.parse(accelerate.__version__) >= version.parse(\"0.16.0\"):\n        # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n        def save_model_hook(models, weights, output_dir):\n            if args.use_ema:\n                ema_model.save_pretrained(os.path.join(output_dir, \"unet_ema\"))\n\n            for i, model in enumerate(models):\n                model.save_pretrained(os.path.join(output_dir, \"unet\"))\n\n                # make sure to pop weight so that corresponding model is not saved again\n                weights.pop()\n\n        def load_model_hook(models, input_dir):\n            if args.use_ema:\n       ",
      "metadata": {
        "source": "examples/unconditional_image_generation/train_unconditional.py",
        "range": {
          "start": { "row": 287, "column": 0 },
          "end": { "row": 287, "column": 0 }
        }
      }
    }
  ],
  [
    "128",
    {
      "pageContent": "def run_command(command: List[str], return_stdout=False):\n    \"\"\"\n    Runs `command` with `subprocess.check_output` and will potentially return the `stdout`. Will also properly capture\n    if an error occurred while running `command`\n    \"\"\"\n    try:\n        output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n        if return_stdout:\n            if hasattr(output, \"decode\"):\n                output = output.decode(\"utf-8\")\n            return output\n    except subprocess.CalledProcessError as e:\n        raise SubprocessCallException(\n            f\"Command `{' '.join(command)}` failed with the following error:\\n\\n{e.output.decode()}\"\n        ) from e",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "129",
    {
      "pageContent": "class ExamplesTestsAccelerate(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls._tmpdir = tempfile.mkdtemp()\n        cls.configPath = os.path.join(cls._tmpdir, \"default_config.yml\")\n\n        write_basic_config(save_location=cls.configPath)\n        cls._launch_args = [\"accelerate\", \"launch\", \"--config_file\", cls.configPath]\n\n    @classmethod\n    def tearDownClass(cls):\n        super().tearDownClass()\n        shutil.rmtree(cls._tmpdir)\n\n    def test_train_unconditional(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            test_args = f\"\"\"\n                examples/unconditional_image_generation/train_unconditional.py\n                --dataset_name hf-internal-testing/dummy_image_class_data\n                --model_config_name_or_path diffusers/ddpm_dummy\n                --resolution 64\n                --output_dir {tmpdir}\n                --train_batch_size 2\n                --num_epochs 1\n                --gradient_accumulation_steps 1\n                --ddpm_num_inference_steps 2\n                --learning_rate 1e-3\n                --lr_warmup_steps 5\n                \"\"\".split()\n\n            run_command(self._launch_args + test_args, return_stdout=True)\n            # save_pretrained smoke test\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"unet\", \"diffusion_pytorch_model.bin\")))\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"scheduler\", \"scheduler_config.json\")))\n\n    def test_textual_inversion(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            test",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 61, "column": 0 },
          "end": { "row": 61, "column": 0 }
        }
      }
    }
  ],
  [
    "130",
    {
      "pageContent": "def test_train_unconditional(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            test_args = f\"\"\"\n                examples/unconditional_image_generation/train_unconditional.py\n                --dataset_name hf-internal-testing/dummy_image_class_data\n                --model_config_name_or_path diffusers/ddpm_dummy\n                --resolution 64\n                --output_dir {tmpdir}\n                --train_batch_size 2\n                --num_epochs 1\n                --gradient_accumulation_steps 1\n                --ddpm_num_inference_steps 2\n                --learning_rate 1e-3\n                --lr_warmup_steps 5\n                \"\"\".split()\n\n            run_command(self._launch_args + test_args, return_stdout=True)\n            # save_pretrained smoke test\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"unet\", \"diffusion_pytorch_model.bin\")))\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"scheduler\", \"scheduler_config.json\")))",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 76, "column": 4 },
          "end": { "row": 76, "column": 4 }
        }
      }
    }
  ],
  [
    "131",
    {
      "pageContent": "def test_textual_inversion(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            test_args = f\"\"\"\n                examples/textual_inversion/textual_inversion.py\n                --pretrained_model_name_or_path hf-internal-testing/tiny-stable-diffusion-pipe\n                --train_data_dir docs/source/en/imgs\n                --learnable_property object\n                --placeholder_token <cat-toy>\n                --initializer_token a\n                --resolution 64\n                --train_batch_size 1\n                --gradient_accumulation_steps 1\n                --max_train_steps 2\n                --learning_rate 5.0e-04\n                --scale_lr\n                --lr_scheduler constant\n                --lr_warmup_steps 0\n                --output_dir {tmpdir}\n                \"\"\".split()\n\n            run_command(self._launch_args + test_args)\n            # save_pretrained smoke test\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"learned_embeds.bin\")))",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 97, "column": 4 },
          "end": { "row": 97, "column": 4 }
        }
      }
    }
  ],
  [
    "132",
    {
      "pageContent": "def test_dreambooth(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            test_args = f\"\"\"\n                examples/dreambooth/train_dreambooth.py\n                --pretrained_model_name_or_path hf-internal-testing/tiny-stable-diffusion-pipe\n                --instance_data_dir docs/source/en/imgs\n                --instance_prompt photo\n                --resolution 64\n                --train_batch_size 1\n                --gradient_accumulation_steps 1\n                --max_train_steps 2\n                --learning_rate 5.0e-04\n                --scale_lr\n                --lr_scheduler constant\n                --lr_warmup_steps 0\n                --output_dir {tmpdir}\n                \"\"\".split()\n\n            run_command(self._launch_args + test_args)\n            # save_pretrained smoke test\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"unet\", \"diffusion_pytorch_model.bin\")))\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"scheduler\", \"scheduler_config.json\")))",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 121, "column": 4 },
          "end": { "row": 121, "column": 4 }
        }
      }
    }
  ],
  [
    "133",
    {
      "pageContent": "def test_dreambooth_checkpointing(self):\n        instance_prompt = \"photo\"\n        pretrained_model_name_or_path = \"hf-internal-testing/tiny-stable-diffusion-pipe\"\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Run training script with checkpointing\n            # max_train_steps == 5, checkpointing_steps == 2\n            # Should create checkpoints at steps 2, 4\n\n            initial_run_args = f\"\"\"\n                examples/dreambooth/train_dreambooth.py\n                --pretrained_model_name_or_path {pretrained_model_name_or_path}\n                --instance_data_dir docs/source/en/imgs\n                --instance_prompt {instance_prompt}\n                --resolution 64\n                --train_batch_size 1\n                --gradient_accumulation_steps 1\n                --max_train_steps 5\n                --learning_rate 5.0e-04\n                --scale_lr\n                --lr_scheduler constant\n                --lr_warmup_steps 0\n                --output_dir {tmpdir}\n                --checkpointing_steps=2\n                --seed=0\n                \"\"\".split()\n\n            run_command(self._launch_args + initial_run_args)\n\n            # check can run the original fully trained output pipeline\n            pipe = DiffusionPipeline.from_pretrained(tmpdir, safety_checker=None)\n            pipe(instance_prompt, num_inference_steps=2)\n\n            # check checkpoint directories exist\n            self.assertTrue(os.path.isdir(os.path.join(tmpdir, \"checkpoint-2\")))\n            self.assertTrue(os.path.isdir(os.path.join(tmpdir, \"checkpoint-4\")))\n\n            # chec",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 144, "column": 4 },
          "end": { "row": 144, "column": 4 }
        }
      }
    }
  ],
  [
    "134",
    {
      "pageContent": "def test_text_to_image(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            test_args = f\"\"\"\n                examples/text_to_image/train_text_to_image.py\n                --pretrained_model_name_or_path hf-internal-testing/tiny-stable-diffusion-pipe\n                --dataset_name hf-internal-testing/dummy_image_text_data\n                --resolution 64\n                --center_crop\n                --random_flip\n                --train_batch_size 1\n                --gradient_accumulation_steps 1\n                --max_train_steps 2\n                --learning_rate 5.0e-04\n                --scale_lr\n                --lr_scheduler constant\n                --lr_warmup_steps 0\n                --output_dir {tmpdir}\n                \"\"\".split()\n\n            run_command(self._launch_args + test_args)\n            # save_pretrained smoke test\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"unet\", \"diffusion_pytorch_model.bin\")))\n            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"scheduler\", \"scheduler_config.json\")))",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 223, "column": 4 },
          "end": { "row": 223, "column": 4 }
        }
      }
    }
  ],
  [
    "135",
    {
      "pageContent": "def test_text_to_image_checkpointing(self):\n        pretrained_model_name_or_path = \"hf-internal-testing/tiny-stable-diffusion-pipe\"\n        prompt = \"a prompt\"\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Run training script with checkpointing\n            # max_train_steps == 5, checkpointing_steps == 2\n            # Should create checkpoints at steps 2, 4\n\n            initial_run_args = f\"\"\"\n                examples/text_to_image/train_text_to_image.py\n                --pretrained_model_name_or_path {pretrained_model_name_or_path}\n                --dataset_name hf-internal-testing/dummy_image_text_data\n                --resolution 64\n                --center_crop\n                --random_flip\n                --train_batch_size 1\n                --gradient_accumulation_steps 1\n                --max_train_steps 5\n                --learning_rate 5.0e-04\n                --scale_lr\n                --lr_scheduler constant\n                --lr_warmup_steps 0\n                --output_dir {tmpdir}\n                --checkpointing_steps=2\n                --seed=0\n                \"\"\".split()\n\n            run_command(self._launch_args + initial_run_args)\n\n            pipe = DiffusionPipeline.from_pretrained(tmpdir, safety_checker=None)\n            pipe(prompt, num_inference_steps=2)\n\n            # check checkpoint directories exist\n            self.assertTrue(os.path.isdir(os.path.join(tmpdir, \"checkpoint-2\")))\n            self.assertTrue(os.path.isdir(os.path.join(tmpdir, \"checkpoint-4\")))\n\n            # check can run an intermediate checkpoint\n            une",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 247, "column": 4 },
          "end": { "row": 247, "column": 4 }
        }
      }
    }
  ],
  [
    "136",
    {
      "pageContent": "def test_text_to_image_checkpointing_use_ema(self):\n        pretrained_model_name_or_path = \"hf-internal-testing/tiny-stable-diffusion-pipe\"\n        prompt = \"a prompt\"\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Run training script with checkpointing\n            # max_train_steps == 5, checkpointing_steps == 2\n            # Should create checkpoints at steps 2, 4\n\n            initial_run_args = f\"\"\"\n                examples/text_to_image/train_text_to_image.py\n                --pretrained_model_name_or_path {pretrained_model_name_or_path}\n                --dataset_name hf-internal-testing/dummy_image_text_data\n                --resolution 64\n                --center_crop\n                --random_flip\n                --train_batch_size 1\n                --gradient_accumulation_steps 1\n                --max_train_steps 5\n                --learning_rate 5.0e-04\n                --scale_lr\n                --lr_scheduler constant\n                --lr_warmup_steps 0\n                --output_dir {tmpdir}\n                --checkpointing_steps=2\n                --use_ema\n                --seed=0\n                \"\"\".split()\n\n            run_command(self._launch_args + initial_run_args)\n\n            pipe = DiffusionPipeline.from_pretrained(tmpdir, safety_checker=None)\n            pipe(prompt, num_inference_steps=2)\n\n            # check checkpoint directories exist\n            self.assertTrue(os.path.isdir(os.path.join(tmpdir, \"checkpoint-2\")))\n            self.assertTrue(os.path.isdir(os.path.join(tmpdir, \"checkpoint-4\")))\n\n            # check can run an inter",
      "metadata": {
        "source": "examples/test_examples.py",
        "range": {
          "start": { "row": 327, "column": 4 },
          "end": { "row": 327, "column": 4 }
        }
      }
    }
  ],
  [
    "137",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--pretrained_vae_name_or_path\",\n        type=str,\n        default=None,\n        help=\"Path to pretrained vae or vae identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify ima",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 46, "column": 0 },
          "end": { "row": 46, "column": 0 }
        }
      }
    }
  ],
  [
    "138",
    {
      "pageContent": "class DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        class_num=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            if class_num is not None:\n                self.num_class_images = min(len(self.class_images_path), class_num)\n            else:\n                self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        se",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 220, "column": 0 },
          "end": { "row": 220, "column": 0 }
        }
      }
    }
  ],
  [
    "139",
    {
      "pageContent": "def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        class_num=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            if class_num is not None:\n                self.num_class_images = min(len(self.class_images_path), class_num)\n            else:\n                self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "140",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            self.instance_prompt,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                padding=\"do_not_pad\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids\n\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 275, "column": 4 },
          "end": { "row": 275, "column": 4 }
        }
      }
    }
  ],
  [
    "141",
    {
      "pageContent": "class PromptDataset(Dataset):\n    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n\n    def __init__(self, prompt, num_samples):\n        self.prompt = prompt\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 303, "column": 0 },
          "end": { "row": 303, "column": 0 }
        }
      }
    }
  ],
  [
    "142",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 313, "column": 4 },
          "end": { "row": 313, "column": 4 }
        }
      }
    }
  ],
  [
    "143",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 320, "column": 0 },
          "end": { "row": 320, "column": 0 }
        }
      }
    }
  ],
  [
    "144",
    {
      "pageContent": "def main():\n    args = parse_args()\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    # Setup logging, we only want one process per machine to log things on the screen.\n    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n    if jax.process_index() == 0:\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    rng = jax.random.PRNGKey(args.seed)\n\n    if args.with_prior_preservation:\n        class_images_dir = Path(args.class_data_dir)\n        if not class_images_dir.exists():\n            class_images_dir.mkdir(parents=True)\n        cur_class_images = len(list(class_images_dir.iterdir()))\n\n        if cur_class_images < args.num_class_images:\n            pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n                args.pretrained_model_name_or_path, safety_checker=None, revision=args.revision\n            )\n            pipeline.set_progress_bar_config(disable=True)\n\n            num_new_images = args.num_class_images - cur_class_images\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n            total_sample_batch_size = args.sample_batch_size * jax.local_device_count()\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_si",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 334, "column": 0 },
          "end": { "row": 334, "column": 0 }
        }
      }
    }
  ],
  [
    "145",
    {
      "pageContent": "def collate_fn(examples):\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n        pixel_values = [example[\"instance_images\"] for example in examples]\n\n        # Concat class and instance examples for prior preservation.\n        # We do this to avoid doing two forward passes.\n        if args.with_prior_preservation:\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n            pixel_values += [example[\"class_images\"] for example in examples]\n\n        pixel_values = torch.stack(pixel_values)\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n        input_ids = tokenizer.pad(\n            {\"input_ids\": input_ids}, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\"\n        ).input_ids\n\n        batch = {\n            \"input_ids\": input_ids,\n            \"pixel_values\": pixel_values,\n        }\n        batch = {k: v.numpy() for k, v in batch.items()}\n        return batch",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 431, "column": 4 },
          "end": { "row": 431, "column": 4 }
        }
      }
    }
  ],
  [
    "146",
    {
      "pageContent": "def train_step(unet_state, text_encoder_state, vae_params, batch, train_rng):\n        dropout_rng, sample_rng, new_train_rng = jax.random.split(train_rng, 3)\n\n        if args.train_text_encoder:\n            params = {\"text_encoder\": text_encoder_state.params, \"unet\": unet_state.params}\n        else:\n            params = {\"unet\": unet_state.params}\n\n        def compute_loss(params):\n            # Convert images to latent space\n            vae_outputs = vae.apply(\n                {\"params\": vae_params}, batch[\"pixel_values\"], deterministic=True, method=vae.encode\n            )\n            latents = vae_outputs.latent_dist.sample(sample_rng)\n            # (NHWC) -> (NCHW)\n            latents = jnp.transpose(latents, (0, 3, 1, 2))\n            latents = latents * vae.config.scaling_factor\n\n            # Sample noise that we'll add to the latents\n            noise_rng, timestep_rng = jax.random.split(sample_rng)\n            noise = jax.random.normal(noise_rng, latents.shape)\n            # Sample a random timestep for each image\n            bsz = latents.shape[0]\n            timesteps = jax.random.randint(\n                timestep_rng,\n                (bsz,),\n                0,\n                noise_scheduler.config.num_train_timesteps,\n            )\n\n            # Add noise to the latents according to the noise magnitude at each timestep\n            # (this is the forward diffusion process)\n            noisy_latents = noise_scheduler.add_noise(noise_scheduler_state, latents, noise, timesteps)\n\n            # Get the text embedding for conditioning\n            if args.train_text_en",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 524, "column": 4 },
          "end": { "row": 524, "column": 4 }
        }
      }
    }
  ],
  [
    "147",
    {
      "pageContent": "def checkpoint(step=None):\n        # Create the pipeline using the trained modules and save it.\n        scheduler, _ = FlaxPNDMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n        safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n            \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n        )\n        pipeline = FlaxStableDiffusionPipeline(\n            text_encoder=text_encoder,\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        )\n\n        outdir = os.path.join(args.output_dir, str(step)) if step else args.output_dir\n        pipeline.save_pretrained(\n            outdir,\n            params={\n                \"text_encoder\": get_params_to_save(text_encoder_state.params),\n                \"vae\": get_params_to_save(vae_params),\n                \"unet\": get_params_to_save(unet_state.params),\n                \"safety_checker\": safety_checker.params,\n            },\n        )\n\n        if args.push_to_hub:\n            message = f\"checkpoint-{step}\" if step is not None else \"End of training\"\n            repo.push_to_hub(commit_message=message, blocking=False, auto_lfs_prune=True)",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_flax.py",
        "range": {
          "start": { "row": 641, "column": 4 },
          "end": { "row": 641, "column": 4 }
        }
      }
    }
  ],
  [
    "148",
    {
      "pageContent": "def log_validation(text_encoder, tokenizer, unet, vae, args, accelerator, weight_dtype, epoch):\n    logger.info(\n        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n        f\" {args.validation_prompt}.\"\n    )\n    # create pipeline (note: unet and vae are loaded again in float32)\n    pipeline = DiffusionPipeline.from_pretrained(\n        args.pretrained_model_name_or_path,\n        text_encoder=accelerator.unwrap_model(text_encoder),\n        tokenizer=tokenizer,\n        unet=accelerator.unwrap_model(unet),\n        vae=vae,\n        revision=args.revision,\n        torch_dtype=weight_dtype,\n    )\n    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n    pipeline = pipeline.to(accelerator.device)\n    pipeline.set_progress_bar_config(disable=True)\n\n    # run inference\n    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)\n    images = []\n    for _ in range(args.num_validation_images):\n        with torch.autocast(\"cuda\"):\n            image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\n        images.append(image)\n\n    for tracker in accelerator.trackers:\n        if tracker.name == \"tensorboard\":\n            np_images = np.stack([np.asarray(img) for img in images])\n            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\n        if tracker.name == \"wandb\":\n            tracker.log(\n                {\n                    \"validation\": [\n                        wandb.Image(i",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "149",
    {
      "pageContent": "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=revision,\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"RobertaSeriesModelWithTransformation\":\n        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n\n        return RobertaSeriesModelWithTransformation\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 108, "column": 0 },
          "end": { "row": 108, "column": 0 }
        }
      }
    }
  ],
  [
    "150",
    {
      "pageContent": "def parse_args(input_args=None):\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=(\n            \"Revision of pretrained model identifier from huggingface.co/models. Trainable model components should be\"\n            \" float32 precision.\"\n        ),\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    p",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 128, "column": 0 },
          "end": { "row": 128, "column": 0 }
        }
      }
    }
  ],
  [
    "151",
    {
      "pageContent": "class DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        class_num=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(f\"Instance {self.instance_data_root} images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            if class_num is not None:\n                self.num_class_images = min(len(self.class_images_path), class_num)\n            else:\n                self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_d",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 443, "column": 0 },
          "end": { "row": 443, "column": 0 }
        }
      }
    }
  ],
  [
    "152",
    {
      "pageContent": "def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        class_num=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(f\"Instance {self.instance_data_root} images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            if class_num is not None:\n                self.num_class_images = min(len(self.class_images_path), class_num)\n            else:\n                self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCro",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 449, "column": 4 },
          "end": { "row": 449, "column": 4 }
        }
      }
    }
  ],
  [
    "153",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            self.instance_prompt,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            ).input_ids\n\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 498, "column": 4 },
          "end": { "row": 498, "column": 4 }
        }
      }
    }
  ],
  [
    "154",
    {
      "pageContent": "def collate_fn(examples, with_prior_preservation=False):\n    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n    pixel_values = [example[\"instance_images\"] for example in examples]\n\n    # Concat class and instance examples for prior preservation.\n    # We do this to avoid doing two forward passes.\n    if with_prior_preservation:\n        input_ids += [example[\"class_prompt_ids\"] for example in examples]\n        pixel_values += [example[\"class_images\"] for example in examples]\n\n    pixel_values = torch.stack(pixel_values)\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n    input_ids = torch.cat(input_ids, dim=0)\n\n    batch = {\n        \"input_ids\": input_ids,\n        \"pixel_values\": pixel_values,\n    }\n    return batch",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 528, "column": 0 },
          "end": { "row": 528, "column": 0 }
        }
      }
    }
  ],
  [
    "155",
    {
      "pageContent": "class PromptDataset(Dataset):\n    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n\n    def __init__(self, prompt, num_samples):\n        self.prompt = prompt\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 550, "column": 0 },
          "end": { "row": 550, "column": 0 }
        }
      }
    }
  ],
  [
    "156",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 560, "column": 4 },
          "end": { "row": 560, "column": 4 }
        }
      }
    }
  ],
  [
    "157",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 567, "column": 0 },
          "end": { "row": 567, "column": 0 }
        }
      }
    }
  ],
  [
    "158",
    {
      "pageContent": "def main(args):\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n        raise ValueError(\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n        )\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth.py",
        "range": {
          "start": { "row": 577, "column": 0 },
          "end": { "row": 577, "column": 0 }
        }
      }
    }
  ],
  [
    "159",
    {
      "pageContent": "def save_model_card(repo_name, images=None, base_model=str, prompt=str, repo_folder=None):\n    img_str = \"\"\n    for i, image in enumerate(images):\n        image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n        img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n\n    yaml = f\"\"\"\n---\nlicense: creativeml-openrail-m\nbase_model: {base_model}\ninstance_prompt: {prompt}\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- diffusers\n- lora\ninference: true\n---\n    \"\"\"\n    model_card = f\"\"\"\n# LoRA DreamBooth - {repo_name}\n\nThese are LoRA adaption weights for {base_model}. The weights were trained on {prompt} using [DreamBooth](https://dreambooth.github.io/). You can find some example images in the following. \\n\n{img_str}\n\"\"\"\n    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n        f.write(yaml + model_card)",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 61, "column": 0 },
          "end": { "row": 61, "column": 0 }
        }
      }
    }
  ],
  [
    "160",
    {
      "pageContent": "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=revision,\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"RobertaSeriesModelWithTransformation\":\n        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n\n        return RobertaSeriesModelWithTransformation\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 91, "column": 0 },
          "end": { "row": 91, "column": 0 }
        }
      }
    }
  ],
  [
    "161",
    {
      "pageContent": "def parse_args(input_args=None):\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--validation_prompt\",\n        type=str,\n        default=None,\n    ",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 111, "column": 0 },
          "end": { "row": 111, "column": 0 }
        }
      }
    }
  ],
  [
    "162",
    {
      "pageContent": "class DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        class_num=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            if class_num is not None:\n                self.num_class_images = min(len(self.class_images_path), class_num)\n            else:\n                self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        se",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 406, "column": 0 },
          "end": { "row": 406, "column": 0 }
        }
      }
    }
  ],
  [
    "163",
    {
      "pageContent": "def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        class_num=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            if class_num is not None:\n                self.num_class_images = min(len(self.class_images_path), class_num)\n            else:\n                self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 412, "column": 4 },
          "end": { "row": 412, "column": 4 }
        }
      }
    }
  ],
  [
    "164",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            self.instance_prompt,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            ).input_ids\n\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 461, "column": 4 },
          "end": { "row": 461, "column": 4 }
        }
      }
    }
  ],
  [
    "165",
    {
      "pageContent": "def collate_fn(examples, with_prior_preservation=False):\n    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n    pixel_values = [example[\"instance_images\"] for example in examples]\n\n    # Concat class and instance examples for prior preservation.\n    # We do this to avoid doing two forward passes.\n    if with_prior_preservation:\n        input_ids += [example[\"class_prompt_ids\"] for example in examples]\n        pixel_values += [example[\"class_images\"] for example in examples]\n\n    pixel_values = torch.stack(pixel_values)\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n    input_ids = torch.cat(input_ids, dim=0)\n\n    batch = {\n        \"input_ids\": input_ids,\n        \"pixel_values\": pixel_values,\n    }\n    return batch",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 491, "column": 0 },
          "end": { "row": 491, "column": 0 }
        }
      }
    }
  ],
  [
    "166",
    {
      "pageContent": "class PromptDataset(Dataset):\n    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n\n    def __init__(self, prompt, num_samples):\n        self.prompt = prompt\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 513, "column": 0 },
          "end": { "row": 513, "column": 0 }
        }
      }
    }
  ],
  [
    "167",
    {
      "pageContent": "def __getitem__(self, index):\n        example = {}\n        example[\"prompt\"] = self.prompt\n        example[\"index\"] = index\n        return example",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 523, "column": 4 },
          "end": { "row": 523, "column": 4 }
        }
      }
    }
  ],
  [
    "168",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 530, "column": 0 },
          "end": { "row": 530, "column": 0 }
        }
      }
    }
  ],
  [
    "169",
    {
      "pageContent": "def main(args):\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set th",
      "metadata": {
        "source": "examples/dreambooth/train_dreambooth_lora.py",
        "range": {
          "start": { "row": 540, "column": 0 },
          "end": { "row": 540, "column": 0 }
        }
      }
    }
  ],
  [
    "170",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that  Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n   ",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_flax.py",
        "range": {
          "start": { "row": 41, "column": 0 },
          "end": { "row": 41, "column": 0 }
        }
      }
    }
  ],
  [
    "171",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_flax.py",
        "range": {
          "start": { "row": 224, "column": 0 },
          "end": { "row": 224, "column": 0 }
        }
      }
    }
  ],
  [
    "172",
    {
      "pageContent": "def main():\n    args = parse_args()\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    # Setup logging, we only want one process per machine to log things on the screen.\n    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n    if jax.process_index() == 0:\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if jax.process_index() == 0:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            create_repo(repo_name, exist_ok=True, token=args.hub_token)\n            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Get the datasets: you can either provide your own training and evaluation files (see below)\n    # or specify a Dataset from the hub (the dataset will be downloaded ",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_flax.py",
        "range": {
          "start": { "row": 243, "column": 0 },
          "end": { "row": 243, "column": 0 }
        }
      }
    }
  ],
  [
    "173",
    {
      "pageContent": "def tokenize_captions(examples, is_train=True):\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n            elif isinstance(caption, (list, np.ndarray)):\n                # take a random caption if there are multiple\n                captions.append(random.choice(caption) if is_train else caption[0])\n            else:\n                raise ValueError(\n                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n                )\n        inputs = tokenizer(captions, max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n        input_ids = inputs.input_ids\n        return input_ids",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_flax.py",
        "range": {
          "start": { "row": 328, "column": 4 },
          "end": { "row": 328, "column": 4 }
        }
      }
    }
  ],
  [
    "174",
    {
      "pageContent": "def preprocess_train(examples):\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples)\n\n        return examples",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_flax.py",
        "range": {
          "start": { "row": 354, "column": 4 },
          "end": { "row": 354, "column": 4 }
        }
      }
    }
  ],
  [
    "175",
    {
      "pageContent": "def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n        input_ids = [example[\"input_ids\"] for example in examples]\n\n        padded_tokens = tokenizer.pad(\n            {\"input_ids\": input_ids}, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\"\n        )\n        batch = {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": padded_tokens.input_ids,\n        }\n        batch = {k: v.numpy() for k, v in batch.items()}\n\n        return batch",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_flax.py",
        "range": {
          "start": { "row": 367, "column": 4 },
          "end": { "row": 367, "column": 4 }
        }
      }
    }
  ],
  [
    "176",
    {
      "pageContent": "def train_step(state, text_encoder_params, vae_params, batch, train_rng):\n        dropout_rng, sample_rng, new_train_rng = jax.random.split(train_rng, 3)\n\n        def compute_loss(params):\n            # Convert images to latent space\n            vae_outputs = vae.apply(\n                {\"params\": vae_params}, batch[\"pixel_values\"], deterministic=True, method=vae.encode\n            )\n            latents = vae_outputs.latent_dist.sample(sample_rng)\n            # (NHWC) -> (NCHW)\n            latents = jnp.transpose(latents, (0, 3, 1, 2))\n            latents = latents * vae.config.scaling_factor\n\n            # Sample noise that we'll add to the latents\n            noise_rng, timestep_rng = jax.random.split(sample_rng)\n            noise = jax.random.normal(noise_rng, latents.shape)\n            # Sample a random timestep for each image\n            bsz = latents.shape[0]\n            timesteps = jax.random.randint(\n                timestep_rng,\n                (bsz,),\n                0,\n                noise_scheduler.config.num_train_timesteps,\n            )\n\n            # Add noise to the latents according to the noise magnitude at each timestep\n            # (this is the forward diffusion process)\n            noisy_latents = noise_scheduler.add_noise(noise_scheduler_state, latents, noise, timesteps)\n\n            # Get the text embedding for conditioning\n            encoder_hidden_states = text_encoder(\n                batch[\"input_ids\"],\n                params=text_encoder_params,\n                train=False,\n            )[0]\n\n            # Predict the noise residual and compute",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_flax.py",
        "range": {
          "start": { "row": 438, "column": 4 },
          "end": { "row": 438, "column": 4 }
        }
      }
    }
  ],
  [
    "177",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that  Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n   ",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "178",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 316, "column": 0 },
          "end": { "row": 316, "column": 0 }
        }
      }
    }
  ],
  [
    "179",
    {
      "pageContent": "def main():\n    args = parse_args()\n\n    if args.non_ema_revision is not None:\n        deprecate(\n            \"non_ema_revision!=None\",\n            \"0.15.0\",\n            message=(\n                \"Downloading 'non_ema' weights from revision branches of the Hub is deprecated. Please make sure to\"\n                \" use `--variant=non_ema` instead.\"\n            ),\n        )\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 331, "column": 0 },
          "end": { "row": 331, "column": 0 }
        }
      }
    }
  ],
  [
    "180",
    {
      "pageContent": "def tokenize_captions(examples, is_train=True):\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n            elif isinstance(caption, (list, np.ndarray)):\n                # take a random caption if there are multiple\n                captions.append(random.choice(caption) if is_train else caption[0])\n            else:\n                raise ValueError(\n                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n                )\n        inputs = tokenizer(\n            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return inputs.input_ids",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 547, "column": 4 },
          "end": { "row": 547, "column": 4 }
        }
      }
    }
  ],
  [
    "181",
    {
      "pageContent": "def preprocess_train(examples):\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples)\n        return examples",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 575, "column": 4 },
          "end": { "row": 575, "column": 4 }
        }
      }
    }
  ],
  [
    "182",
    {
      "pageContent": "def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image.py",
        "range": {
          "start": { "row": 587, "column": 4 },
          "end": { "row": 587, "column": 4 }
        }
      }
    }
  ],
  [
    "183",
    {
      "pageContent": "def save_model_card(repo_name, images=None, base_model=str, dataset_name=str, repo_folder=None):\n    img_str = \"\"\n    for i, image in enumerate(images):\n        image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n        img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n\n    yaml = f\"\"\"\n---\nlicense: creativeml-openrail-m\nbase_model: {base_model}\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- diffusers\n- lora\ninference: true\n---\n    \"\"\"\n    model_card = f\"\"\"\n# LoRA text2image fine-tuning - {repo_name}\nThese are LoRA adaption weights for {base_model}. The weights were fine-tuned on the {dataset_name} dataset. You can find some example images in the following. \\n\n{img_str}\n\"\"\"\n    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n        f.write(yaml + model_card)",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 55, "column": 0 },
          "end": { "row": 55, "column": 0 }
        }
      }
    }
  ],
  [
    "184",
    {
      "pageContent": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that  Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n   ",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 83, "column": 0 },
          "end": { "row": 83, "column": 0 }
        }
      }
    }
  ],
  [
    "185",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 348, "column": 0 },
          "end": { "row": 348, "column": 0 }
        }
      }
    }
  ],
  [
    "186",
    {
      "pageContent": "def main():\n    args = parse_args()\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n        project_config=accelerator_project_config,\n    )\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 363, "column": 0 },
          "end": { "row": 363, "column": 0 }
        }
      }
    }
  ],
  [
    "187",
    {
      "pageContent": "def tokenize_captions(examples, is_train=True):\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n            elif isinstance(caption, (list, np.ndarray)):\n                # take a random caption if there are multiple\n                captions.append(random.choice(caption) if is_train else caption[0])\n            else:\n                raise ValueError(\n                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n                )\n        inputs = tokenizer(\n            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return inputs.input_ids",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 577, "column": 4 },
          "end": { "row": 577, "column": 4 }
        }
      }
    }
  ],
  [
    "188",
    {
      "pageContent": "def preprocess_train(examples):\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples)\n        return examples",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 605, "column": 4 },
          "end": { "row": 605, "column": 4 }
        }
      }
    }
  ],
  [
    "189",
    {
      "pageContent": "def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}",
      "metadata": {
        "source": "examples/text_to_image/train_text_to_image_lora.py",
        "range": {
          "start": { "row": 617, "column": 4 },
          "end": { "row": 617, "column": 4 }
        }
      }
    }
  ],
  [
    "190",
    {
      "pageContent": "class TextInpainting(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text based inpainting using Stable Diffusion.\n    Uses CLIPSeg to get a mask from the given text, then calls the Inpainting pipeline with the generated mask\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        segmentation_model ([`CLIPSegForImageSegmentation`]):\n            CLIPSeg Model to generate mask from the given text. Please refer to the [model card]() for details.\n        segmentation_processor ([`CLIPSegProcessor`]):\n            CLIPSeg processor to get image, text features to translate prompt to English, if necessary. Please refer to the\n            [model card](https://huggingface.co/docs/transformers/model_doc/clipseg) for details.\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n       ",
      "metadata": {
        "source": "examples/community/text_inpainting.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "191",
    {
      "pageContent": "def __init__(\n        self,\n        segmentation_model: CLIPSegForImageSegmentation,\n        segmentation_processor: CLIPSegProcessor,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"skip_prk_steps\") and scheduler.config.skip_prk_steps is False:\n            deprecation_message = (\n                f\"The configurati",
      "metadata": {
        "source": "examples/community/text_inpainting.py",
        "range": {
          "start": { "row": 58, "column": 4 },
          "end": { "row": 58, "column": 4 }
        }
      }
    }
  ],
  [
    "192",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/text_inpainting.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "193",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/text_inpainting.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "194",
    {
      "pageContent": "def enable_sequential_cpu_offload(self):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(\"cuda\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "examples/community/text_inpainting.py",
        "range": {
          "start": { "row": 149, "column": 4 },
          "end": { "row": 149, "column": 4 }
        }
      }
    }
  ],
  [
    "195",
    {
      "pageContent": "def grab_wildcard_values(wildcard_option_dict: Dict[str, List[str]] = {}, wildcard_files: List[str] = []):\n    for wildcard_file in wildcard_files:\n        filename = get_filename(wildcard_file)\n        read_values = read_wildcard_values(wildcard_file)\n        if filename not in wildcard_option_dict:\n            wildcard_option_dict[filename] = []\n        wildcard_option_dict[filename].extend(read_values)\n    return wildcard_option_dict",
      "metadata": {
        "source": "examples/community/wildcard_stable_diffusion.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "196",
    {
      "pageContent": "def replace_prompt_with_wildcards(\n    prompt: str, wildcard_option_dict: Dict[str, List[str]] = {}, wildcard_files: List[str] = []\n):\n    new_prompt = prompt\n\n    # get wildcard options\n    wildcard_option_dict = grab_wildcard_values(wildcard_option_dict, wildcard_files)\n\n    for m in global_re_wildcard.finditer(new_prompt):\n        wildcard_value = m.group()\n        replace_value = random.choice(wildcard_option_dict[wildcard_value.strip(\"__\")])\n        new_prompt = new_prompt.replace(wildcard_value, replace_value, 1)\n\n    return new_prompt",
      "metadata": {
        "source": "examples/community/wildcard_stable_diffusion.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "197",
    {
      "pageContent": "class WildcardStableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Example Usage:\n        pipe = WildcardStableDiffusionPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\",\n\n            torch_dtype=torch.float16,\n        )\n        prompt = \"__animal__ sitting on a __object__ wearing a __clothing__\"\n        out = pipe(\n            prompt,\n            wildcard_option_dict={\n                \"clothing\":[\"hat\", \"shirt\", \"scarf\", \"beret\"]\n            },\n            wildcard_files=[\"object.txt\", \"animal.txt\"],\n            num_prompt_samples=1\n        )\n\n\n    Pipeline for text-to-image generation with wild cards using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DCo",
      "metadata": {
        "source": "examples/community/wildcard_stable_diffusion.py",
        "range": {
          "start": { "row": 65, "column": 0 },
          "end": { "row": 65, "column": 0 }
        }
      }
    }
  ],
  [
    "198",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not ex",
      "metadata": {
        "source": "examples/community/wildcard_stable_diffusion.py",
        "range": {
          "start": { "row": 110, "column": 4 },
          "end": { "row": 110, "column": 4 }
        }
      }
    }
  ],
  [
    "199",
    {
      "pageContent": "def parse_prompt_attention(text):\n    \"\"\"\n    Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n    Accepted tokens are:\n      (abc) - increases attention to abc by a multiplier of 1.1\n      (abc:3.12) - increases attention to abc by a multiplier of 3.12\n      [abc] - decreases attention to abc by a multiplier of 1.1\n      \\( - literal character '('\n      \\[ - literal character '['\n      \\) - literal character ')'\n      \\] - literal character ']'\n      \\\\ - literal character '\\'\n      anything else - just text\n    >>> parse_prompt_attention('normal text')\n    [['normal text', 1.0]]\n    >>> parse_prompt_attention('an (important) word')\n    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n    >>> parse_prompt_attention('(unbalanced')\n    [['unbalanced', 1.1]]\n    >>> parse_prompt_attention('\\(literal\\]')\n    [['(literal]', 1.0]]\n    >>> parse_prompt_attention('(unnecessary)(parens)')\n    [['unnecessaryparens', 1.1]]\n    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n    [['a ', 1.0],\n     ['house', 1.5730000000000004],\n     [' ', 1.1],\n     ['on', 1.0],\n     [' a ', 1.1],\n     ['hill', 0.55],\n     [', sun, ', 1.1],\n     ['sky', 1.4641000000000006],\n     ['.', 1.1]]\n    \"\"\"\n\n    res = []\n    round_brackets = []\n    square_brackets = []\n\n    round_bracket_multiplier = 1.1\n    square_bracket_multiplier = 1 / 1.1\n\n    def multiply_range(start_position, multiplier):\n        for p in range(start_position, len(res)):\n            res[p][1] *= multiplier\n\n    for m in re_attention.finditer(text):\n   ",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 60, "column": 0 },
          "end": { "row": 60, "column": 0 }
        }
      }
    }
  ],
  [
    "200",
    {
      "pageContent": "def get_prompts_with_weights(pipe: StableDiffusionPipeline, prompt: List[str], max_length: int):\n    r\"\"\"\n    Tokenize a list of prompts and return its tokens with weights of each token.\n\n    No padding, starting or ending token is included.\n    \"\"\"\n    tokens = []\n    weights = []\n    truncated = False\n    for text in prompt:\n        texts_and_weights = parse_prompt_attention(text)\n        text_token = []\n        text_weight = []\n        for word, weight in texts_and_weights:\n            # tokenize and discard the starting and the ending token\n            token = pipe.tokenizer(word).input_ids[1:-1]\n            text_token += token\n            # copy the weight by length of token\n            text_weight += [weight] * len(token)\n            # stop if the text is too long (longer than truncation limit)\n            if len(text_token) > max_length:\n                truncated = True\n                break\n        # truncate\n        if len(text_token) > max_length:\n            truncated = True\n            text_token = text_token[:max_length]\n            text_weight = text_weight[:max_length]\n        tokens.append(text_token)\n        weights.append(text_weight)\n    if truncated:\n        logger.warning(\"Prompt was truncated. Try to shorten the prompt or increase max_embeddings_multiples\")\n    return tokens, weights",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 146, "column": 0 },
          "end": { "row": 146, "column": 0 }
        }
      }
    }
  ],
  [
    "201",
    {
      "pageContent": "def pad_tokens_and_weights(tokens, weights, max_length, bos, eos, no_boseos_middle=True, chunk_length=77):\n    r\"\"\"\n    Pad the tokens (with starting and ending tokens) and weights (with 1.0) to max_length.\n    \"\"\"\n    max_embeddings_multiples = (max_length - 2) // (chunk_length - 2)\n    weights_length = max_length if no_boseos_middle else max_embeddings_multiples * chunk_length\n    for i in range(len(tokens)):\n        tokens[i] = [bos] + tokens[i] + [eos] * (max_length - 1 - len(tokens[i]))\n        if no_boseos_middle:\n            weights[i] = [1.0] + weights[i] + [1.0] * (max_length - 1 - len(weights[i]))\n        else:\n            w = []\n            if len(weights[i]) == 0:\n                w = [1.0] * weights_length\n            else:\n                for j in range(max_embeddings_multiples):\n                    w.append(1.0)  # weight for starting token in this chunk\n                    w += weights[i][j * (chunk_length - 2) : min(len(weights[i]), (j + 1) * (chunk_length - 2))]\n                    w.append(1.0)  # weight for ending token in this chunk\n                w += [1.0] * (weights_length - len(w))\n            weights[i] = w[:]\n\n    return tokens, weights",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 181, "column": 0 },
          "end": { "row": 181, "column": 0 }
        }
      }
    }
  ],
  [
    "202",
    {
      "pageContent": "def get_unweighted_text_embeddings(\n    pipe: StableDiffusionPipeline,\n    text_input: torch.Tensor,\n    chunk_length: int,\n    no_boseos_middle: Optional[bool] = True,\n):\n    \"\"\"\n    When the length of tokens is a multiple of the capacity of the text encoder,\n    it should be split into chunks and sent to the text encoder individually.\n    \"\"\"\n    max_embeddings_multiples = (text_input.shape[1] - 2) // (chunk_length - 2)\n    if max_embeddings_multiples > 1:\n        text_embeddings = []\n        for i in range(max_embeddings_multiples):\n            # extract the i-th chunk\n            text_input_chunk = text_input[:, i * (chunk_length - 2) : (i + 1) * (chunk_length - 2) + 2].clone()\n\n            # cover the head and the tail by the starting and the ending tokens\n            text_input_chunk[:, 0] = text_input[0, 0]\n            text_input_chunk[:, -1] = text_input[0, -1]\n            text_embedding = pipe.text_encoder(text_input_chunk)[0]\n\n            if no_boseos_middle:\n                if i == 0:\n                    # discard the ending token\n                    text_embedding = text_embedding[:, :-1]\n                elif i == max_embeddings_multiples - 1:\n                    # discard the starting token\n                    text_embedding = text_embedding[:, 1:]\n                else:\n                    # discard both starting and ending tokens\n                    text_embedding = text_embedding[:, 1:-1]\n\n            text_embeddings.append(text_embedding)\n        text_embeddings = torch.concat(text_embeddings, axis=1)\n    else:\n        text_embeddings = pipe.text_encoder(tex",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 206, "column": 0 },
          "end": { "row": 206, "column": 0 }
        }
      }
    }
  ],
  [
    "203",
    {
      "pageContent": "def get_weighted_text_embeddings(\n    pipe: StableDiffusionPipeline,\n    prompt: Union[str, List[str]],\n    uncond_prompt: Optional[Union[str, List[str]]] = None,\n    max_embeddings_multiples: Optional[int] = 3,\n    no_boseos_middle: Optional[bool] = False,\n    skip_parsing: Optional[bool] = False,\n    skip_weighting: Optional[bool] = False,\n):\n    r\"\"\"\n    Prompts can be assigned with local weights using brackets. For example,\n    prompt 'A (very beautiful) masterpiece' highlights the words 'very beautiful',\n    and the embedding tokens corresponding to the words get multiplied by a constant, 1.1.\n\n    Also, to regularize of the embedding, the weighted embedding would be scaled to preserve the original mean.\n\n    Args:\n        pipe (`StableDiffusionPipeline`):\n            Pipe to provide access to the tokenizer and the text encoder.\n        prompt (`str` or `List[str]`):\n            The prompt or prompts to guide the image generation.\n        uncond_prompt (`str` or `List[str]`):\n            The unconditional prompt or prompts for guide the image generation. If unconditional prompt\n            is provided, the embeddings of prompt and uncond_prompt are concatenated.\n        max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n            The max multiple length of prompt embeddings compared to the max output length of text encoder.\n        no_boseos_middle (`bool`, *optional*, defaults to `False`):\n            If the length of text token is multiples of the capacity of text encoder, whether reserve the starting and\n            ending token in each of the chunk in",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 246, "column": 0 },
          "end": { "row": 246, "column": 0 }
        }
      }
    }
  ],
  [
    "204",
    {
      "pageContent": "def preprocess_image(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 376, "column": 0 },
          "end": { "row": 376, "column": 0 }
        }
      }
    }
  ],
  [
    "205",
    {
      "pageContent": "def preprocess_mask(mask, scale_factor=8):\n    mask = mask.convert(\"L\")\n    w, h = mask.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    mask = mask.resize((w // scale_factor, h // scale_factor), resample=PIL_INTERPOLATION[\"nearest\"])\n    mask = np.array(mask).astype(np.float32) / 255.0\n    mask = np.tile(mask, (4, 1, 1))\n    mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?\n    mask = 1 - mask  # repaint white, keep black\n    mask = torch.from_numpy(mask)\n    return mask",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 386, "column": 0 },
          "end": { "row": 386, "column": 0 }
        }
      }
    }
  ],
  [
    "206",
    {
      "pageContent": "class StableDiffusionLongPromptWeightingPipeline(StableDiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion without tokens length limit, and support parsing\n    weighting in prompt.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether ",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 399, "column": 0 },
          "end": { "row": 399, "column": 0 }
        }
      }
    }
  ],
  [
    "207",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt,\n        max_embeddings_multiples,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `list(int)`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n                The max multiple length of prompt embeddings compared to the max output length of text encoder.\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        if negative_prompt is None:\n            negative_prompt = [\"\"] * batch_size\n        elif isinstance(negative_prompt, str):\n            negative_prompt = [negative_prompt] * batch_size\n        if batch_size != len(negative_prompt):\n            raise ValueError(\n                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                f\" {prompt} has batch size {batch_size}. Please mak",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 498, "column": 4 },
          "end": { "row": 498, "column": 4 }
        }
      }
    }
  ],
  [
    "208",
    {
      "pageContent": "def check_inputs(self, prompt, height, width, strength, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 556, "column": 4 },
          "end": { "row": 556, "column": 4 }
        }
      }
    }
  ],
  [
    "209",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device, is_text2img):\n        if is_text2img:\n            return self.scheduler.timesteps.to(device), num_inference_steps\n        else:\n            # get the original timestep using init_timestep\n            offset = self.scheduler.config.get(\"steps_offset\", 0)\n            init_timestep = int(num_inference_steps * strength) + offset\n            init_timestep = min(init_timestep, num_inference_steps)\n\n            t_start = max(num_inference_steps - init_timestep + offset, 0)\n            timesteps = self.scheduler.timesteps[t_start:].to(device)\n            return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 574, "column": 4 },
          "end": { "row": 574, "column": 4 }
        }
      }
    }
  ],
  [
    "210",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 587, "column": 4 },
          "end": { "row": 587, "column": 4 }
        }
      }
    }
  ],
  [
    "211",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / 0.18215 * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 597, "column": 4 },
          "end": { "row": 597, "column": 4 }
        }
      }
    }
  ],
  [
    "212",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 605, "column": 4 },
          "end": { "row": 605, "column": 4 }
        }
      }
    }
  ],
  [
    "213",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, height, width, dtype, device, generator, latents=None):\n        if image is None:\n            shape = (\n                batch_size,\n                self.unet.in_channels,\n                height // self.vae_scale_factor,\n                width // self.vae_scale_factor,\n            )\n\n            if latents is None:\n                if device.type == \"mps\":\n                    # randn does not work reproducibly on mps\n                    latents = torch.randn(shape, generator=generator, device=\"cpu\", dtype=dtype).to(device)\n                else:\n                    latents = torch.randn(shape, generator=generator, device=device, dtype=dtype)\n            else:\n                if latents.shape != shape:\n                    raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n                latents = latents.to(device)\n\n            # scale the initial noise by the standard deviation required by the scheduler\n            latents = latents * self.scheduler.init_noise_sigma\n            return latents, None, None\n        else:\n            init_latent_dist = self.vae.encode(image).latent_dist\n            init_latents = init_latent_dist.sample(generator=generator)\n            init_latents = 0.18215 * init_latents\n            init_latents = torch.cat([init_latents] * batch_size, dim=0)\n            init_latents_orig = init_latents\n            shape = init_latents.shape\n\n            # add noise to latents using the timesteps\n            if device.type == \"mps\":\n                noise = torch.randn(shape,",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 622, "column": 4 },
          "end": { "row": 622, "column": 4 }
        }
      }
    }
  ],
  [
    "214",
    {
      "pageContent": "def text2img(\n        self,\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        height: int = 512,\n        width: int = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        is_cancelled_callback: Optional[Callable[[], bool]] = None,\n        callback_steps: int = 1,\n    ):\n        r\"\"\"\n        Function for text-to-image generation.\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            height (`int`, *optional*, defaults to 512):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to 512):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image a",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 862, "column": 4 },
          "end": { "row": 862, "column": 4 }
        }
      }
    }
  ],
  [
    "215",
    {
      "pageContent": "def img2img(\n        self,\n        image: Union[torch.FloatTensor, PIL.Image.Image],\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[torch.Generator] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        is_cancelled_callback: Optional[Callable[[], bool]] = None,\n        callback_steps: int = 1,\n    ):\n        r\"\"\"\n        Function for image-to-image generation.\n        Args:\n            image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1.\n                `image` will",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 957, "column": 4 },
          "end": { "row": 957, "column": 4 }
        }
      }
    }
  ],
  [
    "216",
    {
      "pageContent": "def inpaint(\n        self,\n        image: Union[torch.FloatTensor, PIL.Image.Image],\n        mask_image: Union[torch.FloatTensor, PIL.Image.Image],\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[torch.Generator] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        is_cancelled_callback: Optional[Callable[[], bool]] = None,\n        callback_steps: int = 1,\n    ):\n        r\"\"\"\n        Function for inpaint.\n        Args:\n            image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one c",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion.py",
        "range": {
          "start": { "row": 1051, "column": 4 },
          "end": { "row": 1051, "column": 4 }
        }
      }
    }
  ],
  [
    "217",
    {
      "pageContent": "def make_transparency_mask(size, overlap_pixels, remove_borders=[]):\n    size_x = size[0] - overlap_pixels * 2\n    size_y = size[1] - overlap_pixels * 2\n    for letter in [\"l\", \"r\"]:\n        if letter in remove_borders:\n            size_x += overlap_pixels\n    for letter in [\"t\", \"b\"]:\n        if letter in remove_borders:\n            size_y += overlap_pixels\n    mask = np.ones((size_y, size_x), dtype=np.uint8) * 255\n    mask = np.pad(mask, mode=\"linear_ramp\", pad_width=overlap_pixels, end_values=0)\n\n    if \"l\" in remove_borders:\n        mask = mask[:, overlap_pixels : mask.shape[1]]\n    if \"r\" in remove_borders:\n        mask = mask[:, 0 : mask.shape[1] - overlap_pixels]\n    if \"t\" in remove_borders:\n        mask = mask[overlap_pixels : mask.shape[0], :]\n    if \"b\" in remove_borders:\n        mask = mask[0 : mask.shape[0] - overlap_pixels, :]\n    return mask",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "218",
    {
      "pageContent": "def clamp_rect(rect: [int], min: [int], max: [int]):\n    return (\n        clamp(rect[0], min[0], max[0]),\n        clamp(rect[1], min[1], max[1]),\n        clamp(rect[2], min[0], max[0]),\n        clamp(rect[3], min[1], max[1]),\n    )",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 55, "column": 0 },
          "end": { "row": 55, "column": 0 }
        }
      }
    }
  ],
  [
    "219",
    {
      "pageContent": "def add_overlap_rect(rect: [int], overlap: int, image_size: [int]):\n    rect = list(rect)\n    rect[0] -= overlap\n    rect[1] -= overlap\n    rect[2] += overlap\n    rect[3] += overlap\n    rect = clamp_rect(rect, [0, 0], [image_size[0], image_size[1]])\n    return rect",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "220",
    {
      "pageContent": "def squeeze_tile(tile, original_image, original_slice, slice_x):\n    result = Image.new(\"RGB\", (tile.size[0] + original_slice, tile.size[1]))\n    result.paste(\n        original_image.resize((tile.size[0], tile.size[1]), Image.BICUBIC).crop(\n            (slice_x, 0, slice_x + original_slice, tile.size[1])\n        ),\n        (0, 0),\n    )\n    result.paste(tile, (original_slice, 0))\n    return result",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 74, "column": 0 },
          "end": { "row": 74, "column": 0 }
        }
      }
    }
  ],
  [
    "221",
    {
      "pageContent": "def unsqueeze_tile(tile, original_image_slice):\n    crop_rect = (original_image_slice * 4, 0, tile.size[0], tile.size[1])\n    tile = tile.crop(crop_rect)\n    return tile",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 86, "column": 0 },
          "end": { "row": 86, "column": 0 }
        }
      }
    }
  ],
  [
    "222",
    {
      "pageContent": "class StableDiffusionTiledUpscalePipeline(StableDiffusionUpscalePipeline):\n    r\"\"\"\n    Pipeline for tile-based text-guided image super-resolution using Stable Diffusion 2, trading memory for compute\n    to create gigantic images.\n\n    This model inherits from [`StableDiffusionUpscalePipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        low_res_scheduler ([`SchedulerMixin`]):\n            A scheduler used to add initial noise to the low res conditioning image. It must be an instance of\n            [`DDPMScheduler`].\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 97, "column": 0 },
          "end": { "row": 97, "column": 0 }
        }
      }
    }
  ],
  [
    "223",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        low_res_scheduler: DDPMScheduler,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        max_noise_level: int = 350,\n    ):\n        super().__init__(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            low_res_scheduler=low_res_scheduler,\n            scheduler=scheduler,\n            max_noise_level=max_noise_level,\n        )",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "224",
    {
      "pageContent": "def _process_tile(self, original_image_slice, x, y, tile_size, tile_border, image, final_image, **kwargs):\n        torch.manual_seed(0)\n        crop_rect = (\n            min(image.size[0] - (tile_size + original_image_slice), x * tile_size),\n            min(image.size[1] - (tile_size + original_image_slice), y * tile_size),\n            min(image.size[0], (x + 1) * tile_size),\n            min(image.size[1], (y + 1) * tile_size),\n        )\n        crop_rect_with_overlap = add_overlap_rect(crop_rect, tile_border, image.size)\n        tile = image.crop(crop_rect_with_overlap)\n        translated_slice_x = ((crop_rect[0] + ((crop_rect[2] - crop_rect[0]) / 2)) / image.size[0]) * tile.size[0]\n        translated_slice_x = translated_slice_x - (original_image_slice / 2)\n        translated_slice_x = max(0, translated_slice_x)\n        to_input = squeeze_tile(tile, image, original_image_slice, translated_slice_x)\n        orig_input_size = to_input.size\n        to_input = to_input.resize((tile_size, tile_size), Image.BICUBIC)\n        upscaled_tile = super(StableDiffusionTiledUpscalePipeline, self).__call__(image=to_input, **kwargs).images[0]\n        upscaled_tile = upscaled_tile.resize((orig_input_size[0] * 4, orig_input_size[1] * 4), Image.BICUBIC)\n        upscaled_tile = unsqueeze_tile(upscaled_tile, original_image_slice)\n        upscaled_tile = upscaled_tile.resize((tile.size[0] * 4, tile.size[1] * 4), Image.BICUBIC)\n        remove_borders = []\n        if x == 0:\n            remove_borders.append(\"l\")\n        elif crop_rect[2] == image.size[0]:\n            remove_borders.append(\"r\")\n  ",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 144, "column": 4 },
          "end": { "row": 144, "column": 4 }
        }
      }
    }
  ],
  [
    "225",
    {
      "pageContent": "def main():\n    # Run a demo\n    model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n    pipe = StableDiffusionTiledUpscalePipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16)\n    pipe = pipe.to(\"cuda\")\n    image = Image.open(\"../../docs/source/imgs/diffusers_library.jpg\")\n\n    def callback(obj):\n        print(f\"progress: {obj['progress']:.4f}\")\n        obj[\"image\"].save(\"diffusers_library_progress.jpg\")\n\n    final_image = pipe(image=image, prompt=\"Black font, white background, vector\", noise_level=40, callback=callback)\n    final_image.save(\"diffusers_library.jpg\")",
      "metadata": {
        "source": "examples/community/tiled_upscaling.py",
        "range": {
          "start": { "row": 281, "column": 0 },
          "end": { "row": 281, "column": 0 }
        }
      }
    }
  ],
  [
    "226",
    {
      "pageContent": "def _encode_image(self, image, device, num_images_per_prompt, do_classifier_free_guidance):\n    image = image.to(device=device)\n    image_embeddings = image  # take image as image_embeddings\n    image_embeddings = image_embeddings.unsqueeze(1)\n\n    # duplicate image embeddings for each generation per prompt, using mps friendly method\n    bs_embed, seq_len, _ = image_embeddings.shape\n    image_embeddings = image_embeddings.repeat(1, num_images_per_prompt, 1)\n    image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n    if do_classifier_free_guidance:\n        uncond_embeddings = torch.zeros_like(image_embeddings)\n\n        # For classifier free guidance, we need to do two forward passes.\n        # Here we concatenate the unconditional and text embeddings into a single batch\n        # to avoid doing two forward passes\n        image_embeddings = torch.cat([uncond_embeddings, image_embeddings])\n\n    return image_embeddings",
      "metadata": {
        "source": "examples/community/stable_unclip.py",
        "range": {
          "start": { "row": 16, "column": 0 },
          "end": { "row": 16, "column": 0 }
        }
      }
    }
  ],
  [
    "227",
    {
      "pageContent": "class StableUnCLIPPipeline(DiffusionPipeline):\n    def __init__(\n        self,\n        prior: PriorTransformer,\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModelWithProjection,\n        prior_scheduler: UnCLIPScheduler,\n        decoder_pipe_kwargs: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        decoder_pipe_kwargs = dict(image_encoder=None) if decoder_pipe_kwargs is None else decoder_pipe_kwargs\n\n        decoder_pipe_kwargs[\"torch_dtype\"] = decoder_pipe_kwargs.get(\"torch_dtype\", None) or prior.dtype\n\n        self.decoder_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n            \"lambdalabs/sd-image-variations-diffusers\", **decoder_pipe_kwargs\n        )\n\n        # replace `_encode_image` method\n        self.decoder_pipe._encode_image = types.MethodType(_encode_image, self.decoder_pipe)\n\n        self.register_modules(\n            prior=prior,\n            tokenizer=tokenizer,\n            text_encoder=text_encoder,\n            prior_scheduler=prior_scheduler,\n        )\n\n    def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        text_model_output: Optional[Union[CLIPTextModelOutput, Tuple]] = None,\n        text_attention_mask: Optional[torch.Tensor] = None,\n    ):\n        if text_model_output is None:\n            batch_size = len(prompt) if isinstance(prompt, list) else 1\n            # get prompt text embeddings\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length",
      "metadata": {
        "source": "examples/community/stable_unclip.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "228",
    {
      "pageContent": "def __init__(\n        self,\n        prior: PriorTransformer,\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModelWithProjection,\n        prior_scheduler: UnCLIPScheduler,\n        decoder_pipe_kwargs: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        decoder_pipe_kwargs = dict(image_encoder=None) if decoder_pipe_kwargs is None else decoder_pipe_kwargs\n\n        decoder_pipe_kwargs[\"torch_dtype\"] = decoder_pipe_kwargs.get(\"torch_dtype\", None) or prior.dtype\n\n        self.decoder_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n            \"lambdalabs/sd-image-variations-diffusers\", **decoder_pipe_kwargs\n        )\n\n        # replace `_encode_image` method\n        self.decoder_pipe._encode_image = types.MethodType(_encode_image, self.decoder_pipe)\n\n        self.register_modules(\n            prior=prior,\n            tokenizer=tokenizer,\n            text_encoder=text_encoder,\n            prior_scheduler=prior_scheduler,\n        )",
      "metadata": {
        "source": "examples/community/stable_unclip.py",
        "range": {
          "start": { "row": 38, "column": 4 },
          "end": { "row": 38, "column": 4 }
        }
      }
    }
  ],
  [
    "229",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        text_model_output: Optional[Union[CLIPTextModelOutput, Tuple]] = None,\n        text_attention_mask: Optional[torch.Tensor] = None,\n    ):\n        if text_model_output is None:\n            batch_size = len(prompt) if isinstance(prompt, list) else 1\n            # get prompt text embeddings\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n            text_mask = text_inputs.attention_mask.bool().to(device)\n\n            if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n                removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n\n            text_encoder_output = self.text_encoder(text_input_ids.to(device))\n\n            text_embeddings = text_encoder_output.text_embeds\n            text_encoder_hidden_states = text_encoder_output.last_hidden_state\n\n        else:\n            batch_size = text_model_output[0].shape[0]\n            text_embed",
      "metadata": {
        "source": "examples/community/stable_unclip.py",
        "range": {
          "start": { "row": 66, "column": 4 },
          "end": { "row": 66, "column": 4 }
        }
      }
    }
  ],
  [
    "230",
    {
      "pageContent": "def prepare_latents(self, shape, dtype, device, generator, latents, scheduler):\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        latents = latents * scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "examples/community/stable_unclip.py",
        "range": {
          "start": { "row": 168, "column": 4 },
          "end": { "row": 168, "column": 4 }
        }
      }
    }
  ],
  [
    "231",
    {
      "pageContent": "def detect_language(pipe, prompt, batch_size):\n    \"\"\"helper function to detect language(s) of prompt\"\"\"\n\n    if batch_size == 1:\n        preds = pipe(prompt, top_k=1, truncation=True, max_length=128)\n        return preds[0][\"label\"]\n    else:\n        detected_languages = []\n        for p in prompt:\n            preds = pipe(p, top_k=1, truncation=True, max_length=128)\n            detected_languages.append(preds[0][\"label\"])\n\n        return detected_languages",
      "metadata": {
        "source": "examples/community/multilingual_stable_diffusion.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "232",
    {
      "pageContent": "def translate_prompt(prompt, translation_tokenizer, translation_model, device):\n    \"\"\"helper function to translate prompt to English\"\"\"\n\n    encoded_prompt = translation_tokenizer(prompt, return_tensors=\"pt\").to(device)\n    generated_tokens = translation_model.generate(**encoded_prompt, max_new_tokens=1000)\n    en_trans = translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\n    return en_trans[0]",
      "metadata": {
        "source": "examples/community/multilingual_stable_diffusion.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "233",
    {
      "pageContent": "class MultilingualStableDiffusion(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion in different languages.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        detection_pipeline ([`pipeline`]):\n            Transformers pipeline to detect prompt's language.\n        translation_model ([`MBartForConditionalGeneration`]):\n            Model to translate prompt to English, if necessary. Please refer to the\n            [model card](https://huggingface.co/docs/transformers/model_doc/mbart) for details.\n        translation_tokenizer ([`MBart50TokenizerFast`]):\n            Tokenizer of the translation model.\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to ",
      "metadata": {
        "source": "examples/community/multilingual_stable_diffusion.py",
        "range": {
          "start": { "row": 50, "column": 0 },
          "end": { "row": 50, "column": 0 }
        }
      }
    }
  ],
  [
    "234",
    {
      "pageContent": "def __init__(\n        self,\n        detection_pipeline: pipeline,\n        translation_model: MBartForConditionalGeneration,\n        translation_tokenizer: MBart50TokenizerFast,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.",
      "metadata": {
        "source": "examples/community/multilingual_stable_diffusion.py",
        "range": {
          "start": { "row": 85, "column": 4 },
          "end": { "row": 85, "column": 4 }
        }
      }
    }
  ],
  [
    "235",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/multilingual_stable_diffusion.py",
        "range": {
          "start": { "row": 137, "column": 4 },
          "end": { "row": 137, "column": 4 }
        }
      }
    }
  ],
  [
    "236",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/multilingual_stable_diffusion.py",
        "range": {
          "start": { "row": 156, "column": 4 },
          "end": { "row": 156, "column": 4 }
        }
      }
    }
  ],
  [
    "237",
    {
      "pageContent": "def prepare_mask_and_masked_image(image, mask):\n    image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    mask = np.array(mask.convert(\"L\"))\n    mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask)\n\n    masked_image = image * (mask < 0.5)\n\n    return mask, masked_image",
      "metadata": {
        "source": "examples/community/img2img_inpainting.py",
        "range": {
          "start": { "row": 20, "column": 0 },
          "end": { "row": 20, "column": 0 }
        }
      }
    }
  ],
  [
    "238",
    {
      "pageContent": "def check_size(image, height, width):\n    if isinstance(image, PIL.Image.Image):\n        w, h = image.size\n    elif isinstance(image, torch.Tensor):\n        *_, h, w = image.shape\n\n    if h != height or w != width:\n        raise ValueError(f\"Image size should be {height}x{width}, but got {h}x{w}\")",
      "metadata": {
        "source": "examples/community/img2img_inpainting.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "239",
    {
      "pageContent": "def overlay_inner_image(image, inner_image, paste_offset: Tuple[int] = (0, 0)):\n    inner_image = inner_image.convert(\"RGBA\")\n    image = image.convert(\"RGB\")\n\n    image.paste(inner_image, paste_offset, inner_image)\n    image = image.convert(\"RGB\")\n\n    return image",
      "metadata": {
        "source": "examples/community/img2img_inpainting.py",
        "range": {
          "start": { "row": 47, "column": 0 },
          "end": { "row": 47, "column": 0 }
        }
      }
    }
  ],
  [
    "240",
    {
      "pageContent": "class ImageToImageInpaintingPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image-to-image inpainting using Stable Diffusion. *This is an experimental feature*.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latens. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensi",
      "metadata": {
        "source": "examples/community/img2img_inpainting.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "241",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not ex",
      "metadata": {
        "source": "examples/community/img2img_inpainting.py",
        "range": {
          "start": { "row": 85, "column": 4 },
          "end": { "row": 85, "column": 4 }
        }
      }
    }
  ],
  [
    "242",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/img2img_inpainting.py",
        "range": {
          "start": { "row": 131, "column": 4 },
          "end": { "row": 131, "column": 4 }
        }
      }
    }
  ],
  [
    "243",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/img2img_inpainting.py",
        "range": {
          "start": { "row": 150, "column": 4 },
          "end": { "row": 150, "column": 4 }
        }
      }
    }
  ],
  [
    "244",
    {
      "pageContent": "class CheckpointMergerPipeline(DiffusionPipeline):\n    \"\"\"\n    A class that that supports merging diffusion models based on the discussion here:\n    https://github.com/huggingface/diffusers/issues/877\n\n    Example usage:-\n\n    pipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", custom_pipeline=\"checkpoint_merger.py\")\n\n    merged_pipe = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\"prompthero/openjourney\"], interp = 'inv_sigmoid', alpha = 0.8, force = True)\n\n    merged_pipe.to('cuda')\n\n    prompt = \"An astronaut riding a unicycle on Mars\"\n\n    results = merged_pipe(prompt)\n\n    ## For more details, see the docstring for the merge method.\n\n    \"\"\"\n\n    def __init__(self):\n        self.register_to_config()\n        super().__init__()\n\n    def _compare_model_configs(self, dict0, dict1):\n        if dict0 == dict1:\n            return True\n        else:\n            config0, meta_keys0 = self._remove_meta_keys(dict0)\n            config1, meta_keys1 = self._remove_meta_keys(dict1)\n            if config0 == config1:\n                print(f\"Warning !: Mismatch in keys {meta_keys0} and {meta_keys1}.\")\n                return True\n        return False\n\n    def _remove_meta_keys(self, config_dict: Dict):\n        meta_keys = []\n        temp_dict = config_dict.copy()\n        for key in config_dict.keys():\n            if key.startswith(\"_\"):\n                temp_dict.pop(key)\n                meta_keys.append(key)\n        return (temp_dict, meta_keys)\n\n    @torch.no_grad()\n    def merge(self, pretrained_model_name_or_path_list: List[Union[str, os.PathLike]], **kwargs):\n   ",
      "metadata": {
        "source": "examples/community/checkpoint_merger.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "245",
    {
      "pageContent": "def _compare_model_configs(self, dict0, dict1):\n        if dict0 == dict1:\n            return True\n        else:\n            config0, meta_keys0 = self._remove_meta_keys(dict0)\n            config1, meta_keys1 = self._remove_meta_keys(dict1)\n            if config0 == config1:\n                print(f\"Warning !: Mismatch in keys {meta_keys0} and {meta_keys1}.\")\n                return True\n        return False",
      "metadata": {
        "source": "examples/community/checkpoint_merger.py",
        "range": {
          "start": { "row": 44, "column": 4 },
          "end": { "row": 44, "column": 4 }
        }
      }
    }
  ],
  [
    "246",
    {
      "pageContent": "def _remove_meta_keys(self, config_dict: Dict):\n        meta_keys = []\n        temp_dict = config_dict.copy()\n        for key in config_dict.keys():\n            if key.startswith(\"_\"):\n                temp_dict.pop(key)\n                meta_keys.append(key)\n        return (temp_dict, meta_keys)",
      "metadata": {
        "source": "examples/community/checkpoint_merger.py",
        "range": {
          "start": { "row": 55, "column": 4 },
          "end": { "row": 55, "column": 4 }
        }
      }
    }
  ],
  [
    "247",
    {
      "pageContent": "def prepare_image(image):\n    if isinstance(image, torch.Tensor):\n        # Batch single image\n        if image.ndim == 3:\n            image = image.unsqueeze(0)\n\n        image = image.to(dtype=torch.float32)\n    else:\n        # preprocess image\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [np.array(i.convert(\"RGB\"))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    return image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 63, "column": 0 },
          "end": { "row": 63, "column": 0 }
        }
      }
    }
  ],
  [
    "248",
    {
      "pageContent": "def prepare_controlnet_conditioning_image(\n    controlnet_conditioning_image, width, height, batch_size, num_images_per_prompt, device, dtype\n):\n    if not isinstance(controlnet_conditioning_image, torch.Tensor):\n        if isinstance(controlnet_conditioning_image, PIL.Image.Image):\n            controlnet_conditioning_image = [controlnet_conditioning_image]\n\n        if isinstance(controlnet_conditioning_image[0], PIL.Image.Image):\n            controlnet_conditioning_image = [\n                np.array(i.resize((width, height), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :]\n                for i in controlnet_conditioning_image\n            ]\n            controlnet_conditioning_image = np.concatenate(controlnet_conditioning_image, axis=0)\n            controlnet_conditioning_image = np.array(controlnet_conditioning_image).astype(np.float32) / 255.0\n            controlnet_conditioning_image = controlnet_conditioning_image.transpose(0, 3, 1, 2)\n            controlnet_conditioning_image = torch.from_numpy(controlnet_conditioning_image)\n        elif isinstance(controlnet_conditioning_image[0], torch.Tensor):\n            controlnet_conditioning_image = torch.cat(controlnet_conditioning_image, dim=0)\n\n    image_batch_size = controlnet_conditioning_image.shape[0]\n\n    if image_batch_size == 1:\n        repeat_by = batch_size\n    else:\n        # image batch size is the same as prompt batch size\n        repeat_by = num_images_per_prompt\n\n    controlnet_conditioning_image = controlnet_conditioning_image.repeat_interleave(repeat_by, dim=0)\n\n    controlnet_conditioning_image = controlnet_",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 87, "column": 0 },
          "end": { "row": 87, "column": 0 }
        }
      }
    }
  ],
  [
    "249",
    {
      "pageContent": "class StableDiffusionControlNetImg2ImgPipeline(DiffusionPipeline):\n    \"\"\"\n    Inspired by: https://github.com/haofanwang/ControlNet-for-Diffusers/\n    \"\"\"\n\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        controlnet: ControlNetModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature ext",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 121, "column": 0 },
          "end": { "row": 121, "column": 0 }
        }
      }
    }
  ],
  [
    "250",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        controlnet: ControlNetModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modu",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 128, "column": 4 },
          "end": { "row": 128, "column": 4 }
        }
      }
    }
  ],
  [
    "251",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 171, "column": 4 },
          "end": { "row": 171, "column": 4 }
        }
      }
    }
  ],
  [
    "252",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 180, "column": 4 },
          "end": { "row": 180, "column": 4 }
        }
      }
    }
  ],
  [
    "253",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae, controlnet, and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.controlnet]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 187, "column": 4 },
          "end": { "row": 187, "column": 4 }
        }
      }
    }
  ],
  [
    "254",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            # the safety checker can offload the vae again\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # control net hook has be manually offloaded as it alternates with unet\n        cpu_offload_with_hook(self.controlnet, device)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 208, "column": 4 },
          "end": { "row": 208, "column": 4 }
        }
      }
    }
  ],
  [
    "255",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 254, "column": 4 },
          "end": { "row": 254, "column": 4 }
        }
      }
    }
  ],
  [
    "256",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 392, "column": 4 },
          "end": { "row": 392, "column": 4 }
        }
      }
    }
  ],
  [
    "257",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 402, "column": 4 },
          "end": { "row": 402, "column": 4 }
        }
      }
    }
  ],
  [
    "258",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 410, "column": 4 },
          "end": { "row": 410, "column": 4 }
        }
      }
    }
  ],
  [
    "259",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        image,\n        controlnet_conditioning_image,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n        strength=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Canno",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 427, "column": 4 },
          "end": { "row": 427, "column": 4 }
        }
      }
    }
  ],
  [
    "260",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:]\n\n        return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 546, "column": 4 },
          "end": { "row": 546, "column": 4 }
        }
      }
    }
  ],
  [
    "261",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n            )\n\n        image = image.to(device=device, dtype=dtype)\n\n        batch_size = batch_size * num_images_per_prompt\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            init_latents = [\n                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n            ]\n            init_latents = torch.cat(init_latents, dim=0)\n        else:\n            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n            raise ValueError(\n                f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n            )\n        else:\n            init_latents = torch.cat([init_latents], dim=0)\n\n        shape = init_latents.shape\n        noise = randn_tensor(shape, gener",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 555, "column": 4 },
          "end": { "row": 555, "column": 4 }
        }
      }
    }
  ],
  [
    "262",
    {
      "pageContent": "def _default_height_width(self, height, width, image):\n        if isinstance(image, list):\n            image = image[0]\n\n        if height is None:\n            if isinstance(image, PIL.Image.Image):\n                height = image.height\n            elif isinstance(image, torch.Tensor):\n                height = image.shape[3]\n\n            height = (height // 8) * 8  # round down to nearest multiple of 8\n\n        if width is None:\n            if isinstance(image, PIL.Image.Image):\n                width = image.width\n            elif isinstance(image, torch.Tensor):\n                width = image.shape[2]\n\n            width = (width // 8) * 8  # round down to nearest multiple of 8\n\n        return height, width",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_img2img.py",
        "range": {
          "start": { "row": 596, "column": 4 },
          "end": { "row": 596, "column": 4 }
        }
      }
    }
  ],
  [
    "263",
    {
      "pageContent": "class MagicMixPipeline(DiffusionPipeline):\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler],\n    ):\n        super().__init__()\n\n        self.register_modules(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet, scheduler=scheduler)\n\n    # convert PIL image to latents\n    def encode(self, img):\n        with torch.no_grad():\n            latent = self.vae.encode(tfms.ToTensor()(img).unsqueeze(0).to(self.device) * 2 - 1)\n            latent = 0.18215 * latent.latent_dist.sample()\n        return latent\n\n    # convert latents to PIL image\n    def decode(self, latent):\n        latent = (1 / 0.18215) * latent\n        with torch.no_grad():\n            img = self.vae.decode(latent).sample\n        img = (img / 2 + 0.5).clamp(0, 1)\n        img = img.detach().cpu().permute(0, 2, 3, 1).numpy()\n        img = (img * 255).round().astype(\"uint8\")\n        return Image.fromarray(img[0])\n\n    # convert prompt into text embeddings, also unconditional embeddings\n    def prep_text(self, prompt):\n        text_input = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        text_embedding = self.text_encoder(text_input.input_ids.to(self.device))[0]\n\n        uncond_input = self.tokenizer(\n            \"\",\n            padding=\"max_length\",\n          ",
      "metadata": {
        "source": "examples/community/magic_mix.py",
        "range": {
          "start": { "row": 18, "column": 0 },
          "end": { "row": 18, "column": 0 }
        }
      }
    }
  ],
  [
    "264",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler],\n    ):\n        super().__init__()\n\n        self.register_modules(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet, scheduler=scheduler)",
      "metadata": {
        "source": "examples/community/magic_mix.py",
        "range": {
          "start": { "row": 19, "column": 4 },
          "end": { "row": 19, "column": 4 }
        }
      }
    }
  ],
  [
    "265",
    {
      "pageContent": "def encode(self, img):\n        with torch.no_grad():\n            latent = self.vae.encode(tfms.ToTensor()(img).unsqueeze(0).to(self.device) * 2 - 1)\n            latent = 0.18215 * latent.latent_dist.sample()\n        return latent",
      "metadata": {
        "source": "examples/community/magic_mix.py",
        "range": {
          "start": { "row": 32, "column": 4 },
          "end": { "row": 32, "column": 4 }
        }
      }
    }
  ],
  [
    "266",
    {
      "pageContent": "def decode(self, latent):\n        latent = (1 / 0.18215) * latent\n        with torch.no_grad():\n            img = self.vae.decode(latent).sample\n        img = (img / 2 + 0.5).clamp(0, 1)\n        img = img.detach().cpu().permute(0, 2, 3, 1).numpy()\n        img = (img * 255).round().astype(\"uint8\")\n        return Image.fromarray(img[0])",
      "metadata": {
        "source": "examples/community/magic_mix.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "267",
    {
      "pageContent": "def prep_text(self, prompt):\n        text_input = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        text_embedding = self.text_encoder(text_input.input_ids.to(self.device))[0]\n\n        uncond_input = self.tokenizer(\n            \"\",\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        uncond_embedding = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n\n        return torch.cat([uncond_embedding, text_embedding])",
      "metadata": {
        "source": "examples/community/magic_mix.py",
        "range": {
          "start": { "row": 49, "column": 4 },
          "end": { "row": 49, "column": 4 }
        }
      }
    }
  ],
  [
    "268",
    {
      "pageContent": "def __call__(\n        self,\n        img: Image.Image,\n        prompt: str,\n        kmin: float = 0.3,\n        kmax: float = 0.6,\n        mix_factor: float = 0.5,\n        seed: int = 42,\n        steps: int = 50,\n        guidance_scale: float = 7.5,\n    ) -> Image.Image:\n        tmin = steps - int(kmin * steps)\n        tmax = steps - int(kmax * steps)\n\n        text_embeddings = self.prep_text(prompt)\n\n        self.scheduler.set_timesteps(steps)\n\n        width, height = img.size\n        encoded = self.encode(img)\n\n        torch.manual_seed(seed)\n        noise = torch.randn(\n            (1, self.unet.in_channels, height // 8, width // 8),\n        ).to(self.device)\n\n        latents = self.scheduler.add_noise(\n            encoded,\n            noise,\n            timesteps=self.scheduler.timesteps[tmax],\n        )\n\n        input = torch.cat([latents] * 2)\n\n        input = self.scheduler.scale_model_input(input, self.scheduler.timesteps[tmax])\n\n        with torch.no_grad():\n            pred = self.unet(\n                input,\n                self.scheduler.timesteps[tmax],\n                encoder_hidden_states=text_embeddings,\n            ).sample\n\n        pred_uncond, pred_text = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n        latents = self.scheduler.step(pred, self.scheduler.timesteps[tmax], latents).prev_sample\n\n        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n            if i > tmax:\n                if i < tmin:  # layout generation phase\n                    orig_latents = self.scheduler.add_noise(\n                   ",
      "metadata": {
        "source": "examples/community/magic_mix.py",
        "range": {
          "start": { "row": 72, "column": 4 },
          "end": { "row": 72, "column": 4 }
        }
      }
    }
  ],
  [
    "269",
    {
      "pageContent": "def parse_prompt_attention(text):\n    \"\"\"\n    Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n    Accepted tokens are:\n      (abc) - increases attention to abc by a multiplier of 1.1\n      (abc:3.12) - increases attention to abc by a multiplier of 3.12\n      [abc] - decreases attention to abc by a multiplier of 1.1\n      \\( - literal character '('\n      \\[ - literal character '['\n      \\) - literal character ')'\n      \\] - literal character ']'\n      \\\\ - literal character '\\'\n      anything else - just text\n    >>> parse_prompt_attention('normal text')\n    [['normal text', 1.0]]\n    >>> parse_prompt_attention('an (important) word')\n    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n    >>> parse_prompt_attention('(unbalanced')\n    [['unbalanced', 1.1]]\n    >>> parse_prompt_attention('\\(literal\\]')\n    [['(literal]', 1.0]]\n    >>> parse_prompt_attention('(unnecessary)(parens)')\n    [['unnecessaryparens', 1.1]]\n    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n    [['a ', 1.0],\n     ['house', 1.5730000000000004],\n     [' ', 1.1],\n     ['on', 1.0],\n     [' a ', 1.1],\n     ['hill', 0.55],\n     [', sun, ', 1.1],\n     ['sky', 1.4641000000000006],\n     ['.', 1.1]]\n    \"\"\"\n\n    res = []\n    round_brackets = []\n    square_brackets = []\n\n    round_bracket_multiplier = 1.1\n    square_bracket_multiplier = 1 / 1.1\n\n    def multiply_range(start_position, multiplier):\n        for p in range(start_position, len(res)):\n            res[p][1] *= multiplier\n\n    for m in re_attention.finditer(text):\n   ",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 77, "column": 0 },
          "end": { "row": 77, "column": 0 }
        }
      }
    }
  ],
  [
    "270",
    {
      "pageContent": "def get_prompts_with_weights(pipe, prompt: List[str], max_length: int):\n    r\"\"\"\n    Tokenize a list of prompts and return its tokens with weights of each token.\n\n    No padding, starting or ending token is included.\n    \"\"\"\n    tokens = []\n    weights = []\n    truncated = False\n    for text in prompt:\n        texts_and_weights = parse_prompt_attention(text)\n        text_token = []\n        text_weight = []\n        for word, weight in texts_and_weights:\n            # tokenize and discard the starting and the ending token\n            token = pipe.tokenizer(word, return_tensors=\"np\").input_ids[0, 1:-1]\n            text_token += list(token)\n            # copy the weight by length of token\n            text_weight += [weight] * len(token)\n            # stop if the text is too long (longer than truncation limit)\n            if len(text_token) > max_length:\n                truncated = True\n                break\n        # truncate\n        if len(text_token) > max_length:\n            truncated = True\n            text_token = text_token[:max_length]\n            text_weight = text_weight[:max_length]\n        tokens.append(text_token)\n        weights.append(text_weight)\n    if truncated:\n        logger.warning(\"Prompt was truncated. Try to shorten the prompt or increase max_embeddings_multiples\")\n    return tokens, weights",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 163, "column": 0 },
          "end": { "row": 163, "column": 0 }
        }
      }
    }
  ],
  [
    "271",
    {
      "pageContent": "def pad_tokens_and_weights(tokens, weights, max_length, bos, eos, no_boseos_middle=True, chunk_length=77):\n    r\"\"\"\n    Pad the tokens (with starting and ending tokens) and weights (with 1.0) to max_length.\n    \"\"\"\n    max_embeddings_multiples = (max_length - 2) // (chunk_length - 2)\n    weights_length = max_length if no_boseos_middle else max_embeddings_multiples * chunk_length\n    for i in range(len(tokens)):\n        tokens[i] = [bos] + tokens[i] + [eos] * (max_length - 1 - len(tokens[i]))\n        if no_boseos_middle:\n            weights[i] = [1.0] + weights[i] + [1.0] * (max_length - 1 - len(weights[i]))\n        else:\n            w = []\n            if len(weights[i]) == 0:\n                w = [1.0] * weights_length\n            else:\n                for j in range(max_embeddings_multiples):\n                    w.append(1.0)  # weight for starting token in this chunk\n                    w += weights[i][j * (chunk_length - 2) : min(len(weights[i]), (j + 1) * (chunk_length - 2))]\n                    w.append(1.0)  # weight for ending token in this chunk\n                w += [1.0] * (weights_length - len(w))\n            weights[i] = w[:]\n\n    return tokens, weights",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 198, "column": 0 },
          "end": { "row": 198, "column": 0 }
        }
      }
    }
  ],
  [
    "272",
    {
      "pageContent": "def get_unweighted_text_embeddings(\n    pipe,\n    text_input: np.array,\n    chunk_length: int,\n    no_boseos_middle: Optional[bool] = True,\n):\n    \"\"\"\n    When the length of tokens is a multiple of the capacity of the text encoder,\n    it should be split into chunks and sent to the text encoder individually.\n    \"\"\"\n    max_embeddings_multiples = (text_input.shape[1] - 2) // (chunk_length - 2)\n    if max_embeddings_multiples > 1:\n        text_embeddings = []\n        for i in range(max_embeddings_multiples):\n            # extract the i-th chunk\n            text_input_chunk = text_input[:, i * (chunk_length - 2) : (i + 1) * (chunk_length - 2) + 2].copy()\n\n            # cover the head and the tail by the starting and the ending tokens\n            text_input_chunk[:, 0] = text_input[0, 0]\n            text_input_chunk[:, -1] = text_input[0, -1]\n\n            text_embedding = pipe.text_encoder(input_ids=text_input_chunk)[0]\n\n            if no_boseos_middle:\n                if i == 0:\n                    # discard the ending token\n                    text_embedding = text_embedding[:, :-1]\n                elif i == max_embeddings_multiples - 1:\n                    # discard the starting token\n                    text_embedding = text_embedding[:, 1:]\n                else:\n                    # discard both starting and ending tokens\n                    text_embedding = text_embedding[:, 1:-1]\n\n            text_embeddings.append(text_embedding)\n        text_embeddings = np.concatenate(text_embeddings, axis=1)\n    else:\n        text_embeddings = pipe.text_encoder(input_ids=text_input",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 223, "column": 0 },
          "end": { "row": 223, "column": 0 }
        }
      }
    }
  ],
  [
    "273",
    {
      "pageContent": "def get_weighted_text_embeddings(\n    pipe,\n    prompt: Union[str, List[str]],\n    uncond_prompt: Optional[Union[str, List[str]]] = None,\n    max_embeddings_multiples: Optional[int] = 4,\n    no_boseos_middle: Optional[bool] = False,\n    skip_parsing: Optional[bool] = False,\n    skip_weighting: Optional[bool] = False,\n    **kwargs,\n):\n    r\"\"\"\n    Prompts can be assigned with local weights using brackets. For example,\n    prompt 'A (very beautiful) masterpiece' highlights the words 'very beautiful',\n    and the embedding tokens corresponding to the words get multiplied by a constant, 1.1.\n\n    Also, to regularize of the embedding, the weighted embedding would be scaled to preserve the original mean.\n\n    Args:\n        pipe (`OnnxStableDiffusionPipeline`):\n            Pipe to provide access to the tokenizer and the text encoder.\n        prompt (`str` or `List[str]`):\n            The prompt or prompts to guide the image generation.\n        uncond_prompt (`str` or `List[str]`):\n            The unconditional prompt or prompts for guide the image generation. If unconditional prompt\n            is provided, the embeddings of prompt and uncond_prompt are concatenated.\n        max_embeddings_multiples (`int`, *optional*, defaults to `1`):\n            The max multiple length of prompt embeddings compared to the max output length of text encoder.\n        no_boseos_middle (`bool`, *optional*, defaults to `False`):\n            If the length of text token is multiples of the capacity of text encoder, whether reserve the starting and\n            ending token in each of the chunk in the mi",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 264, "column": 0 },
          "end": { "row": 264, "column": 0 }
        }
      }
    }
  ],
  [
    "274",
    {
      "pageContent": "def preprocess_image(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 403, "column": 0 },
          "end": { "row": 403, "column": 0 }
        }
      }
    }
  ],
  [
    "275",
    {
      "pageContent": "def preprocess_mask(mask, scale_factor=8):\n    mask = mask.convert(\"L\")\n    w, h = mask.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    mask = mask.resize((w // scale_factor, h // scale_factor), resample=PIL_INTERPOLATION[\"nearest\"])\n    mask = np.array(mask).astype(np.float32) / 255.0\n    mask = np.tile(mask, (4, 1, 1))\n    mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?\n    mask = 1 - mask  # repaint white, keep black\n    return mask",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 412, "column": 0 },
          "end": { "row": 412, "column": 0 }
        }
      }
    }
  ],
  [
    "276",
    {
      "pageContent": "class OnnxStableDiffusionLongPromptWeightingPipeline(OnnxStableDiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion without tokens length limit, and support parsing\n    weighting in prompt.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n    \"\"\"\n    if version.parse(version.parse(diffusers.__version__).base_version) >= version.parse(\"0.9.0\"):\n\n        def __init__(\n            self,\n            vae_encoder: OnnxRuntimeModel,\n            vae_decoder: OnnxRuntimeModel,\n            text_encoder: OnnxRuntimeModel,\n            tokenizer: CLIPTokenizer,\n            unet: OnnxRuntimeModel,\n            scheduler: SchedulerMixin,\n            safety_checker: OnnxRuntimeModel,\n            feature_extractor: CLIPFeatureExtractor,\n            requires_safety_checker: bool = True,\n        ):\n            super().__init__(\n                vae_encoder=vae_encoder,\n                vae_decoder=vae_decoder,\n                text_encoder=text_encoder,\n                tokenizer=tokenizer,\n                unet=unet,\n                scheduler=scheduler,\n                safety_checker=safety_checker,\n                feature_extractor=feature_extractor,\n                requires_safety_checker=requires_safety_checker,\n            )\n            self.__init__additional__()\n\n    else:\n\n        def __init__(\n            self,\n            vae_encoder: OnnxRuntimeModel,\n            vae_d",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 424, "column": 0 },
          "end": { "row": 424, "column": 0 }
        }
      }
    }
  ],
  [
    "277",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt,\n        max_embeddings_multiples,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `list(int)`):\n                prompt to be encoded\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n                The max multiple length of prompt embeddings compared to the max output length of text encoder.\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        if negative_prompt is None:\n            negative_prompt = [\"\"] * batch_size\n        elif isinstance(negative_prompt, str):\n            negative_prompt = [negative_prompt] * batch_size\n        if batch_size != len(negative_prompt):\n            raise ValueError(\n                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                \" the batch size of `",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 488, "column": 4 },
          "end": { "row": 488, "column": 4 }
        }
      }
    }
  ],
  [
    "278",
    {
      "pageContent": "def check_inputs(self, prompt, height, width, strength, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 539, "column": 4 },
          "end": { "row": 539, "column": 4 }
        }
      }
    }
  ],
  [
    "279",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, is_text2img):\n        if is_text2img:\n            return self.scheduler.timesteps, num_inference_steps\n        else:\n            # get the original timestep using init_timestep\n            offset = self.scheduler.config.get(\"steps_offset\", 0)\n            init_timestep = int(num_inference_steps * strength) + offset\n            init_timestep = min(init_timestep, num_inference_steps)\n\n            t_start = max(num_inference_steps - init_timestep + offset, 0)\n            timesteps = self.scheduler.timesteps[t_start:]\n            return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 557, "column": 4 },
          "end": { "row": 557, "column": 4 }
        }
      }
    }
  ],
  [
    "280",
    {
      "pageContent": "def run_safety_checker(self, image):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(\n                self.numpy_to_pil(image), return_tensors=\"np\"\n            ).pixel_values.astype(image.dtype)\n            # There will throw an error if use safety_checker directly and batchsize>1\n            images, has_nsfw_concept = [], []\n            for i in range(image.shape[0]):\n                image_i, has_nsfw_concept_i = self.safety_checker(\n                    clip_input=safety_checker_input[i : i + 1], images=image[i : i + 1]\n                )\n                images.append(image_i)\n                has_nsfw_concept.append(has_nsfw_concept_i[0])\n            image = np.concatenate(images)\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 570, "column": 4 },
          "end": { "row": 570, "column": 4 }
        }
      }
    }
  ],
  [
    "281",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / 0.18215 * latents\n        # image = self.vae_decoder(latent_sample=latents)[0]\n        # it seems likes there is a strange result for using half-precision vae decoder if batchsize>1\n        image = np.concatenate(\n            [self.vae_decoder(latent_sample=latents[i : i + 1])[0] for i in range(latents.shape[0])]\n        )\n        image = np.clip(image / 2 + 0.5, 0, 1)\n        image = image.transpose((0, 2, 3, 1))\n        return image",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 588, "column": 4 },
          "end": { "row": 588, "column": 4 }
        }
      }
    }
  ],
  [
    "282",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 599, "column": 4 },
          "end": { "row": 599, "column": 4 }
        }
      }
    }
  ],
  [
    "283",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, height, width, dtype, generator, latents=None):\n        if image is None:\n            shape = (\n                batch_size,\n                self.unet_in_channels,\n                height // self.vae_scale_factor,\n                width // self.vae_scale_factor,\n            )\n\n            if latents is None:\n                latents = torch.randn(shape, generator=generator, device=\"cpu\").numpy().astype(dtype)\n            else:\n                if latents.shape != shape:\n                    raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n\n            # scale the initial noise by the standard deviation required by the scheduler\n            latents = (torch.from_numpy(latents) * self.scheduler.init_noise_sigma).numpy()\n            return latents, None, None\n        else:\n            init_latents = self.vae_encoder(sample=image)[0]\n            init_latents = 0.18215 * init_latents\n            init_latents = np.concatenate([init_latents] * batch_size, axis=0)\n            init_latents_orig = init_latents\n            shape = init_latents.shape\n\n            # add noise to latents using the timesteps\n            noise = torch.randn(shape, generator=generator, device=\"cpu\").numpy().astype(dtype)\n            latents = self.scheduler.add_noise(\n                torch.from_numpy(init_latents), torch.from_numpy(noise), timestep\n            ).numpy()\n            return latents, init_latents_orig, noise",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 616, "column": 4 },
          "end": { "row": 616, "column": 4 }
        }
      }
    }
  ],
  [
    "284",
    {
      "pageContent": "def text2img(\n        self,\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        height: int = 512,\n        width: int = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[np.ndarray] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, np.ndarray], None]] = None,\n        callback_steps: int = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function for text-to-image generation.\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            height (`int`, *optional*, defaults to 512):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to 512):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            g",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 864, "column": 4 },
          "end": { "row": 864, "column": 4 }
        }
      }
    }
  ],
  [
    "285",
    {
      "pageContent": "def img2img(\n        self,\n        image: Union[np.ndarray, PIL.Image.Image],\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[torch.Generator] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, np.ndarray], None]] = None,\n        callback_steps: int = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function for image-to-image generation.\n        Args:\n            image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or ndarray representing an image batch, that will be used as the starting point for the\n                process.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1.\n                `image` will be used as a starting point, adding more noise to it the larger the `",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 956, "column": 4 },
          "end": { "row": 956, "column": 4 }
        }
      }
    }
  ],
  [
    "286",
    {
      "pageContent": "def inpaint(\n        self,\n        image: Union[np.ndarray, PIL.Image.Image],\n        mask_image: Union[np.ndarray, PIL.Image.Image],\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[torch.Generator] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, np.ndarray], None]] = None,\n        callback_steps: int = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function for inpaint.\n        Args:\n            image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n       ",
      "metadata": {
        "source": "examples/community/lpw_stable_diffusion_onnx.py",
        "range": {
          "start": { "row": 1047, "column": 4 },
          "end": { "row": 1047, "column": 4 }
        }
      }
    }
  ],
  [
    "287",
    {
      "pageContent": "class StableDiffusionMegaPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionMegaSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to th",
      "metadata": {
        "source": "examples/community/stable_diffusion_mega.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "288",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n         ",
      "metadata": {
        "source": "examples/community/stable_diffusion_mega.py",
        "range": {
          "start": { "row": 54, "column": 4 },
          "end": { "row": 54, "column": 4 }
        }
      }
    }
  ],
  [
    "289",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/stable_diffusion_mega.py",
        "range": {
          "start": { "row": 95, "column": 4 },
          "end": { "row": 95, "column": 4 }
        }
      }
    }
  ],
  [
    "290",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/stable_diffusion_mega.py",
        "range": {
          "start": { "row": 114, "column": 4 },
          "end": { "row": 114, "column": 4 }
        }
      }
    }
  ],
  [
    "291",
    {
      "pageContent": "def slerp(t, v0, v1, DOT_THRESHOLD=0.9995):\n    \"\"\"helper function to spherically interpolate two arrays v1 v2\"\"\"\n\n    if not isinstance(v0, np.ndarray):\n        inputs_are_torch = True\n        input_device = v0.device\n        v0 = v0.cpu().numpy()\n        v1 = v1.cpu().numpy()\n\n    dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n    if np.abs(dot) > DOT_THRESHOLD:\n        v2 = (1 - t) * v0 + t * v1\n    else:\n        theta_0 = np.arccos(dot)\n        sin_theta_0 = np.sin(theta_0)\n        theta_t = theta_0 * t\n        sin_theta_t = np.sin(theta_t)\n        s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n        s1 = sin_theta_t / sin_theta_0\n        v2 = s0 * v0 + s1 * v1\n\n    if inputs_are_torch:\n        v2 = torch.from_numpy(v2).to(input_device)\n\n    return v2",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "292",
    {
      "pageContent": "class StableDiffusionWalkPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [m",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "293",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not ex",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 76, "column": 4 },
          "end": { "row": 76, "column": 4 }
        }
      }
    }
  ],
  [
    "294",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "295",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "296",
    {
      "pageContent": "def embed_text(self, text):\n        \"\"\"takes in text and turns it into text embeddings\"\"\"\n        text_input = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        with torch.no_grad():\n            embed = self.text_encoder(text_input.input_ids.to(self.device))[0]\n        return embed",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 402, "column": 4 },
          "end": { "row": 402, "column": 4 }
        }
      }
    }
  ],
  [
    "297",
    {
      "pageContent": "def get_noise(self, seed, dtype=torch.float32, height=512, width=512):\n        \"\"\"Takes in random seed and returns corresponding noise vector\"\"\"\n        return torch.randn(\n            (1, self.unet.in_channels, height // 8, width // 8),\n            generator=torch.Generator(device=self.device).manual_seed(seed),\n            device=self.device,\n            dtype=dtype,\n        )",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 415, "column": 4 },
          "end": { "row": 415, "column": 4 }
        }
      }
    }
  ],
  [
    "298",
    {
      "pageContent": "def walk(\n        self,\n        prompts: List[str],\n        seeds: List[int],\n        num_interpolation_steps: Optional[int] = 6,\n        output_dir: Optional[str] = \"./dreams\",\n        name: Optional[str] = None,\n        batch_size: Optional[int] = 1,\n        height: Optional[int] = 512,\n        width: Optional[int] = 512,\n        guidance_scale: Optional[float] = 7.5,\n        num_inference_steps: Optional[int] = 50,\n        eta: Optional[float] = 0.0,\n    ) -> List[str]:\n        \"\"\"\n        Walks through a series of prompts and seeds, interpolating between them and saving the results to disk.\n\n        Args:\n            prompts (`List[str]`):\n                List of prompts to generate images for.\n            seeds (`List[int]`):\n                List of seeds corresponding to provided prompts. Must be the same length as prompts.\n            num_interpolation_steps (`int`, *optional*, defaults to 6):\n                Number of interpolation steps to take between prompts.\n            output_dir (`str`, *optional*, defaults to `./dreams`):\n                Directory to save the generated images to.\n            name (`str`, *optional*, defaults to `None`):\n                Subdirectory of `output_dir` to save the generated images to. If `None`, the name will\n                be the current time.\n            batch_size (`int`, *optional*, defaults to 1):\n                Number of images to generate at once.\n            height (`int`, *optional*, defaults to 512):\n                Height of the generated images.\n            width (`int`, *optional*, defaults to 512):\n                ",
      "metadata": {
        "source": "examples/community/interpolate_stable_diffusion.py",
        "range": {
          "start": { "row": 424, "column": 4 },
          "end": { "row": 424, "column": 4 }
        }
      }
    }
  ],
  [
    "299",
    {
      "pageContent": "class ModelWrapper:\n    def __init__(self, model, alphas_cumprod):\n        self.model = model\n        self.alphas_cumprod = alphas_cumprod\n\n    def apply_model(self, *args, **kwargs):\n        if len(args) == 3:\n            encoder_hidden_states = args[-1]\n            args = args[:2]\n        if kwargs.get(\"cond\", None) is not None:\n            encoder_hidden_states = kwargs.pop(\"cond\")\n        return self.model(*args, encoder_hidden_states=encoder_hidden_states, **kwargs).sample",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "300",
    {
      "pageContent": "def apply_model(self, *args, **kwargs):\n        if len(args) == 3:\n            encoder_hidden_states = args[-1]\n            args = args[:2]\n        if kwargs.get(\"cond\", None) is not None:\n            encoder_hidden_states = kwargs.pop(\"cond\")\n        return self.model(*args, encoder_hidden_states=encoder_hidden_states, **kwargs).sample",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 34, "column": 4 },
          "end": { "row": 34, "column": 4 }
        }
      }
    }
  ],
  [
    "301",
    {
      "pageContent": "class StableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 43, "column": 0 },
          "end": { "row": 43, "column": 0 }
        }
      }
    }
  ],
  [
    "302",
    {
      "pageContent": "def __init__(\n        self,\n        vae,\n        text_encoder,\n        tokenizer,\n        unet,\n        scheduler,\n        safety_checker,\n        feature_extractor,\n    ):\n        super().__init__()\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        # get correct sigmas from LMS\n        scheduler = LMSDiscreteScheduler.from_config(scheduler.config)\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n\n        model = ModelWrapper(unet, scheduler.alphas_cumprod)\n        if scheduler.prediction_type == \"v_prediction\":\n            self.k_diffusion_model = CompVisVDenoiser(model)\n        else:\n            self.k_diffusion_model = CompVisDenoise",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 72, "column": 4 },
          "end": { "row": 72, "column": 4 }
        }
      }
    }
  ],
  [
    "303",
    {
      "pageContent": "def set_scheduler(self, scheduler_type: str):\n        library = importlib.import_module(\"k_diffusion\")\n        sampling = getattr(library, \"sampling\")\n        self.sampler = getattr(sampling, scheduler_type)",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 116, "column": 4 },
          "end": { "row": 116, "column": 4 }
        }
      }
    }
  ],
  [
    "304",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 121, "column": 4 },
          "end": { "row": 121, "column": 4 }
        }
      }
    }
  ],
  [
    "305",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 140, "column": 4 },
          "end": { "row": 140, "column": 4 }
        }
      }
    }
  ],
  [
    "306",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 148, "column": 4 },
          "end": { "row": 148, "column": 4 }
        }
      }
    }
  ],
  [
    "307",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `list(int)`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n        if not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "308",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 288, "column": 4 },
          "end": { "row": 288, "column": 4 }
        }
      }
    }
  ],
  [
    "309",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / 0.18215 * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 298, "column": 4 },
          "end": { "row": 298, "column": 4 }
        }
      }
    }
  ],
  [
    "310",
    {
      "pageContent": "def check_inputs(self, prompt, height, width, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 306, "column": 4 },
          "end": { "row": 306, "column": 4 }
        }
      }
    }
  ],
  [
    "311",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // 8, width // 8)\n        if latents is None:\n            if device.type == \"mps\":\n                # randn does not work reproducibly on mps\n                latents = torch.randn(shape, generator=generator, device=\"cpu\", dtype=dtype).to(device)\n            else:\n                latents = torch.randn(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        return latents",
      "metadata": {
        "source": "examples/community/sd_text2img_k_diffusion.py",
        "range": {
          "start": { "row": 321, "column": 4 },
          "end": { "row": 321, "column": 4 }
        }
      }
    }
  ],
  [
    "312",
    {
      "pageContent": "class StableDiffusionComparisonPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for parallel comparison of Stable Diffusion v1-v4\n    This pipeline inherits from DiffusionPipeline and depends on the use of an Auth Token for\n    downloading pre-trained checkpoints from Hugging Face Hub.\n    If using Hugging Face Hub, pass the Model ID for Stable Diffusion v1.4 as the previous 3 checkpoints will be loaded\n    automatically.\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionMegaSafetyChecker`]):\n            Classification module that estimates whether generated images could ",
      "metadata": {
        "source": "examples/community/stable_diffusion_comparison.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "313",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super()._init_()\n\n        self.pipe1 = StableDiffusionPipeline.from_pretrained(pipe1_model_id)\n        self.pipe2 = StableDiffusionPipeline.from_pretrained(pipe2_model_id)\n        self.pipe3 = StableDiffusionPipeline.from_pretrained(pipe3_model_id)\n        self.pipe4 = StableDiffusionPipeline(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n            requires_safety_checker=requires_safety_checker,\n        )\n\n        self.register_modules(pipeline1=self.pipe1, pipeline2=self.pipe2, pipeline3=self.pipe3, pipeline4=self.pipe4)",
      "metadata": {
        "source": "examples/community/stable_diffusion_comparison.py",
        "range": {
          "start": { "row": 52, "column": 4 },
          "end": { "row": 52, "column": 4 }
        }
      }
    }
  ],
  [
    "314",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/stable_diffusion_comparison.py",
        "range": {
          "start": { "row": 85, "column": 4 },
          "end": { "row": 85, "column": 4 }
        }
      }
    }
  ],
  [
    "315",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/stable_diffusion_comparison.py",
        "range": {
          "start": { "row": 102, "column": 4 },
          "end": { "row": 102, "column": 4 }
        }
      }
    }
  ],
  [
    "316",
    {
      "pageContent": "class UnetSchedulerOneForwardPipeline(DiffusionPipeline):\n    def __init__(self, unet, scheduler):\n        super().__init__()\n\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    def __call__(self):\n        image = torch.randn(\n            (1, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),\n        )\n        timestep = 1\n\n        model_output = self.unet(image, timestep).sample\n        scheduler_output = self.scheduler.step(model_output, timestep, image).prev_sample\n\n        result = scheduler_output - scheduler_output + torch.ones_like(scheduler_output)\n\n        return result",
      "metadata": {
        "source": "examples/community/one_step_unet.py",
        "range": {
          "start": { "row": 6, "column": 0 },
          "end": { "row": 6, "column": 0 }
        }
      }
    }
  ],
  [
    "317",
    {
      "pageContent": "def __init__(self, unet, scheduler):\n        super().__init__()\n\n        self.register_modules(unet=unet, scheduler=scheduler)",
      "metadata": {
        "source": "examples/community/one_step_unet.py",
        "range": {
          "start": { "row": 7, "column": 4 },
          "end": { "row": 7, "column": 4 }
        }
      }
    }
  ],
  [
    "318",
    {
      "pageContent": "def __call__(self):\n        image = torch.randn(\n            (1, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),\n        )\n        timestep = 1\n\n        model_output = self.unet(image, timestep).sample\n        scheduler_output = self.scheduler.step(model_output, timestep, image).prev_sample\n\n        result = scheduler_output - scheduler_output + torch.ones_like(scheduler_output)\n\n        return result",
      "metadata": {
        "source": "examples/community/one_step_unet.py",
        "range": {
          "start": { "row": 12, "column": 4 },
          "end": { "row": 12, "column": 4 }
        }
      }
    }
  ],
  [
    "319",
    {
      "pageContent": "def prepare_image(image):\n    if isinstance(image, torch.Tensor):\n        # Batch single image\n        if image.ndim == 3:\n            image = image.unsqueeze(0)\n\n        image = image.to(dtype=torch.float32)\n    else:\n        # preprocess image\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [np.array(i.convert(\"RGB\"))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    return image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 126, "column": 0 },
          "end": { "row": 126, "column": 0 }
        }
      }
    }
  ],
  [
    "320",
    {
      "pageContent": "def prepare_mask_image(mask_image):\n    if isinstance(mask_image, torch.Tensor):\n        if mask_image.ndim == 2:\n            # Batch and add channel dim for single mask\n            mask_image = mask_image.unsqueeze(0).unsqueeze(0)\n        elif mask_image.ndim == 3 and mask_image.shape[0] == 1:\n            # Single mask, the 0'th dimension is considered to be\n            # the existing batch size of 1\n            mask_image = mask_image.unsqueeze(0)\n        elif mask_image.ndim == 3 and mask_image.shape[0] != 1:\n            # Batch of mask, the 0'th dimension is considered to be\n            # the batching dimension\n            mask_image = mask_image.unsqueeze(1)\n\n        # Binarize mask\n        mask_image[mask_image < 0.5] = 0\n        mask_image[mask_image >= 0.5] = 1\n    else:\n        # preprocess mask\n        if isinstance(mask_image, (PIL.Image.Image, np.ndarray)):\n            mask_image = [mask_image]\n\n        if isinstance(mask_image, list) and isinstance(mask_image[0], PIL.Image.Image):\n            mask_image = np.concatenate([np.array(m.convert(\"L\"))[None, None, :] for m in mask_image], axis=0)\n            mask_image = mask_image.astype(np.float32) / 255.0\n        elif isinstance(mask_image, list) and isinstance(mask_image[0], np.ndarray):\n            mask_image = np.concatenate([m[None, None, :] for m in mask_image], axis=0)\n\n        mask_image[mask_image < 0.5] = 0\n        mask_image[mask_image >= 0.5] = 1\n        mask_image = torch.from_numpy(mask_image)\n\n    return mask_image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 150, "column": 0 },
          "end": { "row": 150, "column": 0 }
        }
      }
    }
  ],
  [
    "321",
    {
      "pageContent": "def prepare_controlnet_conditioning_image(\n    controlnet_conditioning_image, width, height, batch_size, num_images_per_prompt, device, dtype\n):\n    if not isinstance(controlnet_conditioning_image, torch.Tensor):\n        if isinstance(controlnet_conditioning_image, PIL.Image.Image):\n            controlnet_conditioning_image = [controlnet_conditioning_image]\n\n        if isinstance(controlnet_conditioning_image[0], PIL.Image.Image):\n            controlnet_conditioning_image = [\n                np.array(i.resize((width, height), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :]\n                for i in controlnet_conditioning_image\n            ]\n            controlnet_conditioning_image = np.concatenate(controlnet_conditioning_image, axis=0)\n            controlnet_conditioning_image = np.array(controlnet_conditioning_image).astype(np.float32) / 255.0\n            controlnet_conditioning_image = controlnet_conditioning_image.transpose(0, 3, 1, 2)\n            controlnet_conditioning_image = torch.from_numpy(controlnet_conditioning_image)\n        elif isinstance(controlnet_conditioning_image[0], torch.Tensor):\n            controlnet_conditioning_image = torch.cat(controlnet_conditioning_image, dim=0)\n\n    image_batch_size = controlnet_conditioning_image.shape[0]\n\n    if image_batch_size == 1:\n        repeat_by = batch_size\n    else:\n        # image batch size is the same as prompt batch size\n        repeat_by = num_images_per_prompt\n\n    controlnet_conditioning_image = controlnet_conditioning_image.repeat_interleave(repeat_by, dim=0)\n\n    controlnet_conditioning_image = controlnet_",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 185, "column": 0 },
          "end": { "row": 185, "column": 0 }
        }
      }
    }
  ],
  [
    "322",
    {
      "pageContent": "class StableDiffusionControlNetInpaintImg2ImgPipeline(DiffusionPipeline):\n    \"\"\"\n    Inspired by: https://github.com/haofanwang/ControlNet-for-Diffusers/\n    \"\"\"\n\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        controlnet: ControlNetModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feat",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 219, "column": 0 },
          "end": { "row": 219, "column": 0 }
        }
      }
    }
  ],
  [
    "323",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        controlnet: ControlNetModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modu",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "324",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 269, "column": 4 },
          "end": { "row": 269, "column": 4 }
        }
      }
    }
  ],
  [
    "325",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 278, "column": 4 },
          "end": { "row": 278, "column": 4 }
        }
      }
    }
  ],
  [
    "326",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae, controlnet, and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.controlnet]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "327",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            # the safety checker can offload the vae again\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # control net hook has be manually offloaded as it alternates with unet\n        cpu_offload_with_hook(self.controlnet, device)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 306, "column": 4 },
          "end": { "row": 306, "column": 4 }
        }
      }
    }
  ],
  [
    "328",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 352, "column": 4 },
          "end": { "row": 352, "column": 4 }
        }
      }
    }
  ],
  [
    "329",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 490, "column": 4 },
          "end": { "row": 490, "column": 4 }
        }
      }
    }
  ],
  [
    "330",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 500, "column": 4 },
          "end": { "row": 500, "column": 4 }
        }
      }
    }
  ],
  [
    "331",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 508, "column": 4 },
          "end": { "row": 508, "column": 4 }
        }
      }
    }
  ],
  [
    "332",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        image,\n        mask_image,\n        controlnet_conditioning_image,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n        strength=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n   ",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 525, "column": 4 },
          "end": { "row": 525, "column": 4 }
        }
      }
    }
  ],
  [
    "333",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:]\n\n        return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 681, "column": 4 },
          "end": { "row": 681, "column": 4 }
        }
      }
    }
  ],
  [
    "334",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n            )\n\n        image = image.to(device=device, dtype=dtype)\n\n        batch_size = batch_size * num_images_per_prompt\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            init_latents = [\n                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n            ]\n            init_latents = torch.cat(init_latents, dim=0)\n        else:\n            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n            raise ValueError(\n                f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n            )\n        else:\n            init_latents = torch.cat([init_latents], dim=0)\n\n        shape = init_latents.shape\n        noise = randn_tensor(shape, gener",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 690, "column": 4 },
          "end": { "row": 690, "column": 4 }
        }
      }
    }
  ],
  [
    "335",
    {
      "pageContent": "def prepare_mask_latents(self, mask_image, batch_size, height, width, dtype, device, do_classifier_free_guidance):\n        # resize the mask to latents shape as we concatenate the mask to the latents\n        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n        # and half precision\n        mask_image = F.interpolate(mask_image, size=(height // self.vae_scale_factor, width // self.vae_scale_factor))\n        mask_image = mask_image.to(device=device, dtype=dtype)\n\n        # duplicate mask for each generation per prompt, using mps friendly method\n        if mask_image.shape[0] < batch_size:\n            if not batch_size % mask_image.shape[0] == 0:\n                raise ValueError(\n                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n                    f\" a total batch size of {batch_size}, but {mask_image.shape[0]} masks were passed. Make sure the number\"\n                    \" of masks that you pass is divisible by the total requested batch size.\"\n                )\n            mask_image = mask_image.repeat(batch_size // mask_image.shape[0], 1, 1, 1)\n\n        mask_image = torch.cat([mask_image] * 2) if do_classifier_free_guidance else mask_image\n\n        mask_image_latents = mask_image\n\n        return mask_image_latents",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 731, "column": 4 },
          "end": { "row": 731, "column": 4 }
        }
      }
    }
  ],
  [
    "336",
    {
      "pageContent": "def prepare_masked_image_latents(\n        self, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance\n    ):\n        masked_image = masked_image.to(device=device, dtype=dtype)\n\n        # encode the mask image into latents space so we can concatenate it to the latents\n        if isinstance(generator, list):\n            masked_image_latents = [\n                self.vae.encode(masked_image[i : i + 1]).latent_dist.sample(generator=generator[i])\n                for i in range(batch_size)\n            ]\n            masked_image_latents = torch.cat(masked_image_latents, dim=0)\n        else:\n            masked_image_latents = self.vae.encode(masked_image).latent_dist.sample(generator=generator)\n        masked_image_latents = self.vae.config.scaling_factor * masked_image_latents\n\n        # duplicate masked_image_latents for each generation per prompt, using mps friendly method\n        if masked_image_latents.shape[0] < batch_size:\n            if not batch_size % masked_image_latents.shape[0] == 0:\n                raise ValueError(\n                    \"The passed images and the required batch size don't match. Images are supposed to be duplicated\"\n                    f\" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed.\"\n                    \" Make sure the number of images that you pass is divisible by the total requested batch size.\"\n                )\n            masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1)\n\n        masked_image_latents = (\n",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 754, "column": 4 },
          "end": { "row": 754, "column": 4 }
        }
      }
    }
  ],
  [
    "337",
    {
      "pageContent": "def _default_height_width(self, height, width, image):\n        if isinstance(image, list):\n            image = image[0]\n\n        if height is None:\n            if isinstance(image, PIL.Image.Image):\n                height = image.height\n            elif isinstance(image, torch.Tensor):\n                height = image.shape[3]\n\n            height = (height // 8) * 8  # round down to nearest multiple of 8\n\n        if width is None:\n            if isinstance(image, PIL.Image.Image):\n                width = image.width\n            elif isinstance(image, torch.Tensor):\n                width = image.shape[2]\n\n            width = (width // 8) * 8  # round down to nearest multiple of 8\n\n        return height, width",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint_img2img.py",
        "range": {
          "start": { "row": 788, "column": 4 },
          "end": { "row": 788, "column": 4 }
        }
      }
    }
  ],
  [
    "338",
    {
      "pageContent": "def slerp(val, low, high):\n    \"\"\"\n    Find the interpolation point between the 'low' and 'high' values for the given 'val'. See https://en.wikipedia.org/wiki/Slerp for more details on the topic.\n    \"\"\"\n    low_norm = low / torch.norm(low)\n    high_norm = high / torch.norm(high)\n    omega = torch.acos((low_norm * high_norm))\n    so = torch.sin(omega)\n    res = (torch.sin((1.0 - val) * omega) / so) * low + (torch.sin(val * omega) / so) * high\n    return res",
      "metadata": {
        "source": "examples/community/unclip_text_interpolation.py",
        "range": {
          "start": { "row": 23, "column": 0 },
          "end": { "row": 23, "column": 0 }
        }
      }
    }
  ],
  [
    "339",
    {
      "pageContent": "class UnCLIPTextInterpolationPipeline(DiffusionPipeline):\n\n    \"\"\"\n    Pipeline for prompt-to-prompt interpolation on CLIP text embeddings and using the UnCLIP / Dall-E to decode them to images.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        text_encoder ([`CLIPTextModelWithProjection`]):\n            Frozen text-encoder.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        prior ([`PriorTransformer`]):\n            The canonincal unCLIP prior to approximate the image embedding from the text embedding.\n        text_proj ([`UnCLIPTextProjModel`]):\n            Utility class to prepare and combine the embeddings before they are passed to the decoder.\n        decoder ([`UNet2DConditionModel`]):\n            The decoder to invert the image embedding into an image.\n        super_res_first ([`UNet2DModel`]):\n            Super resolution unet. Used in all but the last step of the super resolution diffusion process.\n        super_res_last ([`UNet2DModel`]):\n            Super resolution unet. Used in the last step of the super resolution diffusion process.\n        prior_scheduler ([`UnCLIPScheduler`]):\n            Scheduler used in the prior denoising process. Just a modified DDPMScheduler.\n        decoder_scheduler ([`UnCLIPScheduler`]):\n    ",
      "metadata": {
        "source": "examples/community/unclip_text_interpolation.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "340",
    {
      "pageContent": "def __init__(\n        self,\n        prior: PriorTransformer,\n        decoder: UNet2DConditionModel,\n        text_encoder: CLIPTextModelWithProjection,\n        tokenizer: CLIPTokenizer,\n        text_proj: UnCLIPTextProjModel,\n        super_res_first: UNet2DModel,\n        super_res_last: UNet2DModel,\n        prior_scheduler: UnCLIPScheduler,\n        decoder_scheduler: UnCLIPScheduler,\n        super_res_scheduler: UnCLIPScheduler,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            prior=prior,\n            decoder=decoder,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            text_proj=text_proj,\n            super_res_first=super_res_first,\n            super_res_last=super_res_last,\n            prior_scheduler=prior_scheduler,\n            decoder_scheduler=decoder_scheduler,\n            super_res_scheduler=super_res_scheduler,\n        )",
      "metadata": {
        "source": "examples/community/unclip_text_interpolation.py",
        "range": {
          "start": { "row": 81, "column": 4 },
          "end": { "row": 81, "column": 4 }
        }
      }
    }
  ],
  [
    "341",
    {
      "pageContent": "def prepare_latents(self, shape, dtype, device, generator, latents, scheduler):\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        latents = latents * scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "examples/community/unclip_text_interpolation.py",
        "range": {
          "start": { "row": 110, "column": 4 },
          "end": { "row": 110, "column": 4 }
        }
      }
    }
  ],
  [
    "342",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        text_model_output: Optional[Union[CLIPTextModelOutput, Tuple]] = None,\n        text_attention_mask: Optional[torch.Tensor] = None,\n    ):\n        if text_model_output is None:\n            batch_size = len(prompt) if isinstance(prompt, list) else 1\n            # get prompt text embeddings\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n            text_mask = text_inputs.attention_mask.bool().to(device)\n\n            untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n                text_input_ids, untruncated_ids\n            ):\n                removed_text = self.tokenizer.batch_decode(\n                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n                )\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n\n            text_encoder_output = self.text_encoder(",
      "metadata": {
        "source": "examples/community/unclip_text_interpolation.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "343",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, the pipeline's\n        models have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only\n        when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        # TODO: self.prior.post_process_latents is not covered by the offload hooks, so it fails if added to the list\n        models = [\n            self.decoder,\n            self.text_proj,\n            self.text_encoder,\n            self.super_res_first,\n            self.super_res_last,\n        ]\n        for cpu_offloaded_model in models:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "examples/community/unclip_text_interpolation.py",
        "range": {
          "start": { "row": 214, "column": 4 },
          "end": { "row": 214, "column": 4 }
        }
      }
    }
  ],
  [
    "344",
    {
      "pageContent": "class ComposableStableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to ",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "345",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[\n            DDIMScheduler,\n            PNDMScheduler,\n            LMSDiscreteScheduler,\n            EulerDiscreteScheduler,\n            EulerAncestralDiscreteScheduler,\n            DPMSolverMultistepScheduler,\n        ],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and schedu",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 71, "column": 4 },
          "end": { "row": 71, "column": 4 }
        }
      }
    }
  ],
  [
    "346",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 167, "column": 4 },
          "end": { "row": 167, "column": 4 }
        }
      }
    }
  ],
  [
    "347",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 176, "column": 4 },
          "end": { "row": 176, "column": 4 }
        }
      }
    }
  ],
  [
    "348",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate\n            # fix by only offloading self.safety_checker for now\n            cpu_offload(self.safety_checker.vision_model, device)",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "349",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `list(int)`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only hand",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 223, "column": 4 },
          "end": { "row": 223, "column": 4 }
        }
      }
    }
  ],
  [
    "350",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 328, "column": 4 },
          "end": { "row": 328, "column": 4 }
        }
      }
    }
  ],
  [
    "351",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / 0.18215 * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 338, "column": 4 },
          "end": { "row": 338, "column": 4 }
        }
      }
    }
  ],
  [
    "352",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 346, "column": 4 },
          "end": { "row": 346, "column": 4 }
        }
      }
    }
  ],
  [
    "353",
    {
      "pageContent": "def check_inputs(self, prompt, height, width, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 363, "column": 4 },
          "end": { "row": 363, "column": 4 }
        }
      }
    }
  ],
  [
    "354",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if latents is None:\n            if device.type == \"mps\":\n                # randn does not work reproducibly on mps\n                latents = torch.randn(shape, generator=generator, device=\"cpu\", dtype=dtype).to(device)\n            else:\n                latents = torch.randn(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "examples/community/composable_stable_diffusion.py",
        "range": {
          "start": { "row": 378, "column": 4 },
          "end": { "row": 378, "column": 4 }
        }
      }
    }
  ],
  [
    "355",
    {
      "pageContent": "class MakeCutouts(nn.Module):\n    def __init__(self, cut_size, cut_power=1.0):\n        super().__init__()\n\n        self.cut_size = cut_size\n        self.cut_power = cut_power\n\n    def forward(self, pixel_values, num_cutouts):\n        sideY, sideX = pixel_values.shape[2:4]\n        max_size = min(sideX, sideY)\n        min_size = min(sideX, sideY, self.cut_size)\n        cutouts = []\n        for _ in range(num_cutouts):\n            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)\n            offsetx = torch.randint(0, sideX - size + 1, ())\n            offsety = torch.randint(0, sideY - size + 1, ())\n            cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]\n            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n        return torch.cat(cutouts)",
      "metadata": {
        "source": "examples/community/clip_guided_stable_diffusion.py",
        "range": {
          "start": { "row": 20, "column": 0 },
          "end": { "row": 20, "column": 0 }
        }
      }
    }
  ],
  [
    "356",
    {
      "pageContent": "def __init__(self, cut_size, cut_power=1.0):\n        super().__init__()\n\n        self.cut_size = cut_size\n        self.cut_power = cut_power",
      "metadata": {
        "source": "examples/community/clip_guided_stable_diffusion.py",
        "range": {
          "start": { "row": 21, "column": 4 },
          "end": { "row": 21, "column": 4 }
        }
      }
    }
  ],
  [
    "357",
    {
      "pageContent": "def forward(self, pixel_values, num_cutouts):\n        sideY, sideX = pixel_values.shape[2:4]\n        max_size = min(sideX, sideY)\n        min_size = min(sideX, sideY, self.cut_size)\n        cutouts = []\n        for _ in range(num_cutouts):\n            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)\n            offsetx = torch.randint(0, sideX - size + 1, ())\n            offsety = torch.randint(0, sideY - size + 1, ())\n            cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]\n            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n        return torch.cat(cutouts)",
      "metadata": {
        "source": "examples/community/clip_guided_stable_diffusion.py",
        "range": {
          "start": { "row": 27, "column": 4 },
          "end": { "row": 27, "column": 4 }
        }
      }
    }
  ],
  [
    "358",
    {
      "pageContent": "def spherical_dist_loss(x, y):\n    x = F.normalize(x, dim=-1)\n    y = F.normalize(y, dim=-1)\n    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)",
      "metadata": {
        "source": "examples/community/clip_guided_stable_diffusion.py",
        "range": {
          "start": { "row": 41, "column": 0 },
          "end": { "row": 41, "column": 0 }
        }
      }
    }
  ],
  [
    "359",
    {
      "pageContent": "class CLIPGuidedStableDiffusion(DiffusionPipeline):\n    \"\"\"CLIP guided stable diffusion based on the amazing repo by @crowsonkb and @Jack000\n    - https://github.com/Jack000/glid-3-xl\n    - https://github.dev/crowsonkb/k-diffusion\n    \"\"\"\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        clip_model: CLIPModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler],\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            clip_model=clip_model,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            feature_extractor=feature_extractor,\n        )\n\n        self.normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n        self.cut_out_size = (\n            feature_extractor.size\n            if isinstance(feature_extractor.size, int)\n            else feature_extractor.size[\"shortest_edge\"]\n        )\n        self.make_cutouts = MakeCutouts(self.cut_out_size)\n\n        set_requires_grad(self.text_encoder, False)\n        set_requires_grad(self.clip_model, False)\n\n    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.u",
      "metadata": {
        "source": "examples/community/clip_guided_stable_diffusion.py",
        "range": {
          "start": { "row": 52, "column": 0 },
          "end": { "row": 52, "column": 0 }
        }
      }
    }
  ],
  [
    "360",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        clip_model: CLIPModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler],\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            clip_model=clip_model,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            feature_extractor=feature_extractor,\n        )\n\n        self.normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n        self.cut_out_size = (\n            feature_extractor.size\n            if isinstance(feature_extractor.size, int)\n            else feature_extractor.size[\"shortest_edge\"]\n        )\n        self.make_cutouts = MakeCutouts(self.cut_out_size)\n\n        set_requires_grad(self.text_encoder, False)\n        set_requires_grad(self.clip_model, False)",
      "metadata": {
        "source": "examples/community/clip_guided_stable_diffusion.py",
        "range": {
          "start": { "row": 58, "column": 4 },
          "end": { "row": 58, "column": 4 }
        }
      }
    }
  ],
  [
    "361",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/clip_guided_stable_diffusion.py",
        "range": {
          "start": { "row": 90, "column": 4 },
          "end": { "row": 90, "column": 4 }
        }
      }
    }
  ],
  [
    "362",
    {
      "pageContent": "def slerp(val, low, high):\n    \"\"\"\n    Find the interpolation point between the 'low' and 'high' values for the given 'val'. See https://en.wikipedia.org/wiki/Slerp for more details on the topic.\n    \"\"\"\n    low_norm = low / torch.norm(low)\n    high_norm = high / torch.norm(high)\n    omega = torch.acos((low_norm * high_norm))\n    so = torch.sin(omega)\n    res = (torch.sin((1.0 - val) * omega) / so) * low + (torch.sin(val * omega) / so) * high\n    return res",
      "metadata": {
        "source": "examples/community/unclip_image_interpolation.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "363",
    {
      "pageContent": "class UnCLIPImageInterpolationPipeline(DiffusionPipeline):\n    \"\"\"\n    Pipeline to generate variations from an input image using unCLIP\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        text_encoder ([`CLIPTextModelWithProjection`]):\n            Frozen text-encoder.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `image_encoder`.\n        image_encoder ([`CLIPVisionModelWithProjection`]):\n            Frozen CLIP image-encoder. unCLIP Image Variation uses the vision portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModelWithProjection),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        text_proj ([`UnCLIPTextProjModel`]):\n            Utility class to prepare and combine the embeddings before they are passed to the decoder.\n        decoder ([`UNet2DConditionModel`]):\n            The decoder to invert the image embedding into an image.\n        super_res_first ([`UNet2DModel`]):\n            Super resolution unet. Used in all but the last step of the super resolution ",
      "metadata": {
        "source": "examples/community/unclip_image_interpolation.py",
        "range": {
          "start": { "row": 39, "column": 0 },
          "end": { "row": 39, "column": 0 }
        }
      }
    }
  ],
  [
    "364",
    {
      "pageContent": "def __init__(\n        self,\n        decoder: UNet2DConditionModel,\n        text_encoder: CLIPTextModelWithProjection,\n        tokenizer: CLIPTokenizer,\n        text_proj: UnCLIPTextProjModel,\n        feature_extractor: CLIPFeatureExtractor,\n        image_encoder: CLIPVisionModelWithProjection,\n        super_res_first: UNet2DModel,\n        super_res_last: UNet2DModel,\n        decoder_scheduler: UnCLIPScheduler,\n        super_res_scheduler: UnCLIPScheduler,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            decoder=decoder,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            text_proj=text_proj,\n            feature_extractor=feature_extractor,\n            image_encoder=image_encoder,\n            super_res_first=super_res_first,\n            super_res_last=super_res_last,\n            decoder_scheduler=decoder_scheduler,\n            super_res_scheduler=super_res_scheduler,\n        )",
      "metadata": {
        "source": "examples/community/unclip_image_interpolation.py",
        "range": {
          "start": { "row": 86, "column": 4 },
          "end": { "row": 86, "column": 4 }
        }
      }
    }
  ],
  [
    "365",
    {
      "pageContent": "def prepare_latents(self, shape, dtype, device, generator, latents, scheduler):\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        latents = latents * scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "examples/community/unclip_image_interpolation.py",
        "range": {
          "start": { "row": 115, "column": 4 },
          "end": { "row": 115, "column": 4 }
        }
      }
    }
  ],
  [
    "366",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance):\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        text_mask = text_inputs.attention_mask.bool().to(device)\n        text_encoder_output = self.text_encoder(text_input_ids.to(device))\n\n        prompt_embeds = text_encoder_output.text_embeds\n        text_encoder_hidden_states = text_encoder_output.last_hidden_state\n\n        prompt_embeds = prompt_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n        text_encoder_hidden_states = text_encoder_hidden_states.repeat_interleave(num_images_per_prompt, dim=0)\n        text_mask = text_mask.repeat_interleave(num_images_per_prompt, dim=0)\n\n        if do_classifier_free_guidance:\n            uncond_tokens = [\"\"] * batch_size\n\n            max_length = text_input_ids.shape[-1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            uncond_text_mask = uncond_input.attention_mask.bool().to(device)\n            negative_prompt_embeds_text_encoder_output = self.text_encoder(uncond_input.input_ids.to(device))\n\n            negative_prompt_embeds = negative_prompt",
      "metadata": {
        "source": "examples/community/unclip_image_interpolation.py",
        "range": {
          "start": { "row": 127, "column": 4 },
          "end": { "row": 127, "column": 4 }
        }
      }
    }
  ],
  [
    "367",
    {
      "pageContent": "def _encode_image(self, image, device, num_images_per_prompt, image_embeddings: Optional[torch.Tensor] = None):\n        dtype = next(self.image_encoder.parameters()).dtype\n\n        if image_embeddings is None:\n            if not isinstance(image, torch.Tensor):\n                image = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n            image = image.to(device=device, dtype=dtype)\n            image_embeddings = self.image_encoder(image).image_embeds\n\n        image_embeddings = image_embeddings.repeat_interleave(num_images_per_prompt, dim=0)\n\n        return image_embeddings",
      "metadata": {
        "source": "examples/community/unclip_image_interpolation.py",
        "range": {
          "start": { "row": 191, "column": 4 },
          "end": { "row": 191, "column": 4 }
        }
      }
    }
  ],
  [
    "368",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, the pipeline's\n        models have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only\n        when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        models = [\n            self.decoder,\n            self.text_proj,\n            self.text_encoder,\n            self.super_res_first,\n            self.super_res_last,\n        ]\n        for cpu_offloaded_model in models:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "examples/community/unclip_image_interpolation.py",
        "range": {
          "start": { "row": 206, "column": 4 },
          "end": { "row": 206, "column": 4 }
        }
      }
    }
  ],
  [
    "369",
    {
      "pageContent": "def preprocess(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "examples/community/imagic_stable_diffusion.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "370",
    {
      "pageContent": "class ImagicStableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for imagic image editing.\n    See paper here: https://arxiv.org/pdf/2210.09276.pdf\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offsensive or harmful.\n   ",
      "metadata": {
        "source": "examples/community/imagic_stable_diffusion.py",
        "range": {
          "start": { "row": 58, "column": 0 },
          "end": { "row": 58, "column": 0 }
        }
      }
    }
  ],
  [
    "371",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )",
      "metadata": {
        "source": "examples/community/imagic_stable_diffusion.py",
        "range": {
          "start": { "row": 86, "column": 4 },
          "end": { "row": 86, "column": 4 }
        }
      }
    }
  ],
  [
    "372",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/imagic_stable_diffusion.py",
        "range": {
          "start": { "row": 107, "column": 4 },
          "end": { "row": 107, "column": 4 }
        }
      }
    }
  ],
  [
    "373",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/imagic_stable_diffusion.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "374",
    {
      "pageContent": "def train(\n        self,\n        prompt: Union[str, List[str]],\n        image: Union[torch.FloatTensor, PIL.Image.Image],\n        height: Optional[int] = 512,\n        width: Optional[int] = 512,\n        generator: Optional[torch.Generator] = None,\n        embedding_learning_rate: float = 0.001,\n        diffusion_model_learning_rate: float = 2e-6,\n        text_embedding_optimization_steps: int = 500,\n        model_fine_tuning_optimization_steps: int = 1000,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            height (`int`, *optional*, defaults to 512):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to 512):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate ima",
      "metadata": {
        "source": "examples/community/imagic_stable_diffusion.py",
        "range": {
          "start": { "row": 132, "column": 4 },
          "end": { "row": 132, "column": 4 }
        }
      }
    }
  ],
  [
    "375",
    {
      "pageContent": "def prepare_image(image):\n    if isinstance(image, torch.Tensor):\n        # Batch single image\n        if image.ndim == 3:\n            image = image.unsqueeze(0)\n\n        image = image.to(dtype=torch.float32)\n    else:\n        # preprocess image\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [np.array(i.convert(\"RGB\"))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    return image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 126, "column": 0 },
          "end": { "row": 126, "column": 0 }
        }
      }
    }
  ],
  [
    "376",
    {
      "pageContent": "def prepare_mask_image(mask_image):\n    if isinstance(mask_image, torch.Tensor):\n        if mask_image.ndim == 2:\n            # Batch and add channel dim for single mask\n            mask_image = mask_image.unsqueeze(0).unsqueeze(0)\n        elif mask_image.ndim == 3 and mask_image.shape[0] == 1:\n            # Single mask, the 0'th dimension is considered to be\n            # the existing batch size of 1\n            mask_image = mask_image.unsqueeze(0)\n        elif mask_image.ndim == 3 and mask_image.shape[0] != 1:\n            # Batch of mask, the 0'th dimension is considered to be\n            # the batching dimension\n            mask_image = mask_image.unsqueeze(1)\n\n        # Binarize mask\n        mask_image[mask_image < 0.5] = 0\n        mask_image[mask_image >= 0.5] = 1\n    else:\n        # preprocess mask\n        if isinstance(mask_image, (PIL.Image.Image, np.ndarray)):\n            mask_image = [mask_image]\n\n        if isinstance(mask_image, list) and isinstance(mask_image[0], PIL.Image.Image):\n            mask_image = np.concatenate([np.array(m.convert(\"L\"))[None, None, :] for m in mask_image], axis=0)\n            mask_image = mask_image.astype(np.float32) / 255.0\n        elif isinstance(mask_image, list) and isinstance(mask_image[0], np.ndarray):\n            mask_image = np.concatenate([m[None, None, :] for m in mask_image], axis=0)\n\n        mask_image[mask_image < 0.5] = 0\n        mask_image[mask_image >= 0.5] = 1\n        mask_image = torch.from_numpy(mask_image)\n\n    return mask_image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 150, "column": 0 },
          "end": { "row": 150, "column": 0 }
        }
      }
    }
  ],
  [
    "377",
    {
      "pageContent": "def prepare_controlnet_conditioning_image(\n    controlnet_conditioning_image, width, height, batch_size, num_images_per_prompt, device, dtype\n):\n    if not isinstance(controlnet_conditioning_image, torch.Tensor):\n        if isinstance(controlnet_conditioning_image, PIL.Image.Image):\n            controlnet_conditioning_image = [controlnet_conditioning_image]\n\n        if isinstance(controlnet_conditioning_image[0], PIL.Image.Image):\n            controlnet_conditioning_image = [\n                np.array(i.resize((width, height), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :]\n                for i in controlnet_conditioning_image\n            ]\n            controlnet_conditioning_image = np.concatenate(controlnet_conditioning_image, axis=0)\n            controlnet_conditioning_image = np.array(controlnet_conditioning_image).astype(np.float32) / 255.0\n            controlnet_conditioning_image = controlnet_conditioning_image.transpose(0, 3, 1, 2)\n            controlnet_conditioning_image = torch.from_numpy(controlnet_conditioning_image)\n        elif isinstance(controlnet_conditioning_image[0], torch.Tensor):\n            controlnet_conditioning_image = torch.cat(controlnet_conditioning_image, dim=0)\n\n    image_batch_size = controlnet_conditioning_image.shape[0]\n\n    if image_batch_size == 1:\n        repeat_by = batch_size\n    else:\n        # image batch size is the same as prompt batch size\n        repeat_by = num_images_per_prompt\n\n    controlnet_conditioning_image = controlnet_conditioning_image.repeat_interleave(repeat_by, dim=0)\n\n    controlnet_conditioning_image = controlnet_",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 185, "column": 0 },
          "end": { "row": 185, "column": 0 }
        }
      }
    }
  ],
  [
    "378",
    {
      "pageContent": "class StableDiffusionControlNetInpaintPipeline(DiffusionPipeline):\n    \"\"\"\n    Inspired by: https://github.com/haofanwang/ControlNet-for-Diffusers/\n    \"\"\"\n\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        controlnet: ControlNetModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature ext",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 219, "column": 0 },
          "end": { "row": 219, "column": 0 }
        }
      }
    }
  ],
  [
    "379",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        controlnet: ControlNetModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modu",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "380",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 269, "column": 4 },
          "end": { "row": 269, "column": 4 }
        }
      }
    }
  ],
  [
    "381",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 278, "column": 4 },
          "end": { "row": 278, "column": 4 }
        }
      }
    }
  ],
  [
    "382",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae, controlnet, and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.controlnet]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "383",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            # the safety checker can offload the vae again\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # control net hook has be manually offloaded as it alternates with unet\n        cpu_offload_with_hook(self.controlnet, device)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 306, "column": 4 },
          "end": { "row": 306, "column": 4 }
        }
      }
    }
  ],
  [
    "384",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 352, "column": 4 },
          "end": { "row": 352, "column": 4 }
        }
      }
    }
  ],
  [
    "385",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 490, "column": 4 },
          "end": { "row": 490, "column": 4 }
        }
      }
    }
  ],
  [
    "386",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 500, "column": 4 },
          "end": { "row": 500, "column": 4 }
        }
      }
    }
  ],
  [
    "387",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 508, "column": 4 },
          "end": { "row": 508, "column": 4 }
        }
      }
    }
  ],
  [
    "388",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        image,\n        mask_image,\n        controlnet_conditioning_image,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot f",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 525, "column": 4 },
          "end": { "row": 525, "column": 4 }
        }
      }
    }
  ],
  [
    "389",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n\n        return latents",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 677, "column": 4 },
          "end": { "row": 677, "column": 4 }
        }
      }
    }
  ],
  [
    "390",
    {
      "pageContent": "def prepare_mask_latents(self, mask_image, batch_size, height, width, dtype, device, do_classifier_free_guidance):\n        # resize the mask to latents shape as we concatenate the mask to the latents\n        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n        # and half precision\n        mask_image = F.interpolate(mask_image, size=(height // self.vae_scale_factor, width // self.vae_scale_factor))\n        mask_image = mask_image.to(device=device, dtype=dtype)\n\n        # duplicate mask for each generation per prompt, using mps friendly method\n        if mask_image.shape[0] < batch_size:\n            if not batch_size % mask_image.shape[0] == 0:\n                raise ValueError(\n                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n                    f\" a total batch size of {batch_size}, but {mask_image.shape[0]} masks were passed. Make sure the number\"\n                    \" of masks that you pass is divisible by the total requested batch size.\"\n                )\n            mask_image = mask_image.repeat(batch_size // mask_image.shape[0], 1, 1, 1)\n\n        mask_image = torch.cat([mask_image] * 2) if do_classifier_free_guidance else mask_image\n\n        mask_image_latents = mask_image\n\n        return mask_image_latents",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 695, "column": 4 },
          "end": { "row": 695, "column": 4 }
        }
      }
    }
  ],
  [
    "391",
    {
      "pageContent": "def prepare_masked_image_latents(\n        self, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance\n    ):\n        masked_image = masked_image.to(device=device, dtype=dtype)\n\n        # encode the mask image into latents space so we can concatenate it to the latents\n        if isinstance(generator, list):\n            masked_image_latents = [\n                self.vae.encode(masked_image[i : i + 1]).latent_dist.sample(generator=generator[i])\n                for i in range(batch_size)\n            ]\n            masked_image_latents = torch.cat(masked_image_latents, dim=0)\n        else:\n            masked_image_latents = self.vae.encode(masked_image).latent_dist.sample(generator=generator)\n        masked_image_latents = self.vae.config.scaling_factor * masked_image_latents\n\n        # duplicate masked_image_latents for each generation per prompt, using mps friendly method\n        if masked_image_latents.shape[0] < batch_size:\n            if not batch_size % masked_image_latents.shape[0] == 0:\n                raise ValueError(\n                    \"The passed images and the required batch size don't match. Images are supposed to be duplicated\"\n                    f\" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed.\"\n                    \" Make sure the number of images that you pass is divisible by the total requested batch size.\"\n                )\n            masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1)\n\n        masked_image_latents = (\n",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 718, "column": 4 },
          "end": { "row": 718, "column": 4 }
        }
      }
    }
  ],
  [
    "392",
    {
      "pageContent": "def _default_height_width(self, height, width, image):\n        if isinstance(image, list):\n            image = image[0]\n\n        if height is None:\n            if isinstance(image, PIL.Image.Image):\n                height = image.height\n            elif isinstance(image, torch.Tensor):\n                height = image.shape[3]\n\n            height = (height // 8) * 8  # round down to nearest multiple of 8\n\n        if width is None:\n            if isinstance(image, PIL.Image.Image):\n                width = image.width\n            elif isinstance(image, torch.Tensor):\n                width = image.shape[2]\n\n            width = (width // 8) * 8  # round down to nearest multiple of 8\n\n        return height, width",
      "metadata": {
        "source": "examples/community/stable_diffusion_controlnet_inpaint.py",
        "range": {
          "start": { "row": 752, "column": 4 },
          "end": { "row": 752, "column": 4 }
        }
      }
    }
  ],
  [
    "393",
    {
      "pageContent": "class SpeechToImagePipeline(DiffusionPipeline):\n    def __init__(\n        self,\n        speech_model: WhisperForConditionalGeneration,\n        speech_processor: WhisperProcessor,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        self.register_modules(\n            speech_model=speech_model,\n            speech_processor=speech_processor,\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            feature_extractor=feature_ex",
      "metadata": {
        "source": "examples/community/speech_to_image_diffusion.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "394",
    {
      "pageContent": "def __init__(\n        self,\n        speech_model: WhisperForConditionalGeneration,\n        speech_processor: WhisperProcessor,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        self.register_modules(\n            speech_model=speech_model,\n            speech_processor=speech_processor,\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            feature_extractor=feature_extractor,\n        )",
      "metadata": {
        "source": "examples/community/speech_to_image_diffusion.py",
        "range": {
          "start": { "row": 29, "column": 4 },
          "end": { "row": 29, "column": 4 }
        }
      }
    }
  ],
  [
    "395",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        if slice_size == \"auto\":\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/speech_to_image_diffusion.py",
        "range": {
          "start": { "row": 64, "column": 4 },
          "end": { "row": 64, "column": 4 }
        }
      }
    }
  ],
  [
    "396",
    {
      "pageContent": "class SeedResizeStableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to ",
      "metadata": {
        "source": "examples/community/seed_resize_stable_diffusion.py",
        "range": {
          "start": { "row": 20, "column": 0 },
          "end": { "row": 20, "column": 0 }
        }
      }
    }
  ],
  [
    "397",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        super().__init__()\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )",
      "metadata": {
        "source": "examples/community/seed_resize_stable_diffusion.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "398",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = self.unet.config.attention_head_dim // 2\n        self.unet.set_attention_slice(slice_size)",
      "metadata": {
        "source": "examples/community/seed_resize_stable_diffusion.py",
        "range": {
          "start": { "row": 69, "column": 4 },
          "end": { "row": 69, "column": 4 }
        }
      }
    }
  ],
  [
    "399",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "examples/community/seed_resize_stable_diffusion.py",
        "range": {
          "start": { "row": 88, "column": 4 },
          "end": { "row": 88, "column": 4 }
        }
      }
    }
  ],
  [
    "400",
    {
      "pageContent": "def decimal_to_bits(x, bits=BITS):\n    \"\"\"expects image tensor ranging from 0 to 1, outputs bit tensor ranging from -1 to 1\"\"\"\n    device = x.device\n\n    x = (x * 255).int().clamp(0, 255)\n\n    mask = 2 ** torch.arange(bits - 1, -1, -1, device=device)\n    mask = rearrange(mask, \"d -> d 1 1\")\n    x = rearrange(x, \"b c h w -> b c 1 h w\")\n\n    bits = ((x & mask) != 0).float()\n    bits = rearrange(bits, \"b c d h w -> b (c d) h w\")\n    bits = bits * 2 - 1\n    return bits",
      "metadata": {
        "source": "examples/community/bit_diffusion.py",
        "range": {
          "start": { "row": 14, "column": 0 },
          "end": { "row": 14, "column": 0 }
        }
      }
    }
  ],
  [
    "401",
    {
      "pageContent": "def bits_to_decimal(x, bits=BITS):\n    \"\"\"expects bits from -1 to 1, outputs image tensor from 0 to 1\"\"\"\n    device = x.device\n\n    x = (x > 0).int()\n    mask = 2 ** torch.arange(bits - 1, -1, -1, device=device, dtype=torch.int32)\n\n    mask = rearrange(mask, \"d -> d 1 1\")\n    x = rearrange(x, \"b (c d) h w -> b c d h w\", d=8)\n    dec = reduce(x * mask, \"b c d h w -> b c h w\", \"sum\")\n    return (dec / 255).clamp(0.0, 1.0)",
      "metadata": {
        "source": "examples/community/bit_diffusion.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "402",
    {
      "pageContent": "def ddim_bit_scheduler_step(\n    self,\n    model_output: torch.FloatTensor,\n    timestep: int,\n    sample: torch.FloatTensor,\n    eta: float = 0.0,\n    use_clipped_model_output: bool = True,\n    generator=None,\n    return_dict: bool = True,\n) -> Union[DDIMSchedulerOutput, Tuple]:\n    \"\"\"\n    Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n    process from the learned model outputs (most often the predicted noise).\n    Args:\n        model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n        timestep (`int`): current discrete timestep in the diffusion chain.\n        sample (`torch.FloatTensor`):\n            current instance of sample being created by diffusion process.\n        eta (`float`): weight of noise for added noise in diffusion step.\n        use_clipped_model_output (`bool`): TODO\n        generator: random number generator.\n        return_dict (`bool`): option for returning tuple rather than DDIMSchedulerOutput class\n    Returns:\n        [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] or `tuple`:\n        [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n        returning a tuple, the first element is the sample tensor.\n    \"\"\"\n    if self.num_inference_steps is None:\n        raise ValueError(\n            \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n        )\n\n    # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n    # Ideally, read DDIM pa",
      "metadata": {
        "source": "examples/community/bit_diffusion.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "403",
    {
      "pageContent": "def ddpm_bit_scheduler_step(\n    self,\n    model_output: torch.FloatTensor,\n    timestep: int,\n    sample: torch.FloatTensor,\n    prediction_type=\"epsilon\",\n    generator=None,\n    return_dict: bool = True,\n) -> Union[DDPMSchedulerOutput, Tuple]:\n    \"\"\"\n    Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n    process from the learned model outputs (most often the predicted noise).\n    Args:\n        model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n        timestep (`int`): current discrete timestep in the diffusion chain.\n        sample (`torch.FloatTensor`):\n            current instance of sample being created by diffusion process.\n        prediction_type (`str`, default `epsilon`):\n            indicates whether the model predicts the noise (epsilon), or the samples (`sample`).\n        generator: random number generator.\n        return_dict (`bool`): option for returning tuple rather than DDPMSchedulerOutput class\n    Returns:\n        [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] or `tuple`:\n        [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n        returning a tuple, the first element is the sample tensor.\n    \"\"\"\n    t = timestep\n\n    if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [\"learned\", \"learned_range\"]:\n        model_output, predicted_variance = torch.split(model_output, sample.shape[1], dim=1)\n    else:\n        predicted_variance = None\n\n    # 1. compute alphas, betas\n    alpha_pro",
      "metadata": {
        "source": "examples/community/bit_diffusion.py",
        "range": {
          "start": { "row": 134, "column": 0 },
          "end": { "row": 134, "column": 0 }
        }
      }
    }
  ],
  [
    "404",
    {
      "pageContent": "class BitDiffusion(DiffusionPipeline):\n    def __init__(\n        self,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, DDPMScheduler],\n        bit_scale: Optional[float] = 1.0,\n    ):\n        super().__init__()\n        self.bit_scale = bit_scale\n        self.scheduler.step = (\n            ddim_bit_scheduler_step if isinstance(scheduler, DDIMScheduler) else ddpm_bit_scheduler_step\n        )\n\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        height: Optional[int] = 256,\n        width: Optional[int] = 256,\n        num_inference_steps: Optional[int] = 50,\n        generator: Optional[torch.Generator] = None,\n        batch_size: Optional[int] = 1,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[Tuple, ImagePipelineOutput]:\n        latents = torch.randn(\n            (batch_size, self.unet.in_channels, height, width),\n            generator=generator,\n        )\n        latents = decimal_to_bits(latents) * self.bit_scale\n        latents = latents.to(self.device)\n\n        self.scheduler.set_timesteps(num_inference_steps)\n\n        for t in self.progress_bar(self.scheduler.timesteps):\n            # predict the noise residual\n            noise_pred = self.unet(latents, t).sample\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n\n        image = bits_to_decimal(latents)\n\n        if output_type == \"pil\":\n            image = self.numpy_to_p",
      "metadata": {
        "source": "examples/community/bit_diffusion.py",
        "range": {
          "start": { "row": 212, "column": 0 },
          "end": { "row": 212, "column": 0 }
        }
      }
    }
  ],
  [
    "405",
    {
      "pageContent": "def __init__(\n        self,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, DDPMScheduler],\n        bit_scale: Optional[float] = 1.0,\n    ):\n        super().__init__()\n        self.bit_scale = bit_scale\n        self.scheduler.step = (\n            ddim_bit_scheduler_step if isinstance(scheduler, DDIMScheduler) else ddpm_bit_scheduler_step\n        )\n\n        self.register_modules(unet=unet, scheduler=scheduler)",
      "metadata": {
        "source": "examples/community/bit_diffusion.py",
        "range": {
          "start": { "row": 213, "column": 4 },
          "end": { "row": 213, "column": 4 }
        }
      }
    }
  ],
  [
    "406",
    {
      "pageContent": "class CreateModelCardTest(unittest.TestCase):\n    @patch(\"diffusers.utils.hub_utils.get_full_repo_name\")\n    def test_create_model_card(self, repo_name_mock: Mock) -> None:\n        repo_name_mock.return_value = \"full_repo_name\"\n        with TemporaryDirectory() as tmpdir:\n            # Dummy args values\n            args = Mock()\n            args.output_dir = tmpdir\n            args.local_rank = 0\n            args.hub_token = \"hub_token\"\n            args.dataset_name = \"dataset_name\"\n            args.learning_rate = 0.01\n            args.train_batch_size = 100000\n            args.eval_batch_size = 10000\n            args.gradient_accumulation_steps = 0.01\n            args.adam_beta1 = 0.02\n            args.adam_beta2 = 0.03\n            args.adam_weight_decay = 0.0005\n            args.adam_epsilon = 0.000001\n            args.lr_scheduler = 1\n            args.lr_warmup_steps = 10\n            args.ema_inv_gamma = 0.001\n            args.ema_power = 0.1\n            args.ema_max_decay = 0.2\n            args.mixed_precision = True\n\n            # Model card mush be rendered and saved\n            diffusers.utils.hub_utils.create_model_card(args, model_name=\"model_name\")\n            self.assertTrue((Path(tmpdir) / \"README.md\").is_file())",
      "metadata": {
        "source": "tests/test_hub_utils.py",
        "range": {
          "start": { "row": 22, "column": 0 },
          "end": { "row": 22, "column": 0 }
        }
      }
    }
  ],
  [
    "407",
    {
      "pageContent": "class FlaxSchedulerCommonTest(unittest.TestCase):\n    scheduler_classes = ()\n    forward_default_kwargs = ()\n\n    @property\n    def dummy_sample(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        key1, key2 = random.split(random.PRNGKey(0))\n        sample = random.uniform(key1, (batch_size, num_channels, height, width))\n\n        return sample, key2\n\n    @property\n    def dummy_sample_deter(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        num_elems = batch_size * num_channels * height * width\n        sample = jnp.arange(num_elems)\n        sample = sample.reshape(num_channels, height, width, batch_size)\n        sample = sample / num_elems\n        return jnp.transpose(sample, (3, 0, 1, 2))\n\n    def get_scheduler_config(self):\n        raise NotImplementedError\n\n    def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            sample, key = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            state = scheduler.create_state()\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n  ",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_flax.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "408",
    {
      "pageContent": "class FlaxDDPMSchedulerTest(FlaxSchedulerCommonTest):\n    scheduler_classes = (FlaxDDPMScheduler,)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"variance_type\": \"fixed_small\",\n            \"clip_sample\": True,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [1, 5, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.0001, 0.001, 0.01, 0.1], [0.002, 0.02, 0.2, 2]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"squaredcos_cap_v2\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_variance_type(self):\n        for variance in [\"fixed_small\", \"fixed_large\", \"other\"]:\n            self.check_over_configs(variance_type=variance)\n\n    def test_clip_sample(self):\n        for clip_sample in [True, False]:\n            self.check_over_configs(clip_sample=clip_sample)\n\n    def test_time_indices(self):\n        for t in [0, 500, 999]:\n            self.check_over_forward(time_step=t)\n\n    def test_variance(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n        state = scheduler.create_state()\n\n        assert j",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_flax.py",
        "range": {
          "start": { "row": 254, "column": 0 },
          "end": { "row": 254, "column": 0 }
        }
      }
    }
  ],
  [
    "409",
    {
      "pageContent": "class FlaxDDIMSchedulerTest(FlaxSchedulerCommonTest):\n    scheduler_classes = (FlaxDDIMScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 50),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def full_loop(self, **config):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n        state = scheduler.create_state()\n        key1, key2 = random.split(random.PRNGKey(0))\n\n        num_inference_steps = 10\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n\n        state = scheduler.set_timesteps(state, num_inference_steps)\n\n        for t in state.timesteps:\n            residual = model(sample, t)\n            output = scheduler.step(state, residual, t, sample)\n            sample = output.prev_sample\n            state = output.state\n            key1, key2 = random.split(key2)\n\n        return sample\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            sample, _ = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = ",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_flax.py",
        "range": {
          "start": { "row": 345, "column": 0 },
          "end": { "row": 345, "column": 0 }
        }
      }
    }
  ],
  [
    "410",
    {
      "pageContent": "class FlaxPNDMSchedulerTest(FlaxSchedulerCommonTest):\n    scheduler_classes = (FlaxPNDMScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 50),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample, _ = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = jnp.array([residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05])\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            state = scheduler.create_state()\n            state = scheduler.set_timesteps(state, num_inference_steps, shape=sample.shape)\n            # copy over dummy past residuals\n            state = state.replace(ets=dummy_past_residuals[:])\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler, new_state = scheduler_class.from_pretrained(tmpdirname)\n                new_state = new_scheduler.set_timesteps(new_state, num_inference_steps, shape=sample.shape)\n                # copy over dummy past residuals\n                new_sta",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_flax.py",
        "range": {
          "start": { "row": 630, "column": 0 },
          "end": { "row": 630, "column": 0 }
        }
      }
    }
  ],
  [
    "411",
    {
      "pageContent": "class DEISMultistepSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DEISMultistepScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 25),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            outp",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 14, "column": 0 },
          "end": { "row": 14, "column": 0 }
        }
      }
    }
  ],
  [
    "412",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 18, "column": 4 },
          "end": { "row": 18, "column": 4 }
        }
      }
    }
  ],
  [
    "413",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output, new_output = sample, sample\n            for t in range(time_step, time_step + scheduler.config.solver_order + 1):\n                output = scheduler.step(residual, t, output, **kwargs).prev_sample\n                new_output = new_scheduler.step(residual, t, new_output, **kwargs).prev_sample\n\n                assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 30, "column": 4 },
          "end": { "row": 30, "column": 4 }
        }
      }
    }
  ],
  [
    "414",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n\n            # copy over dummy past residuals (must be after setting timesteps)\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                # copy over dummy past residuals\n                new_scheduler.set_timesteps(num_inference_steps)\n\n                # copy over dummy past residual (must be after setting timesteps)\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 61, "column": 4 },
          "end": { "row": 61, "column": 4 }
        }
      }
    }
  ],
  [
    "415",
    {
      "pageContent": "def full_loop(self, scheduler=None, **config):\n        if scheduler is None:\n            scheduler_class = self.scheduler_classes[0]\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 90, "column": 4 },
          "end": { "row": 90, "column": 4 }
        }
      }
    }
  ],
  [
    "416",
    {
      "pageContent": "def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            # copy over dummy past residuals (must be done after set_timesteps)\n            dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            time_step_0 = scheduler.timesteps[5]\n            time_step_1 = scheduler.timesteps[6]\n\n            output_0 = scheduler.step(residual, time_step_0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step(residual, time_step_1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 111, "column": 4 },
          "end": { "row": 111, "column": 4 }
        }
      }
    }
  ],
  [
    "417",
    {
      "pageContent": "def test_switch(self):\n        # make sure that iterating over schedulers with same config names gives same results\n        # for defaults\n        scheduler = DEISMultistepScheduler(**self.get_scheduler_config())\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.23916) < 1e-3\n\n        scheduler = DPMSolverSinglestepScheduler.from_config(scheduler.config)\n        scheduler = DPMSolverMultistepScheduler.from_config(scheduler.config)\n        scheduler = UniPCMultistepScheduler.from_config(scheduler.config)\n        scheduler = DEISMultistepScheduler.from_config(scheduler.config)\n\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.23916) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "418",
    {
      "pageContent": "def test_thresholding(self):\n        self.check_over_configs(thresholding=False)\n        for order in [1, 2, 3]:\n            for solver_type in [\"logrho\"]:\n                for threshold in [0.5, 1.0, 2.0]:\n                    for prediction_type in [\"epsilon\", \"sample\"]:\n                        self.check_over_configs(\n                            thresholding=True,\n                            prediction_type=prediction_type,\n                            sample_max_value=threshold,\n                            algorithm_type=\"deis\",\n                            solver_order=order,\n                            solver_type=solver_type,\n                        )",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 164, "column": 4 },
          "end": { "row": 164, "column": 4 }
        }
      }
    }
  ],
  [
    "419",
    {
      "pageContent": "def test_solver_order_and_type(self):\n        for algorithm_type in [\"deis\"]:\n            for solver_type in [\"logrho\"]:\n                for order in [1, 2, 3]:\n                    for prediction_type in [\"epsilon\", \"sample\"]:\n                        self.check_over_configs(\n                            solver_order=order,\n                            solver_type=solver_type,\n                            prediction_type=prediction_type,\n                            algorithm_type=algorithm_type,\n                        )\n                        sample = self.full_loop(\n                            solver_order=order,\n                            solver_type=solver_type,\n                            prediction_type=prediction_type,\n                            algorithm_type=algorithm_type,\n                        )\n                        assert not torch.isnan(sample).any(), \"Samples have nan numbers\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "420",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.23916) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 210, "column": 4 },
          "end": { "row": 210, "column": 4 }
        }
      }
    }
  ],
  [
    "421",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.091) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 216, "column": 4 },
          "end": { "row": 216, "column": 4 }
        }
      }
    }
  ],
  [
    "422",
    {
      "pageContent": "def test_fp16_support(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(thresholding=True, dynamic_thresholding_ratio=0)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter.half()\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        assert sample.dtype == torch.float16",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_deis.py",
        "range": {
          "start": { "row": 222, "column": 4 },
          "end": { "row": 222, "column": 4 }
        }
      }
    }
  ],
  [
    "423",
    {
      "pageContent": "class PNDMSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (PNDMScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 50),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.ets = dummy_past_residuals[:]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.ets = dummy_past_residuals[:]\n\n            output = scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.ste",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 9, "column": 0 },
          "end": { "row": 9, "column": 0 }
        }
      }
    }
  ],
  [
    "424",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 13, "column": 4 },
          "end": { "row": 13, "column": 4 }
        }
      }
    }
  ],
  [
    "425",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.ets = dummy_past_residuals[:]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.ets = dummy_past_residuals[:]\n\n            output = scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step_plms(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step_plms(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5,",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 24, "column": 4 },
          "end": { "row": 24, "column": 4 }
        }
      }
    }
  ],
  [
    "426",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n\n            # copy over dummy past residuals (must be after setting timesteps)\n            scheduler.ets = dummy_past_residuals[:]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                # copy over dummy past residuals\n                new_scheduler.set_timesteps(num_inference_steps)\n\n                # copy over dummy past residual (must be after setting timesteps)\n                new_scheduler.ets = dummy_past_residuals[:]\n\n            output = scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step_plms(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step_plms(re",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 58, "column": 4 },
          "end": { "row": 58, "column": 4 }
        }
      }
    }
  ],
  [
    "427",
    {
      "pageContent": "def full_loop(self, **config):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.prk_timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step_prk(residual, t, sample).prev_sample\n\n        for i, t in enumerate(scheduler.plms_timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step_plms(residual, t, sample).prev_sample\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 92, "column": 4 },
          "end": { "row": 92, "column": 4 }
        }
      }
    }
  ],
  [
    "428",
    {
      "pageContent": "def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            # copy over dummy past residuals (must be done after set_timesteps)\n            dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n            scheduler.ets = dummy_past_residuals[:]\n\n            output_0 = scheduler.step_prk(residual, 0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step_prk(residual, 1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)\n\n            output_0 = scheduler.step_plms(residual, 0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step_plms(residual, 1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 112, "column": 4 },
          "end": { "row": 112, "column": 4 }
        }
      }
    }
  ],
  [
    "429",
    {
      "pageContent": "def test_steps_offset(self):\n        for steps_offset in [0, 1]:\n            self.check_over_configs(steps_offset=steps_offset)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(steps_offset=1)\n        scheduler = scheduler_class(**scheduler_config)\n        scheduler.set_timesteps(10)\n        assert torch.equal(\n            scheduler.timesteps,\n            torch.LongTensor(\n                [901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]\n            ),\n        )",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 149, "column": 4 },
          "end": { "row": 149, "column": 4 }
        }
      }
    }
  ],
  [
    "430",
    {
      "pageContent": "def test_pow_of_3_inference_steps(self):\n        # earlier version of set_timesteps() caused an error indexing alpha's with inference steps as power of 3\n        num_inference_steps = 27\n\n        for scheduler_class in self.scheduler_classes:\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            scheduler.set_timesteps(num_inference_steps)\n\n            # before power of 3 fix, would error on first step, so we only need to do two\n            for i, t in enumerate(scheduler.prk_timesteps[:2]):\n                sample = scheduler.step_prk(residual, t, sample).prev_sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 184, "column": 4 },
          "end": { "row": 184, "column": 4 }
        }
      }
    }
  ],
  [
    "431",
    {
      "pageContent": "def test_inference_plms_no_past_residuals(self):\n        with self.assertRaises(ValueError):\n            scheduler_class = self.scheduler_classes[0]\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            scheduler.step_plms(self.dummy_sample, 1, self.dummy_sample).prev_sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 201, "column": 4 },
          "end": { "row": 201, "column": 4 }
        }
      }
    }
  ],
  [
    "432",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 198.1318) < 1e-2\n        assert abs(result_mean.item() - 0.2580) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 209, "column": 4 },
          "end": { "row": 209, "column": 4 }
        }
      }
    }
  ],
  [
    "433",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 67.3986) < 1e-2\n        assert abs(result_mean.item() - 0.0878) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 217, "column": 4 },
          "end": { "row": 217, "column": 4 }
        }
      }
    }
  ],
  [
    "434",
    {
      "pageContent": "def test_full_loop_with_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=True, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 230.0399) < 1e-2\n        assert abs(result_mean.item() - 0.2995) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 225, "column": 4 },
          "end": { "row": 225, "column": 4 }
        }
      }
    }
  ],
  [
    "435",
    {
      "pageContent": "def test_full_loop_with_no_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=False, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 186.9482) < 1e-2\n        assert abs(result_mean.item() - 0.2434) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_pndm.py",
        "range": {
          "start": { "row": 234, "column": 4 },
          "end": { "row": 234, "column": 4 }
        }
      }
    }
  ],
  [
    "436",
    {
      "pageContent": "class SchedulerObject(SchedulerMixin, ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n    ):\n        pass",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "437",
    {
      "pageContent": "class SchedulerObject2(SchedulerMixin, ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        f=[1, 3],\n    ):\n        pass",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "438",
    {
      "pageContent": "class SchedulerObject3(SchedulerMixin, ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n        f=[1, 3],\n    ):\n        pass",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 72, "column": 0 },
          "end": { "row": 72, "column": 0 }
        }
      }
    }
  ],
  [
    "439",
    {
      "pageContent": "class SchedulerBaseTests(unittest.TestCase):\n    def test_save_load_from_different_config(self):\n        obj = SchedulerObject()\n\n        # mock add obj class to `diffusers`\n        setattr(diffusers, \"SchedulerObject\", SchedulerObject)\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n            with CaptureLogger(logger) as cap_logger_1:\n                config = SchedulerObject2.load_config(tmpdirname)\n                new_obj_1 = SchedulerObject2.from_config(config)\n\n            # now save a config parameter that is not expected\n            with open(os.path.join(tmpdirname, SchedulerObject.config_name), \"r\") as f:\n                data = json.load(f)\n                data[\"unexpected\"] = True\n\n            with open(os.path.join(tmpdirname, SchedulerObject.config_name), \"w\") as f:\n                json.dump(data, f)\n\n            with CaptureLogger(logger) as cap_logger_2:\n                config = SchedulerObject.load_config(tmpdirname)\n                new_obj_2 = SchedulerObject.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_3:\n                config = SchedulerObject2.load_config(tmpdirname)\n                new_obj_3 = SchedulerObject2.from_config(config)\n\n        assert new_obj_1.__class__ == SchedulerObject2\n        assert new_obj_2.__class__ == SchedulerObject\n        assert new_obj_3.__class__ == SchedulerObject2\n\n        assert cap_logger_1.out == \"\"\n        assert (\n            cap_logger_2.out\n            == \"The config att",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 88, "column": 0 },
          "end": { "row": 88, "column": 0 }
        }
      }
    }
  ],
  [
    "440",
    {
      "pageContent": "def test_save_load_from_different_config(self):\n        obj = SchedulerObject()\n\n        # mock add obj class to `diffusers`\n        setattr(diffusers, \"SchedulerObject\", SchedulerObject)\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n            with CaptureLogger(logger) as cap_logger_1:\n                config = SchedulerObject2.load_config(tmpdirname)\n                new_obj_1 = SchedulerObject2.from_config(config)\n\n            # now save a config parameter that is not expected\n            with open(os.path.join(tmpdirname, SchedulerObject.config_name), \"r\") as f:\n                data = json.load(f)\n                data[\"unexpected\"] = True\n\n            with open(os.path.join(tmpdirname, SchedulerObject.config_name), \"w\") as f:\n                json.dump(data, f)\n\n            with CaptureLogger(logger) as cap_logger_2:\n                config = SchedulerObject.load_config(tmpdirname)\n                new_obj_2 = SchedulerObject.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_3:\n                config = SchedulerObject2.load_config(tmpdirname)\n                new_obj_3 = SchedulerObject2.from_config(config)\n\n        assert new_obj_1.__class__ == SchedulerObject2\n        assert new_obj_2.__class__ == SchedulerObject\n        assert new_obj_3.__class__ == SchedulerObject2\n\n        assert cap_logger_1.out == \"\"\n        assert (\n            cap_logger_2.out\n            == \"The config attributes {'unexpected': True} were passed to Sched",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 89, "column": 4 },
          "end": { "row": 89, "column": 4 }
        }
      }
    }
  ],
  [
    "441",
    {
      "pageContent": "def test_save_load_compatible_schedulers(self):\n        SchedulerObject2._compatibles = [\"SchedulerObject\"]\n        SchedulerObject._compatibles = [\"SchedulerObject2\"]\n\n        obj = SchedulerObject()\n\n        # mock add obj class to `diffusers`\n        setattr(diffusers, \"SchedulerObject\", SchedulerObject)\n        setattr(diffusers, \"SchedulerObject2\", SchedulerObject2)\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n\n            # now save a config parameter that is expected by another class, but not origin class\n            with open(os.path.join(tmpdirname, SchedulerObject.config_name), \"r\") as f:\n                data = json.load(f)\n                data[\"f\"] = [0, 0]\n                data[\"unexpected\"] = True\n\n            with open(os.path.join(tmpdirname, SchedulerObject.config_name), \"w\") as f:\n                json.dump(data, f)\n\n            with CaptureLogger(logger) as cap_logger:\n                config = SchedulerObject.load_config(tmpdirname)\n                new_obj = SchedulerObject.from_config(config)\n\n        assert new_obj.__class__ == SchedulerObject\n\n        assert (\n            cap_logger.out\n            == \"The config attributes {'unexpected': True} were passed to SchedulerObject, but are not expected and\"\n            \" will\"\n            \" be ignored. Please verify your config.json configuration file.\\n\"\n        )",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 131, "column": 4 },
          "end": { "row": 131, "column": 4 }
        }
      }
    }
  ],
  [
    "442",
    {
      "pageContent": "def test_save_load_from_different_config_comp_schedulers(self):\n        SchedulerObject3._compatibles = [\"SchedulerObject\", \"SchedulerObject2\"]\n        SchedulerObject2._compatibles = [\"SchedulerObject\", \"SchedulerObject3\"]\n        SchedulerObject._compatibles = [\"SchedulerObject2\", \"SchedulerObject3\"]\n\n        obj = SchedulerObject()\n\n        # mock add obj class to `diffusers`\n        setattr(diffusers, \"SchedulerObject\", SchedulerObject)\n        setattr(diffusers, \"SchedulerObject2\", SchedulerObject2)\n        setattr(diffusers, \"SchedulerObject3\", SchedulerObject3)\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n        logger.setLevel(diffusers.logging.INFO)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n\n            with CaptureLogger(logger) as cap_logger_1:\n                config = SchedulerObject.load_config(tmpdirname)\n                new_obj_1 = SchedulerObject.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_2:\n                config = SchedulerObject2.load_config(tmpdirname)\n                new_obj_2 = SchedulerObject2.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_3:\n                config = SchedulerObject3.load_config(tmpdirname)\n                new_obj_3 = SchedulerObject3.from_config(config)\n\n        assert new_obj_1.__class__ == SchedulerObject\n        assert new_obj_2.__class__ == SchedulerObject2\n        assert new_obj_3.__class__ == SchedulerObject3\n\n        assert cap_logger_1.out == \"\"\n        assert cap_logger_2.out == \"{'f'} w",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 167, "column": 4 },
          "end": { "row": 167, "column": 4 }
        }
      }
    }
  ],
  [
    "443",
    {
      "pageContent": "class SchedulerCommonTest(unittest.TestCase):\n    scheduler_classes = ()\n    forward_default_kwargs = ()\n\n    @property\n    def dummy_sample(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        sample = torch.rand((batch_size, num_channels, height, width))\n\n        return sample\n\n    @property\n    def dummy_sample_deter(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        num_elems = batch_size * num_channels * height * width\n        sample = torch.arange(num_elems)\n        sample = sample.reshape(num_channels, height, width, batch_size)\n        sample = sample / num_elems\n        sample = sample.permute(3, 0, 1, 2)\n\n        return sample\n\n    def get_scheduler_config(self):\n        raise NotImplementedError\n\n    def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            # TODO(Suraj) - delete the following two lines once DDPM, DDIM, and PNDM have timesteps casted to float by default\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                time_step = float(time_step)\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n       ",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 205, "column": 0 },
          "end": { "row": 205, "column": 0 }
        }
      }
    }
  ],
  [
    "444",
    {
      "pageContent": "def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 238, "column": 4 },
          "end": { "row": 238, "column": 4 }
        }
      }
    }
  ],
  [
    "445",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            # TODO(Suraj) - delete the following two lines once DDPM, DDIM, and PNDM have timesteps casted to float by default\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                time_step = float(time_step)\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, time_step)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n                new_scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_infere",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 244, "column": 4 },
          "end": { "row": 244, "column": 4 }
        }
      }
    }
  ],
  [
    "446",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        kwargs.update(forward_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                time_step = float(time_step)\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, time_step)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n                new_scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 292, "column": 4 },
          "end": { "row": 292, "column": 4 }
        }
      }
    }
  ],
  [
    "447",
    {
      "pageContent": "def test_from_save_pretrained(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            timestep = 1\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                timestep = float(timestep)\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, timestep)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n                new_scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n                kwargs[\"genera",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 334, "column": 4 },
          "end": { "row": 334, "column": 4 }
        }
      }
    }
  ],
  [
    "448",
    {
      "pageContent": "def test_compatibles(self):\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n\n            scheduler = scheduler_class(**scheduler_config)\n\n            assert all(c is not None for c in scheduler.compatibles)\n\n            for comp_scheduler_cls in scheduler.compatibles:\n                comp_scheduler = comp_scheduler_cls.from_config(scheduler.config)\n                assert comp_scheduler is not None\n\n            new_scheduler = scheduler_class.from_config(comp_scheduler.config)\n\n            new_scheduler_config = {k: v for k, v in new_scheduler.config.items() if k in scheduler.config}\n            scheduler_diff = {k: v for k, v in new_scheduler.config.items() if k not in scheduler.config}\n\n            # make sure that configs are essentially identical\n            assert new_scheduler_config == dict(scheduler.config)\n\n            # make sure that only differences are for configs that are not in init\n            init_keys = inspect.signature(scheduler_class.__init__).parameters.keys()\n            assert set(scheduler_diff.keys()).intersection(set(init_keys)) == set()",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 376, "column": 4 },
          "end": { "row": 376, "column": 4 }
        }
      }
    }
  ],
  [
    "449",
    {
      "pageContent": "def test_from_pretrained(self):\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n\n            scheduler = scheduler_class(**scheduler_config)\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_pretrained(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            assert scheduler.config == new_scheduler.config",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 400, "column": 4 },
          "end": { "row": 400, "column": 4 }
        }
      }
    }
  ],
  [
    "450",
    {
      "pageContent": "def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        timestep_0 = 0\n        timestep_1 = 1\n\n        for scheduler_class in self.scheduler_classes:\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                timestep_0 = float(timestep_0)\n                timestep_1 = float(timestep_1)\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, timestep_0)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            output_0 = scheduler.step(residual, timestep_0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step(residual, timestep_1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 412, "column": 4 },
          "end": { "row": 412, "column": 4 }
        }
      }
    }
  ],
  [
    "451",
    {
      "pageContent": "def test_scheduler_outputs_equivalence(self):\n        def set_nan_tensor_to_zero(t):\n            t[t != t] = 0\n            return t\n\n        def recursive_check(tuple_object, dict_object):\n            if isinstance(tuple_object, (List, Tuple)):\n                for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object.values()):\n                    recursive_check(tuple_iterable_value, dict_iterable_value)\n            elif isinstance(tuple_object, Dict):\n                for tuple_iterable_value, dict_iterable_value in zip(tuple_object.values(), dict_object.values()):\n                    recursive_check(tuple_iterable_value, dict_iterable_value)\n            elif tuple_object is None:\n                return\n            else:\n                self.assertTrue(\n                    torch.allclose(\n                        set_nan_tensor_to_zero(tuple_object), set_nan_tensor_to_zero(dict_object), atol=1e-5\n                    ),\n                    msg=(\n                        \"Tuple and dict output are not equal. Difference:\"\n                        f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"\n                        f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"\n                        f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"\n                    ),\n                )\n\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", 50)\n\n        timestep = 0\n        if len(self.scheduler_classes) > 0 and ",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 448, "column": 4 },
          "end": { "row": 448, "column": 4 }
        }
      }
    }
  ],
  [
    "452",
    {
      "pageContent": "def test_scheduler_public_api(self):\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class != VQDiffusionScheduler:\n                self.assertTrue(\n                    hasattr(scheduler, \"init_noise_sigma\"),\n                    f\"{scheduler_class} does not implement a required attribute `init_noise_sigma`\",\n                )\n                self.assertTrue(\n                    hasattr(scheduler, \"scale_model_input\"),\n                    (\n                        f\"{scheduler_class} does not implement a required class method `scale_model_input(sample,\"\n                        \" timestep)`\"\n                    ),\n                )\n            self.assertTrue(\n                hasattr(scheduler, \"step\"),\n                f\"{scheduler_class} does not implement a required class method `step(...)`\",\n            )\n\n            if scheduler_class != VQDiffusionScheduler:\n                sample = self.dummy_sample\n                scaled_sample = scheduler.scale_model_input(sample, 0.0)\n                self.assertEqual(sample.shape, scaled_sample.shape)",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 520, "column": 4 },
          "end": { "row": 520, "column": 4 }
        }
      }
    }
  ],
  [
    "453",
    {
      "pageContent": "def test_add_noise_device(self):\n        for scheduler_class in self.scheduler_classes:\n            if scheduler_class == IPNDMScheduler:\n                continue\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(100)\n\n            sample = self.dummy_sample.to(torch_device)\n            scaled_sample = scheduler.scale_model_input(sample, 0.0)\n            self.assertEqual(sample.shape, scaled_sample.shape)\n\n            noise = torch.randn_like(scaled_sample).to(torch_device)\n            t = scheduler.timesteps[5][None]\n            noised = scheduler.add_noise(scaled_sample, noise, t)\n            self.assertEqual(noised.shape, scaled_sample.shape)",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 547, "column": 4 },
          "end": { "row": 547, "column": 4 }
        }
      }
    }
  ],
  [
    "454",
    {
      "pageContent": "def test_deprecated_kwargs(self):\n        for scheduler_class in self.scheduler_classes:\n            has_kwarg_in_model_class = \"kwargs\" in inspect.signature(scheduler_class.__init__).parameters\n            has_deprecated_kwarg = len(scheduler_class._deprecated_kwargs) > 0\n\n            if has_kwarg_in_model_class and not has_deprecated_kwarg:\n                raise ValueError(\n                    f\"{scheduler_class} has `**kwargs` in its __init__ method but has not defined any deprecated\"\n                    \" kwargs under the `_deprecated_kwargs` class attribute. Make sure to either remove `**kwargs` if\"\n                    \" there are no deprecated arguments or add the deprecated argument with `_deprecated_kwargs =\"\n                    \" [<deprecated_argument>]`\"\n                )\n\n            if not has_kwarg_in_model_class and has_deprecated_kwarg:\n                raise ValueError(\n                    f\"{scheduler_class} doesn't have `**kwargs` in its __init__ method but has defined deprecated\"\n                    \" kwargs under the `_deprecated_kwargs` class attribute. Make sure to either add the `**kwargs`\"\n                    f\" argument to {self.model_class}.__init__ if there are deprecated arguments or remove the\"\n                    \" deprecated argument from `_deprecated_kwargs = [<deprecated_argument>]`\"\n                )",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 564, "column": 4 },
          "end": { "row": 564, "column": 4 }
        }
      }
    }
  ],
  [
    "455",
    {
      "pageContent": "def test_trained_betas(self):\n        for scheduler_class in self.scheduler_classes:\n            if scheduler_class == VQDiffusionScheduler:\n                continue\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config, trained_betas=np.array([0.1, 0.3]))\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_pretrained(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            assert scheduler.betas.tolist() == new_scheduler.betas.tolist()",
      "metadata": {
        "source": "tests/schedulers/test_schedulers.py",
        "range": {
          "start": { "row": 585, "column": 4 },
          "end": { "row": 585, "column": 4 }
        }
      }
    }
  ],
  [
    "456",
    {
      "pageContent": "class LMSDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (LMSDiscreteScheduler,)\n    num_inference_steps = 10\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [10, 50, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.00001, 0.0001, 0.001], [0.0002, 0.002, 0.02]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"scaled_linear\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_time_indices(self):\n        for t in [0, 500, 800]:\n            self.check_over_forward(time_step=t)\n\n    def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n\n        for i, t in enumerate(scheduler.timesteps):\n          ",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_lms.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "457",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_lms.py",
        "range": {
          "start": { "row": 12, "column": 4 },
          "end": { "row": 12, "column": 4 }
        }
      }
    }
  ],
  [
    "458",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 1006.388) < 1e-2\n        assert abs(result_mean.item() - 1.31) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_lms.py",
        "range": {
          "start": { "row": 43, "column": 4 },
          "end": { "row": 43, "column": 4 }
        }
      }
    }
  ],
  [
    "459",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 0.0017) < 1e-2\n        assert abs(result_mean.item() - 2.2676e-06) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_lms.py",
        "range": {
          "start": { "row": 67, "column": 4 },
          "end": { "row": 67, "column": 4 }
        }
      }
    }
  ],
  [
    "460",
    {
      "pageContent": "def test_full_loop_device(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps, device=torch_device)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 1006.388) < 1e-2\n        assert abs(result_mean.item() - 1.31) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_lms.py",
        "range": {
          "start": { "row": 91, "column": 4 },
          "end": { "row": 91, "column": 4 }
        }
      }
    }
  ],
  [
    "461",
    {
      "pageContent": "class EulerDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (EulerDiscreteScheduler,)\n    num_inference_steps = 10\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [10, 50, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.00001, 0.0001, 0.001], [0.0002, 0.002, 0.02]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"scaled_linear\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_m",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "462",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler.py",
        "range": {
          "start": { "row": 12, "column": 4 },
          "end": { "row": 12, "column": 4 }
        }
      }
    }
  ],
  [
    "463",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 10.0807) < 1e-2\n        assert abs(result_mean.item() - 0.0131) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "464",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 0.0002) < 1e-2\n        assert abs(result_mean.item() - 2.2676e-06) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler.py",
        "range": {
          "start": { "row": 66, "column": 4 },
          "end": { "row": 66, "column": 4 }
        }
      }
    }
  ],
  [
    "465",
    {
      "pageContent": "def test_full_loop_device(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps, device=torch_device)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for t in scheduler.timesteps:\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 10.0807) < 1e-2\n        assert abs(result_mean.item() - 0.0131) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "466",
    {
      "pageContent": "class DPMSolverSinglestepSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DPMSolverSinglestepScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 25),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"prediction_type\": \"epsilon\",\n            \"thresholding\": False,\n            \"sample_max_value\": 1.0,\n            \"algorithm_type\": \"dpmsolver++\",\n            \"solver_type\": \"midpoint\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_sched",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 14, "column": 0 },
          "end": { "row": 14, "column": 0 }
        }
      }
    }
  ],
  [
    "467",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"prediction_type\": \"epsilon\",\n            \"thresholding\": False,\n            \"sample_max_value\": 1.0,\n            \"algorithm_type\": \"dpmsolver++\",\n            \"solver_type\": \"midpoint\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 18, "column": 4 },
          "end": { "row": 18, "column": 4 }
        }
      }
    }
  ],
  [
    "468",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output, new_output = sample, sample\n            for t in range(time_step, time_step + scheduler.config.solver_order + 1):\n                output = scheduler.step(residual, t, output, **kwargs).prev_sample\n                new_output = new_scheduler.step(residual, t, new_output, **kwargs).prev_sample\n\n                assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 35, "column": 4 },
          "end": { "row": 35, "column": 4 }
        }
      }
    }
  ],
  [
    "469",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n\n            # copy over dummy past residuals (must be after setting timesteps)\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                # copy over dummy past residuals\n                new_scheduler.set_timesteps(num_inference_steps)\n\n                # copy over dummy past residual (must be after setting timesteps)\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 66, "column": 4 },
          "end": { "row": 66, "column": 4 }
        }
      }
    }
  ],
  [
    "470",
    {
      "pageContent": "def full_loop(self, scheduler=None, **config):\n        if scheduler is None:\n            scheduler_class = self.scheduler_classes[0]\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 95, "column": 4 },
          "end": { "row": 95, "column": 4 }
        }
      }
    }
  ],
  [
    "471",
    {
      "pageContent": "def test_switch(self):\n        # make sure that iterating over schedulers with same config names gives same results\n        # for defaults\n        scheduler = DPMSolverSinglestepScheduler(**self.get_scheduler_config())\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.2791) < 1e-3\n\n        scheduler = DEISMultistepScheduler.from_config(scheduler.config)\n        scheduler = DPMSolverMultistepScheduler.from_config(scheduler.config)\n        scheduler = UniPCMultistepScheduler.from_config(scheduler.config)\n        scheduler = DPMSolverSinglestepScheduler.from_config(scheduler.config)\n\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.2791) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 120, "column": 4 },
          "end": { "row": 120, "column": 4 }
        }
      }
    }
  ],
  [
    "472",
    {
      "pageContent": "def test_thresholding(self):\n        self.check_over_configs(thresholding=False)\n        for order in [1, 2, 3]:\n            for solver_type in [\"midpoint\", \"heun\"]:\n                for threshold in [0.5, 1.0, 2.0]:\n                    for prediction_type in [\"epsilon\", \"sample\"]:\n                        self.check_over_configs(\n                            thresholding=True,\n                            prediction_type=prediction_type,\n                            sample_max_value=threshold,\n                            algorithm_type=\"dpmsolver++\",\n                            solver_order=order,\n                            solver_type=solver_type,\n                        )",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 139, "column": 4 },
          "end": { "row": 139, "column": 4 }
        }
      }
    }
  ],
  [
    "473",
    {
      "pageContent": "def test_solver_order_and_type(self):\n        for algorithm_type in [\"dpmsolver\", \"dpmsolver++\"]:\n            for solver_type in [\"midpoint\", \"heun\"]:\n                for order in [1, 2, 3]:\n                    for prediction_type in [\"epsilon\", \"sample\"]:\n                        self.check_over_configs(\n                            solver_order=order,\n                            solver_type=solver_type,\n                            prediction_type=prediction_type,\n                            algorithm_type=algorithm_type,\n                        )\n                        sample = self.full_loop(\n                            solver_order=order,\n                            solver_type=solver_type,\n                            prediction_type=prediction_type,\n                            algorithm_type=algorithm_type,\n                        )\n                        assert not torch.isnan(sample).any(), \"Samples have nan numbers\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 158, "column": 4 },
          "end": { "row": 158, "column": 4 }
        }
      }
    }
  ],
  [
    "474",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.2791) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 185, "column": 4 },
          "end": { "row": 185, "column": 4 }
        }
      }
    }
  ],
  [
    "475",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.1453) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 191, "column": 4 },
          "end": { "row": 191, "column": 4 }
        }
      }
    }
  ],
  [
    "476",
    {
      "pageContent": "def test_fp16_support(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(thresholding=True, dynamic_thresholding_ratio=0)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter.half()\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        assert sample.dtype == torch.float16",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_single.py",
        "range": {
          "start": { "row": 197, "column": 4 },
          "end": { "row": 197, "column": 4 }
        }
      }
    }
  ],
  [
    "477",
    {
      "pageContent": "class EulerAncestralDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (EulerAncestralDiscreteScheduler,)\n    num_inference_steps = 10\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [10, 50, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.00001, 0.0001, 0.001], [0.0002, 0.002, 0.02]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"scaled_linear\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample =",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler_ancestral.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "478",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler_ancestral.py",
        "range": {
          "start": { "row": 12, "column": 4 },
          "end": { "row": 12, "column": 4 }
        }
      }
    }
  ],
  [
    "479",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 152.3192) < 1e-2\n        assert abs(result_mean.item() - 0.1983) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler_ancestral.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "480",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 108.4439) < 1e-2\n        assert abs(result_mean.item() - 0.1412) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler_ancestral.py",
        "range": {
          "start": { "row": 66, "column": 4 },
          "end": { "row": 66, "column": 4 }
        }
      }
    }
  ],
  [
    "481",
    {
      "pageContent": "def test_full_loop_device(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps, device=torch_device)\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for t in scheduler.timesteps:\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 152.3192) < 1e-2\n        assert abs(result_mean.item() - 0.1983) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_euler_ancestral.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "482",
    {
      "pageContent": "class VQDiffusionSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (VQDiffusionScheduler,)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_vec_classes\": 4097,\n            \"num_train_timesteps\": 100,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def dummy_sample(self, num_vec_classes):\n        batch_size = 4\n        height = 8\n        width = 8\n\n        sample = torch.randint(0, num_vec_classes, (batch_size, height * width))\n\n        return sample\n\n    @property\n    def dummy_sample_deter(self):\n        assert False\n\n    def dummy_model(self, num_vec_classes):\n        def model(sample, t, *args):\n            batch_size, num_latent_pixels = sample.shape\n            logits = torch.rand((batch_size, num_vec_classes - 1, num_latent_pixels))\n            return_value = F.log_softmax(logits.double(), dim=1).float()\n            return return_value\n\n        return model\n\n    def test_timesteps(self):\n        for timesteps in [2, 5, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_num_vec_classes(self):\n        for num_vec_classes in [5, 100, 1000, 4000]:\n            self.check_over_configs(num_vec_classes=num_vec_classes)\n\n    def test_time_indices(self):\n        for t in [0, 50, 99]:\n            self.check_over_forward(time_step=t)\n\n    def test_add_noise_device(self):\n        pass",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_vq_diffusion.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "483",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_vec_classes\": 4097,\n            \"num_train_timesteps\": 100,\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_vq_diffusion.py",
        "range": {
          "start": { "row": 11, "column": 4 },
          "end": { "row": 11, "column": 4 }
        }
      }
    }
  ],
  [
    "484",
    {
      "pageContent": "def dummy_sample(self, num_vec_classes):\n        batch_size = 4\n        height = 8\n        width = 8\n\n        sample = torch.randint(0, num_vec_classes, (batch_size, height * width))\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_vq_diffusion.py",
        "range": {
          "start": { "row": 20, "column": 4 },
          "end": { "row": 20, "column": 4 }
        }
      }
    }
  ],
  [
    "485",
    {
      "pageContent": "def dummy_model(self, num_vec_classes):\n        def model(sample, t, *args):\n            batch_size, num_latent_pixels = sample.shape\n            logits = torch.rand((batch_size, num_vec_classes - 1, num_latent_pixels))\n            return_value = F.log_softmax(logits.double(), dim=1).float()\n            return return_value\n\n        return model",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_vq_diffusion.py",
        "range": {
          "start": { "row": 33, "column": 4 },
          "end": { "row": 33, "column": 4 }
        }
      }
    }
  ],
  [
    "486",
    {
      "pageContent": "class KDPM2DiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (KDPM2DiscreteScheduler,)\n    num_inference_steps = 10\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [10, 50, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.00001, 0.0001, 0.001], [0.0002, 0.002, 0.02]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"scaled_linear\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_full_loop_with_v_prediction(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_mode",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_discrete.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "487",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_discrete.py",
        "range": {
          "start": { "row": 12, "column": 4 },
          "end": { "row": 12, "column": 4 }
        }
      }
    }
  ],
  [
    "488",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        if torch_device in [\"cpu\", \"mps\"]:\n            assert abs(result_sum.item() - 4.6934e-07) < 1e-2\n            assert abs(result_mean.item() - 6.1112e-10) < 1e-3\n        else:\n            # CUDA\n            assert abs(result_sum.item() - 4.693428650170972e-07) < 1e-2\n            assert abs(result_mean.item() - 0.0002) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_discrete.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "489",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        if torch_device == \"mps\":\n            return\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        if torch_device in [\"cpu\", \"mps\"]:\n            assert abs(result_sum.item() - 20.4125) < 1e-2\n            assert abs(result_mean.item() - 0.0266) < 1e-3\n        else:\n            # CUDA\n            assert abs(result_sum.item() - 20.4125) < 1e-2\n            assert abs(result_mean.item() - 0.0266) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_discrete.py",
        "range": {
          "start": { "row": 69, "column": 4 },
          "end": { "row": 69, "column": 4 }
        }
      }
    }
  ],
  [
    "490",
    {
      "pageContent": "def test_full_loop_device(self):\n        if torch_device == \"mps\":\n            return\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps, device=torch_device)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter.to(torch_device) * scheduler.init_noise_sigma\n\n        for t in scheduler.timesteps:\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        if str(torch_device).startswith(\"cpu\"):\n            # The following sum varies between 148 and 156 on mps. Why?\n            assert abs(result_sum.item() - 20.4125) < 1e-2\n            assert abs(result_mean.item() - 0.0266) < 1e-3\n        else:\n            # CUDA\n            assert abs(result_sum.item() - 20.4125) < 1e-2\n            assert abs(result_mean.item() - 0.0266) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_discrete.py",
        "range": {
          "start": { "row": 101, "column": 4 },
          "end": { "row": 101, "column": 4 }
        }
      }
    }
  ],
  [
    "491",
    {
      "pageContent": "class HeunDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (HeunDiscreteScheduler,)\n    num_inference_steps = 10\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [10, 50, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.00001, 0.0001, 0.001], [0.0002, 0.002, 0.02]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"scaled_linear\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_out",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_heun.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "492",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_heun.py",
        "range": {
          "start": { "row": 12, "column": 4 },
          "end": { "row": 12, "column": 4 }
        }
      }
    }
  ],
  [
    "493",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        if torch_device in [\"cpu\", \"mps\"]:\n            assert abs(result_sum.item() - 0.1233) < 1e-2\n            assert abs(result_mean.item() - 0.0002) < 1e-3\n        else:\n            # CUDA\n            assert abs(result_sum.item() - 0.1233) < 1e-2\n            assert abs(result_mean.item() - 0.0002) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_heun.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "494",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        if torch_device in [\"cpu\", \"mps\"]:\n            assert abs(result_sum.item() - 4.6934e-07) < 1e-2\n            assert abs(result_mean.item() - 6.1112e-10) < 1e-3\n        else:\n            # CUDA\n            assert abs(result_sum.item() - 4.693428650170972e-07) < 1e-2\n            assert abs(result_mean.item() - 0.0002) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_heun.py",
        "range": {
          "start": { "row": 69, "column": 4 },
          "end": { "row": 69, "column": 4 }
        }
      }
    }
  ],
  [
    "495",
    {
      "pageContent": "def test_full_loop_device(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps, device=torch_device)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter.to(torch_device) * scheduler.init_noise_sigma\n\n        for t in scheduler.timesteps:\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        if str(torch_device).startswith(\"cpu\"):\n            # The following sum varies between 148 and 156 on mps. Why?\n            assert abs(result_sum.item() - 0.1233) < 1e-2\n            assert abs(result_mean.item() - 0.0002) < 1e-3\n        elif str(torch_device).startswith(\"mps\"):\n            # Larger tolerance on mps\n            assert abs(result_mean.item() - 0.0002) < 1e-2\n        else:\n            # CUDA\n            assert abs(result_sum.item() - 0.1233) < 1e-2\n            assert abs(result_mean.item() - 0.0002) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_heun.py",
        "range": {
          "start": { "row": 99, "column": 4 },
          "end": { "row": 99, "column": 4 }
        }
      }
    }
  ],
  [
    "496",
    {
      "pageContent": "class KDPM2AncestralDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (KDPM2AncestralDiscreteScheduler,)\n    num_inference_steps = 10\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [10, 50, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.00001, 0.0001, 0.001], [0.0002, 0.002, 0.02]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"scaled_linear\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_full_loop_no_noise(self):\n        if torch_device == \"mps\":\n            return\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = schedu",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_ancestral.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "497",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_ancestral.py",
        "range": {
          "start": { "row": 12, "column": 4 },
          "end": { "row": 12, "column": 4 }
        }
      }
    }
  ],
  [
    "498",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        if torch_device == \"mps\":\n            return\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 13849.3877) < 1e-2\n        assert abs(result_mean.item() - 18.0331) < 5e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_ancestral.py",
        "range": {
          "start": { "row": 35, "column": 4 },
          "end": { "row": 35, "column": 4 }
        }
      }
    }
  ],
  [
    "499",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        if torch_device == \"mps\":\n            return\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        generator = torch.manual_seed(0)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 328.9970) < 1e-2\n        assert abs(result_mean.item() - 0.4284) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_ancestral.py",
        "range": {
          "start": { "row": 68, "column": 4 },
          "end": { "row": 68, "column": 4 }
        }
      }
    }
  ],
  [
    "500",
    {
      "pageContent": "def test_full_loop_device(self):\n        if torch_device == \"mps\":\n            return\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps, device=torch_device)\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter.to(torch_device) * scheduler.init_noise_sigma\n\n        for t in scheduler.timesteps:\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 13849.3818) < 1e-1\n        assert abs(result_mean.item() - 18.0331) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_kdpm2_ancestral.py",
        "range": {
          "start": { "row": 97, "column": 4 },
          "end": { "row": 97, "column": 4 }
        }
      }
    }
  ],
  [
    "501",
    {
      "pageContent": "class UniPCMultistepSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (UniPCMultistepScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 25),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"solver_type\": \"bh1\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.co",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 14, "column": 0 },
          "end": { "row": 14, "column": 0 }
        }
      }
    }
  ],
  [
    "502",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"solver_type\": \"bh1\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 18, "column": 4 },
          "end": { "row": 18, "column": 4 }
        }
      }
    }
  ],
  [
    "503",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output, new_output = sample, sample\n            for t in range(time_step, time_step + scheduler.config.solver_order + 1):\n                output = scheduler.step(residual, t, output, **kwargs).prev_sample\n                new_output = new_scheduler.step(residual, t, new_output, **kwargs).prev_sample\n\n                assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 31, "column": 4 },
          "end": { "row": 31, "column": 4 }
        }
      }
    }
  ],
  [
    "504",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n\n            # copy over dummy past residuals (must be after setting timesteps)\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                # copy over dummy past residuals\n                new_scheduler.set_timesteps(num_inference_steps)\n\n                # copy over dummy past residual (must be after setting timesteps)\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 59, "column": 4 },
          "end": { "row": 59, "column": 4 }
        }
      }
    }
  ],
  [
    "505",
    {
      "pageContent": "def full_loop(self, scheduler=None, **config):\n        if scheduler is None:\n            scheduler_class = self.scheduler_classes[0]\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 88, "column": 4 },
          "end": { "row": 88, "column": 4 }
        }
      }
    }
  ],
  [
    "506",
    {
      "pageContent": "def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            # copy over dummy past residuals (must be done after set_timesteps)\n            dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            time_step_0 = scheduler.timesteps[5]\n            time_step_1 = scheduler.timesteps[6]\n\n            output_0 = scheduler.step(residual, time_step_0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step(residual, time_step_1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 109, "column": 4 },
          "end": { "row": 109, "column": 4 }
        }
      }
    }
  ],
  [
    "507",
    {
      "pageContent": "def test_switch(self):\n        # make sure that iterating over schedulers with same config names gives same results\n        # for defaults\n        scheduler = UniPCMultistepScheduler(**self.get_scheduler_config())\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.2521) < 1e-3\n\n        scheduler = DPMSolverSinglestepScheduler.from_config(scheduler.config)\n        scheduler = DEISMultistepScheduler.from_config(scheduler.config)\n        scheduler = DPMSolverMultistepScheduler.from_config(scheduler.config)\n        scheduler = UniPCMultistepScheduler.from_config(scheduler.config)\n\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.2521) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 139, "column": 4 },
          "end": { "row": 139, "column": 4 }
        }
      }
    }
  ],
  [
    "508",
    {
      "pageContent": "def test_thresholding(self):\n        self.check_over_configs(thresholding=False)\n        for order in [1, 2, 3]:\n            for solver_type in [\"bh1\", \"bh2\"]:\n                for threshold in [0.5, 1.0, 2.0]:\n                    for prediction_type in [\"epsilon\", \"sample\"]:\n                        self.check_over_configs(\n                            thresholding=True,\n                            prediction_type=prediction_type,\n                            sample_max_value=threshold,\n                            solver_order=order,\n                            solver_type=solver_type,\n                        )",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 162, "column": 4 },
          "end": { "row": 162, "column": 4 }
        }
      }
    }
  ],
  [
    "509",
    {
      "pageContent": "def test_solver_order_and_type(self):\n        for solver_type in [\"bh1\", \"bh2\"]:\n            for order in [1, 2, 3]:\n                for prediction_type in [\"epsilon\", \"sample\"]:\n                    self.check_over_configs(\n                        solver_order=order,\n                        solver_type=solver_type,\n                        prediction_type=prediction_type,\n                    )\n                    sample = self.full_loop(\n                        solver_order=order,\n                        solver_type=solver_type,\n                        prediction_type=prediction_type,\n                    )\n                    assert not torch.isnan(sample).any(), \"Samples have nan numbers\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 180, "column": 4 },
          "end": { "row": 180, "column": 4 }
        }
      }
    }
  ],
  [
    "510",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.2521) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 204, "column": 4 },
          "end": { "row": 204, "column": 4 }
        }
      }
    }
  ],
  [
    "511",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.1096) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 210, "column": 4 },
          "end": { "row": 210, "column": 4 }
        }
      }
    }
  ],
  [
    "512",
    {
      "pageContent": "def test_fp16_support(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(thresholding=True, dynamic_thresholding_ratio=0)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter.half()\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        assert sample.dtype == torch.float16",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unipc.py",
        "range": {
          "start": { "row": 216, "column": 4 },
          "end": { "row": 216, "column": 4 }
        }
      }
    }
  ],
  [
    "513",
    {
      "pageContent": "class DPMSolverMultistepSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DPMSolverMultistepScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 25),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"prediction_type\": \"epsilon\",\n            \"thresholding\": False,\n            \"sample_max_value\": 1.0,\n            \"algorithm_type\": \"dpmsolver++\",\n            \"solver_type\": \"midpoint\",\n            \"lower_order_final\": False,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 14, "column": 0 },
          "end": { "row": 14, "column": 0 }
        }
      }
    }
  ],
  [
    "514",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"prediction_type\": \"epsilon\",\n            \"thresholding\": False,\n            \"sample_max_value\": 1.0,\n            \"algorithm_type\": \"dpmsolver++\",\n            \"solver_type\": \"midpoint\",\n            \"lower_order_final\": False,\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 18, "column": 4 },
          "end": { "row": 18, "column": 4 }
        }
      }
    }
  ],
  [
    "515",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output, new_output = sample, sample\n            for t in range(time_step, time_step + scheduler.config.solver_order + 1):\n                output = scheduler.step(residual, t, output, **kwargs).prev_sample\n                new_output = new_scheduler.step(residual, t, new_output, **kwargs).prev_sample\n\n                assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 36, "column": 4 },
          "end": { "row": 36, "column": 4 }
        }
      }
    }
  ],
  [
    "516",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n\n            # copy over dummy past residuals (must be after setting timesteps)\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                # copy over dummy past residuals\n                new_scheduler.set_timesteps(num_inference_steps)\n\n                # copy over dummy past residual (must be after setting timesteps)\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 67, "column": 4 },
          "end": { "row": 67, "column": 4 }
        }
      }
    }
  ],
  [
    "517",
    {
      "pageContent": "def full_loop(self, scheduler=None, **config):\n        if scheduler is None:\n            scheduler_class = self.scheduler_classes[0]\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 96, "column": 4 },
          "end": { "row": 96, "column": 4 }
        }
      }
    }
  ],
  [
    "518",
    {
      "pageContent": "def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            # copy over dummy past residuals (must be done after set_timesteps)\n            dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            time_step_0 = scheduler.timesteps[5]\n            time_step_1 = scheduler.timesteps[6]\n\n            output_0 = scheduler.step(residual, time_step_0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step(residual, time_step_1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 113, "column": 4 },
          "end": { "row": 113, "column": 4 }
        }
      }
    }
  ],
  [
    "519",
    {
      "pageContent": "def test_thresholding(self):\n        self.check_over_configs(thresholding=False)\n        for order in [1, 2, 3]:\n            for solver_type in [\"midpoint\", \"heun\"]:\n                for threshold in [0.5, 1.0, 2.0]:\n                    for prediction_type in [\"epsilon\", \"sample\"]:\n                        self.check_over_configs(\n                            thresholding=True,\n                            prediction_type=prediction_type,\n                            sample_max_value=threshold,\n                            algorithm_type=\"dpmsolver++\",\n                            solver_order=order,\n                            solver_type=solver_type,\n                        )",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "520",
    {
      "pageContent": "def test_solver_order_and_type(self):\n        for algorithm_type in [\"dpmsolver\", \"dpmsolver++\"]:\n            for solver_type in [\"midpoint\", \"heun\"]:\n                for order in [1, 2, 3]:\n                    for prediction_type in [\"epsilon\", \"sample\"]:\n                        self.check_over_configs(\n                            solver_order=order,\n                            solver_type=solver_type,\n                            prediction_type=prediction_type,\n                            algorithm_type=algorithm_type,\n                        )\n                        sample = self.full_loop(\n                            solver_order=order,\n                            solver_type=solver_type,\n                            prediction_type=prediction_type,\n                            algorithm_type=algorithm_type,\n                        )\n                        assert not torch.isnan(sample).any(), \"Samples have nan numbers\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 166, "column": 4 },
          "end": { "row": 166, "column": 4 }
        }
      }
    }
  ],
  [
    "521",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.3301) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 193, "column": 4 },
          "end": { "row": 193, "column": 4 }
        }
      }
    }
  ],
  [
    "522",
    {
      "pageContent": "def test_full_loop_no_noise_thres(self):\n        sample = self.full_loop(thresholding=True, dynamic_thresholding_ratio=0.87, sample_max_value=0.5)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.6405) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 199, "column": 4 },
          "end": { "row": 199, "column": 4 }
        }
      }
    }
  ],
  [
    "523",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.2251) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 205, "column": 4 },
          "end": { "row": 205, "column": 4 }
        }
      }
    }
  ],
  [
    "524",
    {
      "pageContent": "def test_switch(self):\n        # make sure that iterating over schedulers with same config names gives same results\n        # for defaults\n        scheduler = DPMSolverMultistepScheduler(**self.get_scheduler_config())\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.3301) < 1e-3\n\n        scheduler = DPMSolverSinglestepScheduler.from_config(scheduler.config)\n        scheduler = UniPCMultistepScheduler.from_config(scheduler.config)\n        scheduler = DEISMultistepScheduler.from_config(scheduler.config)\n        scheduler = DPMSolverMultistepScheduler.from_config(scheduler.config)\n\n        sample = self.full_loop(scheduler=scheduler)\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 0.3301) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 211, "column": 4 },
          "end": { "row": 211, "column": 4 }
        }
      }
    }
  ],
  [
    "525",
    {
      "pageContent": "def test_fp16_support(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(thresholding=True, dynamic_thresholding_ratio=0)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter.half()\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        assert sample.dtype == torch.float16",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_dpm_multi.py",
        "range": {
          "start": { "row": 230, "column": 4 },
          "end": { "row": 230, "column": 4 }
        }
      }
    }
  ],
  [
    "526",
    {
      "pageContent": "class UnCLIPSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (UnCLIPScheduler,)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"variance_type\": \"fixed_small_log\",\n            \"clip_sample\": True,\n            \"clip_sample_range\": 1.0,\n            \"prediction_type\": \"epsilon\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [1, 5, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_variance_type(self):\n        for variance in [\"fixed_small_log\", \"learned_range\"]:\n            self.check_over_configs(variance_type=variance)\n\n    def test_clip_sample(self):\n        for clip_sample in [True, False]:\n            self.check_over_configs(clip_sample=clip_sample)\n\n    def test_clip_sample_range(self):\n        for clip_sample_range in [1, 5, 10, 20]:\n            self.check_over_configs(clip_sample_range=clip_sample_range)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"sample\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_time_indices(self):\n        for time_step in [0, 500, 999]:\n            for prev_timestep in [None, 5, 100, 250, 500, 750]:\n                if prev_timestep is not None and prev_timestep >= time_step:\n                    continue\n\n                self.check_over_forward(time_step=time_step, prev_timestep=prev_timestep)\n\n    def test_variance_fixed_small_log(self):\n        scheduler_class = self.schedule",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unclip.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "527",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"variance_type\": \"fixed_small_log\",\n            \"clip_sample\": True,\n            \"clip_sample_range\": 1.0,\n            \"prediction_type\": \"epsilon\",\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unclip.py",
        "range": {
          "start": { "row": 11, "column": 4 },
          "end": { "row": 11, "column": 4 }
        }
      }
    }
  ],
  [
    "528",
    {
      "pageContent": "def test_time_indices(self):\n        for time_step in [0, 500, 999]:\n            for prev_timestep in [None, 5, 100, 250, 500, 750]:\n                if prev_timestep is not None and prev_timestep >= time_step:\n                    continue\n\n                self.check_over_forward(time_step=time_step, prev_timestep=prev_timestep)",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unclip.py",
        "range": {
          "start": { "row": 43, "column": 4 },
          "end": { "row": 43, "column": 4 }
        }
      }
    }
  ],
  [
    "529",
    {
      "pageContent": "def test_variance_fixed_small_log(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(variance_type=\"fixed_small_log\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        assert torch.sum(torch.abs(scheduler._get_variance(0) - 1.0000e-10)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(487) - 0.0549625)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(999) - 0.9994987)) < 1e-5",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unclip.py",
        "range": {
          "start": { "row": 51, "column": 4 },
          "end": { "row": 51, "column": 4 }
        }
      }
    }
  ],
  [
    "530",
    {
      "pageContent": "def test_variance_learned_range(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(variance_type=\"learned_range\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        predicted_variance = 0.5\n\n        assert scheduler._get_variance(1, predicted_variance=predicted_variance) - -10.1712790 < 1e-5\n        assert scheduler._get_variance(487, predicted_variance=predicted_variance) - -5.7998052 < 1e-5\n        assert scheduler._get_variance(999, predicted_variance=predicted_variance) - -0.0010011 < 1e-5",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unclip.py",
        "range": {
          "start": { "row": 60, "column": 4 },
          "end": { "row": 60, "column": 4 }
        }
      }
    }
  ],
  [
    "531",
    {
      "pageContent": "def test_full_loop(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        timesteps = scheduler.timesteps\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        generator = torch.manual_seed(0)\n\n        for i, t in enumerate(timesteps):\n            # 1. predict noise residual\n            residual = model(sample, t)\n\n            # 2. predict previous mean of sample x_t-1\n            pred_prev_sample = scheduler.step(residual, t, sample, generator=generator).prev_sample\n\n            sample = pred_prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 252.2682495) < 1e-2\n        assert abs(result_mean.item() - 0.3284743) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unclip.py",
        "range": {
          "start": { "row": 71, "column": 4 },
          "end": { "row": 71, "column": 4 }
        }
      }
    }
  ],
  [
    "532",
    {
      "pageContent": "def test_full_loop_skip_timesteps(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(25)\n\n        timesteps = scheduler.timesteps\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        generator = torch.manual_seed(0)\n\n        for i, t in enumerate(timesteps):\n            # 1. predict noise residual\n            residual = model(sample, t)\n\n            if i + 1 == timesteps.shape[0]:\n                prev_timestep = None\n            else:\n                prev_timestep = timesteps[i + 1]\n\n            # 2. predict previous mean of sample x_t-1\n            pred_prev_sample = scheduler.step(\n                residual, t, sample, prev_timestep=prev_timestep, generator=generator\n            ).prev_sample\n\n            sample = pred_prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 258.2044983) < 1e-2\n        assert abs(result_mean.item() - 0.3362038) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_unclip.py",
        "range": {
          "start": { "row": 97, "column": 4 },
          "end": { "row": 97, "column": 4 }
        }
      }
    }
  ],
  [
    "533",
    {
      "pageContent": "class DDPMSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DDPMScheduler,)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"variance_type\": \"fixed_small\",\n            \"clip_sample\": True,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [1, 5, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.0001, 0.001, 0.01, 0.1], [0.002, 0.02, 0.2, 2]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"squaredcos_cap_v2\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_variance_type(self):\n        for variance in [\"fixed_small\", \"fixed_large\", \"other\"]:\n            self.check_over_configs(variance_type=variance)\n\n    def test_clip_sample(self):\n        for clip_sample in [True, False]:\n            self.check_over_configs(clip_sample=clip_sample)\n\n    def test_thresholding(self):\n        self.check_over_configs(thresholding=False)\n        for threshold in [0.5, 1.0, 2.0]:\n            for prediction_type in [\"epsilon\", \"sample\", \"v_prediction\"]:\n                self.check_over_configs(\n                    thresholding=True,\n                    prediction_type=prediction_type,\n                    sample_max_value=thresh",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddpm.py",
        "range": {
          "start": { "row": 7, "column": 0 },
          "end": { "row": 7, "column": 0 }
        }
      }
    }
  ],
  [
    "534",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"variance_type\": \"fixed_small\",\n            \"clip_sample\": True,\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddpm.py",
        "range": {
          "start": { "row": 10, "column": 4 },
          "end": { "row": 10, "column": 4 }
        }
      }
    }
  ],
  [
    "535",
    {
      "pageContent": "def test_thresholding(self):\n        self.check_over_configs(thresholding=False)\n        for threshold in [0.5, 1.0, 2.0]:\n            for prediction_type in [\"epsilon\", \"sample\", \"v_prediction\"]:\n                self.check_over_configs(\n                    thresholding=True,\n                    prediction_type=prediction_type,\n                    sample_max_value=threshold,\n                )",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddpm.py",
        "range": {
          "start": { "row": 43, "column": 4 },
          "end": { "row": 43, "column": 4 }
        }
      }
    }
  ],
  [
    "536",
    {
      "pageContent": "def test_variance(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        assert torch.sum(torch.abs(scheduler._get_variance(0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(487) - 0.00979)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(999) - 0.02)) < 1e-5",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddpm.py",
        "range": {
          "start": { "row": 61, "column": 4 },
          "end": { "row": 61, "column": 4 }
        }
      }
    }
  ],
  [
    "537",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_trained_timesteps = len(scheduler)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        generator = torch.manual_seed(0)\n\n        for t in reversed(range(num_trained_timesteps)):\n            # 1. predict noise residual\n            residual = model(sample, t)\n\n            # 2. predict previous mean of sample x_t-1\n            pred_prev_sample = scheduler.step(residual, t, sample, generator=generator).prev_sample\n\n            # if t > 0:\n            #     noise = self.dummy_sample_deter\n            #     variance = scheduler.get_variance(t) ** (0.5) * noise\n            #\n            # sample = pred_prev_sample + variance\n            sample = pred_prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 258.9606) < 1e-2\n        assert abs(result_mean.item() - 0.3372) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddpm.py",
        "range": {
          "start": { "row": 70, "column": 4 },
          "end": { "row": 70, "column": 4 }
        }
      }
    }
  ],
  [
    "538",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(prediction_type=\"v_prediction\")\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_trained_timesteps = len(scheduler)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        generator = torch.manual_seed(0)\n\n        for t in reversed(range(num_trained_timesteps)):\n            # 1. predict noise residual\n            residual = model(sample, t)\n\n            # 2. predict previous mean of sample x_t-1\n            pred_prev_sample = scheduler.step(residual, t, sample, generator=generator).prev_sample\n\n            # if t > 0:\n            #     noise = self.dummy_sample_deter\n            #     variance = scheduler.get_variance(t) ** (0.5) * noise\n            #\n            # sample = pred_prev_sample + variance\n            sample = pred_prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 202.0296) < 1e-2\n        assert abs(result_mean.item() - 0.2631) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddpm.py",
        "range": {
          "start": { "row": 101, "column": 4 },
          "end": { "row": 101, "column": 4 }
        }
      }
    }
  ],
  [
    "539",
    {
      "pageContent": "class DDIMSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DDIMScheduler,)\n    forward_default_kwargs = ((\"eta\", 0.0), (\"num_inference_steps\", 50))\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"clip_sample\": True,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def full_loop(self, **config):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps, eta = 10, 0.0\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n\n        scheduler.set_timesteps(num_inference_steps)\n\n        for t in scheduler.timesteps:\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample, eta).prev_sample\n\n        return sample\n\n    def test_timesteps(self):\n        for timesteps in [100, 500, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_steps_offset(self):\n        for steps_offset in [0, 1]:\n            self.check_over_configs(steps_offset=steps_offset)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(steps_offset=1)\n        scheduler = scheduler_class(**scheduler_config)\n        scheduler.set_timesteps(5)\n        assert torch.equal(scheduler.timesteps, torch.LongTensor([801, 601, 401, 201, 1",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 7, "column": 0 },
          "end": { "row": 7, "column": 0 }
        }
      }
    }
  ],
  [
    "540",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"clip_sample\": True,\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 11, "column": 4 },
          "end": { "row": 11, "column": 4 }
        }
      }
    }
  ],
  [
    "541",
    {
      "pageContent": "def full_loop(self, **config):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps, eta = 10, 0.0\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n\n        scheduler.set_timesteps(num_inference_steps)\n\n        for t in scheduler.timesteps:\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample, eta).prev_sample\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 23, "column": 4 },
          "end": { "row": 23, "column": 4 }
        }
      }
    }
  ],
  [
    "542",
    {
      "pageContent": "def test_steps_offset(self):\n        for steps_offset in [0, 1]:\n            self.check_over_configs(steps_offset=steps_offset)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(steps_offset=1)\n        scheduler = scheduler_class(**scheduler_config)\n        scheduler.set_timesteps(5)\n        assert torch.equal(scheduler.timesteps, torch.LongTensor([801, 601, 401, 201, 1]))",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 45, "column": 4 },
          "end": { "row": 45, "column": 4 }
        }
      }
    }
  ],
  [
    "543",
    {
      "pageContent": "def test_thresholding(self):\n        self.check_over_configs(thresholding=False)\n        for threshold in [0.5, 1.0, 2.0]:\n            for prediction_type in [\"epsilon\", \"v_prediction\"]:\n                self.check_over_configs(\n                    thresholding=True,\n                    prediction_type=prediction_type,\n                    sample_max_value=threshold,\n                )",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 71, "column": 4 },
          "end": { "row": 71, "column": 4 }
        }
      }
    }
  ],
  [
    "544",
    {
      "pageContent": "def test_variance(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        assert torch.sum(torch.abs(scheduler._get_variance(0, 0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(420, 400) - 0.14771)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(980, 960) - 0.32460)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(0, 0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(487, 486) - 0.00979)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(999, 998) - 0.02)) < 1e-5",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "545",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 172.0067) < 1e-2\n        assert abs(result_mean.item() - 0.223967) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 105, "column": 4 },
          "end": { "row": 105, "column": 4 }
        }
      }
    }
  ],
  [
    "546",
    {
      "pageContent": "def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 52.5302) < 1e-2\n        assert abs(result_mean.item() - 0.0684) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 114, "column": 4 },
          "end": { "row": 114, "column": 4 }
        }
      }
    }
  ],
  [
    "547",
    {
      "pageContent": "def test_full_loop_with_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=True, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 149.8295) < 1e-2\n        assert abs(result_mean.item() - 0.1951) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 123, "column": 4 },
          "end": { "row": 123, "column": 4 }
        }
      }
    }
  ],
  [
    "548",
    {
      "pageContent": "def test_full_loop_with_no_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=False, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 149.0784) < 1e-2\n        assert abs(result_mean.item() - 0.1941) < 1e-3",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ddim.py",
        "range": {
          "start": { "row": 132, "column": 4 },
          "end": { "row": 132, "column": 4 }
        }
      }
    }
  ],
  [
    "549",
    {
      "pageContent": "class IPNDMSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (IPNDMScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 50),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\"num_train_timesteps\": 1000}\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.ets = dummy_past_residuals[:]\n\n            if time_step is None:\n                time_step = scheduler.timesteps[len(scheduler.timesteps) // 2]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.ets = dummy_past_residuals[:]\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, tim",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ipndm.py",
        "range": {
          "start": { "row": 9, "column": 0 },
          "end": { "row": 9, "column": 0 }
        }
      }
    }
  ],
  [
    "550",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\"num_train_timesteps\": 1000}\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ipndm.py",
        "range": {
          "start": { "row": 13, "column": 4 },
          "end": { "row": 13, "column": 4 }
        }
      }
    }
  ],
  [
    "551",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.ets = dummy_past_residuals[:]\n\n            if time_step is None:\n                time_step = scheduler.timesteps[len(scheduler.timesteps) // 2]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.ets = dummy_past_residuals[:]\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, time_step, sam",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ipndm.py",
        "range": {
          "start": { "row": 18, "column": 4 },
          "end": { "row": 18, "column": 4 }
        }
      }
    }
  ],
  [
    "552",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n\n            # copy over dummy past residuals (must be after setting timesteps)\n            scheduler.ets = dummy_past_residuals[:]\n\n            if time_step is None:\n                time_step = scheduler.timesteps[len(scheduler.timesteps) // 2]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                # copy over dummy past residuals\n                new_scheduler.set_timesteps(num_inference_steps)\n\n                # copy over dummy past residual (must be after setting timesteps)\n                new_scheduler.ets = dummy_past_residuals[:]\n\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step(r",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ipndm.py",
        "range": {
          "start": { "row": 55, "column": 4 },
          "end": { "row": 55, "column": 4 }
        }
      }
    }
  ],
  [
    "553",
    {
      "pageContent": "def full_loop(self, **config):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 10\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        for i, t in enumerate(scheduler.timesteps):\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample).prev_sample\n\n        return sample",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ipndm.py",
        "range": {
          "start": { "row": 92, "column": 4 },
          "end": { "row": 92, "column": 4 }
        }
      }
    }
  ],
  [
    "554",
    {
      "pageContent": "def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            # copy over dummy past residuals (must be done after set_timesteps)\n            dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.1, residual + 0.05]\n            scheduler.ets = dummy_past_residuals[:]\n\n            time_step_0 = scheduler.timesteps[5]\n            time_step_1 = scheduler.timesteps[6]\n\n            output_0 = scheduler.step(residual, time_step_0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step(residual, time_step_1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)\n\n            output_0 = scheduler.step(residual, time_step_0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step(residual, time_step_1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n       ",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ipndm.py",
        "range": {
          "start": { "row": 112, "column": 4 },
          "end": { "row": 112, "column": 4 }
        }
      }
    }
  ],
  [
    "555",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_mean.item() - 2540529) < 10",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_ipndm.py",
        "range": {
          "start": { "row": 156, "column": 4 },
          "end": { "row": 156, "column": 4 }
        }
      }
    }
  ],
  [
    "556",
    {
      "pageContent": "class ScoreSdeVeSchedulerTest(unittest.TestCase):\n    # TODO adapt with class SchedulerCommonTest (scheduler needs Numpy Integration)\n    scheduler_classes = (ScoreSdeVeScheduler,)\n    forward_default_kwargs = ()\n\n    @property\n    def dummy_sample(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        sample = torch.rand((batch_size, num_channels, height, width))\n\n        return sample\n\n    @property\n    def dummy_sample_deter(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        num_elems = batch_size * num_channels * height * width\n        sample = torch.arange(num_elems)\n        sample = sample.reshape(num_channels, height, width, batch_size)\n        sample = sample / num_elems\n        sample = sample.permute(3, 0, 1, 2)\n\n        return sample\n\n    def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 2000,\n            \"snr\": 0.15,\n            \"sigma_min\": 0.01,\n            \"sigma_max\": 1348,\n            \"sampling_eps\": 1e-5,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        for scheduler_class in self.scheduler_classes:\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_score_sde_ve.py",
        "range": {
          "start": { "row": 9, "column": 0 },
          "end": { "row": 9, "column": 0 }
        }
      }
    }
  ],
  [
    "557",
    {
      "pageContent": "def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_score_sde_ve.py",
        "range": {
          "start": { "row": 40, "column": 4 },
          "end": { "row": 40, "column": 4 }
        }
      }
    }
  ],
  [
    "558",
    {
      "pageContent": "def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 2000,\n            \"snr\": 0.15,\n            \"sigma_min\": 0.01,\n            \"sigma_max\": 1348,\n            \"sampling_eps\": 1e-5,\n        }\n\n        config.update(**kwargs)\n        return config",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_score_sde_ve.py",
        "range": {
          "start": { "row": 46, "column": 4 },
          "end": { "row": 46, "column": 4 }
        }
      }
    }
  ],
  [
    "559",
    {
      "pageContent": "def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        for scheduler_class in self.scheduler_classes:\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            output = scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n            new_output = new_scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step_correct(residual, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n            new_output = new_scheduler.step_correct(\n                residual, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler correction are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_score_sde_ve.py",
        "range": {
          "start": { "row": 58, "column": 4 },
          "end": { "row": 58, "column": 4 }
        }
      }
    }
  ],
  [
    "560",
    {
      "pageContent": "def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        kwargs.update(forward_kwargs)\n\n        for scheduler_class in self.scheduler_classes:\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            output = scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n            new_output = new_scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step_correct(residual, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n            new_output = new_scheduler.step_correct(\n                residual, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler correction are not identical\"",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_score_sde_ve.py",
        "range": {
          "start": { "row": 88, "column": 4 },
          "end": { "row": 88, "column": 4 }
        }
      }
    }
  ],
  [
    "561",
    {
      "pageContent": "def test_full_loop_no_noise(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 3\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n\n        scheduler.set_sigmas(num_inference_steps)\n        scheduler.set_timesteps(num_inference_steps)\n        generator = torch.manual_seed(0)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sigma_t = scheduler.sigmas[i]\n\n            for _ in range(scheduler.config.correct_steps):\n                with torch.no_grad():\n                    model_output = model(sample, sigma_t)\n                sample = scheduler.step_correct(model_output, sample, generator=generator, **kwargs).prev_sample\n\n            with torch.no_grad():\n                model_output = model(sample, sigma_t)\n\n            output = scheduler.step_pred(model_output, t, sample, generator=generator, **kwargs)\n            sample, _ = output.prev_sample, output.prev_sample_mean\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert np.isclose(result_sum.item(), 14372758528.0)\n        assert np.isclose(result_mean.item(), 18714530.0)",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_score_sde_ve.py",
        "range": {
          "start": { "row": 131, "column": 4 },
          "end": { "row": 131, "column": 4 }
        }
      }
    }
  ],
  [
    "562",
    {
      "pageContent": "def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            output_0 = scheduler.step_pred(residual, 0, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n            output_1 = scheduler.step_pred(residual, 1, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)",
      "metadata": {
        "source": "tests/schedulers/test_scheduler_score_sde_ve.py",
        "range": {
          "start": { "row": 167, "column": 4 },
          "end": { "row": 167, "column": 4 }
        }
      }
    }
  ],
  [
    "563",
    {
      "pageContent": "class DDIMPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = DDIMPipeline\n    params = UNCONDITIONAL_IMAGE_GENERATION_PARAMS\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\n        \"num_images_per_prompt\",\n        \"latents\",\n        \"callback\",\n        \"callback_steps\",\n    }\n    batch_params = UNCONDITIONAL_IMAGE_GENERATION_BATCH_PARAMS\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        scheduler = DDIMScheduler()\n        components = {\"unet\": unet, \"scheduler\": scheduler}\n        return components\n\n    def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"batch_size\": 1,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_inference(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n        pipe = self.pipeline_class(**components)\n        pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = se",
      "metadata": {
        "source": "tests/pipelines/ddim/test_ddim.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "564",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        scheduler = DDIMScheduler()\n        components = {\"unet\": unet, \"scheduler\": scheduler}\n        return components",
      "metadata": {
        "source": "tests/pipelines/ddim/test_ddim.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "565",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"batch_size\": 1,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/ddim/test_ddim.py",
        "range": {
          "start": { "row": 57, "column": 4 },
          "end": { "row": 57, "column": 4 }
        }
      }
    }
  ],
  [
    "566",
    {
      "pageContent": "def test_inference(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n        pipe = self.pipeline_class(**components)\n        pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        self.assertEqual(image.shape, (1, 32, 32, 3))\n        expected_slice = np.array(\n            [1.000e00, 5.717e-01, 4.717e-01, 1.000e00, 0.000e00, 1.000e00, 3.000e-04, 0.000e00, 9.000e-04]\n        )\n        max_diff = np.abs(image_slice.flatten() - expected_slice).max()\n        self.assertLessEqual(max_diff, 1e-3)",
      "metadata": {
        "source": "tests/pipelines/ddim/test_ddim.py",
        "range": {
          "start": { "row": 70, "column": 4 },
          "end": { "row": 70, "column": 4 }
        }
      }
    }
  ],
  [
    "567",
    {
      "pageContent": "class DDIMPipelineIntegrationTests(unittest.TestCase):\n    def test_inference_cifar10(self):\n        model_id = \"google/ddpm-cifar10-32\"\n\n        unet = UNet2DModel.from_pretrained(model_id)\n        scheduler = DDIMScheduler()\n\n        ddim = DDIMPipeline(unet=unet, scheduler=scheduler)\n        ddim.to(torch_device)\n        ddim.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = ddim(generator=generator, eta=0.0, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.1723, 0.1617, 0.1600, 0.1626, 0.1497, 0.1513, 0.1505, 0.1442, 0.1453])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_inference_ema_bedroom(self):\n        model_id = \"google/ddpm-ema-bedroom-256\"\n\n        unet = UNet2DModel.from_pretrained(model_id)\n        scheduler = DDIMScheduler.from_pretrained(model_id)\n\n        ddpm = DDIMPipeline(unet=unet, scheduler=scheduler)\n        ddpm.to(torch_device)\n        ddpm.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = ddpm(generator=generator, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 256, 256, 3)\n        expected_slice = np.array([0.0060, 0.0201, 0.0344, 0.0024, 0.0018, 0.0002, 0.0022, 0.0000, 0.0069])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/ddim/test_ddim.py",
        "range": {
          "start": { "row": 92, "column": 0 },
          "end": { "row": 92, "column": 0 }
        }
      }
    }
  ],
  [
    "568",
    {
      "pageContent": "class RepaintPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = RePaintPipeline\n    params = IMAGE_INPAINTING_PARAMS - {\"width\", \"height\", \"guidance_scale\"}\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\n        \"latents\",\n        \"num_images_per_prompt\",\n        \"callback\",\n        \"callback_steps\",\n    }\n    batch_params = IMAGE_INPAINTING_BATCH_PARAMS\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        torch.manual_seed(0)\n        unet = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        scheduler = RePaintScheduler()\n        components = {\"unet\": unet, \"scheduler\": scheduler}\n        return components\n\n    def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        image = np.random.RandomState(seed).standard_normal((1, 3, 32, 32))\n        image = torch.from_numpy(image).to(device=device, dtype=torch.float32)\n        mask = (image > 0).to(device=device, dtype=torch.float32)\n        inputs = {\n            \"image\": image,\n            \"mask_image\": mask,\n            \"generator\": generator,\n            \"num_inference_steps\": 5,\n            \"e",
      "metadata": {
        "source": "tests/pipelines/repaint/test_repaint.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "569",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        torch.manual_seed(0)\n        unet = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        scheduler = RePaintScheduler()\n        components = {\"unet\": unet, \"scheduler\": scheduler}\n        return components",
      "metadata": {
        "source": "tests/pipelines/repaint/test_repaint.py",
        "range": {
          "start": { "row": 43, "column": 4 },
          "end": { "row": 43, "column": 4 }
        }
      }
    }
  ],
  [
    "570",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        image = np.random.RandomState(seed).standard_normal((1, 3, 32, 32))\n        image = torch.from_numpy(image).to(device=device, dtype=torch.float32)\n        mask = (image > 0).to(device=device, dtype=torch.float32)\n        inputs = {\n            \"image\": image,\n            \"mask_image\": mask,\n            \"generator\": generator,\n            \"num_inference_steps\": 5,\n            \"eta\": 0.0,\n            \"jump_length\": 2,\n            \"jump_n_sample\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/repaint/test_repaint.py",
        "range": {
          "start": { "row": 59, "column": 4 },
          "end": { "row": 59, "column": 4 }
        }
      }
    }
  ],
  [
    "571",
    {
      "pageContent": "def test_repaint(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = RePaintPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([1.0000, 0.5426, 0.5497, 0.2200, 1.0000, 1.0000, 0.5623, 1.0000, 0.6274])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/repaint/test_repaint.py",
        "range": {
          "start": { "row": 79, "column": 4 },
          "end": { "row": 79, "column": 4 }
        }
      }
    }
  ],
  [
    "572",
    {
      "pageContent": "class RepaintPipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_celebahq(self):\n        original_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/\"\n            \"repaint/celeba_hq_256.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/repaint/mask_256.png\"\n        )\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/\"\n            \"repaint/celeba_hq_256_result.npy\"\n        )\n\n        model_id = \"google/ddpm-ema-celebahq-256\"\n        unet = UNet2DModel.from_pretrained(model_id)\n        scheduler = RePaintScheduler.from_pretrained(model_id)\n\n        repaint = RePaintPipeline(unet=unet, scheduler=scheduler).to(torch_device)\n        repaint.set_progress_bar_config(disable=None)\n        repaint.enable_attention_slicing()\n\n        generator = torch.manual_seed(0)\n        output = repaint(\n            original_image,\n            mask_image,\n            num_inference_steps=250,\n            eta=0.0,\n            jump_length=10,\n            jump_n_sample=10,\n            generator=generator,\n            output_type=\"np\",\n        )\n        image = output.images[0]\n\n        assert image.shape == (256, 256, 3)\n        assert np.abs(expected_image - image).mean() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/repaint/test_repaint.py",
        "range": {
          "start": { "row": 120, "column": 0 },
          "end": { "row": 120, "column": 0 }
        }
      }
    }
  ],
  [
    "573",
    {
      "pageContent": "class AltDiffusionPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = AltDiffusionPipeline\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n\n        # TODO: address the non-deterministic text encoder (fails for save-load tests)\n        # torch.manual_seed(0)\n        # text_encoder_config = RobertaSeriesConfig(\n        #     hidden_size=32,\n        #     project_dim=32,\n        #     intermediate_size=37,\n        #     layer_norm_eps=1e-05,\n        #     num_attention_heads=4,\n        #     num_hidden_layers=5,\n        #     vocab_size=5002,\n ",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "574",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n\n        # TODO: address the non-deterministic text encoder (fails for save-load tests)\n        # torch.manual_seed(0)\n        # text_encoder_config = RobertaSeriesConfig(\n        #     hidden_size=32,\n        #     project_dim=32,\n        #     intermediate_size=37,\n        #     layer_norm_eps=1e-05,\n        #     num_attention_heads=4,\n        #     num_hidden_layers=5,\n        #     vocab_size=5002,\n        # )\n        # text_encoder = RobertaSeriesModelWithTransformation(text_encoder_config)\n\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n      ",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "575",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion.py",
        "range": {
          "start": { "row": 113, "column": 4 },
          "end": { "row": 113, "column": 4 }
        }
      }
    }
  ],
  [
    "576",
    {
      "pageContent": "def test_alt_diffusion_ddim(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        torch.manual_seed(0)\n        text_encoder_config = RobertaSeriesConfig(\n            hidden_size=32,\n            project_dim=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            vocab_size=5002,\n        )\n        # TODO: remove after fixing the non-deterministic text encoder\n        text_encoder = RobertaSeriesModelWithTransformation(text_encoder_config)\n        components[\"text_encoder\"] = text_encoder\n\n        alt_pipe = AltDiffusionPipeline(**components)\n        alt_pipe = alt_pipe.to(device)\n        alt_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = \"A photo of an astronaut\"\n        output = alt_pipe(**inputs)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array(\n            [0.5748162, 0.60447145, 0.48821217, 0.50100636, 0.5431185, 0.45763683, 0.49657696, 0.48132733, 0.47573093]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion.py",
        "range": {
          "start": { "row": 127, "column": 4 },
          "end": { "row": 127, "column": 4 }
        }
      }
    }
  ],
  [
    "577",
    {
      "pageContent": "def test_alt_diffusion_pndm(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        text_encoder_config = RobertaSeriesConfig(\n            hidden_size=32,\n            project_dim=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            vocab_size=5002,\n        )\n        # TODO: remove after fixing the non-deterministic text encoder\n        text_encoder = RobertaSeriesModelWithTransformation(text_encoder_config)\n        components[\"text_encoder\"] = text_encoder\n        alt_pipe = AltDiffusionPipeline(**components)\n        alt_pipe = alt_pipe.to(device)\n        alt_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = alt_pipe(**inputs)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array(\n            [0.51605093, 0.5707241, 0.47365507, 0.50578886, 0.5633877, 0.4642503, 0.5182081, 0.48763484, 0.49084237]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion.py",
        "range": {
          "start": { "row": 162, "column": 4 },
          "end": { "row": 162, "column": 4 }
        }
      }
    }
  ],
  [
    "578",
    {
      "pageContent": "class AltDiffusionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_alt_diffusion(self):\n        # make sure here that pndm scheduler skips prk\n        alt_pipe = AltDiffusionPipeline.from_pretrained(\"BAAI/AltDiffusion\", safety_checker=None)\n        alt_pipe = alt_pipe.to(torch_device)\n        alt_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.manual_seed(0)\n        output = alt_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=20, output_type=\"np\")\n\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.1010, 0.0800, 0.0794, 0.0885, 0.0843, 0.0762, 0.0769, 0.0729, 0.0586])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_alt_diffusion_fast_ddim(self):\n        scheduler = DDIMScheduler.from_pretrained(\"BAAI/AltDiffusion\", subfolder=\"scheduler\")\n\n        alt_pipe = AltDiffusionPipeline.from_pretrained(\"BAAI/AltDiffusion\", scheduler=scheduler, safety_checker=None)\n        alt_pipe = alt_pipe.to(torch_device)\n        alt_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.manual_seed(0)\n\n        output = alt_pipe([prompt], generator=generator, num_inference_steps=2, output_type=\"numpy\")\n       ",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion.py",
        "range": {
          "start": { "row": 199, "column": 0 },
          "end": { "row": 199, "column": 0 }
        }
      }
    }
  ],
  [
    "579",
    {
      "pageContent": "class AltDiffusionImg2ImgPipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    @property\n    def dummy_cond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        return model\n\n    @property\n    def dummy_vae(self):\n        torch.manual_seed(0)\n        model = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        return model\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = RobertaSeriesConfig(\n            hidden_size=32,\n            project_dim=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "580",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 36, "column": 4 },
          "end": { "row": 36, "column": 4 }
        }
      }
    }
  ],
  [
    "581",
    {
      "pageContent": "def test_stable_diffusion_img2img_default_case(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = XLMRobertaTokenizer.from_pretrained(\"hf-internal-testing/tiny-xlm-roberta\")\n        tokenizer.model_max_length = 77\n\n        init_image = self.dummy_image.to(device)\n\n        # make sure here that pndm scheduler skips prk\n        alt_pipe = AltDiffusionImg2ImgPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        alt_pipe = alt_pipe.to(device)\n        alt_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = alt_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            image=init_image,\n        )\n\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = alt_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            image=init_image,\n            return",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 109, "column": 4 },
          "end": { "row": 109, "column": 4 }
        }
      }
    }
  ],
  [
    "582",
    {
      "pageContent": "class AltDiffusionImg2ImgPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_diffusion_img2img_pipeline_default(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/img2img/sketch-mountains-input.jpg\"\n        )\n        init_image = init_image.resize((768, 512))\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/img2img/fantasy_landscape_alt.npy\"\n        )\n\n        model_id = \"BAAI/AltDiffusion\"\n        pipe = AltDiffusionImg2ImgPipeline.from_pretrained(\n            model_id,\n            safety_checker=None,\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        prompt = \"A fantasy landscape, trending on artstation\"\n\n        generator = torch.manual_seed(0)\n        output = pipe(\n            prompt=prompt,\n            image=init_image,\n            strength=0.75,\n            guidance_scale=7.5,\n            generator=generator,\n            output_type=\"np\",\n        )\n        image = output.images[0]\n\n        assert image.shape == (512, 768, 3)\n        # img2img is flaky across GPUs even in fp32, so using MAE here\n        assert np.abs(expected_image - image).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 249, "column": 0 },
          "end": { "row": 249, "column": 0 }
        }
      }
    }
  ],
  [
    "583",
    {
      "pageContent": "class VQDiffusionPipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def num_embed(self):\n        return 12\n\n    @property\n    def num_embeds_ada_norm(self):\n        return 12\n\n    @property\n    def text_embedder_hidden_size(self):\n        return 32\n\n    @property\n    def dummy_vqvae(self):\n        torch.manual_seed(0)\n        model = VQModel(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=3,\n            num_vq_embeddings=self.num_embed,\n            vq_embed_dim=3,\n        )\n        return model\n\n    @property\n    def dummy_tokenizer(self):\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n        return tokenizer\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=self.text_embedder_hidden_size,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        return CLIPTextModel(config)\n\n    @property\n    def dummy_transformer(self):\n        torch.manual_seed(0)\n\n        height = 12\n  ",
      "metadata": {
        "source": "tests/pipelines/vq_diffusion/test_vq_diffusion.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "584",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/vq_diffusion/test_vq_diffusion.py",
        "range": {
          "start": { "row": 32, "column": 4 },
          "end": { "row": 32, "column": 4 }
        }
      }
    }
  ],
  [
    "585",
    {
      "pageContent": "def test_vq_diffusion(self):\n        device = \"cpu\"\n\n        vqvae = self.dummy_vqvae\n        text_encoder = self.dummy_text_encoder\n        tokenizer = self.dummy_tokenizer\n        transformer = self.dummy_transformer\n        scheduler = VQDiffusionScheduler(self.num_embed)\n        learned_classifier_free_sampling_embeddings = LearnedClassifierFreeSamplingEmbeddings(learnable=False)\n\n        pipe = VQDiffusionPipeline(\n            vqvae=vqvae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            transformer=transformer,\n            scheduler=scheduler,\n            learned_classifier_free_sampling_embeddings=learned_classifier_free_sampling_embeddings,\n        )\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"teddy bear playing in the pool\"\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = pipe([prompt], generator=generator, num_inference_steps=2, output_type=\"np\")\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = pipe(\n            [prompt], generator=generator, output_type=\"np\", return_dict=False, num_inference_steps=2\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 24, 24, 3)\n\n        expected_slice = np.array([0.6583, 0.6410, 0.5325, 0.5635, 0.5563, 0.4234, 0.6008, 0.5491, 0.4880])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert ",
      "metadata": {
        "source": "tests/pipelines/vq_diffusion/test_vq_diffusion.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "586",
    {
      "pageContent": "def test_vq_diffusion_classifier_free_sampling(self):\n        device = \"cpu\"\n\n        vqvae = self.dummy_vqvae\n        text_encoder = self.dummy_text_encoder\n        tokenizer = self.dummy_tokenizer\n        transformer = self.dummy_transformer\n        scheduler = VQDiffusionScheduler(self.num_embed)\n        learned_classifier_free_sampling_embeddings = LearnedClassifierFreeSamplingEmbeddings(\n            learnable=True, hidden_size=self.text_embedder_hidden_size, length=tokenizer.model_max_length\n        )\n\n        pipe = VQDiffusionPipeline(\n            vqvae=vqvae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            transformer=transformer,\n            scheduler=scheduler,\n            learned_classifier_free_sampling_embeddings=learned_classifier_free_sampling_embeddings,\n        )\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"teddy bear playing in the pool\"\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = pipe([prompt], generator=generator, num_inference_steps=2, output_type=\"np\")\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = pipe(\n            [prompt], generator=generator, output_type=\"np\", return_dict=False, num_inference_steps=2\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 24, 24, 3)\n\n        expected_slice = np.array([0.6647, 0.6531, 0.5303, 0.5891, 0.5726,",
      "metadata": {
        "source": "tests/pipelines/vq_diffusion/test_vq_diffusion.py",
        "range": {
          "start": { "row": 150, "column": 4 },
          "end": { "row": 150, "column": 4 }
        }
      }
    }
  ],
  [
    "587",
    {
      "pageContent": "class VQDiffusionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_vq_diffusion_classifier_free_sampling(self):\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/vq_diffusion/teddy_bear_pool_classifier_free_sampling.npy\"\n        )\n\n        pipeline = VQDiffusionPipeline.from_pretrained(\"microsoft/vq-diffusion-ithq\")\n        pipeline = pipeline.to(torch_device)\n        pipeline.set_progress_bar_config(disable=None)\n\n        # requires GPU generator for gumbel softmax\n        # don't use GPU generator in tests though\n        generator = torch.Generator(device=torch_device).manual_seed(0)\n        output = pipeline(\n            \"teddy bear playing in the pool\",\n            num_images_per_prompt=1,\n            generator=generator,\n            output_type=\"np\",\n        )\n\n        image = output.images[0]\n\n        assert image.shape == (256, 256, 3)\n        assert np.abs(expected_image - image).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/vq_diffusion/test_vq_diffusion.py",
        "range": {
          "start": { "row": 197, "column": 0 },
          "end": { "row": 197, "column": 0 }
        }
      }
    }
  ],
  [
    "588",
    {
      "pageContent": "class ScoreSdeVeipelineFastTests(unittest.TestCase):\n    @property\n    def dummy_uncond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    def test_inference(self):\n        unet = self.dummy_uncond_unet\n        scheduler = ScoreSdeVeScheduler()\n\n        sde_ve = ScoreSdeVePipeline(unet=unet, scheduler=scheduler)\n        sde_ve.to(torch_device)\n        sde_ve.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = sde_ve(num_inference_steps=2, output_type=\"numpy\", generator=generator).images\n\n        generator = torch.manual_seed(0)\n        image_from_tuple = sde_ve(num_inference_steps=2, output_type=\"numpy\", generator=generator, return_dict=False)[\n            0\n        ]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/score_sde_ve/test_score_sde_ve.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "589",
    {
      "pageContent": "def test_inference(self):\n        unet = self.dummy_uncond_unet\n        scheduler = ScoreSdeVeScheduler()\n\n        sde_ve = ScoreSdeVePipeline(unet=unet, scheduler=scheduler)\n        sde_ve.to(torch_device)\n        sde_ve.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = sde_ve(num_inference_steps=2, output_type=\"numpy\", generator=generator).images\n\n        generator = torch.manual_seed(0)\n        image_from_tuple = sde_ve(num_inference_steps=2, output_type=\"numpy\", generator=generator, return_dict=False)[\n            0\n        ]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/score_sde_ve/test_score_sde_ve.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "590",
    {
      "pageContent": "class ScoreSdeVePipelineIntegrationTests(unittest.TestCase):\n    def test_inference(self):\n        model_id = \"google/ncsnpp-church-256\"\n        model = UNet2DModel.from_pretrained(model_id)\n\n        scheduler = ScoreSdeVeScheduler.from_pretrained(model_id)\n\n        sde_ve = ScoreSdeVePipeline(unet=model, scheduler=scheduler)\n        sde_ve.to(torch_device)\n        sde_ve.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = sde_ve(num_inference_steps=10, output_type=\"numpy\", generator=generator).images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 256, 256, 3)\n\n        expected_slice = np.array([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/score_sde_ve/test_score_sde_ve.py",
        "range": {
          "start": { "row": 70, "column": 0 },
          "end": { "row": 70, "column": 0 }
        }
      }
    }
  ],
  [
    "591",
    {
      "pageContent": "class IsSafetensorsCompatibleTests(unittest.TestCase):\n    def test_all_is_compatible(self):\n        filenames = [\n            \"safety_checker/pytorch_model.bin\",\n            \"safety_checker/model.safetensors\",\n            \"vae/diffusion_pytorch_model.bin\",\n            \"vae/diffusion_pytorch_model.safetensors\",\n            \"text_encoder/pytorch_model.bin\",\n            \"text_encoder/model.safetensors\",\n            \"unet/diffusion_pytorch_model.bin\",\n            \"unet/diffusion_pytorch_model.safetensors\",\n        ]\n        self.assertTrue(is_safetensors_compatible(filenames))\n\n    def test_diffusers_model_is_compatible(self):\n        filenames = [\n            \"unet/diffusion_pytorch_model.bin\",\n            \"unet/diffusion_pytorch_model.safetensors\",\n        ]\n        self.assertTrue(is_safetensors_compatible(filenames))\n\n    def test_diffusers_model_is_not_compatible(self):\n        filenames = [\n            \"safety_checker/pytorch_model.bin\",\n            \"safety_checker/model.safetensors\",\n            \"vae/diffusion_pytorch_model.bin\",\n            \"vae/diffusion_pytorch_model.safetensors\",\n            \"text_encoder/pytorch_model.bin\",\n            \"text_encoder/model.safetensors\",\n            \"unet/diffusion_pytorch_model.bin\",\n            # Removed: 'unet/diffusion_pytorch_model.safetensors',\n        ]\n        self.assertFalse(is_safetensors_compatible(filenames))\n\n    def test_transformer_model_is_compatible(self):\n        filenames = [\n            \"text_encoder/pytorch_model.bin\",\n            \"text_encoder/model.safetensors\",\n        ]\n        self.assertTrue(is_safetensors",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 5, "column": 0 },
          "end": { "row": 5, "column": 0 }
        }
      }
    }
  ],
  [
    "592",
    {
      "pageContent": "def test_all_is_compatible(self):\n        filenames = [\n            \"safety_checker/pytorch_model.bin\",\n            \"safety_checker/model.safetensors\",\n            \"vae/diffusion_pytorch_model.bin\",\n            \"vae/diffusion_pytorch_model.safetensors\",\n            \"text_encoder/pytorch_model.bin\",\n            \"text_encoder/model.safetensors\",\n            \"unet/diffusion_pytorch_model.bin\",\n            \"unet/diffusion_pytorch_model.safetensors\",\n        ]\n        self.assertTrue(is_safetensors_compatible(filenames))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 6, "column": 4 },
          "end": { "row": 6, "column": 4 }
        }
      }
    }
  ],
  [
    "593",
    {
      "pageContent": "def test_diffusers_model_is_compatible(self):\n        filenames = [\n            \"unet/diffusion_pytorch_model.bin\",\n            \"unet/diffusion_pytorch_model.safetensors\",\n        ]\n        self.assertTrue(is_safetensors_compatible(filenames))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 19, "column": 4 },
          "end": { "row": 19, "column": 4 }
        }
      }
    }
  ],
  [
    "594",
    {
      "pageContent": "def test_diffusers_model_is_not_compatible(self):\n        filenames = [\n            \"safety_checker/pytorch_model.bin\",\n            \"safety_checker/model.safetensors\",\n            \"vae/diffusion_pytorch_model.bin\",\n            \"vae/diffusion_pytorch_model.safetensors\",\n            \"text_encoder/pytorch_model.bin\",\n            \"text_encoder/model.safetensors\",\n            \"unet/diffusion_pytorch_model.bin\",\n            # Removed: 'unet/diffusion_pytorch_model.safetensors',\n        ]\n        self.assertFalse(is_safetensors_compatible(filenames))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 26, "column": 4 },
          "end": { "row": 26, "column": 4 }
        }
      }
    }
  ],
  [
    "595",
    {
      "pageContent": "def test_transformer_model_is_compatible(self):\n        filenames = [\n            \"text_encoder/pytorch_model.bin\",\n            \"text_encoder/model.safetensors\",\n        ]\n        self.assertTrue(is_safetensors_compatible(filenames))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "596",
    {
      "pageContent": "def test_transformer_model_is_not_compatible(self):\n        filenames = [\n            \"safety_checker/pytorch_model.bin\",\n            \"safety_checker/model.safetensors\",\n            \"vae/diffusion_pytorch_model.bin\",\n            \"vae/diffusion_pytorch_model.safetensors\",\n            \"text_encoder/pytorch_model.bin\",\n            # Removed: 'text_encoder/model.safetensors',\n            \"unet/diffusion_pytorch_model.bin\",\n            \"unet/diffusion_pytorch_model.safetensors\",\n        ]\n        self.assertFalse(is_safetensors_compatible(filenames))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 46, "column": 4 },
          "end": { "row": 46, "column": 4 }
        }
      }
    }
  ],
  [
    "597",
    {
      "pageContent": "def test_all_is_compatible_variant(self):\n        filenames = [\n            \"safety_checker/pytorch_model.fp16.bin\",\n            \"safety_checker/model.fp16.safetensors\",\n            \"vae/diffusion_pytorch_model.fp16.bin\",\n            \"vae/diffusion_pytorch_model.fp16.safetensors\",\n            \"text_encoder/pytorch_model.fp16.bin\",\n            \"text_encoder/model.fp16.safetensors\",\n            \"unet/diffusion_pytorch_model.fp16.bin\",\n            \"unet/diffusion_pytorch_model.fp16.safetensors\",\n        ]\n        variant = \"fp16\"\n        self.assertTrue(is_safetensors_compatible(filenames, variant=variant))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 59, "column": 4 },
          "end": { "row": 59, "column": 4 }
        }
      }
    }
  ],
  [
    "598",
    {
      "pageContent": "def test_diffusers_model_is_compatible_variant(self):\n        filenames = [\n            \"unet/diffusion_pytorch_model.fp16.bin\",\n            \"unet/diffusion_pytorch_model.fp16.safetensors\",\n        ]\n        variant = \"fp16\"\n        self.assertTrue(is_safetensors_compatible(filenames, variant=variant))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 73, "column": 4 },
          "end": { "row": 73, "column": 4 }
        }
      }
    }
  ],
  [
    "599",
    {
      "pageContent": "def test_diffusers_model_is_compatible_variant_partial(self):\n        # pass variant but use the non-variant filenames\n        filenames = [\n            \"unet/diffusion_pytorch_model.bin\",\n            \"unet/diffusion_pytorch_model.safetensors\",\n        ]\n        variant = \"fp16\"\n        self.assertTrue(is_safetensors_compatible(filenames, variant=variant))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 81, "column": 4 },
          "end": { "row": 81, "column": 4 }
        }
      }
    }
  ],
  [
    "600",
    {
      "pageContent": "def test_diffusers_model_is_not_compatible_variant(self):\n        filenames = [\n            \"safety_checker/pytorch_model.fp16.bin\",\n            \"safety_checker/model.fp16.safetensors\",\n            \"vae/diffusion_pytorch_model.fp16.bin\",\n            \"vae/diffusion_pytorch_model.fp16.safetensors\",\n            \"text_encoder/pytorch_model.fp16.bin\",\n            \"text_encoder/model.fp16.safetensors\",\n            \"unet/diffusion_pytorch_model.fp16.bin\",\n            # Removed: 'unet/diffusion_pytorch_model.fp16.safetensors',\n        ]\n        variant = \"fp16\"\n        self.assertFalse(is_safetensors_compatible(filenames, variant=variant))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 90, "column": 4 },
          "end": { "row": 90, "column": 4 }
        }
      }
    }
  ],
  [
    "601",
    {
      "pageContent": "def test_transformer_model_is_compatible_variant(self):\n        filenames = [\n            \"text_encoder/pytorch_model.fp16.bin\",\n            \"text_encoder/model.fp16.safetensors\",\n        ]\n        variant = \"fp16\"\n        self.assertTrue(is_safetensors_compatible(filenames, variant=variant))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 104, "column": 4 },
          "end": { "row": 104, "column": 4 }
        }
      }
    }
  ],
  [
    "602",
    {
      "pageContent": "def test_transformer_model_is_compatible_variant_partial(self):\n        # pass variant but use the non-variant filenames\n        filenames = [\n            \"text_encoder/pytorch_model.bin\",\n            \"text_encoder/model.safetensors\",\n        ]\n        variant = \"fp16\"\n        self.assertTrue(is_safetensors_compatible(filenames, variant=variant))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 112, "column": 4 },
          "end": { "row": 112, "column": 4 }
        }
      }
    }
  ],
  [
    "603",
    {
      "pageContent": "def test_transformer_model_is_not_compatible_variant(self):\n        filenames = [\n            \"safety_checker/pytorch_model.fp16.bin\",\n            \"safety_checker/model.fp16.safetensors\",\n            \"vae/diffusion_pytorch_model.fp16.bin\",\n            \"vae/diffusion_pytorch_model.fp16.safetensors\",\n            \"text_encoder/pytorch_model.fp16.bin\",\n            # 'text_encoder/model.fp16.safetensors',\n            \"unet/diffusion_pytorch_model.fp16.bin\",\n            \"unet/diffusion_pytorch_model.fp16.safetensors\",\n        ]\n        variant = \"fp16\"\n        self.assertFalse(is_safetensors_compatible(filenames, variant=variant))",
      "metadata": {
        "source": "tests/pipelines/test_pipeline_utils.py",
        "range": {
          "start": { "row": 121, "column": 4 },
          "end": { "row": 121, "column": 4 }
        }
      }
    }
  ],
  [
    "604",
    {
      "pageContent": "class PipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def dummy_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            sample_size=(32, 64),\n            in_channels=1,\n            out_channels=1,\n            layers_per_block=2,\n            block_out_channels=(128, 128),\n            down_block_types=(\"AttnDownBlock2D\", \"DownBlock2D\"),\n            up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\"),\n        )\n        return model\n\n    @property\n    def dummy_unet_condition(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            sample_size=(64, 32),\n            in_channels=1,\n            out_channels=1,\n            layers_per_block=2,\n            block_out_channels=(128, 128),\n            down_block_types=(\"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n            up_block_types=(\"UpBlock2D\", \"CrossAttnUpBlock2D\"),\n            cross_attention_dim=10,\n        )\n        return model\n\n    @property\n    def dummy_vqvae_and_unet(self):\n        torch.manual_seed(0)\n        vqvae = AutoencoderKL(\n            sample_size=(128, 64),\n            in_channels=1,\n            out_channels=1,\n            latent_channels=1,\n            layers_per_block=2,\n            block_out_channels=(128, 128),\n            down_block_types=(\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"),\n            up_block_types=(\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"),\n        )\n        unet = UNet2DModel(\n            sampl",
      "metadata": {
        "source": "tests/pipelines/audio_diffusion/test_audio_diffusion.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "605",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/audio_diffusion/test_audio_diffusion.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "606",
    {
      "pageContent": "class PipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_audio_diffusion(self):\n        device = torch_device\n\n        pipe = DiffusionPipeline.from_pretrained(\"teticio/audio-diffusion-ddim-256\")\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=device).manual_seed(42)\n        output = pipe(generator=generator)\n        audio = output.audios[0]\n        image = output.images[0]\n\n        assert audio.shape == (1, (pipe.unet.sample_size[1] - 1) * pipe.mel.hop_length)\n        assert image.height == pipe.unet.sample_size[0] and image.width == pipe.unet.sample_size[1]\n        image_slice = np.frombuffer(image.tobytes(), dtype=\"uint8\")[:10]\n        expected_slice = np.array([151, 167, 154, 144, 122, 134, 121, 105, 70, 26])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() == 0",
      "metadata": {
        "source": "tests/pipelines/audio_diffusion/test_audio_diffusion.py",
        "range": {
          "start": { "row": 166, "column": 0 },
          "end": { "row": 166, "column": 0 }
        }
      }
    }
  ],
  [
    "607",
    {
      "pageContent": "class StableUnCLIPImg2ImgPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableUnCLIPImg2ImgPipeline\n    params = TEXT_GUIDED_IMAGE_VARIATION_PARAMS\n    batch_params = TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        embedder_hidden_size = 32\n        embedder_projection_dim = embedder_hidden_size\n\n        # image encoding components\n\n        feature_extractor = CLIPFeatureExtractor(crop_size=32, size=32)\n\n        image_encoder = CLIPVisionModelWithProjection(\n            CLIPVisionConfig(\n                hidden_size=embedder_hidden_size,\n                projection_dim=embedder_projection_dim,\n                num_hidden_layers=5,\n                num_attention_heads=4,\n                image_size=32,\n                intermediate_size=37,\n                patch_size=1,\n            )\n        )\n\n        # regular denoising components\n\n        torch.manual_seed(0)\n        image_normalizer = StableUnCLIPImageNormalizer(embedding_dim=embedder_hidden_size)\n        image_noising_scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\")\n\n        torch.manual_seed(0)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        torch.manual_seed(0)\n        text_encoder = CLIPTextModel(\n            CLIPTextConfig(\n                bos_token_id=0,\n                eos_token_id=2,\n                hidden_size=embedder_hidden_size,\n                projection_dim=32,\n                intermediate_size=37,\n                layer_norm_eps=1e-05,\n                num_attention_heads=4,\n        ",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "608",
    {
      "pageContent": "def get_dummy_components(self):\n        embedder_hidden_size = 32\n        embedder_projection_dim = embedder_hidden_size\n\n        # image encoding components\n\n        feature_extractor = CLIPFeatureExtractor(crop_size=32, size=32)\n\n        image_encoder = CLIPVisionModelWithProjection(\n            CLIPVisionConfig(\n                hidden_size=embedder_hidden_size,\n                projection_dim=embedder_projection_dim,\n                num_hidden_layers=5,\n                num_attention_heads=4,\n                image_size=32,\n                intermediate_size=37,\n                patch_size=1,\n            )\n        )\n\n        # regular denoising components\n\n        torch.manual_seed(0)\n        image_normalizer = StableUnCLIPImageNormalizer(embedding_dim=embedder_hidden_size)\n        image_noising_scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\")\n\n        torch.manual_seed(0)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        torch.manual_seed(0)\n        text_encoder = CLIPTextModel(\n            CLIPTextConfig(\n                bos_token_id=0,\n                eos_token_id=2,\n                hidden_size=embedder_hidden_size,\n                projection_dim=32,\n                intermediate_size=37,\n                layer_norm_eps=1e-05,\n                num_attention_heads=4,\n                num_hidden_layers=5,\n                pad_token_id=1,\n                vocab_size=1000,\n            )\n        )\n\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            sample_size=32,\n            in_channels=4,\n      ",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 32, "column": 4 },
          "end": { "row": 32, "column": 4 }
        }
      }
    }
  ],
  [
    "609",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0, pil_image=True):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n\n        input_image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n\n        if pil_image:\n            input_image = input_image * 0.5 + 0.5\n            input_image = input_image.clamp(0, 1)\n            input_image = input_image.cpu().permute(0, 2, 3, 1).float().numpy()\n            input_image = DiffusionPipeline.numpy_to_pil(input_image)[0]\n\n        return {\n            \"prompt\": \"An anime racoon running a marathon\",\n            \"image\": input_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"output_type\": \"np\",\n        }",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 126, "column": 4 },
          "end": { "row": 126, "column": 4 }
        }
      }
    }
  ],
  [
    "610",
    {
      "pageContent": "def test_attention_slicing_forward_pass(self):\n        test_max_difference = torch_device in [\"cpu\", \"mps\"]\n\n        self._test_attention_slicing_forward_pass(test_max_difference=test_max_difference)",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 150, "column": 4 },
          "end": { "row": 150, "column": 4 }
        }
      }
    }
  ],
  [
    "611",
    {
      "pageContent": "def test_inference_batch_single_identical(self):\n        test_max_difference = torch_device in [\"cpu\", \"mps\"]\n\n        self._test_inference_batch_single_identical(test_max_difference=test_max_difference)",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 157, "column": 4 },
          "end": { "row": 157, "column": 4 }
        }
      }
    }
  ],
  [
    "612",
    {
      "pageContent": "class StableUnCLIPImg2ImgPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_unclip_l_img2img(self):\n        input_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/turtle.png\"\n        )\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/stable_unclip_2_1_l_img2img_anime_turtle_fp16.npy\"\n        )\n\n        pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n            \"fusing/stable-unclip-2-1-l-img2img\", torch_dtype=torch.float16\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        # stable unclip will oom when integration tests are run on a V100,\n        # so turn on memory savings\n        pipe.enable_attention_slicing()\n        pipe.enable_sequential_cpu_offload()\n\n        generator = torch.Generator(device=\"cpu\").manual_seed(0)\n        output = pipe(\"anime turle\", image=input_image, generator=generator, output_type=\"np\")\n\n        image = output.images[0]\n\n        assert image.shape == (768, 768, 3)\n\n        assert_mean_pixel_difference(image, expected_image)\n\n    def test_stable_unclip_h_img2img(self):\n        input_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/turtle.png\"\n        )\n\n        expected_image = lo",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 172, "column": 0 },
          "end": { "row": 172, "column": 0 }
        }
      }
    }
  ],
  [
    "613",
    {
      "pageContent": "class StableUnCLIPPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableUnCLIPPipeline\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n\n    # TODO(will) Expected attn_bias.stride(1) == 0 to be true, but got false\n    test_xformers_attention = False\n\n    def get_dummy_components(self):\n        embedder_hidden_size = 32\n        embedder_projection_dim = embedder_hidden_size\n\n        # prior components\n\n        torch.manual_seed(0)\n        prior_tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        torch.manual_seed(0)\n        prior_text_encoder = CLIPTextModelWithProjection(\n            CLIPTextConfig(\n                bos_token_id=0,\n                eos_token_id=2,\n                hidden_size=embedder_hidden_size,\n                projection_dim=embedder_projection_dim,\n                intermediate_size=37,\n                layer_norm_eps=1e-05,\n                num_attention_heads=4,\n                num_hidden_layers=5,\n                pad_token_id=1,\n                vocab_size=1000,\n            )\n        )\n\n        torch.manual_seed(0)\n        prior = PriorTransformer(\n            num_attention_heads=2,\n            attention_head_dim=12,\n            embedding_dim=embedder_projection_dim,\n            num_layers=1,\n        )\n\n        torch.manual_seed(0)\n        prior_scheduler = DDPMScheduler(\n            variance_type=\"fixed_small_log\",\n            prediction_type=\"sample\",\n            num_train_timesteps=1000,\n            clip_sample=True,\n            clip_sample_range=5.0,\n  ",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "614",
    {
      "pageContent": "def get_dummy_components(self):\n        embedder_hidden_size = 32\n        embedder_projection_dim = embedder_hidden_size\n\n        # prior components\n\n        torch.manual_seed(0)\n        prior_tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        torch.manual_seed(0)\n        prior_text_encoder = CLIPTextModelWithProjection(\n            CLIPTextConfig(\n                bos_token_id=0,\n                eos_token_id=2,\n                hidden_size=embedder_hidden_size,\n                projection_dim=embedder_projection_dim,\n                intermediate_size=37,\n                layer_norm_eps=1e-05,\n                num_attention_heads=4,\n                num_hidden_layers=5,\n                pad_token_id=1,\n                vocab_size=1000,\n            )\n        )\n\n        torch.manual_seed(0)\n        prior = PriorTransformer(\n            num_attention_heads=2,\n            attention_head_dim=12,\n            embedding_dim=embedder_projection_dim,\n            num_layers=1,\n        )\n\n        torch.manual_seed(0)\n        prior_scheduler = DDPMScheduler(\n            variance_type=\"fixed_small_log\",\n            prediction_type=\"sample\",\n            num_train_timesteps=1000,\n            clip_sample=True,\n            clip_sample_range=5.0,\n            beta_schedule=\"squaredcos_cap_v2\",\n        )\n\n        # regular denoising components\n\n        torch.manual_seed(0)\n        image_normalizer = StableUnCLIPImageNormalizer(embedding_dim=embedder_hidden_size)\n        image_noising_scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\")\n\n        torch.manu",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip.py",
        "range": {
          "start": { "row": 29, "column": 4 },
          "end": { "row": 29, "column": 4 }
        }
      }
    }
  ],
  [
    "615",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"prior_num_inference_steps\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip.py",
        "range": {
          "start": { "row": 148, "column": 4 },
          "end": { "row": 148, "column": 4 }
        }
      }
    }
  ],
  [
    "616",
    {
      "pageContent": "def test_attention_slicing_forward_pass(self):\n        test_max_difference = torch_device == \"cpu\"\n\n        self._test_attention_slicing_forward_pass(test_max_difference=test_max_difference)",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip.py",
        "range": {
          "start": { "row": 164, "column": 4 },
          "end": { "row": 164, "column": 4 }
        }
      }
    }
  ],
  [
    "617",
    {
      "pageContent": "def test_inference_batch_single_identical(self):\n        test_max_difference = torch_device in [\"cpu\", \"mps\"]\n\n        self._test_inference_batch_single_identical(test_max_difference=test_max_difference)",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip.py",
        "range": {
          "start": { "row": 171, "column": 4 },
          "end": { "row": 171, "column": 4 }
        }
      }
    }
  ],
  [
    "618",
    {
      "pageContent": "class StableUnCLIPPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_unclip(self):\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/stable_unclip_2_1_l_anime_turtle_fp16.npy\"\n        )\n\n        pipe = StableUnCLIPPipeline.from_pretrained(\"fusing/stable-unclip-2-1-l\", torch_dtype=torch.float16)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        # stable unclip will oom when integration tests are run on a V100,\n        # so turn on memory savings\n        pipe.enable_attention_slicing()\n        pipe.enable_sequential_cpu_offload()\n\n        generator = torch.Generator(device=\"cpu\").manual_seed(0)\n        output = pipe(\"anime turle\", generator=generator, output_type=\"np\")\n\n        image = output.images[0]\n\n        assert image.shape == (768, 768, 3)\n\n        assert_mean_pixel_difference(image, expected_image)\n\n    def test_stable_unclip_pipeline_with_sequential_cpu_offloading(self):\n        torch.cuda.empty_cache()\n        torch.cuda.reset_max_memory_allocated()\n        torch.cuda.reset_peak_memory_stats()\n\n        pipe = StableUnCLIPPipeline.from_pretrained(\"fusing/stable-unclip-2-1-l\", torch_dtype=torch.float16)\n        pipe = pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n        pipe.enable_sequential_cpu_offload()\n\n",
      "metadata": {
        "source": "tests/pipelines/stable_unclip/test_stable_unclip.py",
        "range": {
          "start": { "row": 179, "column": 0 },
          "end": { "row": 179, "column": 0 }
        }
      }
    }
  ],
  [
    "619",
    {
      "pageContent": "class SafeDiffusionPipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    @property\n    def dummy_cond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        return model\n\n    @property\n    def dummy_vae(self):\n        torch.manual_seed(0)\n        model = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        return model\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_a",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "620",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py",
        "range": {
          "start": { "row": 34, "column": 4 },
          "end": { "row": 34, "column": 4 }
        }
      }
    }
  ],
  [
    "621",
    {
      "pageContent": "def test_safe_diffusion_ddim(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tup",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "622",
    {
      "pageContent": "def test_stable_diffusion_pndm(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.5095, 0.5674, 0.4668, 0.5126, 0",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py",
        "range": {
          "start": { "row": 161, "column": 4 },
          "end": { "row": 161, "column": 4 }
        }
      }
    }
  ],
  [
    "623",
    {
      "pageContent": "def test_stable_diffusion_no_safety_checker(self):\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-lms-pipe\", safety_checker=None\n        )\n        assert isinstance(pipe, StableDiffusionPipeline)\n        assert isinstance(pipe.scheduler, LMSDiscreteScheduler)\n        assert pipe.safety_checker is None\n\n        image = pipe(\"example prompt\", num_inference_steps=2).images[0]\n        assert image is not None\n\n        # check that there's no error when saving a pipeline with one of the models being None\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe = StableDiffusionPipeline.from_pretrained(tmpdirname)\n\n        # sanity check that the pipeline still works\n        assert pipe.safety_checker is None\n        image = pipe(\"example prompt\", num_inference_steps=2).images[0]\n        assert image is not None",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py",
        "range": {
          "start": { "row": 207, "column": 4 },
          "end": { "row": 207, "column": 4 }
        }
      }
    }
  ],
  [
    "624",
    {
      "pageContent": "class SafeDiffusionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_harm_safe_stable_diffusion(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", safety_checker=None)\n        sd_pipe.scheduler = LMSDiscreteScheduler.from_config(sd_pipe.scheduler.config)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = (\n            \"portrait of girl with smokey eyes makeup in abandoned hotel, grange clothes, redshift, wide high angle\"\n            \" coloured polaroid photograph with flash, kodak film, hyper real, stunning moody cinematography, with\"\n            \" anamorphic lenses, by maripol, fallen angels by wong kar - wai, style of suspiria and neon demon and\"\n            \" children from bahnhof zoo, detailed \"\n        )\n        seed = 4003660346\n        guidance_scale = 7\n\n        # without safety guidance (sld_guidance_scale = 0)\n        generator = torch.manual_seed(seed)\n        output = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=guidance_scale,\n            num_inference_steps=50,\n            output_type=\"np\",\n            width=512,\n            height=512,\n            sld_guidance_scale=0,\n        )\n\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n        expected_slice = [0.2278, 0.2231, 0.2249, 0.2333, 0.2303, 0.1885, 0.2273, 0.2144, 0.2176]",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py",
        "range": {
          "start": { "row": 263, "column": 0 },
          "end": { "row": 263, "column": 0 }
        }
      }
    }
  ],
  [
    "625",
    {
      "pageContent": "class LDMPipelineFastTests(unittest.TestCase):\n    @property\n    def dummy_uncond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    @property\n    def dummy_vq_model(self):\n        torch.manual_seed(0)\n        model = VQModel(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=3,\n        )\n        return model\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        return CLIPTextModel(config)\n\n    def test_inference_uncond(self):\n        unet = self.dummy_uncond_unet\n        scheduler = DDIMScheduler()\n        vae = self.dummy_vq_model\n\n        ldm = LDMPipeline(unet=unet, vqvae=vae, scheduler=scheduler)\n        ldm.to(torch_device)\n        ldm.set_progress_bar_config(disable=None)\n\n     ",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion_uncond.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "626",
    {
      "pageContent": "def test_inference_uncond(self):\n        unet = self.dummy_uncond_unet\n        scheduler = DDIMScheduler()\n        vae = self.dummy_vq_model\n\n        ldm = LDMPipeline(unet=unet, vqvae=vae, scheduler=scheduler)\n        ldm.to(torch_device)\n        ldm.set_progress_bar_config(disable=None)\n\n        # Warmup pass when using mps (see #372)\n        if torch_device == \"mps\":\n            generator = torch.manual_seed(0)\n            _ = ldm(generator=generator, num_inference_steps=1, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        image = ldm(generator=generator, num_inference_steps=2, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        image_from_tuple = ldm(generator=generator, num_inference_steps=2, output_type=\"numpy\", return_dict=False)[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.8512, 0.818, 0.6411, 0.6808, 0.4465, 0.5618, 0.46, 0.6231, 0.5172])\n        tolerance = 1e-2 if torch_device != \"mps\" else 3e-2\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < tolerance\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < tolerance",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion_uncond.py",
        "range": {
          "start": { "row": 72, "column": 4 },
          "end": { "row": 72, "column": 4 }
        }
      }
    }
  ],
  [
    "627",
    {
      "pageContent": "class LDMPipelineIntegrationTests(unittest.TestCase):\n    def test_inference_uncond(self):\n        ldm = LDMPipeline.from_pretrained(\"CompVis/ldm-celebahq-256\")\n        ldm.to(torch_device)\n        ldm.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = ldm(generator=generator, num_inference_steps=5, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 256, 256, 3)\n        expected_slice = np.array([0.4399, 0.44975, 0.46825, 0.474, 0.4359, 0.4581, 0.45095, 0.4341, 0.4447])\n        tolerance = 1e-2 if torch_device != \"mps\" else 3e-2\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < tolerance",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion_uncond.py",
        "range": {
          "start": { "row": 105, "column": 0 },
          "end": { "row": 105, "column": 0 }
        }
      }
    }
  ],
  [
    "628",
    {
      "pageContent": "class LDMSuperResolutionPipelineFastTests(unittest.TestCase):\n    @property\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    @property\n    def dummy_uncond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=6,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    @property\n    def dummy_vq_model(self):\n        torch.manual_seed(0)\n        model = VQModel(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=3,\n        )\n        return model\n\n    def test_inference_superresolution(self):\n        device = \"cpu\"\n        unet = self.dummy_uncond_unet\n        scheduler = DDIMScheduler()\n        vqvae = self.dummy_vq_model\n\n        ldm = LDMSuperResolutionPipeline(unet=unet, vqvae=vqvae, scheduler=scheduler)\n        ldm.to(device)\n        ldm.set_progress_bar_config(disable=None)\n\n        init_image = self.dummy_image.to(device)\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image = ldm(im",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion_superresolution.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "629",
    {
      "pageContent": "def test_inference_superresolution(self):\n        device = \"cpu\"\n        unet = self.dummy_uncond_unet\n        scheduler = DDIMScheduler()\n        vqvae = self.dummy_vq_model\n\n        ldm = LDMSuperResolutionPipeline(unet=unet, vqvae=vqvae, scheduler=scheduler)\n        ldm.to(device)\n        ldm.set_progress_bar_config(disable=None)\n\n        init_image = self.dummy_image.to(device)\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image = ldm(image=init_image, generator=generator, num_inference_steps=2, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.8678, 0.8245, 0.6381, 0.6830, 0.4385, 0.5599, 0.4641, 0.6201, 0.5150])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion_superresolution.py",
        "range": {
          "start": { "row": 66, "column": 4 },
          "end": { "row": 66, "column": 4 }
        }
      }
    }
  ],
  [
    "630",
    {
      "pageContent": "class LDMSuperResolutionPipelineIntegrationTests(unittest.TestCase):\n    def test_inference_superresolution(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/vq_diffusion/teddy_bear_pool.png\"\n        )\n        init_image = init_image.resize((64, 64), resample=PIL_INTERPOLATION[\"lanczos\"])\n\n        ldm = LDMSuperResolutionPipeline.from_pretrained(\"duongna/ldm-super-resolution\", device_map=\"auto\")\n        ldm.to(torch_device)\n        ldm.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = ldm(image=init_image, generator=generator, num_inference_steps=20, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 256, 256, 3)\n        expected_slice = np.array([0.7644, 0.7679, 0.7642, 0.7633, 0.7666, 0.7560, 0.7425, 0.7257, 0.6907])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion_superresolution.py",
        "range": {
          "start": { "row": 111, "column": 0 },
          "end": { "row": 111, "column": 0 }
        }
      }
    }
  ],
  [
    "631",
    {
      "pageContent": "class LDMTextToImagePipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = LDMTextToImagePipeline\n    params = TEXT_TO_IMAGE_PARAMS - {\n        \"negative_prompt\",\n        \"negative_prompt_embeds\",\n        \"cross_attention_kwargs\",\n        \"prompt_embeds\",\n    }\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\n        \"num_images_per_prompt\",\n        \"callback\",\n        \"callback_steps\",\n    }\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=(32, 64),\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"),\n            up_block_types=(\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"),\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n     ",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "632",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=(32, 64),\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"),\n            up_block_types=(\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"),\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vqv",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "633",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion.py",
        "range": {
          "start": { "row": 100, "column": 4 },
          "end": { "row": 100, "column": 4 }
        }
      }
    }
  ],
  [
    "634",
    {
      "pageContent": "def test_inference_text2img(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        pipe = LDMTextToImagePipeline(**components)\n        pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 16, 16, 3)\n        expected_slice = np.array([0.59450, 0.64078, 0.55509, 0.51229, 0.69640, 0.36960, 0.59296, 0.60801, 0.49332])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion.py",
        "range": {
          "start": { "row": 114, "column": 4 },
          "end": { "row": 114, "column": 4 }
        }
      }
    }
  ],
  [
    "635",
    {
      "pageContent": "class LDMTextToImagePipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, dtype=torch.float32, seed=0):\n        generator = torch.manual_seed(seed)\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 32, 32))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_ldm_default_ddim(self):\n        pipe = LDMTextToImagePipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\").to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 256, 256, 3)\n        expected_slice = np.array([0.51825, 0.52850, 0.52543, 0.54258, 0.52304, 0.52569, 0.54363, 0.55276, 0.56878])\n        max_diff = np.abs(expected_slice - image_slice).max()\n        assert max_diff < 1e-3",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion.py",
        "range": {
          "start": { "row": 134, "column": 0 },
          "end": { "row": 134, "column": 0 }
        }
      }
    }
  ],
  [
    "636",
    {
      "pageContent": "class LDMTextToImagePipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, dtype=torch.float32, seed=0):\n        generator = torch.manual_seed(seed)\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 32, 32))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_ldm_default_ddim(self):\n        pipe = LDMTextToImagePipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\").to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/ldm_text2img/ldm_large_256_ddim.npy\"\n        )\n        max_diff = np.abs(expected_image - image).max()\n        assert max_diff < 1e-3",
      "metadata": {
        "source": "tests/pipelines/latent_diffusion/test_latent_diffusion.py",
        "range": {
          "start": { "row": 170, "column": 0 },
          "end": { "row": 170, "column": 0 }
        }
      }
    }
  ],
  [
    "637",
    {
      "pageContent": "class StableDiffusion2PipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionPipeline\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=(2, 4),\n            use_linear_projection=True,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n            sample_size=128,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "638",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=(2, 4),\n            use_linear_projection=True,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n            sample_size=128,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n            # SD2-specific config below\n            hidden_act=\"gelu\",\n            projection_dim=512,\n        )",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 49, "column": 4 },
          "end": { "row": 49, "column": 4 }
        }
      }
    }
  ],
  [
    "639",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 110, "column": 4 },
          "end": { "row": 110, "column": 4 }
        }
      }
    }
  ],
  [
    "640",
    {
      "pageContent": "def test_stable_diffusion_ddim(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.5649, 0.6022, 0.4804, 0.5270, 0.5585, 0.4643, 0.5159, 0.4963, 0.4793])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "641",
    {
      "pageContent": "def test_stable_diffusion_pndm(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = PNDMScheduler(skip_prk_steps=True)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.5099, 0.5677, 0.4671, 0.5128, 0.5697, 0.4676, 0.5277, 0.4964, 0.4946])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 140, "column": 4 },
          "end": { "row": 140, "column": 4 }
        }
      }
    }
  ],
  [
    "642",
    {
      "pageContent": "def test_stable_diffusion_k_lms(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler.from_config(components[\"scheduler\"].config)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.4717, 0.5376, 0.4568, 0.5225, 0.5734, 0.4797, 0.5467, 0.5074, 0.5043])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 157, "column": 4 },
          "end": { "row": 157, "column": 4 }
        }
      }
    }
  ],
  [
    "643",
    {
      "pageContent": "def test_stable_diffusion_k_euler_ancestral(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = EulerAncestralDiscreteScheduler.from_config(components[\"scheduler\"].config)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.4715, 0.5376, 0.4569, 0.5224, 0.5734, 0.4797, 0.5465, 0.5074, 0.5046])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 174, "column": 4 },
          "end": { "row": 174, "column": 4 }
        }
      }
    }
  ],
  [
    "644",
    {
      "pageContent": "def test_stable_diffusion_k_euler(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = EulerDiscreteScheduler.from_config(components[\"scheduler\"].config)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.4717, 0.5376, 0.4568, 0.5225, 0.5734, 0.4797, 0.5467, 0.5074, 0.5043])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 191, "column": 4 },
          "end": { "row": 191, "column": 4 }
        }
      }
    }
  ],
  [
    "645",
    {
      "pageContent": "def test_stable_diffusion_long_prompt(self):\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler.from_config(components[\"scheduler\"].config)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        do_classifier_free_guidance = True\n        negative_prompt = None\n        num_images_per_prompt = 1\n        logger = logging.get_logger(\"diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion\")\n\n        prompt = 25 * \"@\"\n        with CaptureLogger(logger) as cap_logger_3:\n            text_embeddings_3 = sd_pipe._encode_prompt(\n                prompt, torch_device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n            )\n\n        prompt = 100 * \"@\"\n        with CaptureLogger(logger) as cap_logger:\n            text_embeddings = sd_pipe._encode_prompt(\n                prompt, torch_device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n            )\n\n        negative_prompt = \"Hello\"\n        with CaptureLogger(logger) as cap_logger_2:\n            text_embeddings_2 = sd_pipe._encode_prompt(\n                prompt, torch_device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n            )\n\n        assert text_embeddings_3.shape == text_embeddings_2.shape == text_embeddings.shape\n        assert text_embeddings.shape[1] == 77\n\n        assert cap_logger.out == cap_logger_2.out\n        # 100 - 77 + 1 (BOS token) + 1 (EOS token) = 25\n        assert cap_logger.ou",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 208, "column": 4 },
          "end": { "row": 208, "column": 4 }
        }
      }
    }
  ],
  [
    "646",
    {
      "pageContent": "class StableDiffusion2PipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 64, 64))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"prompt\": \"a photograph of an astronaut riding a horse\",\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_default_ddim(self):\n        pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-base\")\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.49493, 0.47896, 0.40798, 0.54214, 0.53212, 0.48202, 0.47656, 0.46329, 0.48506])\n        assert np.abs(image_slice - expected_slice).max() < 1e-4\n\n    def test_stable_diffusion_pndm(self):\n        pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-base\")\n        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config)\n        pi",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 249, "column": 0 },
          "end": { "row": 249, "column": 0 }
        }
      }
    }
  ],
  [
    "647",
    {
      "pageContent": "class StableDiffusion2PipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 64, 64))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"prompt\": \"a photograph of an astronaut riding a horse\",\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_2_0_default_ddim(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-base\").to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_2_text2img/stable_diffusion_2_0_base_ddim.npy\"\n        )\n        max_diff = np.abs(expected_image - image).max()\n        assert max_diff < 1e-3\n\n    def test_stable_diffusion_2_1_default_pndm(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(torch_device)\n        sd_pipe.set_progress_bar_c",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion.py",
        "range": {
          "start": { "row": 452, "column": 0 },
          "end": { "row": 452, "column": 0 }
        }
      }
    }
  ],
  [
    "648",
    {
      "pageContent": "class StableDiffusionAttendAndExcitePipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionAttendAndExcitePipeline\n    test_attention_slicing = False\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS.union({\"token_indices\"})\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=(2, 4),\n            use_linear_projection=True,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n            sample_size=128,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n     ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "649",
    {
      "pageContent": "class StableDiffusionAttendAndExcitePipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_attend_and_excite_fp16(self):\n        generator = torch.manual_seed(51)\n\n        pipe = StableDiffusionAttendAndExcitePipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\", safety_checker=None, torch_dtype=torch.float16\n        )\n        pipe.to(\"cuda\")\n\n        prompt = \"a painting of an elephant with glasses\"\n        token_indices = [5, 7]\n\n        image = pipe(\n            prompt=prompt,\n            token_indices=token_indices,\n            guidance_scale=7.5,\n            generator=generator,\n            num_inference_steps=5,\n            max_iter_to_alter=5,\n            output_type=\"numpy\",\n        ).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/attend-and-excite/elephant_glasses.npy\"\n        )\n        assert np.abs((expected_image - image).max()) < 5e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 147, "column": 0 },
          "end": { "row": 147, "column": 0 }
        }
      }
    }
  ],
  [
    "650",
    {
      "pageContent": "class StableDiffusion2VPredictionPipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def dummy_cond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=(2, 4),\n            use_linear_projection=True,\n        )\n        return model\n\n    @property\n    def dummy_vae(self):\n        torch.manual_seed(0)\n        model = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n            sample_size=128,\n        )\n        return model\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "651",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "652",
    {
      "pageContent": "def test_stable_diffusion_v_pred_ddim(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n            prediction_type=\"v_prediction\",\n        )\n\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=None,\n            requires_safety_checker=False,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            return_dict=False,\n ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py",
        "range": {
          "start": { "row": 96, "column": 4 },
          "end": { "row": 96, "column": 4 }
        }
      }
    }
  ],
  [
    "653",
    {
      "pageContent": "def test_stable_diffusion_v_pred_k_euler(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = EulerDiscreteScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", prediction_type=\"v_prediction\"\n        )\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=None,\n            requires_safety_checker=False,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = im",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py",
        "range": {
          "start": { "row": 151, "column": 4 },
          "end": { "row": 151, "column": 4 }
        }
      }
    }
  ],
  [
    "654",
    {
      "pageContent": "class StableDiffusion2VPredictionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_diffusion_v_pred_default(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.enable_attention_slicing()\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=7.5, num_inference_steps=20, output_type=\"np\")\n\n        image = output.images\n        image_slice = image[0, 253:256, 253:256, -1]\n\n        assert image.shape == (1, 768, 768, 3)\n        expected_slice = np.array([0.1868, 0.1922, 0.1527, 0.1921, 0.1908, 0.1624, 0.1779, 0.1652, 0.1734])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_stable_diffusion_v_pred_upcast_attention(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16\n        )\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.enable_attention_slicing()\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=7.5, num_inference_steps=20, outpu",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py",
        "range": {
          "start": { "row": 244, "column": 0 },
          "end": { "row": 244, "column": 0 }
        }
      }
    }
  ],
  [
    "655",
    {
      "pageContent": "class StableDiffusion2InpaintPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionInpaintPipeline\n    params = TEXT_GUIDED_IMAGE_INPAINTING_PARAMS\n    batch_params = TEXT_GUIDED_IMAGE_INPAINTING_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=(2, 4),\n            use_linear_projection=True,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n            sample_size=128,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n            # SD2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "656",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=(2, 4),\n            use_linear_projection=True,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n            sample_size=128,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n            # SD2-specific config below\n            hidden_act=\"gelu\",\n            projection_dim=512,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        co",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 40, "column": 4 },
          "end": { "row": 40, "column": 4 }
        }
      }
    }
  ],
  [
    "657",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        # TODO: use tensor inputs instead of PIL, this is here just to leave the old expected_slices untouched\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((64, 64))\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((64, 64))\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 95, "column": 4 },
          "end": { "row": 95, "column": 4 }
        }
      }
    }
  ],
  [
    "658",
    {
      "pageContent": "def test_stable_diffusion_inpaint(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInpaintPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.4727, 0.5735, 0.3941, 0.5446, 0.5926, 0.4394, 0.5062, 0.4654, 0.4476])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 116, "column": 4 },
          "end": { "row": 116, "column": 4 }
        }
      }
    }
  ],
  [
    "659",
    {
      "pageContent": "class StableDiffusionInpaintPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_diffusion_inpaint_pipeline(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/sd2-inpaint/init_image.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-inpaint/mask.png\"\n        )\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-inpaint\"\n            \"/yellow_cat_sitting_on_a_park_bench.npy\"\n        )\n\n        model_id = \"stabilityai/stable-diffusion-2-inpainting\"\n        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, safety_checker=None)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n\n        generator = torch.manual_seed(0)\n        output = pipe(\n            prompt=prompt,\n            image=init_image,\n            mask_image=mask_image,\n            generator=generator,\n            output_type=\"np\",\n        )\n        image = output.images[0]\n\n        assert image.shape == (512, 512, 3)\n        assert np.abs(expected_image - image).max() < 1e-3\n\n    def test_stable_diffusion_inpaint_pipeli",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 135, "column": 0 },
          "end": { "row": 135, "column": 0 }
        }
      }
    }
  ],
  [
    "660",
    {
      "pageContent": "class FlaxStableDiffusion2PipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n\n    def test_stable_diffusion_flax(self):\n        sd_pipe, params = FlaxStableDiffusionPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2\",\n            revision=\"bf16\",\n            dtype=jnp.bfloat16,\n        )\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        num_samples = jax.device_count()\n        prompt = num_samples * [prompt]\n        prompt_ids = sd_pipe.prepare_inputs(prompt)\n\n        params = replicate(params)\n        prompt_ids = shard(prompt_ids)\n\n        prng_seed = jax.random.PRNGKey(0)\n        prng_seed = jax.random.split(prng_seed, jax.device_count())\n\n        images = sd_pipe(prompt_ids, params, prng_seed, num_inference_steps=25, jit=True)[0]\n        assert images.shape == (jax.device_count(), 1, 768, 768, 3)\n\n        images = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\n        image_slice = images[0, 253:256, 253:256, -1]\n\n        output_slice = jnp.asarray(jax.device_get(image_slice.flatten()))\n        expected_slice = jnp.array([0.4238, 0.4414, 0.4395, 0.4453, 0.4629, 0.4590, 0.4531, 0.45508, 0.4512])\n        print(f\"output_slice: {output_slice}\")\n        assert jnp.abs(output_slice - expected_slice).max() < 1e-2\n\n    def test_stable_diffusion_dpm_flax(self):\n        model_id = \"stabilityai/stable-diffusion-2\"\n        scheduler, scheduler_params = FlaxDPMSolverMultistepScheduler.from_pretrained(model_id, s",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_flax.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "661",
    {
      "pageContent": "class StableDiffusionDepth2ImgPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionDepth2ImgPipeline\n    test_save_load_optional_components = False\n    params = TEXT_GUIDED_IMAGE_VARIATION_PARAMS - {\"height\", \"width\"}\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\"latents\"}\n    batch_params = TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS - {\"image\"}\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=5,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            attention_head_dim=(2, 4),\n            use_linear_projection=True,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n    ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py",
        "range": {
          "start": { "row": 61, "column": 0 },
          "end": { "row": 61, "column": 0 }
        }
      }
    }
  ],
  [
    "662",
    {
      "pageContent": "class StableDiffusionDepth2ImgPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/depth2img/two_cats.png\"\n        )\n        inputs = {\n            \"prompt\": \"two tigers\",\n            \"image\": init_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_depth2img_pipeline_default(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-depth\", safety_checker=None\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 480, 640, 3)\n        expected_slice = np.array([0.9057, 0.9365, 0.9258, 0.8937, 0.8555, 0.8541, 0.8260, 0.7747, 0.7421])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-4\n\n    def test_stable_diffusion_depth2img_pipeline_k_lms(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n           ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py",
        "range": {
          "start": { "row": 368, "column": 0 },
          "end": { "row": 368, "column": 0 }
        }
      }
    }
  ],
  [
    "663",
    {
      "pageContent": "class StableDiffusionImg2ImgPipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/depth2img/two_cats.png\"\n        )\n        inputs = {\n            \"prompt\": \"two tigers\",\n            \"image\": init_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_depth2img_pndm(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-depth\")\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_depth2img/stable_diffusion_2_0_pndm.npy\"\n        )\n        max_diff = np.abs(expected_image - image).max()\n        assert max_diff < 1e-3\n\n    def test_depth2img_ddim(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-depth\")\n        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n        pipe.to(torch_device)\n",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py",
        "range": {
          "start": { "row": 506, "column": 0 },
          "end": { "row": 506, "column": 0 }
        }
      }
    }
  ],
  [
    "664",
    {
      "pageContent": "class StableDiffusionLatentUpscalePipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionLatentUpscalePipeline\n    params = TEXT_GUIDED_IMAGE_VARIATION_PARAMS - {\n        \"height\",\n        \"width\",\n        \"cross_attention_kwargs\",\n        \"negative_prompt_embeds\",\n        \"prompt_embeds\",\n    }\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\"num_images_per_prompt\"}\n    batch_params = TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS\n    test_cpu_offload = True\n\n    @property\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 4\n        sizes = (16, 16)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            act_fn=\"gelu\",\n            attention_head_dim=8,\n            norm_num_groups=None,\n            block_out_channels=[32, 32, 64, 64],\n            time_cond_proj_dim=160,\n            conv_in_kernel=1,\n            conv_out_kernel=1,\n            cross_attention_dim=32,\n            down_block_types=(\n                \"KDownBlock2D\",\n                \"KCrossAttnDownBlock2D\",\n                \"KCrossAttnDownBlock2D\",\n                \"KCrossAttnDownBlock2D\",\n            ),\n            in_channels=8,\n            mid_block_type=None,\n            only_cross_attention=False,\n            out_channels=5,\n            resnet_time_scale_shift=\"scale_shift\",\n            time_embedding_type=\"fourier\",\n            timestep_post",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "665",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            act_fn=\"gelu\",\n            attention_head_dim=8,\n            norm_num_groups=None,\n            block_out_channels=[32, 32, 64, 64],\n            time_cond_proj_dim=160,\n            conv_in_kernel=1,\n            conv_out_kernel=1,\n            cross_attention_dim=32,\n            down_block_types=(\n                \"KDownBlock2D\",\n                \"KCrossAttnDownBlock2D\",\n                \"KCrossAttnDownBlock2D\",\n                \"KCrossAttnDownBlock2D\",\n            ),\n            in_channels=8,\n            mid_block_type=None,\n            only_cross_attention=False,\n            out_channels=5,\n            resnet_time_scale_shift=\"scale_shift\",\n            time_embedding_type=\"fourier\",\n            timestep_post_act=\"gelu\",\n            up_block_types=(\"KCrossAttnUpBlock2D\", \"KCrossAttnUpBlock2D\", \"KCrossAttnUpBlock2D\", \"KUpBlock2D\"),\n        )\n        vae = AutoencoderKL(\n            block_out_channels=[32, 32, 64, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\n                \"DownEncoderBlock2D\",\n                \"DownEncoderBlock2D\",\n                \"DownEncoderBlock2D\",\n                \"DownEncoderBlock2D\",\n            ],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        scheduler = EulerDiscreteScheduler(prediction_type=\"sample\")\n        text_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 62, "column": 4 },
          "end": { "row": 62, "column": 4 }
        }
      }
    }
  ],
  [
    "666",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": self.dummy_image.cpu(),\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 128, "column": 4 },
          "end": { "row": 128, "column": 4 }
        }
      }
    }
  ],
  [
    "667",
    {
      "pageContent": "def test_inference(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n        pipe = self.pipeline_class(**components)\n        pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        self.assertEqual(image.shape, (1, 256, 256, 3))\n        expected_slice = np.array(\n            [0.47222412, 0.41921633, 0.44717434, 0.46874192, 0.42588258, 0.46150726, 0.4677534, 0.45583832, 0.48579055]\n        )\n        max_diff = np.abs(image_slice.flatten() - expected_slice).max()\n        self.assertLessEqual(max_diff, 1e-3)",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 142, "column": 4 },
          "end": { "row": 142, "column": 4 }
        }
      }
    }
  ],
  [
    "668",
    {
      "pageContent": "class StableDiffusionLatentUpscalePipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_latent_upscaler_fp16(self):\n        generator = torch.manual_seed(33)\n\n        pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n        pipe.to(\"cuda\")\n\n        upscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(\n            \"stabilityai/sd-x2-latent-upscaler\", torch_dtype=torch.float16\n        )\n        upscaler.to(\"cuda\")\n\n        prompt = \"a photo of an astronaut high resolution, unreal engine, ultra realistic\"\n\n        low_res_latents = pipe(prompt, generator=generator, output_type=\"latent\").images\n\n        image = upscaler(\n            prompt=prompt,\n            image=low_res_latents,\n            num_inference_steps=20,\n            guidance_scale=0,\n            generator=generator,\n            output_type=\"np\",\n        ).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/latent-upscaler/astronaut_1024.npy\"\n        )\n        assert np.abs((expected_image - image).max()) < 5e-1\n\n    def test_latent_upscaler_fp16_image(self):\n        generator = torch.manual_seed(33)\n\n        upscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(\n            \"stabilityai/sd-x2-latent-upscaler\", torch_dtype=torch.float16\n        )\n        upscaler.to(\"cuda\")\n\n        prompt = \"the temple of fire by Ross Tran and Gerardo Dotto",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 167, "column": 0 },
          "end": { "row": 167, "column": 0 }
        }
      }
    }
  ],
  [
    "669",
    {
      "pageContent": "class FlaxStableDiffusionInpaintPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n\n    def test_stable_diffusion_inpaint_pipeline(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/sd2-inpaint/init_image.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-inpaint/mask.png\"\n        )\n\n        model_id = \"xvjiarui/stable-diffusion-2-inpainting\"\n        pipeline, params = FlaxStableDiffusionInpaintPipeline.from_pretrained(model_id, safety_checker=None)\n\n        prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n\n        prng_seed = jax.random.PRNGKey(0)\n        num_inference_steps = 50\n\n        num_samples = jax.device_count()\n        prompt = num_samples * [prompt]\n        init_image = num_samples * [init_image]\n        mask_image = num_samples * [mask_image]\n        prompt_ids, processed_masked_images, processed_masks = pipeline.prepare_inputs(prompt, init_image, mask_image)\n\n        # shard inputs and rng\n        params = replicate(params)\n        prng_seed = jax.random.split(prng_seed, jax.device_count())\n        prompt_ids = shard(prompt_ids)\n        processed_masked_images = shard(processed_masked_images)\n        processed_masks = shard(processed_masks)\n\n        output = pipeline(\n            prompt_ids, processed_masks, processed_masked_images, par",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_flax_inpaint.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "670",
    {
      "pageContent": "class StableDiffusionUpscalePipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    @property\n    def dummy_cond_unet_upscale(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=7,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=8,\n            use_linear_projection=True,\n            only_cross_attention=(True, True, False),\n            num_class_embeds=100,\n        )\n        return model\n\n    @property\n    def dummy_vae(self):\n        torch.manual_seed(0)\n        model = AutoencoderKL(\n            block_out_channels=[32, 32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "671",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 33, "column": 4 },
          "end": { "row": 33, "column": 4 }
        }
      }
    }
  ],
  [
    "672",
    {
      "pageContent": "def test_stable_diffusion_upscale(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet_upscale\n        low_res_scheduler = DDPMScheduler()\n        scheduler = DDIMScheduler(prediction_type=\"v_prediction\")\n        vae = self.dummy_vae\n        text_encoder = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        image = self.dummy_image.cpu().permute(0, 2, 3, 1)[0]\n        low_res_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((64, 64))\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionUpscalePipeline(\n            unet=unet,\n            low_res_scheduler=low_res_scheduler,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            max_noise_level=350,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe(\n            [prompt],\n            image=low_res_image,\n            generator=generator,\n            guidance_scale=6.0,\n            noise_level=20,\n            num_inference_steps=2,\n            output_type=\"np\",\n        )\n\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            image=low_res_image",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 100, "column": 4 },
          "end": { "row": 100, "column": 4 }
        }
      }
    }
  ],
  [
    "673",
    {
      "pageContent": "def test_stable_diffusion_upscale_batch(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet_upscale\n        low_res_scheduler = DDPMScheduler()\n        scheduler = DDIMScheduler(prediction_type=\"v_prediction\")\n        vae = self.dummy_vae\n        text_encoder = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        image = self.dummy_image.cpu().permute(0, 2, 3, 1)[0]\n        low_res_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((64, 64))\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionUpscalePipeline(\n            unet=unet,\n            low_res_scheduler=low_res_scheduler,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            max_noise_level=350,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        output = sd_pipe(\n            2 * [prompt],\n            image=2 * [low_res_image],\n            guidance_scale=6.0,\n            noise_level=20,\n            num_inference_steps=2,\n            output_type=\"np\",\n        )\n        image = output.images\n        assert image.shape[0] == 2\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe(\n            [prompt],\n            image=low_res_image,\n            generator=generator,\n            num_images_p",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 161, "column": 4 },
          "end": { "row": 161, "column": 4 }
        }
      }
    }
  ],
  [
    "674",
    {
      "pageContent": "class StableDiffusionUpscalePipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_diffusion_upscale_pipeline(self):\n        image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/sd2-upscale/low_res_cat.png\"\n        )\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale\"\n            \"/upsampled_cat.npy\"\n        )\n\n        model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n        pipe = StableDiffusionUpscalePipeline.from_pretrained(model_id)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        prompt = \"a cat sitting on a park bench\"\n\n        generator = torch.manual_seed(0)\n        output = pipe(\n            prompt=prompt,\n            image=image,\n            generator=generator,\n            output_type=\"np\",\n        )\n        image = output.images[0]\n\n        assert image.shape == (512, 512, 3)\n        assert np.abs(expected_image - image).max() < 1e-3\n\n    def test_stable_diffusion_upscale_pipeline_fp16(self):\n        image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/sd2-upscale/low_res_cat.png\"\n        )\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-int",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 258, "column": 0 },
          "end": { "row": 258, "column": 0 }
        }
      }
    }
  ],
  [
    "675",
    {
      "pageContent": "class PNDMPipelineFastTests(unittest.TestCase):\n    @property\n    def dummy_uncond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    def test_inference(self):\n        unet = self.dummy_uncond_unet\n        scheduler = PNDMScheduler()\n\n        pndm = PNDMPipeline(unet=unet, scheduler=scheduler)\n        pndm.to(torch_device)\n        pndm.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = pndm(generator=generator, num_inference_steps=20, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        image_from_tuple = pndm(generator=generator, num_inference_steps=20, output_type=\"numpy\", return_dict=False)[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/pndm/test_pndm.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "676",
    {
      "pageContent": "def test_inference(self):\n        unet = self.dummy_uncond_unet\n        scheduler = PNDMScheduler()\n\n        pndm = PNDMPipeline(unet=unet, scheduler=scheduler)\n        pndm.to(torch_device)\n        pndm.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = pndm(generator=generator, num_inference_steps=20, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        image_from_tuple = pndm(generator=generator, num_inference_steps=20, output_type=\"numpy\", return_dict=False)[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/pndm/test_pndm.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "677",
    {
      "pageContent": "class PNDMPipelineIntegrationTests(unittest.TestCase):\n    def test_inference_cifar10(self):\n        model_id = \"google/ddpm-cifar10-32\"\n\n        unet = UNet2DModel.from_pretrained(model_id)\n        scheduler = PNDMScheduler()\n\n        pndm = PNDMPipeline(unet=unet, scheduler=scheduler)\n        pndm.to(torch_device)\n        pndm.set_progress_bar_config(disable=None)\n        generator = torch.manual_seed(0)\n        image = pndm(generator=generator, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.1564, 0.14645, 0.1406, 0.14715, 0.12425, 0.14045, 0.13115, 0.12175, 0.125])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/pndm/test_pndm.py",
        "range": {
          "start": { "row": 68, "column": 0 },
          "end": { "row": 68, "column": 0 }
        }
      }
    }
  ],
  [
    "678",
    {
      "pageContent": "class KarrasVePipelineFastTests(unittest.TestCase):\n    @property\n    def dummy_uncond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    def test_inference(self):\n        unet = self.dummy_uncond_unet\n        scheduler = KarrasVeScheduler()\n\n        pipe = KarrasVePipeline(unet=unet, scheduler=scheduler)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = pipe(num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        image_from_tuple = pipe(num_inference_steps=2, generator=generator, output_type=\"numpy\", return_dict=False)[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/karras_ve/test_karras_ve.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "679",
    {
      "pageContent": "def test_inference(self):\n        unet = self.dummy_uncond_unet\n        scheduler = KarrasVeScheduler()\n\n        pipe = KarrasVePipeline(unet=unet, scheduler=scheduler)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = pipe(num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        image_from_tuple = pipe(num_inference_steps=2, generator=generator, output_type=\"numpy\", return_dict=False)[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/karras_ve/test_karras_ve.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "680",
    {
      "pageContent": "class KarrasVePipelineIntegrationTests(unittest.TestCase):\n    def test_inference(self):\n        model_id = \"google/ncsnpp-celebahq-256\"\n        model = UNet2DModel.from_pretrained(model_id)\n        scheduler = KarrasVeScheduler()\n\n        pipe = KarrasVePipeline(unet=model, scheduler=scheduler)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = pipe(num_inference_steps=20, generator=generator, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n        assert image.shape == (1, 256, 256, 3)\n        expected_slice = np.array([0.578, 0.5811, 0.5924, 0.5809, 0.587, 0.5886, 0.5861, 0.5802, 0.586])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/karras_ve/test_karras_ve.py",
        "range": {
          "start": { "row": 68, "column": 0 },
          "end": { "row": 68, "column": 0 }
        }
      }
    }
  ],
  [
    "681",
    {
      "pageContent": "class VersatileDiffusionDualGuidedPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_remove_unused_weights_save_load(self):\n        pipe = VersatileDiffusionDualGuidedPipeline.from_pretrained(\"shi-labs/versatile-diffusion\")\n        # remove text_unet\n        pipe.remove_unused_weights()\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        second_prompt = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/versatile_diffusion/benz.jpg\"\n        )\n\n        generator = torch.manual_seed(0)\n        image = pipe(\n            prompt=\"first prompt\",\n            image=second_prompt,\n            text_to_image_strength=0.75,\n            generator=generator,\n            guidance_scale=7.5,\n            num_inference_steps=2,\n            output_type=\"numpy\",\n        ).images\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe = VersatileDiffusionDualGuidedPipeline.from_pretrained(tmpdirname)\n\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = generator.manual_seed(0)\n        new_image = pipe(\n            prompt=\"first prompt\",\n            image=second_prompt,\n            text_to_image_strength=0.75,\n            generator=generator,\n            guidance_scale=7.5,\n            num_inference_steps=2,\n            output_type=\"num",
      "metadata": {
        "source": "tests/pipelines/versatile_diffusion/test_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "682",
    {
      "pageContent": "class VersatileDiffusionTextToImagePipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_remove_unused_weights_save_load(self):\n        pipe = VersatileDiffusionTextToImagePipeline.from_pretrained(\"shi-labs/versatile-diffusion\")\n        # remove text_unet\n        pipe.remove_unused_weights()\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger \"\n        generator = torch.manual_seed(0)\n        image = pipe(\n            prompt=prompt, generator=generator, guidance_scale=7.5, num_inference_steps=2, output_type=\"numpy\"\n        ).images\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe = VersatileDiffusionTextToImagePipeline.from_pretrained(tmpdirname)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = generator.manual_seed(0)\n        new_image = pipe(\n            prompt=prompt, generator=generator, guidance_scale=7.5, num_inference_steps=2, output_type=\"numpy\"\n        ).images\n\n        assert np.abs(image - new_image).sum() < 1e-5, \"Models don't have the same forward pass\"\n\n    def test_inference_text2img(self):\n        pipe = VersatileDiffusionTextToImagePipeline.from_pretrained(\n            \"shi-labs/versatile-diffusion\", torch_dtype=torch.float16\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_c",
      "metadata": {
        "source": "tests/pipelines/versatile_diffusion/test_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "683",
    {
      "pageContent": "class VersatileDiffusionImageVariationPipelineIntegrationTests(unittest.TestCase):\n    def test_inference_image_variations(self):\n        pipe = VersatileDiffusionImageVariationPipeline.from_pretrained(\"shi-labs/versatile-diffusion\")\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        image_prompt = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/versatile_diffusion/benz.jpg\"\n        )\n        generator = torch.manual_seed(0)\n        image = pipe(\n            image=image_prompt,\n            generator=generator,\n            guidance_scale=7.5,\n            num_inference_steps=50,\n            output_type=\"numpy\",\n        ).images\n\n        image_slice = image[0, 253:256, 253:256, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.0441, 0.0469, 0.0507, 0.0575, 0.0632, 0.0650, 0.0865, 0.0909, 0.0945])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/versatile_diffusion/test_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "684",
    {
      "pageContent": "class VersatileDiffusionMegaPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_from_save_pretrained(self):\n        pipe = VersatileDiffusionPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", torch_dtype=torch.float16)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/versatile_diffusion/benz.jpg\"\n        )\n\n        generator = torch.manual_seed(0)\n        image = pipe.dual_guided(\n            prompt=\"first prompt\",\n            image=prompt_image,\n            text_to_image_strength=0.75,\n            generator=generator,\n            guidance_scale=7.5,\n            num_inference_steps=2,\n            output_type=\"numpy\",\n        ).images\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe = VersatileDiffusionPipeline.from_pretrained(tmpdirname, torch_dtype=torch.float16)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = generator.manual_seed(0)\n        new_image = pipe.dual_guided(\n            prompt=\"first prompt\",\n            image=prompt_image,\n            text_to_image_strength=0.75,\n            generator=generator,\n            guidance_scale=7.5,\n            num_inference_steps=2,\n            output_type=\"numpy\",\n        ).images\n\n    ",
      "metadata": {
        "source": "tests/pipelines/versatile_diffusion/test_versatile_diffusion_mega.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "685",
    {
      "pageContent": "class DiTPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = DiTPipeline\n    params = CLASS_CONDITIONED_IMAGE_GENERATION_PARAMS\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\n        \"latents\",\n        \"num_images_per_prompt\",\n        \"callback\",\n        \"callback_steps\",\n    }\n    batch_params = CLASS_CONDITIONED_IMAGE_GENERATION_BATCH_PARAMS\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        transformer = Transformer2DModel(\n            sample_size=16,\n            num_layers=2,\n            patch_size=4,\n            attention_head_dim=8,\n            num_attention_heads=2,\n            in_channels=4,\n            out_channels=8,\n            attention_bias=True,\n            activation_fn=\"gelu-approximate\",\n            num_embeds_ada_norm=1000,\n            norm_type=\"ada_norm_zero\",\n            norm_elementwise_affine=False,\n        )\n        vae = AutoencoderKL()\n        scheduler = DDIMScheduler()\n        components = {\"transformer\": transformer.eval(), \"vae\": vae.eval(), \"scheduler\": scheduler}\n        return components\n\n    def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"class_labels\": [1],\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_inference(self):\n",
      "metadata": {
        "source": "tests/pipelines/dit/test_dit.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "686",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        transformer = Transformer2DModel(\n            sample_size=16,\n            num_layers=2,\n            patch_size=4,\n            attention_head_dim=8,\n            num_attention_heads=2,\n            in_channels=4,\n            out_channels=8,\n            attention_bias=True,\n            activation_fn=\"gelu-approximate\",\n            num_embeds_ada_norm=1000,\n            norm_type=\"ada_norm_zero\",\n            norm_elementwise_affine=False,\n        )\n        vae = AutoencoderKL()\n        scheduler = DDIMScheduler()\n        components = {\"transformer\": transformer.eval(), \"vae\": vae.eval(), \"scheduler\": scheduler}\n        return components",
      "metadata": {
        "source": "tests/pipelines/dit/test_dit.py",
        "range": {
          "start": { "row": 47, "column": 4 },
          "end": { "row": 47, "column": 4 }
        }
      }
    }
  ],
  [
    "687",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"class_labels\": [1],\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/dit/test_dit.py",
        "range": {
          "start": { "row": 68, "column": 4 },
          "end": { "row": 68, "column": 4 }
        }
      }
    }
  ],
  [
    "688",
    {
      "pageContent": "def test_inference(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n        pipe = self.pipeline_class(**components)\n        pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        self.assertEqual(image.shape, (1, 16, 16, 3))\n        expected_slice = np.array([0.4380, 0.4141, 0.5159, 0.0000, 0.4282, 0.6680, 0.5485, 0.2545, 0.6719])\n        max_diff = np.abs(image_slice.flatten() - expected_slice).max()\n        self.assertLessEqual(max_diff, 1e-3)",
      "metadata": {
        "source": "tests/pipelines/dit/test_dit.py",
        "range": {
          "start": { "row": 81, "column": 4 },
          "end": { "row": 81, "column": 4 }
        }
      }
    }
  ],
  [
    "689",
    {
      "pageContent": "class DiTPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_dit_256(self):\n        generator = torch.manual_seed(0)\n\n        pipe = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-256\")\n        pipe.to(\"cuda\")\n\n        words = [\"vase\", \"umbrella\", \"white shark\", \"white wolf\"]\n        ids = pipe.get_label_ids(words)\n\n        images = pipe(ids, generator=generator, num_inference_steps=40, output_type=\"np\").images\n\n        for word, image in zip(words, images):\n            expected_image = load_numpy(\n                f\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/dit/{word}.npy\"\n            )\n            assert np.abs((expected_image - image).max()) < 1e-3\n\n    def test_dit_512_fp16(self):\n        pipe = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-512\", torch_dtype=torch.float16)\n        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n        pipe.to(\"cuda\")\n\n        words = [\"vase\", \"umbrella\"]\n        ids = pipe.get_label_ids(words)\n\n        generator = torch.manual_seed(0)\n        images = pipe(ids, generator=generator, num_inference_steps=25, output_type=\"np\").images\n\n        for word, image in zip(words, images):\n            expected_image = load_numpy(\n                \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n                f\"/dit/{word}_fp16.npy\"\n            )\n\n            assert np.abs((expected_image - image).max()) < 7.5e-1",
      "metadata": {
        "source": "tests/pipelines/dit/test_dit.py",
        "range": {
          "start": { "row": 104, "column": 0 },
          "end": { "row": 104, "column": 0 }
        }
      }
    }
  ],
  [
    "690",
    {
      "pageContent": "class UnCLIPPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = UnCLIPPipeline\n    params = TEXT_TO_IMAGE_PARAMS - {\n        \"negative_prompt\",\n        \"height\",\n        \"width\",\n        \"negative_prompt_embeds\",\n        \"guidance_scale\",\n        \"prompt_embeds\",\n        \"cross_attention_kwargs\",\n    }\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n    required_optional_params = [\n        \"generator\",\n        \"return_dict\",\n        \"prior_num_inference_steps\",\n        \"decoder_num_inference_steps\",\n        \"super_res_num_inference_steps\",\n    ]\n    test_xformers_attention = False\n\n    @property\n    def text_embedder_hidden_size(self):\n        return 32\n\n    @property\n    def time_input_dim(self):\n        return 32\n\n    @property\n    def block_out_channels_0(self):\n        return self.time_input_dim\n\n    @property\n    def time_embed_dim(self):\n        return self.time_input_dim * 4\n\n    @property\n    def cross_attention_dim(self):\n        return 100\n\n    @property\n    def dummy_tokenizer(self):\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n        return tokenizer\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=self.text_embedder_hidden_size,\n            projection_dim=self.text_embedder_hidden_size,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n   ",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "691",
    {
      "pageContent": "def get_dummy_components(self):\n        prior = self.dummy_prior\n        decoder = self.dummy_decoder\n        text_proj = self.dummy_text_proj\n        text_encoder = self.dummy_text_encoder\n        tokenizer = self.dummy_tokenizer\n        super_res_first = self.dummy_super_res_first\n        super_res_last = self.dummy_super_res_last\n\n        prior_scheduler = UnCLIPScheduler(\n            variance_type=\"fixed_small_log\",\n            prediction_type=\"sample\",\n            num_train_timesteps=1000,\n            clip_sample_range=5.0,\n        )\n\n        decoder_scheduler = UnCLIPScheduler(\n            variance_type=\"learned_range\",\n            prediction_type=\"epsilon\",\n            num_train_timesteps=1000,\n        )\n\n        super_res_scheduler = UnCLIPScheduler(\n            variance_type=\"fixed_small_log\",\n            prediction_type=\"epsilon\",\n            num_train_timesteps=1000,\n        )\n\n        components = {\n            \"prior\": prior,\n            \"decoder\": decoder,\n            \"text_proj\": text_proj,\n            \"text_encoder\": text_encoder,\n            \"tokenizer\": tokenizer,\n            \"super_res_first\": super_res_first,\n            \"super_res_last\": super_res_last,\n            \"prior_scheduler\": prior_scheduler,\n            \"decoder_scheduler\": decoder_scheduler,\n            \"super_res_scheduler\": super_res_scheduler,\n        }\n\n        return components",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 172, "column": 4 },
          "end": { "row": 172, "column": 4 }
        }
      }
    }
  ],
  [
    "692",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"horse\",\n            \"generator\": generator,\n            \"prior_num_inference_steps\": 2,\n            \"decoder_num_inference_steps\": 2,\n            \"super_res_num_inference_steps\": 2,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 215, "column": 4 },
          "end": { "row": 215, "column": 4 }
        }
      }
    }
  ],
  [
    "693",
    {
      "pageContent": "def test_unclip(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n\n        pipe = self.pipeline_class(**components)\n        pipe = pipe.to(device)\n\n        pipe.set_progress_bar_config(disable=None)\n\n        output = pipe(**self.get_dummy_inputs(device))\n        image = output.images\n\n        image_from_tuple = pipe(\n            **self.get_dummy_inputs(device),\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n\n        expected_slice = np.array(\n            [\n                0.9997,\n                0.9988,\n                0.0028,\n                0.9997,\n                0.9984,\n                0.9965,\n                0.0029,\n                0.9986,\n                0.0025,\n            ]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 230, "column": 4 },
          "end": { "row": 230, "column": 4 }
        }
      }
    }
  ],
  [
    "694",
    {
      "pageContent": "def test_unclip_passed_text_embed(self):\n        device = torch.device(\"cpu\")\n\n        class DummyScheduler:\n            init_noise_sigma = 1\n\n        components = self.get_dummy_components()\n\n        pipe = self.pipeline_class(**components)\n        pipe = pipe.to(device)\n\n        prior = components[\"prior\"]\n        decoder = components[\"decoder\"]\n        super_res_first = components[\"super_res_first\"]\n        tokenizer = components[\"tokenizer\"]\n        text_encoder = components[\"text_encoder\"]\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        dtype = prior.dtype\n        batch_size = 1\n\n        shape = (batch_size, prior.config.embedding_dim)\n        prior_latents = pipe.prepare_latents(\n            shape, dtype=dtype, device=device, generator=generator, latents=None, scheduler=DummyScheduler()\n        )\n        shape = (batch_size, decoder.in_channels, decoder.sample_size, decoder.sample_size)\n        decoder_latents = pipe.prepare_latents(\n            shape, dtype=dtype, device=device, generator=generator, latents=None, scheduler=DummyScheduler()\n        )\n\n        shape = (\n            batch_size,\n            super_res_first.in_channels // 2,\n            super_res_first.sample_size,\n            super_res_first.sample_size,\n        )\n        super_res_latents = pipe.prepare_latents(\n            shape, dtype=dtype, device=device, generator=generator, latents=None, scheduler=DummyScheduler()\n        )\n\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"this is a prompt example\"\n\n        generator = torch.Generator(device=device).m",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 270, "column": 4 },
          "end": { "row": 270, "column": 4 }
        }
      }
    }
  ],
  [
    "695",
    {
      "pageContent": "def test_inference_batch_consistent(self):\n        additional_params_copy_to_batched_inputs = [\n            \"prior_num_inference_steps\",\n            \"decoder_num_inference_steps\",\n            \"super_res_num_inference_steps\",\n        ]\n\n        if torch_device == \"mps\":\n            # TODO: MPS errors with larger batch sizes\n            batch_sizes = [2, 3]\n            self._test_inference_batch_consistent(\n                batch_sizes=batch_sizes,\n                additional_params_copy_to_batched_inputs=additional_params_copy_to_batched_inputs,\n            )\n        else:\n            self._test_inference_batch_consistent(\n                additional_params_copy_to_batched_inputs=additional_params_copy_to_batched_inputs\n            )",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 380, "column": 4 },
          "end": { "row": 380, "column": 4 }
        }
      }
    }
  ],
  [
    "696",
    {
      "pageContent": "class UnCLIPPipelineCPUIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_unclip_karlo_cpu_fp32(self):\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/unclip/karlo_v1_alpha_horse_cpu.npy\"\n        )\n\n        pipeline = UnCLIPPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha\")\n        pipeline.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        output = pipeline(\n            \"horse\",\n            num_images_per_prompt=1,\n            generator=generator,\n            output_type=\"np\",\n        )\n\n        image = output.images[0]\n\n        assert image.shape == (256, 256, 3)\n        assert np.abs(expected_image - image).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 413, "column": 0 },
          "end": { "row": 413, "column": 0 }
        }
      }
    }
  ],
  [
    "697",
    {
      "pageContent": "class UnCLIPPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_unclip_karlo(self):\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/unclip/karlo_v1_alpha_horse_fp16.npy\"\n        )\n\n        pipeline = UnCLIPPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha\", torch_dtype=torch.float16)\n        pipeline = pipeline.to(torch_device)\n        pipeline.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=\"cpu\").manual_seed(0)\n        output = pipeline(\n            \"horse\",\n            generator=generator,\n            output_type=\"np\",\n        )\n\n        image = output.images[0]\n\n        assert image.shape == (256, 256, 3)\n\n        assert_mean_pixel_difference(image, expected_image)\n\n    def test_unclip_pipeline_with_sequential_cpu_offloading(self):\n        torch.cuda.empty_cache()\n        torch.cuda.reset_max_memory_allocated()\n        torch.cuda.reset_peak_memory_stats()\n\n        pipe = UnCLIPPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha\", torch_dtype=torch.float16)\n        pipe = pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n        pipe.enable_sequential_cpu_offload()\n\n        _ = pipe(\n            \"horse\",\n            num_images_per_prompt=1,\n            prior_num_inference_steps=2,\n            decoder_num_inference_steps=2,\n",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip.py",
        "range": {
          "start": { "row": 445, "column": 0 },
          "end": { "row": 445, "column": 0 }
        }
      }
    }
  ],
  [
    "698",
    {
      "pageContent": "class UnCLIPImageVariationPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = UnCLIPImageVariationPipeline\n    params = IMAGE_VARIATION_PARAMS - {\"height\", \"width\", \"guidance_scale\"}\n    batch_params = IMAGE_VARIATION_BATCH_PARAMS\n\n    required_optional_params = [\n        \"generator\",\n        \"return_dict\",\n        \"decoder_num_inference_steps\",\n        \"super_res_num_inference_steps\",\n    ]\n\n    @property\n    def text_embedder_hidden_size(self):\n        return 32\n\n    @property\n    def time_input_dim(self):\n        return 32\n\n    @property\n    def block_out_channels_0(self):\n        return self.time_input_dim\n\n    @property\n    def time_embed_dim(self):\n        return self.time_input_dim * 4\n\n    @property\n    def cross_attention_dim(self):\n        return 100\n\n    @property\n    def dummy_tokenizer(self):\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n        return tokenizer\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=self.text_embedder_hidden_size,\n            projection_dim=self.text_embedder_hidden_size,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        return CLIPTextModelWithProjection(config)\n\n    @property\n    def dummy_image_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPVisi",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 45, "column": 0 },
          "end": { "row": 45, "column": 0 }
        }
      }
    }
  ],
  [
    "699",
    {
      "pageContent": "def get_dummy_components(self):\n        decoder = self.dummy_decoder\n        text_proj = self.dummy_text_proj\n        text_encoder = self.dummy_text_encoder\n        tokenizer = self.dummy_tokenizer\n        super_res_first = self.dummy_super_res_first\n        super_res_last = self.dummy_super_res_last\n\n        decoder_scheduler = UnCLIPScheduler(\n            variance_type=\"learned_range\",\n            prediction_type=\"epsilon\",\n            num_train_timesteps=1000,\n        )\n\n        super_res_scheduler = UnCLIPScheduler(\n            variance_type=\"fixed_small_log\",\n            prediction_type=\"epsilon\",\n            num_train_timesteps=1000,\n        )\n\n        feature_extractor = CLIPImageProcessor(crop_size=32, size=32)\n\n        image_encoder = self.dummy_image_encoder\n\n        return {\n            \"decoder\": decoder,\n            \"text_encoder\": text_encoder,\n            \"tokenizer\": tokenizer,\n            \"text_proj\": text_proj,\n            \"feature_extractor\": feature_extractor,\n            \"image_encoder\": image_encoder,\n            \"super_res_first\": super_res_first,\n            \"super_res_last\": super_res_last,\n            \"decoder_scheduler\": decoder_scheduler,\n            \"super_res_scheduler\": super_res_scheduler,\n        }",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 177, "column": 4 },
          "end": { "row": 177, "column": 4 }
        }
      }
    }
  ],
  [
    "700",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0, pil_image=True):\n        input_image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n\n        if pil_image:\n            input_image = input_image * 0.5 + 0.5\n            input_image = input_image.clamp(0, 1)\n            input_image = input_image.cpu().permute(0, 2, 3, 1).float().numpy()\n            input_image = DiffusionPipeline.numpy_to_pil(input_image)[0]\n\n        return {\n            \"image\": input_image,\n            \"generator\": generator,\n            \"decoder_num_inference_steps\": 2,\n            \"super_res_num_inference_steps\": 2,\n            \"output_type\": \"np\",\n        }",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 214, "column": 4 },
          "end": { "row": 214, "column": 4 }
        }
      }
    }
  ],
  [
    "701",
    {
      "pageContent": "def test_unclip_image_variation_input_tensor(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n\n        pipe = self.pipeline_class(**components)\n        pipe = pipe.to(device)\n\n        pipe.set_progress_bar_config(disable=None)\n\n        pipeline_inputs = self.get_dummy_inputs(device, pil_image=False)\n\n        output = pipe(**pipeline_inputs)\n        image = output.images\n\n        tuple_pipeline_inputs = self.get_dummy_inputs(device, pil_image=False)\n\n        image_from_tuple = pipe(\n            **tuple_pipeline_inputs,\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n\n        expected_slice = np.array(\n            [\n                0.9997,\n                0.0002,\n                0.9997,\n                0.9997,\n                0.9969,\n                0.0023,\n                0.9997,\n                0.9969,\n                0.9970,\n            ]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 235, "column": 4 },
          "end": { "row": 235, "column": 4 }
        }
      }
    }
  ],
  [
    "702",
    {
      "pageContent": "def test_unclip_image_variation_input_image(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n\n        pipe = self.pipeline_class(**components)\n        pipe = pipe.to(device)\n\n        pipe.set_progress_bar_config(disable=None)\n\n        pipeline_inputs = self.get_dummy_inputs(device, pil_image=True)\n\n        output = pipe(**pipeline_inputs)\n        image = output.images\n\n        tuple_pipeline_inputs = self.get_dummy_inputs(device, pil_image=True)\n\n        image_from_tuple = pipe(\n            **tuple_pipeline_inputs,\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n\n        expected_slice = np.array([0.9997, 0.0003, 0.9997, 0.9997, 0.9970, 0.0024, 0.9997, 0.9971, 0.9971])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 279, "column": 4 },
          "end": { "row": 279, "column": 4 }
        }
      }
    }
  ],
  [
    "703",
    {
      "pageContent": "def test_unclip_image_variation_input_list_images(self):\n        device = \"cpu\"\n\n        components = self.get_dummy_components()\n\n        pipe = self.pipeline_class(**components)\n        pipe = pipe.to(device)\n\n        pipe.set_progress_bar_config(disable=None)\n\n        pipeline_inputs = self.get_dummy_inputs(device, pil_image=True)\n        pipeline_inputs[\"image\"] = [\n            pipeline_inputs[\"image\"],\n            pipeline_inputs[\"image\"],\n        ]\n\n        output = pipe(**pipeline_inputs)\n        image = output.images\n\n        tuple_pipeline_inputs = self.get_dummy_inputs(device, pil_image=True)\n        tuple_pipeline_inputs[\"image\"] = [\n            tuple_pipeline_inputs[\"image\"],\n            tuple_pipeline_inputs[\"image\"],\n        ]\n\n        image_from_tuple = pipe(\n            **tuple_pipeline_inputs,\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (2, 64, 64, 3)\n\n        expected_slice = np.array(\n            [\n                0.9997,\n                0.9989,\n                0.0008,\n                0.0021,\n                0.9960,\n                0.0018,\n                0.0014,\n                0.0002,\n                0.9933,\n            ]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 311, "column": 4 },
          "end": { "row": 311, "column": 4 }
        }
      }
    }
  ],
  [
    "704",
    {
      "pageContent": "def test_unclip_passed_image_embed(self):\n        device = torch.device(\"cpu\")\n\n        class DummyScheduler:\n            init_noise_sigma = 1\n\n        components = self.get_dummy_components()\n\n        pipe = self.pipeline_class(**components)\n        pipe = pipe.to(device)\n\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        dtype = pipe.decoder.dtype\n        batch_size = 1\n\n        shape = (batch_size, pipe.decoder.in_channels, pipe.decoder.sample_size, pipe.decoder.sample_size)\n        decoder_latents = pipe.prepare_latents(\n            shape, dtype=dtype, device=device, generator=generator, latents=None, scheduler=DummyScheduler()\n        )\n\n        shape = (\n            batch_size,\n            pipe.super_res_first.in_channels // 2,\n            pipe.super_res_first.sample_size,\n            pipe.super_res_first.sample_size,\n        )\n        super_res_latents = pipe.prepare_latents(\n            shape, dtype=dtype, device=device, generator=generator, latents=None, scheduler=DummyScheduler()\n        )\n\n        pipeline_inputs = self.get_dummy_inputs(device, pil_image=False)\n\n        img_out_1 = pipe(\n            **pipeline_inputs, decoder_latents=decoder_latents, super_res_latents=super_res_latents\n        ).images\n\n        pipeline_inputs = self.get_dummy_inputs(device, pil_image=False)\n        # Don't pass image, instead pass embedding\n        image = pipeline_inputs.pop(\"image\")\n        image_embeddings = pipe.image_encoder(image).image_embeds\n\n        img_out_2 = pipe(\n            **pipeline_input",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 363, "column": 4 },
          "end": { "row": 363, "column": 4 }
        }
      }
    }
  ],
  [
    "705",
    {
      "pageContent": "def test_inference_batch_consistent(self):\n        additional_params_copy_to_batched_inputs = [\n            \"decoder_num_inference_steps\",\n            \"super_res_num_inference_steps\",\n        ]\n\n        if torch_device == \"mps\":\n            # TODO: MPS errors with larger batch sizes\n            batch_sizes = [2, 3]\n            self._test_inference_batch_consistent(\n                batch_sizes=batch_sizes,\n                additional_params_copy_to_batched_inputs=additional_params_copy_to_batched_inputs,\n            )\n        else:\n            self._test_inference_batch_consistent(\n                additional_params_copy_to_batched_inputs=additional_params_copy_to_batched_inputs\n            )",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 441, "column": 4 },
          "end": { "row": 441, "column": 4 }
        }
      }
    }
  ],
  [
    "706",
    {
      "pageContent": "class UnCLIPImageVariationPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_unclip_image_variation_karlo(self):\n        input_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unclip/cat.png\"\n        )\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/unclip/karlo_v1_alpha_cat_variation_fp16.npy\"\n        )\n\n        pipeline = UnCLIPImageVariationPipeline.from_pretrained(\n            \"kakaobrain/karlo-v1-alpha-image-variations\", torch_dtype=torch.float16\n        )\n        pipeline = pipeline.to(torch_device)\n        pipeline.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=\"cpu\").manual_seed(0)\n        output = pipeline(\n            input_image,\n            generator=generator,\n            output_type=\"np\",\n        )\n\n        image = output.images[0]\n\n        assert image.shape == (256, 256, 3)\n\n        assert_mean_pixel_difference(image, expected_image)",
      "metadata": {
        "source": "tests/pipelines/unclip/test_unclip_image_variation.py",
        "range": {
          "start": { "row": 474, "column": 0 },
          "end": { "row": 474, "column": 0 }
        }
      }
    }
  ],
  [
    "707",
    {
      "pageContent": "class PaintByExamplePipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = PaintByExamplePipeline\n    params = IMAGE_GUIDED_IMAGE_INPAINTING_PARAMS\n    batch_params = IMAGE_GUIDED_IMAGE_INPAINTING_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        config = CLIPVisionConfig(\n            hidden_size=32,\n            projection_dim=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            image_size=32,\n            patch_size=4,\n        )\n        image_encoder = PaintByExampleImageEncoder(config, proj_size=32)\n        feature_extractor = CLIPImageProcessor(crop_size=32, size=32)\n\n        components = {\n            \"unet\": unet,\n            \"sche",
      "metadata": {
        "source": "tests/pipelines/paint_by_example/test_paint_by_example.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "708",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        config = CLIPVisionConfig(\n            hidden_size=32,\n            projection_dim=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            image_size=32,\n            patch_size=4,\n        )\n        image_encoder = PaintByExampleImageEncoder(config, proj_size=32)\n        feature_extractor = CLIPImageProcessor(crop_size=32, size=32)\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae\": vae,\n            \"image_encoder\": image_encoder,\n            \"safety_checker\": None,\n            \"feature_extractor\": feature_extractor,\n        }\n        return components",
      "metadata": {
        "source": "tests/pipelines/paint_by_example/test_paint_by_example.py",
        "range": {
          "start": { "row": 41, "column": 4 },
          "end": { "row": 41, "column": 4 }
        }
      }
    }
  ],
  [
    "709",
    {
      "pageContent": "def convert_to_pt(self, image):\n        image = np.array(image.convert(\"RGB\"))\n        image = image[None].transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        return image",
      "metadata": {
        "source": "tests/pipelines/paint_by_example/test_paint_by_example.py",
        "range": {
          "start": { "row": 87, "column": 4 },
          "end": { "row": 87, "column": 4 }
        }
      }
    }
  ],
  [
    "710",
    {
      "pageContent": "def get_dummy_inputs(self, device=\"cpu\", seed=0):\n        # TODO: use tensor inputs instead of PIL, this is here just to leave the old expected_slices untouched\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((64, 64))\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((64, 64))\n        example_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((32, 32))\n\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"example_image\": example_image,\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/paint_by_example/test_paint_by_example.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "711",
    {
      "pageContent": "def test_paint_by_example_inpaint(self):\n        components = self.get_dummy_components()\n\n        # make sure here that pndm scheduler skips prk\n        pipe = PaintByExamplePipeline(**components)\n        pipe = pipe.to(\"cpu\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        output = pipe(**inputs)\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.4701, 0.5555, 0.3994, 0.5107, 0.5691, 0.4517, 0.5125, 0.4769, 0.4539])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/paint_by_example/test_paint_by_example.py",
        "range": {
          "start": { "row": 116, "column": 4 },
          "end": { "row": 116, "column": 4 }
        }
      }
    }
  ],
  [
    "712",
    {
      "pageContent": "def test_paint_by_example_image_tensor(self):\n        device = \"cpu\"\n        inputs = self.get_dummy_inputs()\n        inputs.pop(\"mask_image\")\n        image = self.convert_to_pt(inputs.pop(\"image\"))\n        mask_image = image.clamp(0, 1) / 2\n\n        # make sure here that pndm scheduler skips prk\n        pipe = PaintByExamplePipeline(**self.get_dummy_components())\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        output = pipe(image=image, mask_image=mask_image[:, 0], **inputs)\n        out_1 = output.images\n\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        mask_image = mask_image.cpu().permute(0, 2, 3, 1)[0]\n\n        image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        mask_image = Image.fromarray(np.uint8(mask_image)).convert(\"RGB\")\n\n        output = pipe(**self.get_dummy_inputs())\n        out_2 = output.images\n\n        assert out_1.shape == (1, 64, 64, 3)\n        assert np.abs(out_1.flatten() - out_2.flatten()).max() < 5e-2",
      "metadata": {
        "source": "tests/pipelines/paint_by_example/test_paint_by_example.py",
        "range": {
          "start": { "row": 135, "column": 4 },
          "end": { "row": 135, "column": 4 }
        }
      }
    }
  ],
  [
    "713",
    {
      "pageContent": "class PaintByExamplePipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_paint_by_example(self):\n        # make sure here that pndm scheduler skips prk\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/paint_by_example/dog_in_bucket.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/paint_by_example/mask.png\"\n        )\n        example_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/paint_by_example/panda.jpg\"\n        )\n\n        pipe = PaintByExamplePipeline.from_pretrained(\"Fantasy-Studio/Paint-by-Example\")\n        pipe = pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(321)\n        output = pipe(\n            image=init_image,\n            mask_image=mask_image,\n            example_image=example_image,\n            generator=generator,\n            guidance_scale=5.0,\n            num_inference_steps=50,\n            output_type=\"np\",\n        )\n\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.4834, 0.4811, 0.4874, 0.5122, 0.5081, 0.5144, 0.5291, 0.5290, 0.5374])\n\n        assert",
      "metadata": {
        "source": "tests/pipelines/paint_by_example/test_paint_by_example.py",
        "range": {
          "start": { "row": 165, "column": 0 },
          "end": { "row": 165, "column": 0 }
        }
      }
    }
  ],
  [
    "714",
    {
      "pageContent": "class DanceDiffusionPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = DanceDiffusionPipeline\n    params = UNCONDITIONAL_AUDIO_GENERATION_PARAMS\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\n        \"callback\",\n        \"latents\",\n        \"callback_steps\",\n        \"output_type\",\n        \"num_images_per_prompt\",\n    }\n    batch_params = UNCONDITIONAL_AUDIO_GENERATION_BATCH_PARAMS\n    test_attention_slicing = False\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet1DModel(\n            block_out_channels=(32, 32, 64),\n            extra_in_channels=16,\n            sample_size=512,\n            sample_rate=16_000,\n            in_channels=2,\n            out_channels=2,\n            flip_sin_to_cos=True,\n            use_timestep_embedding=False,\n            time_embedding_type=\"fourier\",\n            mid_block_type=\"UNetMidBlock1D\",\n            down_block_types=(\"DownBlock1DNoSkip\", \"DownBlock1D\", \"AttnDownBlock1D\"),\n            up_block_types=(\"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1DNoSkip\"),\n        )\n        scheduler = IPNDMScheduler()\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n        }\n        return components\n\n    def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"batch_size\": 1,\n            \"generator\": generat",
      "metadata": {
        "source": "tests/pipelines/dance_diffusion/test_dance_diffusion.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "715",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet1DModel(\n            block_out_channels=(32, 32, 64),\n            extra_in_channels=16,\n            sample_size=512,\n            sample_rate=16_000,\n            in_channels=2,\n            out_channels=2,\n            flip_sin_to_cos=True,\n            use_timestep_embedding=False,\n            time_embedding_type=\"fourier\",\n            mid_block_type=\"UNetMidBlock1D\",\n            down_block_types=(\"DownBlock1DNoSkip\", \"DownBlock1D\", \"AttnDownBlock1D\"),\n            up_block_types=(\"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1DNoSkip\"),\n        )\n        scheduler = IPNDMScheduler()\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n        }\n        return components",
      "metadata": {
        "source": "tests/pipelines/dance_diffusion/test_dance_diffusion.py",
        "range": {
          "start": { "row": 46, "column": 4 },
          "end": { "row": 46, "column": 4 }
        }
      }
    }
  ],
  [
    "716",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"batch_size\": 1,\n            \"generator\": generator,\n            \"num_inference_steps\": 4,\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/dance_diffusion/test_dance_diffusion.py",
        "range": {
          "start": { "row": 70, "column": 4 },
          "end": { "row": 70, "column": 4 }
        }
      }
    }
  ],
  [
    "717",
    {
      "pageContent": "def test_dance_diffusion(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        pipe = DanceDiffusionPipeline(**components)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = pipe(**inputs)\n        audio = output.audios\n\n        audio_slice = audio[0, -3:, -3:]\n\n        assert audio.shape == (1, 2, components[\"unet\"].sample_size)\n        expected_slice = np.array([-0.7265, 1.0000, -0.8388, 0.1175, 0.9498, -1.0000])\n        assert np.abs(audio_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/dance_diffusion/test_dance_diffusion.py",
        "range": {
          "start": { "row": 82, "column": 4 },
          "end": { "row": 82, "column": 4 }
        }
      }
    }
  ],
  [
    "718",
    {
      "pageContent": "class PipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_dance_diffusion(self):\n        device = torch_device\n\n        pipe = DanceDiffusionPipeline.from_pretrained(\"harmonai/maestro-150k\")\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        output = pipe(generator=generator, num_inference_steps=100, audio_length_in_s=4.096)\n        audio = output.audios\n\n        audio_slice = audio[0, -3:, -3:]\n\n        assert audio.shape == (1, 2, pipe.unet.sample_size)\n        expected_slice = np.array([-0.0192, -0.0231, -0.0318, -0.0059, 0.0002, -0.0020])\n\n        assert np.abs(audio_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_dance_diffusion_fp16(self):\n        device = torch_device\n\n        pipe = DanceDiffusionPipeline.from_pretrained(\"harmonai/maestro-150k\", torch_dtype=torch.float16)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        output = pipe(generator=generator, num_inference_steps=100, audio_length_in_s=4.096)\n        audio = output.audios\n\n        audio_slice = audio[0, -3:, -3:]\n\n        assert audio.shape == (1, 2, pipe.unet.sample_size)\n        expected_slice = np.array([-0.0367, -0.0488, -0.0771, -0.0525, -0.0444, -0.0341])\n\n        assert np.abs(audio_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/dance_diffusion/test_dance_diffusion.py",
        "range": {
          "start": { "row": 118, "column": 0 },
          "end": { "row": 118, "column": 0 }
        }
      }
    }
  ],
  [
    "719",
    {
      "pageContent": "class StableDiffusionPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionPipeline\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(tex",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "720",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 53, "column": 4 },
          "end": { "row": 53, "column": 4 }
        }
      }
    }
  ],
  [
    "721",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 107, "column": 4 },
          "end": { "row": 107, "column": 4 }
        }
      }
    }
  ],
  [
    "722",
    {
      "pageContent": "def test_stable_diffusion_ddim(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs)\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.5643, 0.6017, 0.4799, 0.5267, 0.5584, 0.4641, 0.5159, 0.4963, 0.4791])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 121, "column": 4 },
          "end": { "row": 121, "column": 4 }
        }
      }
    }
  ],
  [
    "723",
    {
      "pageContent": "def test_stable_diffusion_lora(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        # forward 1\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        # set lora layers\n        lora_attn_procs = create_lora_layers(sd_pipe.unet)\n        sd_pipe.unet.set_attn_processor(lora_attn_procs)\n        sd_pipe = sd_pipe.to(torch_device)\n\n        # forward 2\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs, cross_attention_kwargs={\"scale\": 0.0})\n        image = output.images\n        image_slice_1 = image[0, -3:, -3:, -1]\n\n        # forward 3\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs, cross_attention_kwargs={\"scale\": 0.5})\n        image = output.images\n        image_slice_2 = image[0, -3:, -3:, -1]\n\n        assert np.abs(image_slice - image_slice_1).max() < 1e-2\n        assert np.abs(image_slice - image_slice_2).max() > 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 140, "column": 4 },
          "end": { "row": 140, "column": 4 }
        }
      }
    }
  ],
  [
    "724",
    {
      "pageContent": "def test_stable_diffusion_prompt_embeds(self):\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(torch_device)\n        inputs[\"prompt\"] = 3 * [inputs[\"prompt\"]]\n\n        # forward\n        output = sd_pipe(**inputs)\n        image_slice_1 = output.images[0, -3:, -3:, -1]\n\n        inputs = self.get_dummy_inputs(torch_device)\n        prompt = 3 * [inputs.pop(\"prompt\")]\n\n        text_inputs = sd_pipe.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=sd_pipe.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_inputs = text_inputs[\"input_ids\"].to(torch_device)\n\n        prompt_embeds = sd_pipe.text_encoder(text_inputs)[0]\n\n        inputs[\"prompt_embeds\"] = prompt_embeds\n\n        # forward\n        output = sd_pipe(**inputs)\n        image_slice_2 = output.images[0, -3:, -3:, -1]\n\n        assert np.abs(image_slice_1.flatten() - image_slice_2.flatten()).max() < 1e-4",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 174, "column": 4 },
          "end": { "row": 174, "column": 4 }
        }
      }
    }
  ],
  [
    "725",
    {
      "pageContent": "def test_stable_diffusion_negative_prompt_embeds(self):\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(torch_device)\n        negative_prompt = 3 * [\"this is a negative prompt\"]\n        inputs[\"negative_prompt\"] = negative_prompt\n        inputs[\"prompt\"] = 3 * [inputs[\"prompt\"]]\n\n        # forward\n        output = sd_pipe(**inputs)\n        image_slice_1 = output.images[0, -3:, -3:, -1]\n\n        inputs = self.get_dummy_inputs(torch_device)\n        prompt = 3 * [inputs.pop(\"prompt\")]\n\n        embeds = []\n        for p in [prompt, negative_prompt]:\n            text_inputs = sd_pipe.tokenizer(\n                p,\n                padding=\"max_length\",\n                max_length=sd_pipe.tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            text_inputs = text_inputs[\"input_ids\"].to(torch_device)\n\n            embeds.append(sd_pipe.text_encoder(text_inputs)[0])\n\n        inputs[\"prompt_embeds\"], inputs[\"negative_prompt_embeds\"] = embeds\n\n        # forward\n        output = sd_pipe(**inputs)\n        image_slice_2 = output.images[0, -3:, -3:, -1]\n\n        assert np.abs(image_slice_1.flatten() - image_slice_2.flatten()).max() < 1e-4",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 210, "column": 4 },
          "end": { "row": 210, "column": 4 }
        }
      }
    }
  ],
  [
    "726",
    {
      "pageContent": "def test_stable_diffusion_ddim_factor_8(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs, height=136, width=136)\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 136, 136, 3)\n        expected_slice = np.array([0.5524, 0.5626, 0.6069, 0.4727, 0.386, 0.3995, 0.4613, 0.4328, 0.4269])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 250, "column": 4 },
          "end": { "row": 250, "column": 4 }
        }
      }
    }
  ],
  [
    "727",
    {
      "pageContent": "def test_stable_diffusion_pndm(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe.scheduler = PNDMScheduler(skip_prk_steps=True)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.5094, 0.5674, 0.4667, 0.5125, 0.5696, 0.4674, 0.5277, 0.4964, 0.4945])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 269, "column": 4 },
          "end": { "row": 269, "column": 4 }
        }
      }
    }
  ],
  [
    "728",
    {
      "pageContent": "def test_stable_diffusion_no_safety_checker(self):\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-lms-pipe\", safety_checker=None\n        )\n        assert isinstance(pipe, StableDiffusionPipeline)\n        assert isinstance(pipe.scheduler, LMSDiscreteScheduler)\n        assert pipe.safety_checker is None\n\n        image = pipe(\"example prompt\", num_inference_steps=2).images[0]\n        assert image is not None\n\n        # check that there's no error when saving a pipeline with one of the models being None\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe = StableDiffusionPipeline.from_pretrained(tmpdirname)\n\n        # sanity check that the pipeline still works\n        assert pipe.safety_checker is None\n        image = pipe(\"example prompt\", num_inference_steps=2).images[0]\n        assert image is not None",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 287, "column": 4 },
          "end": { "row": 287, "column": 4 }
        }
      }
    }
  ],
  [
    "729",
    {
      "pageContent": "def test_stable_diffusion_k_lms(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe.scheduler = LMSDiscreteScheduler.from_config(sd_pipe.scheduler.config)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array(\n            [\n                0.47082293033599854,\n                0.5371589064598083,\n                0.4562119245529175,\n                0.5220914483070374,\n                0.5733777284622192,\n                0.4795039892196655,\n                0.5465868711471558,\n                0.5074326395988464,\n                0.5042197108268738,\n            ]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 308, "column": 4 },
          "end": { "row": 308, "column": 4 }
        }
      }
    }
  ],
  [
    "730",
    {
      "pageContent": "def test_stable_diffusion_k_euler_ancestral(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(sd_pipe.scheduler.config)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array(\n            [\n                0.4707113206386566,\n                0.5372191071510315,\n                0.4563021957874298,\n                0.5220003724098206,\n                0.5734264850616455,\n                0.4794946610927582,\n                0.5463782548904419,\n                0.5074145197868347,\n                0.504422664642334,\n            ]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 339, "column": 4 },
          "end": { "row": 339, "column": 4 }
        }
      }
    }
  ],
  [
    "731",
    {
      "pageContent": "def test_stable_diffusion_k_euler(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe.scheduler = EulerDiscreteScheduler.from_config(sd_pipe.scheduler.config)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array(\n            [\n                0.47082313895225525,\n                0.5371587872505188,\n                0.4562119245529175,\n                0.5220913887023926,\n                0.5733776688575745,\n                0.47950395941734314,\n                0.546586811542511,\n                0.5074326992034912,\n                0.5042197108268738,\n            ]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 370, "column": 4 },
          "end": { "row": 370, "column": 4 }
        }
      }
    }
  ],
  [
    "732",
    {
      "pageContent": "def test_stable_diffusion_vae_slicing(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler.from_config(components[\"scheduler\"].config)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        image_count = 4\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * image_count\n        output_1 = sd_pipe(**inputs)\n\n        # make sure sliced vae decode yields the same result\n        sd_pipe.enable_vae_slicing()\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * image_count\n        output_2 = sd_pipe(**inputs)\n\n        # there is a small discrepancy at image borders vs. full batch decode\n        assert np.abs(output_2.images.flatten() - output_1.images.flatten()).max() < 3e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 401, "column": 4 },
          "end": { "row": 401, "column": 4 }
        }
      }
    }
  ],
  [
    "733",
    {
      "pageContent": "def test_stable_diffusion_vae_tiling(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n\n        # make sure here that pndm scheduler skips prk\n        components[\"safety_checker\"] = None\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n\n        # Test that tiled decode at 512x512 yields the same result as the non-tiled decode\n        generator = torch.Generator(device=device).manual_seed(0)\n        output_1 = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n\n        # make sure tiled vae decode yields the same result\n        sd_pipe.enable_vae_tiling()\n        generator = torch.Generator(device=device).manual_seed(0)\n        output_2 = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n\n        assert np.abs(output_2.images.flatten() - output_1.images.flatten()).max() < 5e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 424, "column": 4 },
          "end": { "row": 424, "column": 4 }
        }
      }
    }
  ],
  [
    "734",
    {
      "pageContent": "def test_stable_diffusion_negative_prompt(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = PNDMScheduler(skip_prk_steps=True)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        negative_prompt = \"french fries\"\n        output = sd_pipe(**inputs, negative_prompt=negative_prompt)\n\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array(\n            [\n                0.5108221173286438,\n                0.5688379406929016,\n                0.4685141146183014,\n                0.5098261833190918,\n                0.5657756328582764,\n                0.4631010890007019,\n                0.5226285457611084,\n                0.49129390716552734,\n                0.4899061322212219,\n            ]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 447, "column": 4 },
          "end": { "row": 447, "column": 4 }
        }
      }
    }
  ],
  [
    "735",
    {
      "pageContent": "def test_stable_diffusion_long_prompt(self):\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler.from_config(components[\"scheduler\"].config)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        do_classifier_free_guidance = True\n        negative_prompt = None\n        num_images_per_prompt = 1\n        logger = logging.get_logger(\"diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion\")\n\n        prompt = 25 * \"@\"\n        with CaptureLogger(logger) as cap_logger_3:\n            text_embeddings_3 = sd_pipe._encode_prompt(\n                prompt, torch_device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n            )\n\n        prompt = 100 * \"@\"\n        with CaptureLogger(logger) as cap_logger:\n            text_embeddings = sd_pipe._encode_prompt(\n                prompt, torch_device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n            )\n\n        negative_prompt = \"Hello\"\n        with CaptureLogger(logger) as cap_logger_2:\n            text_embeddings_2 = sd_pipe._encode_prompt(\n                prompt, torch_device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n            )\n\n        assert text_embeddings_3.shape == text_embeddings_2.shape == text_embeddings.shape\n        assert text_embeddings.shape[1] == 77\n\n        assert cap_logger.out == cap_logger_2.out\n        # 100 - 77 + 1 (BOS token) + 1 (EOS token) = 25\n        assert cap_logger.ou",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 479, "column": 4 },
          "end": { "row": 479, "column": 4 }
        }
      }
    }
  ],
  [
    "736",
    {
      "pageContent": "def test_stable_diffusion_height_width_opt(self):\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler.from_config(components[\"scheduler\"].config)\n        sd_pipe = StableDiffusionPipeline(**components)\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"hey\"\n\n        output = sd_pipe(prompt, num_inference_steps=1, output_type=\"np\")\n        image_shape = output.images[0].shape[:2]\n        assert image_shape == (64, 64)\n\n        output = sd_pipe(prompt, num_inference_steps=1, height=96, width=96, output_type=\"np\")\n        image_shape = output.images[0].shape[:2]\n        assert image_shape == (96, 96)\n\n        config = dict(sd_pipe.unet.config)\n        config[\"sample_size\"] = 96\n        sd_pipe.unet = UNet2DConditionModel.from_config(config).to(torch_device)\n        output = sd_pipe(prompt, num_inference_steps=1, output_type=\"np\")\n        image_shape = output.images[0].shape[:2]\n        assert image_shape == (192, 192)",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 517, "column": 4 },
          "end": { "row": 517, "column": 4 }
        }
      }
    }
  ],
  [
    "737",
    {
      "pageContent": "class StableDiffusionPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 64, 64))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"prompt\": \"a photograph of an astronaut riding a horse\",\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_1_1_pndm(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-1\")\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.43625, 0.43554, 0.36670, 0.40660, 0.39703, 0.38658, 0.43936, 0.43557, 0.40592])\n        assert np.abs(image_slice - expected_slice).max() < 1e-4\n\n    def test_stable_diffusion_1_4_pndm(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 544, "column": 0 },
          "end": { "row": 544, "column": 0 }
        }
      }
    }
  ],
  [
    "738",
    {
      "pageContent": "class StableDiffusionPipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 64, 64))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"prompt\": \"a photograph of an astronaut riding a horse\",\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_1_4_pndm(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_text2img/stable_diffusion_1_4_pndm.npy\"\n        )\n        max_diff = np.abs(expected_image - image).max()\n        assert max_diff < 1e-3\n\n    def test_stable_diffusion_1_5_pndm(self):\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs =",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion.py",
        "range": {
          "start": { "row": 888, "column": 0 },
          "end": { "row": 888, "column": 0 }
        }
      }
    }
  ],
  [
    "739",
    {
      "pageContent": "class StableDiffusionPanoramaPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionPanoramaPipeline\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler()\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n     ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "740",
    {
      "pageContent": "class StableDiffusionPanoramaSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, seed=0):\n        generator = torch.manual_seed(seed)\n        inputs = {\n            \"prompt\": \"a photo of the dolomites\",\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_panorama_default(self):\n        model_ckpt = \"stabilityai/stable-diffusion-2-base\"\n        scheduler = DDIMScheduler.from_pretrained(model_ckpt, subfolder=\"scheduler\")\n        pipe = StableDiffusionPanoramaPipeline.from_pretrained(model_ckpt, scheduler=scheduler, safety_checker=None)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 512, 2048, 3)\n\n        expected_slice = np.array(\n            [\n                0.36968392,\n                0.27025372,\n                0.32446766,\n                0.28379387,\n                0.36363274,\n                0.30733347,\n                0.27100027,\n                0.27054125,\n                0.25536096,\n            ]\n        )\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-2\n\n    def test_stable_diffusion_panorama_k_lms(self):\n        pipe = StableDiffusionPanoramaPipeline.from",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 182, "column": 0 },
          "end": { "row": 182, "column": 0 }
        }
      }
    }
  ],
  [
    "741",
    {
      "pageContent": "class OnnxStableDiffusionImg2ImgPipelineFastTests(OnnxPipelineTesterMixin, unittest.TestCase):\n    hub_checkpoint = \"hf-internal-testing/tiny-random-OnnxStableDiffusionPipeline\"\n\n    def get_dummy_inputs(self, seed=0):\n        image = floats_tensor((1, 3, 128, 128), rng=random.Random(seed))\n        generator = np.random.RandomState(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_pipeline_default_ddim(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.69643, 0.58484, 0.50314, 0.58760, 0.55368, 0.59643, 0.51529, 0.41217, 0.49087])\n        assert np.abs(image_slice - expected_slice).max() < 1e-1\n\n    def test_pipeline_pndm(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config, skip_prk_steps=True)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pi",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "742",
    {
      "pageContent": "def get_dummy_inputs(self, seed=0):\n        image = floats_tensor((1, 3, 128, 128), rng=random.Random(seed))\n        generator = np.random.RandomState(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 47, "column": 4 },
          "end": { "row": 47, "column": 4 }
        }
      }
    }
  ],
  [
    "743",
    {
      "pageContent": "def test_pipeline_default_ddim(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.69643, 0.58484, 0.50314, 0.58760, 0.55368, 0.59643, 0.51529, 0.41217, 0.49087])\n        assert np.abs(image_slice - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 61, "column": 4 },
          "end": { "row": 61, "column": 4 }
        }
      }
    }
  ],
  [
    "744",
    {
      "pageContent": "def test_pipeline_pndm(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config, skip_prk_steps=True)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.61710, 0.53390, 0.49310, 0.55622, 0.50982, 0.58240, 0.50716, 0.38629, 0.46856])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 73, "column": 4 },
          "end": { "row": 73, "column": 4 }
        }
      }
    }
  ],
  [
    "745",
    {
      "pageContent": "def test_pipeline_lms(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        # warmup pass to apply optimizations\n        _ = pipe(**self.get_dummy_inputs())\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.52761, 0.59977, 0.49033, 0.49619, 0.54282, 0.50311, 0.47600, 0.40918, 0.45203])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 87, "column": 4 },
          "end": { "row": 87, "column": 4 }
        }
      }
    }
  ],
  [
    "746",
    {
      "pageContent": "def test_pipeline_euler(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.52911, 0.60004, 0.49229, 0.49805, 0.54502, 0.50680, 0.47777, 0.41028, 0.45304])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 104, "column": 4 },
          "end": { "row": 104, "column": 4 }
        }
      }
    }
  ],
  [
    "747",
    {
      "pageContent": "def test_pipeline_euler_ancestral(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.52911, 0.60004, 0.49229, 0.49805, 0.54502, 0.50680, 0.47777, 0.41028, 0.45304])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 118, "column": 4 },
          "end": { "row": 118, "column": 4 }
        }
      }
    }
  ],
  [
    "748",
    {
      "pageContent": "def test_pipeline_dpm_multistep(self):\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.65331, 0.58277, 0.48204, 0.56059, 0.53665, 0.56235, 0.50969, 0.40009, 0.46552])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 132, "column": 4 },
          "end": { "row": 132, "column": 4 }
        }
      }
    }
  ],
  [
    "749",
    {
      "pageContent": "class OnnxStableDiffusionImg2ImgPipelineIntegrationTests(unittest.TestCase):\n    @property\n    def gpu_provider(self):\n        return (\n            \"CUDAExecutionProvider\",\n            {\n                \"gpu_mem_limit\": \"15000000000\",  # 15GB\n                \"arena_extend_strategy\": \"kSameAsRequested\",\n            },\n        )\n\n    @property\n    def gpu_options(self):\n        options = ort.SessionOptions()\n        options.enable_mem_pattern = False\n        return options\n\n    def test_inference_default_pndm(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/img2img/sketch-mountains-input.jpg\"\n        )\n        init_image = init_image.resize((768, 512))\n        # using the PNDM scheduler by default\n        pipe = OnnxStableDiffusionImg2ImgPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\",\n            revision=\"onnx\",\n            safety_checker=None,\n            feature_extractor=None,\n            provider=self.gpu_provider,\n            sess_options=self.gpu_options,\n        )\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A fantasy landscape, trending on artstation\"\n\n        generator = np.random.RandomState(0)\n        output = pipe(\n            prompt=prompt,\n            image=init_image,\n            strength=0.75,\n            guidance_scale=7.5,\n            num_inference_steps=10,\n            generator=generator,\n            output_type=\"np\",\n        )\n        images = output.images\n        image_slice = images[0, 255:258, 383:386, -",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 150, "column": 0 },
          "end": { "row": 150, "column": 0 }
        }
      }
    }
  ],
  [
    "750",
    {
      "pageContent": "class StableDiffusionPix2PixZeroPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionPix2PixZeroPipeline\n    params = TEXT_GUIDED_IMAGE_VARIATION_PARAMS\n    batch_params = TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS\n\n    @classmethod\n    def setUpClass(cls):\n        cls.source_embeds = load_pt(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/src_emb_0.pt\"\n        )\n\n        cls.target_embeds = load_pt(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/tgt_emb_0.pt\"\n        )\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler()\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 43, "column": 0 },
          "end": { "row": 43, "column": 0 }
        }
      }
    }
  ],
  [
    "751",
    {
      "pageContent": "class StableDiffusionPix2PixZeroPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @classmethod\n    def setUpClass(cls):\n        cls.source_embeds = load_pt(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat.pt\"\n        )\n\n        cls.target_embeds = load_pt(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/dog.pt\"\n        )\n\n    def get_inputs(self, seed=0):\n        generator = torch.manual_seed(seed)\n\n        inputs = {\n            \"prompt\": \"turn him into a cyborg\",\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"cross_attention_guidance_amount\": 0.15,\n            \"source_embeds\": self.source_embeds,\n            \"target_embeds\": self.target_embeds,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_pix2pix_zero_default(self):\n        pipe = StableDiffusionPix2PixZeroPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\", safety_checker=None, torch_dtype=torch.float16\n        )\n        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 512",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 201, "column": 0 },
          "end": { "row": 201, "column": 0 }
        }
      }
    }
  ],
  [
    "752",
    {
      "pageContent": "class InversionPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @classmethod\n    def setUpClass(cls):\n        raw_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat_6.png\"\n        )\n\n        raw_image = raw_image.convert(\"RGB\").resize((512, 512))\n\n        cls.raw_image = raw_image\n\n    def test_stable_diffusion_pix2pix_inversion(self):\n        pipe = StableDiffusionPix2PixZeroPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\", safety_checker=None, torch_dtype=torch.float16\n        )\n        pipe.inverse_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n        pipe.inverse_scheduler = DDIMInverseScheduler.from_config(pipe.scheduler.config)\n\n        caption = \"a photography of a cat with flowers\"\n        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n        pipe.enable_model_cpu_offload()\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        output = pipe.invert(caption, image=self.raw_image, generator=generator, num_inference_steps=10)\n        inv_latents = output[0]\n\n        image_slice = inv_latents[0, -3:, -3:, -1].flatten()\n\n        assert inv_latents.shape == (1, 4, 64, 64)\n        expected_slice = np.array([0.8877, 0.0587, 0.7700, -1.6035, -0.5962, 0.4827, -0.6265, 1.0498, -0.8599])\n\n        assert np.abs(expected_slice - image_slice.cpu().numpy()).max() < 5e-2\n\n    def test_stable_diffusion_pix2pix_f",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 329, "column": 0 },
          "end": { "row": 329, "column": 0 }
        }
      }
    }
  ],
  [
    "753",
    {
      "pageContent": "class StableDiffusionImageVariationPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionImageVariationPipeline\n    params = IMAGE_VARIATION_PARAMS\n    batch_params = IMAGE_VARIATION_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        image_encoder_config = CLIPVisionConfig(\n            hidden_size=32,\n            projection_dim=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            image_size=32,\n            patch_size=4,\n        )\n        image_encoder = CLIPVisionModelWithProjection(image_encoder_config)\n        feature_extractor = CLIPImageProcessor(crop_size=32, size=32)\n\n        components = {\n            \"unet\": unet",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 41, "column": 0 },
          "end": { "row": 41, "column": 0 }
        }
      }
    }
  ],
  [
    "754",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        image_encoder_config = CLIPVisionConfig(\n            hidden_size=32,\n            projection_dim=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            image_size=32,\n            patch_size=4,\n        )\n        image_encoder = CLIPVisionModelWithProjection(image_encoder_config)\n        feature_extractor = CLIPImageProcessor(crop_size=32, size=32)\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae\": vae,\n            \"image_encoder\": image_encoder,\n            \"feature_extractor\": feature_extractor,\n            \"safety_checker\": None,\n        }\n        return components",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 46, "column": 4 },
          "end": { "row": 46, "column": 4 }
        }
      }
    }
  ],
  [
    "755",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed))\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((32, 32))\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 92, "column": 4 },
          "end": { "row": 92, "column": 4 }
        }
      }
    }
  ],
  [
    "756",
    {
      "pageContent": "def test_stable_diffusion_img_variation_default_case(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImageVariationPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.5167, 0.5746, 0.4835, 0.4914, 0.5605, 0.4691, 0.5201, 0.4898, 0.4958])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 109, "column": 4 },
          "end": { "row": 109, "column": 4 }
        }
      }
    }
  ],
  [
    "757",
    {
      "pageContent": "def test_stable_diffusion_img_variation_multiple_images(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImageVariationPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"image\"] = 2 * [inputs[\"image\"]]\n        output = sd_pipe(**inputs)\n\n        image = output.images\n\n        image_slice = image[-1, -3:, -3:, -1]\n\n        assert image.shape == (2, 64, 64, 3)\n        expected_slice = np.array([0.6568, 0.5470, 0.5684, 0.5444, 0.5945, 0.6221, 0.5508, 0.5531, 0.5263])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 125, "column": 4 },
          "end": { "row": 125, "column": 4 }
        }
      }
    }
  ],
  [
    "758",
    {
      "pageContent": "class StableDiffusionImageVariationPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_imgvar/input_image_vermeer.png\"\n        )\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 64, 64))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"image\": init_image,\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_img_variation_pipeline_default(self):\n        sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n            \"lambdalabs/sd-image-variations-diffusers\", safety_checker=None\n        )\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.84491, 0.90789, 0.75708, 0.78734, 0.83485, 0.70099, 0.66938, 0.68727, 0.61379])\n        assert np.ab",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 148, "column": 0 },
          "end": { "row": 148, "column": 0 }
        }
      }
    }
  ],
  [
    "759",
    {
      "pageContent": "class StableDiffusionImageVariationPipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_imgvar/input_image_vermeer.png\"\n        )\n        latents = np.random.RandomState(seed).standard_normal((1, 4, 64, 64))\n        latents = torch.from_numpy(latents).to(device=device, dtype=dtype)\n        inputs = {\n            \"image\": init_image,\n            \"latents\": latents,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_img_variation_pndm(self):\n        sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\"fusing/sd-image-variations-diffusers\")\n        sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_imgvar/lambdalabs_variations_pndm.npy\"\n        )\n        max_diff = np.abs(expected_image - image).max()\n        assert max_diff < 1e-3\n\n    def test_img_variation_dpm(s",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 251, "column": 0 },
          "end": { "row": 251, "column": 0 }
        }
      }
    }
  ],
  [
    "760",
    {
      "pageContent": "class OnnxStableDiffusionUpscalePipelineFastTests(OnnxPipelineTesterMixin, unittest.TestCase):\n    # TODO: is there an appropriate internal test set?\n    hub_checkpoint = \"ssube/stable-diffusion-x4-upscaler-onnx\"\n\n    def get_dummy_inputs(self, seed=0):\n        image = floats_tensor((1, 3, 128, 128), rng=random.Random(seed))\n        generator = torch.manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_pipeline_default_ddpm(self):\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        # started as 128, should now be 512\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array(\n            [0.6974782, 0.68902093, 0.70135885, 0.7583618, 0.7804545, 0.7854912, 0.78667426, 0.78743863, 0.78070223]\n        )\n        assert np.abs(image_slice - expected_slice).max() < 1e-1\n\n    def test_pipeline_pndm(self):\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config, skip_prk_steps=True)\n        pipe.set_prog",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 45, "column": 0 },
          "end": { "row": 45, "column": 0 }
        }
      }
    }
  ],
  [
    "761",
    {
      "pageContent": "def get_dummy_inputs(self, seed=0):\n        image = floats_tensor((1, 3, 128, 128), rng=random.Random(seed))\n        generator = torch.manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 49, "column": 4 },
          "end": { "row": 49, "column": 4 }
        }
      }
    }
  ],
  [
    "762",
    {
      "pageContent": "def test_pipeline_default_ddpm(self):\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        # started as 128, should now be 512\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array(\n            [0.6974782, 0.68902093, 0.70135885, 0.7583618, 0.7804545, 0.7854912, 0.78667426, 0.78743863, 0.78070223]\n        )\n        assert np.abs(image_slice - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 62, "column": 4 },
          "end": { "row": 62, "column": 4 }
        }
      }
    }
  ],
  [
    "763",
    {
      "pageContent": "def test_pipeline_pndm(self):\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config, skip_prk_steps=True)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array(\n            [0.6898892, 0.59240556, 0.52499527, 0.58866215, 0.52258235, 0.52572715, 0.62414473, 0.6174387, 0.6214964]\n        )\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 77, "column": 4 },
          "end": { "row": 77, "column": 4 }
        }
      }
    }
  ],
  [
    "764",
    {
      "pageContent": "def test_pipeline_dpm_multistep(self):\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array(\n            [0.7659278, 0.76437664, 0.75579107, 0.7691116, 0.77666986, 0.7727672, 0.7758664, 0.7812226, 0.76942515]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 92, "column": 4 },
          "end": { "row": 92, "column": 4 }
        }
      }
    }
  ],
  [
    "765",
    {
      "pageContent": "def test_pipeline_euler(self):\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array(\n            [0.6974782, 0.68902093, 0.70135885, 0.7583618, 0.7804545, 0.7854912, 0.78667426, 0.78743863, 0.78070223]\n        )\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "766",
    {
      "pageContent": "def test_pipeline_euler_ancestral(self):\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array(\n            [0.77424496, 0.773601, 0.7645288, 0.7769598, 0.7772739, 0.7738688, 0.78187233, 0.77879584, 0.767043]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-1",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 123, "column": 4 },
          "end": { "row": 123, "column": 4 }
        }
      }
    }
  ],
  [
    "767",
    {
      "pageContent": "class OnnxStableDiffusionUpscalePipelineIntegrationTests(unittest.TestCase):\n    @property\n    def gpu_provider(self):\n        return (\n            \"CUDAExecutionProvider\",\n            {\n                \"gpu_mem_limit\": \"15000000000\",  # 15GB\n                \"arena_extend_strategy\": \"kSameAsRequested\",\n            },\n        )\n\n    @property\n    def gpu_options(self):\n        options = ort.SessionOptions()\n        options.enable_mem_pattern = False\n        return options\n\n    def test_inference_default_ddpm(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/img2img/sketch-mountains-input.jpg\"\n        )\n        init_image = init_image.resize((128, 128))\n        # using the PNDM scheduler by default\n        pipe = OnnxStableDiffusionUpscalePipeline.from_pretrained(\n            \"ssube/stable-diffusion-x4-upscaler-onnx\",\n            provider=self.gpu_provider,\n            sess_options=self.gpu_options,\n        )\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A fantasy landscape, trending on artstation\"\n\n        generator = torch.manual_seed(0)\n        output = pipe(\n            prompt=prompt,\n            image=init_image,\n            guidance_scale=7.5,\n            num_inference_steps=10,\n            generator=generator,\n            output_type=\"np\",\n        )\n        images = output.images\n        image_slice = images[0, 255:258, 383:386, -1]\n\n        assert images.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.4883, 0.4947, 0.4980, 0.4975, ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 143, "column": 0 },
          "end": { "row": 143, "column": 0 }
        }
      }
    }
  ],
  [
    "768",
    {
      "pageContent": "class StableDiffusionSAGPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionSAGPipeline\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n     ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "769",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 44, "column": 4 },
          "end": { "row": 44, "column": 4 }
        }
      }
    }
  ],
  [
    "770",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \".\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 1.0,\n            \"sag_scale\": 1.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 98, "column": 4 },
          "end": { "row": 98, "column": 4 }
        }
      }
    }
  ],
  [
    "771",
    {
      "pageContent": "class StableDiffusionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_diffusion_1(self):\n        sag_pipe = StableDiffusionSAGPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n        sag_pipe = sag_pipe.to(torch_device)\n        sag_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \".\"\n        generator = torch.manual_seed(0)\n        output = sag_pipe(\n            [prompt], generator=generator, guidance_scale=7.5, sag_scale=1.0, num_inference_steps=20, output_type=\"np\"\n        )\n\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.1568, 0.1738, 0.1695, 0.1693, 0.1507, 0.1705, 0.1547, 0.1751, 0.1949])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 5e-2\n\n    def test_stable_diffusion_2(self):\n        sag_pipe = StableDiffusionSAGPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n        sag_pipe = sag_pipe.to(torch_device)\n        sag_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \".\"\n        generator = torch.manual_seed(0)\n        output = sag_pipe(\n            [prompt], generator=generator, guidance_scale=7.5, sag_scale=1.0, num_inference_steps=20, output_type=\"np\"\n        )\n\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 116, "column": 0 },
          "end": { "row": 116, "column": 0 }
        }
      }
    }
  ],
  [
    "772",
    {
      "pageContent": "class StableDiffusionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_stable_diffusion_1(self):\n        sd_pipe = StableDiffusionKDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        sd_pipe.set_scheduler(\"sample_euler\")\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=9.0, num_inference_steps=20, output_type=\"np\")\n\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.0447, 0.0492, 0.0468, 0.0408, 0.0383, 0.0408, 0.0354, 0.0380, 0.0339])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_stable_diffusion_2(self):\n        sd_pipe = StableDiffusionKDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n        sd_pipe = sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        sd_pipe.set_scheduler(\"sample_euler\")\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=9.0, num_inference_steps=20, output_type=\"np\")\n\n        image = output.images\n\n        image_slice = imag",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "773",
    {
      "pageContent": "class StableDiffusionInpaintPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionInpaintPipeline\n    params = TEXT_GUIDED_IMAGE_INPAINTING_PARAMS\n    batch_params = TEXT_GUIDED_IMAGE_INPAINTING_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n       ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 43, "column": 0 },
          "end": { "row": 43, "column": 0 }
        }
      }
    }
  ],
  [
    "774",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae\": vae,\n            \"text_encoder\": text_encoder,\n            \"tokenizer\": tokenizer,\n            \"safety_checker\": None,\n            \"feature_extractor\": None,",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "775",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        # TODO: use tensor inputs instead of PIL, this is here just to leave the old expected_slices untouched\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((64, 64))\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((64, 64))\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 96, "column": 4 },
          "end": { "row": 96, "column": 4 }
        }
      }
    }
  ],
  [
    "776",
    {
      "pageContent": "def test_stable_diffusion_inpaint(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInpaintPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.4723, 0.5731, 0.3939, 0.5441, 0.5922, 0.4392, 0.5059, 0.4651, 0.4474])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 117, "column": 4 },
          "end": { "row": 117, "column": 4 }
        }
      }
    }
  ],
  [
    "777",
    {
      "pageContent": "def test_stable_diffusion_inpaint_image_tensor(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInpaintPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = sd_pipe(**inputs)\n        out_pil = output.images\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"image\"] = torch.tensor(np.array(inputs[\"image\"]) / 127.5 - 1).permute(2, 0, 1).unsqueeze(0)\n        inputs[\"mask_image\"] = torch.tensor(np.array(inputs[\"mask_image\"]) / 255).permute(2, 0, 1)[:1].unsqueeze(0)\n        output = sd_pipe(**inputs)\n        out_tensor = output.images\n\n        assert out_pil.shape == (1, 64, 64, 3)\n        assert np.abs(out_pil.flatten() - out_tensor.flatten()).max() < 5e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 133, "column": 4 },
          "end": { "row": 133, "column": 4 }
        }
      }
    }
  ],
  [
    "778",
    {
      "pageContent": "class StableDiffusionInpaintPipelineSlowTests(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_image.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_mask.png\"\n        )\n        inputs = {\n            \"prompt\": \"Face of a yellow cat, high resolution, sitting on a park bench\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_inpaint_ddim(self):\n        pipe = StableDiffusionInpaintPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-inpainting\", safety_checker=None\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs(torch_device)\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.sh",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 156, "column": 0 },
          "end": { "row": 156, "column": 0 }
        }
      }
    }
  ],
  [
    "779",
    {
      "pageContent": "class StableDiffusionInpaintPipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_image.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_mask.png\"\n        )\n        inputs = {\n            \"prompt\": \"Face of a yellow cat, high resolution, sitting on a park bench\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_inpaint_ddim(self):\n        sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\")\n        sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/stable_diffusion_inpaint_ddim.npy\"\n        )\n        max",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 279, "column": 0 },
          "end": { "row": 279, "column": 0 }
        }
      }
    }
  ],
  [
    "780",
    {
      "pageContent": "class StableDiffusionInpaintingPrepareMaskAndMaskedImageTests(unittest.TestCase):\n    def test_pil_inputs(self):\n        im = np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8)\n        im = Image.fromarray(im)\n        mask = np.random.randint(0, 255, (32, 32), dtype=np.uint8) > 127.5\n        mask = Image.fromarray((mask * 255).astype(np.uint8))\n\n        t_mask, t_masked = prepare_mask_and_masked_image(im, mask)\n\n        self.assertTrue(isinstance(t_mask, torch.Tensor))\n        self.assertTrue(isinstance(t_masked, torch.Tensor))\n\n        self.assertEqual(t_mask.ndim, 4)\n        self.assertEqual(t_masked.ndim, 4)\n\n        self.assertEqual(t_mask.shape, (1, 1, 32, 32))\n        self.assertEqual(t_masked.shape, (1, 3, 32, 32))\n\n        self.assertTrue(t_mask.dtype == torch.float32)\n        self.assertTrue(t_masked.dtype == torch.float32)\n\n        self.assertTrue(t_mask.min() >= 0.0)\n        self.assertTrue(t_mask.max() <= 1.0)\n        self.assertTrue(t_masked.min() >= -1.0)\n        self.assertTrue(t_masked.min() <= 1.0)\n\n        self.assertTrue(t_mask.sum() > 0.0)\n\n    def test_np_inputs(self):\n        im_np = np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8)\n        im_pil = Image.fromarray(im_np)\n        mask_np = np.random.randint(0, 255, (32, 32), dtype=np.uint8) > 127.5\n        mask_pil = Image.fromarray((mask_np * 255).astype(np.uint8))\n\n        t_mask_np, t_masked_np = prepare_mask_and_masked_image(im_np, mask_np)\n        t_mask_pil, t_masked_pil = prepare_mask_and_masked_image(im_pil, mask_pil)\n\n        self.assertTrue((t_mask_np == t_mask_pil).all())\n        se",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 371, "column": 0 },
          "end": { "row": 371, "column": 0 }
        }
      }
    }
  ],
  [
    "781",
    {
      "pageContent": "def test_pil_inputs(self):\n        im = np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8)\n        im = Image.fromarray(im)\n        mask = np.random.randint(0, 255, (32, 32), dtype=np.uint8) > 127.5\n        mask = Image.fromarray((mask * 255).astype(np.uint8))\n\n        t_mask, t_masked = prepare_mask_and_masked_image(im, mask)\n\n        self.assertTrue(isinstance(t_mask, torch.Tensor))\n        self.assertTrue(isinstance(t_masked, torch.Tensor))\n\n        self.assertEqual(t_mask.ndim, 4)\n        self.assertEqual(t_masked.ndim, 4)\n\n        self.assertEqual(t_mask.shape, (1, 1, 32, 32))\n        self.assertEqual(t_masked.shape, (1, 3, 32, 32))\n\n        self.assertTrue(t_mask.dtype == torch.float32)\n        self.assertTrue(t_masked.dtype == torch.float32)\n\n        self.assertTrue(t_mask.min() >= 0.0)\n        self.assertTrue(t_mask.max() <= 1.0)\n        self.assertTrue(t_masked.min() >= -1.0)\n        self.assertTrue(t_masked.min() <= 1.0)\n\n        self.assertTrue(t_mask.sum() > 0.0)",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 372, "column": 4 },
          "end": { "row": 372, "column": 4 }
        }
      }
    }
  ],
  [
    "782",
    {
      "pageContent": "def test_np_inputs(self):\n        im_np = np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8)\n        im_pil = Image.fromarray(im_np)\n        mask_np = np.random.randint(0, 255, (32, 32), dtype=np.uint8) > 127.5\n        mask_pil = Image.fromarray((mask_np * 255).astype(np.uint8))\n\n        t_mask_np, t_masked_np = prepare_mask_and_masked_image(im_np, mask_np)\n        t_mask_pil, t_masked_pil = prepare_mask_and_masked_image(im_pil, mask_pil)\n\n        self.assertTrue((t_mask_np == t_mask_pil).all())\n        self.assertTrue((t_masked_np == t_masked_pil).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 399, "column": 4 },
          "end": { "row": 399, "column": 4 }
        }
      }
    }
  ],
  [
    "783",
    {
      "pageContent": "def test_torch_3D_2D_inputs(self):\n        im_tensor = torch.randint(0, 255, (3, 32, 32), dtype=torch.uint8)\n        mask_tensor = torch.randint(0, 255, (32, 32), dtype=torch.uint8) > 127.5\n        im_np = im_tensor.numpy().transpose(1, 2, 0)\n        mask_np = mask_tensor.numpy()\n\n        t_mask_tensor, t_masked_tensor = prepare_mask_and_masked_image(im_tensor / 127.5 - 1, mask_tensor)\n        t_mask_np, t_masked_np = prepare_mask_and_masked_image(im_np, mask_np)\n\n        self.assertTrue((t_mask_tensor == t_mask_np).all())\n        self.assertTrue((t_masked_tensor == t_masked_np).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 411, "column": 4 },
          "end": { "row": 411, "column": 4 }
        }
      }
    }
  ],
  [
    "784",
    {
      "pageContent": "def test_torch_3D_3D_inputs(self):\n        im_tensor = torch.randint(0, 255, (3, 32, 32), dtype=torch.uint8)\n        mask_tensor = torch.randint(0, 255, (1, 32, 32), dtype=torch.uint8) > 127.5\n        im_np = im_tensor.numpy().transpose(1, 2, 0)\n        mask_np = mask_tensor.numpy()[0]\n\n        t_mask_tensor, t_masked_tensor = prepare_mask_and_masked_image(im_tensor / 127.5 - 1, mask_tensor)\n        t_mask_np, t_masked_np = prepare_mask_and_masked_image(im_np, mask_np)\n\n        self.assertTrue((t_mask_tensor == t_mask_np).all())\n        self.assertTrue((t_masked_tensor == t_masked_np).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 423, "column": 4 },
          "end": { "row": 423, "column": 4 }
        }
      }
    }
  ],
  [
    "785",
    {
      "pageContent": "def test_torch_4D_2D_inputs(self):\n        im_tensor = torch.randint(0, 255, (1, 3, 32, 32), dtype=torch.uint8)\n        mask_tensor = torch.randint(0, 255, (32, 32), dtype=torch.uint8) > 127.5\n        im_np = im_tensor.numpy()[0].transpose(1, 2, 0)\n        mask_np = mask_tensor.numpy()\n\n        t_mask_tensor, t_masked_tensor = prepare_mask_and_masked_image(im_tensor / 127.5 - 1, mask_tensor)\n        t_mask_np, t_masked_np = prepare_mask_and_masked_image(im_np, mask_np)\n\n        self.assertTrue((t_mask_tensor == t_mask_np).all())\n        self.assertTrue((t_masked_tensor == t_masked_np).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 435, "column": 4 },
          "end": { "row": 435, "column": 4 }
        }
      }
    }
  ],
  [
    "786",
    {
      "pageContent": "def test_torch_4D_3D_inputs(self):\n        im_tensor = torch.randint(0, 255, (1, 3, 32, 32), dtype=torch.uint8)\n        mask_tensor = torch.randint(0, 255, (1, 32, 32), dtype=torch.uint8) > 127.5\n        im_np = im_tensor.numpy()[0].transpose(1, 2, 0)\n        mask_np = mask_tensor.numpy()[0]\n\n        t_mask_tensor, t_masked_tensor = prepare_mask_and_masked_image(im_tensor / 127.5 - 1, mask_tensor)\n        t_mask_np, t_masked_np = prepare_mask_and_masked_image(im_np, mask_np)\n\n        self.assertTrue((t_mask_tensor == t_mask_np).all())\n        self.assertTrue((t_masked_tensor == t_masked_np).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 447, "column": 4 },
          "end": { "row": 447, "column": 4 }
        }
      }
    }
  ],
  [
    "787",
    {
      "pageContent": "def test_torch_4D_4D_inputs(self):\n        im_tensor = torch.randint(0, 255, (1, 3, 32, 32), dtype=torch.uint8)\n        mask_tensor = torch.randint(0, 255, (1, 1, 32, 32), dtype=torch.uint8) > 127.5\n        im_np = im_tensor.numpy()[0].transpose(1, 2, 0)\n        mask_np = mask_tensor.numpy()[0][0]\n\n        t_mask_tensor, t_masked_tensor = prepare_mask_and_masked_image(im_tensor / 127.5 - 1, mask_tensor)\n        t_mask_np, t_masked_np = prepare_mask_and_masked_image(im_np, mask_np)\n\n        self.assertTrue((t_mask_tensor == t_mask_np).all())\n        self.assertTrue((t_masked_tensor == t_masked_np).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 459, "column": 4 },
          "end": { "row": 459, "column": 4 }
        }
      }
    }
  ],
  [
    "788",
    {
      "pageContent": "def test_torch_batch_4D_3D(self):\n        im_tensor = torch.randint(0, 255, (2, 3, 32, 32), dtype=torch.uint8)\n        mask_tensor = torch.randint(0, 255, (2, 32, 32), dtype=torch.uint8) > 127.5\n\n        im_nps = [im.numpy().transpose(1, 2, 0) for im in im_tensor]\n        mask_nps = [mask.numpy() for mask in mask_tensor]\n\n        t_mask_tensor, t_masked_tensor = prepare_mask_and_masked_image(im_tensor / 127.5 - 1, mask_tensor)\n        nps = [prepare_mask_and_masked_image(i, m) for i, m in zip(im_nps, mask_nps)]\n        t_mask_np = torch.cat([n[0] for n in nps])\n        t_masked_np = torch.cat([n[1] for n in nps])\n\n        self.assertTrue((t_mask_tensor == t_mask_np).all())\n        self.assertTrue((t_masked_tensor == t_masked_np).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 471, "column": 4 },
          "end": { "row": 471, "column": 4 }
        }
      }
    }
  ],
  [
    "789",
    {
      "pageContent": "def test_torch_batch_4D_4D(self):\n        im_tensor = torch.randint(0, 255, (2, 3, 32, 32), dtype=torch.uint8)\n        mask_tensor = torch.randint(0, 255, (2, 1, 32, 32), dtype=torch.uint8) > 127.5\n\n        im_nps = [im.numpy().transpose(1, 2, 0) for im in im_tensor]\n        mask_nps = [mask.numpy()[0] for mask in mask_tensor]\n\n        t_mask_tensor, t_masked_tensor = prepare_mask_and_masked_image(im_tensor / 127.5 - 1, mask_tensor)\n        nps = [prepare_mask_and_masked_image(i, m) for i, m in zip(im_nps, mask_nps)]\n        t_mask_np = torch.cat([n[0] for n in nps])\n        t_masked_np = torch.cat([n[1] for n in nps])\n\n        self.assertTrue((t_mask_tensor == t_mask_np).all())\n        self.assertTrue((t_masked_tensor == t_masked_np).all())",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 486, "column": 4 },
          "end": { "row": 486, "column": 4 }
        }
      }
    }
  ],
  [
    "790",
    {
      "pageContent": "def test_shape_mismatch(self):\n        # test height and width\n        with self.assertRaises(AssertionError):\n            prepare_mask_and_masked_image(torch.randn(3, 32, 32), torch.randn(64, 64))\n        # test batch dim\n        with self.assertRaises(AssertionError):\n            prepare_mask_and_masked_image(torch.randn(2, 3, 32, 32), torch.randn(4, 64, 64))\n        # test batch dim\n        with self.assertRaises(AssertionError):\n            prepare_mask_and_masked_image(torch.randn(2, 3, 32, 32), torch.randn(4, 1, 64, 64))",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 501, "column": 4 },
          "end": { "row": 501, "column": 4 }
        }
      }
    }
  ],
  [
    "791",
    {
      "pageContent": "def test_type_mismatch(self):\n        # test tensors-only\n        with self.assertRaises(TypeError):\n            prepare_mask_and_masked_image(torch.rand(3, 32, 32), torch.rand(3, 32, 32).numpy())\n        # test tensors-only\n        with self.assertRaises(TypeError):\n            prepare_mask_and_masked_image(torch.rand(3, 32, 32).numpy(), torch.rand(3, 32, 32))",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 512, "column": 4 },
          "end": { "row": 512, "column": 4 }
        }
      }
    }
  ],
  [
    "792",
    {
      "pageContent": "def test_channels_first(self):\n        # test channels first for 3D tensors\n        with self.assertRaises(AssertionError):\n            prepare_mask_and_masked_image(torch.rand(32, 32, 3), torch.rand(3, 32, 32))",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 520, "column": 4 },
          "end": { "row": 520, "column": 4 }
        }
      }
    }
  ],
  [
    "793",
    {
      "pageContent": "def test_tensor_range(self):\n        # test im <= 1\n        with self.assertRaises(ValueError):\n            prepare_mask_and_masked_image(torch.ones(3, 32, 32) * 2, torch.rand(32, 32))\n        # test im >= -1\n        with self.assertRaises(ValueError):\n            prepare_mask_and_masked_image(torch.ones(3, 32, 32) * (-2), torch.rand(32, 32))\n        # test mask <= 1\n        with self.assertRaises(ValueError):\n            prepare_mask_and_masked_image(torch.rand(3, 32, 32), torch.ones(32, 32) * 2)\n        # test mask >= 0\n        with self.assertRaises(ValueError):\n            prepare_mask_and_masked_image(torch.rand(3, 32, 32), torch.ones(32, 32) * -1)",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 525, "column": 4 },
          "end": { "row": 525, "column": 4 }
        }
      }
    }
  ],
  [
    "794",
    {
      "pageContent": "class StableDiffusionImg2ImgPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionImg2ImgPipeline\n    params = TEXT_GUIDED_IMAGE_VARIATION_PARAMS - {\"height\", \"width\"}\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\"latents\"}\n    batch_params = TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_enco",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "795",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae\": vae,\n            \"text_encoder\": text_encoder,\n            \"tokenizer\": tokenizer,\n            \"safety_checker\": None,\n            \"feature_extractor\": None,",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "796",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 96, "column": 4 },
          "end": { "row": 96, "column": 4 }
        }
      }
    }
  ],
  [
    "797",
    {
      "pageContent": "def test_stable_diffusion_img2img_default_case(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4492, 0.3865, 0.4222, 0.5854, 0.5139, 0.4379, 0.4193, 0.48, 0.4218])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 112, "column": 4 },
          "end": { "row": 112, "column": 4 }
        }
      }
    }
  ],
  [
    "798",
    {
      "pageContent": "def test_stable_diffusion_img2img_negative_prompt(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        negative_prompt = \"french fries\"\n        output = sd_pipe(**inputs, negative_prompt=negative_prompt)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4065, 0.3783, 0.4050, 0.5266, 0.4781, 0.4252, 0.4203, 0.4692, 0.4365])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 128, "column": 4 },
          "end": { "row": 128, "column": 4 }
        }
      }
    }
  ],
  [
    "799",
    {
      "pageContent": "def test_stable_diffusion_img2img_multiple_init_images(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * 2\n        inputs[\"image\"] = inputs[\"image\"].repeat(2, 1, 1, 1)\n        image = sd_pipe(**inputs).images\n        image_slice = image[-1, -3:, -3:, -1]\n\n        assert image.shape == (2, 32, 32, 3)\n        expected_slice = np.array([0.5144, 0.4447, 0.4735, 0.6676, 0.5526, 0.5454, 0.645, 0.5149, 0.4689])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 146, "column": 4 },
          "end": { "row": 146, "column": 4 }
        }
      }
    }
  ],
  [
    "800",
    {
      "pageContent": "def test_stable_diffusion_img2img_k_lms(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\"\n        )\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4367, 0.4986, 0.4372, 0.6706, 0.5665, 0.444, 0.5864, 0.6019, 0.5203])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 164, "column": 4 },
          "end": { "row": 164, "column": 4 }
        }
      }
    }
  ],
  [
    "801",
    {
      "pageContent": "class StableDiffusionImg2ImgPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_img2img/sketch-mountains-input.png\"\n        )\n        inputs = {\n            \"prompt\": \"a fantasy landscape, concept art, high resolution\",\n            \"image\": init_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_img2img_default(self):\n        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", safety_checker=None)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs(torch_device)\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 512, 768, 3)\n        expected_slice = np.array([0.4300, 0.4662, 0.4930, 0.3990, 0.4307, 0.4525, 0.3719, 0.4064, 0.3923])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-3\n\n    def test_stable_diffusion_img2img_k_lms(self):\n        pipe = StableDiffusionI",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 202, "column": 0 },
          "end": { "row": 202, "column": 0 }
        }
      }
    }
  ],
  [
    "802",
    {
      "pageContent": "class StableDiffusionImg2ImgPipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_img2img/sketch-mountains-input.png\"\n        )\n        inputs = {\n            \"prompt\": \"a fantasy landscape, concept art, high resolution\",\n            \"image\": init_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_img2img_pndm(self):\n        sd_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_img2img/stable_diffusion_1_5_pndm.npy\"\n        )\n        max_diff = np.abs(expected_image - image).max()\n        assert max_diff < 1e-3\n\n    def test_img2img_ddim(self):\n        sd_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n  ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 409, "column": 0 },
          "end": { "row": 409, "column": 0 }
        }
      }
    }
  ],
  [
    "803",
    {
      "pageContent": "class OnnxStableDiffusionPipelineFastTests(OnnxPipelineTesterMixin, unittest.TestCase):\n    hub_checkpoint = \"hf-internal-testing/tiny-random-OnnxStableDiffusionPipeline\"\n\n    def get_dummy_inputs(self, seed=0):\n        generator = np.random.RandomState(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_pipeline_default_ddim(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.65072, 0.58492, 0.48219, 0.55521, 0.53180, 0.55939, 0.50697, 0.39800, 0.46455])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_pipeline_pndm(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config, skip_prk_steps=True)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.6",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "804",
    {
      "pageContent": "def get_dummy_inputs(self, seed=0):\n        generator = np.random.RandomState(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 41, "column": 4 },
          "end": { "row": 41, "column": 4 }
        }
      }
    }
  ],
  [
    "805",
    {
      "pageContent": "def test_pipeline_default_ddim(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.65072, 0.58492, 0.48219, 0.55521, 0.53180, 0.55939, 0.50697, 0.39800, 0.46455])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 52, "column": 4 },
          "end": { "row": 52, "column": 4 }
        }
      }
    }
  ],
  [
    "806",
    {
      "pageContent": "def test_pipeline_pndm(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config, skip_prk_steps=True)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.65863, 0.59425, 0.49326, 0.56313, 0.53875, 0.56627, 0.51065, 0.39777, 0.46330])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 65, "column": 4 },
          "end": { "row": 65, "column": 4 }
        }
      }
    }
  ],
  [
    "807",
    {
      "pageContent": "def test_pipeline_lms(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.53755, 0.60786, 0.47402, 0.49488, 0.51869, 0.49819, 0.47985, 0.38957, 0.44279])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 79, "column": 4 },
          "end": { "row": 79, "column": 4 }
        }
      }
    }
  ],
  [
    "808",
    {
      "pageContent": "def test_pipeline_euler(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.53755, 0.60786, 0.47402, 0.49488, 0.51869, 0.49819, 0.47985, 0.38957, 0.44279])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "809",
    {
      "pageContent": "def test_pipeline_euler_ancestral(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.53817, 0.60812, 0.47384, 0.49530, 0.51894, 0.49814, 0.47984, 0.38958, 0.44271])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 107, "column": 4 },
          "end": { "row": 107, "column": 4 }
        }
      }
    }
  ],
  [
    "810",
    {
      "pageContent": "def test_pipeline_dpm_multistep(self):\n        pipe = OnnxStableDiffusionPipeline.from_pretrained(self.hub_checkpoint, provider=\"CPUExecutionProvider\")\n        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 128, 128, 3)\n        expected_slice = np.array([0.53895, 0.60808, 0.47933, 0.49608, 0.51886, 0.49950, 0.48053, 0.38957, 0.44200])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 121, "column": 4 },
          "end": { "row": 121, "column": 4 }
        }
      }
    }
  ],
  [
    "811",
    {
      "pageContent": "class OnnxStableDiffusionPipelineIntegrationTests(unittest.TestCase):\n    @property\n    def gpu_provider(self):\n        return (\n            \"CUDAExecutionProvider\",\n            {\n                \"gpu_mem_limit\": \"15000000000\",  # 15GB\n                \"arena_extend_strategy\": \"kSameAsRequested\",\n            },\n        )\n\n    @property\n    def gpu_options(self):\n        options = ort.SessionOptions()\n        options.enable_mem_pattern = False\n        return options\n\n    def test_inference_default_pndm(self):\n        # using the PNDM scheduler by default\n        sd_pipe = OnnxStableDiffusionPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\",\n            revision=\"onnx\",\n            safety_checker=None,\n            feature_extractor=None,\n            provider=self.gpu_provider,\n            sess_options=self.gpu_options,\n        )\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        np.random.seed(0)\n        output = sd_pipe([prompt], guidance_scale=6.0, num_inference_steps=10, output_type=\"np\")\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.0452, 0.0390, 0.0087, 0.0350, 0.0617, 0.0364, 0.0544, 0.0523, 0.0720])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_inference_ddim(self):\n        ddim_scheduler = DDIMScheduler.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\", revision=\"onnx\"\n        )\n        s",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 139, "column": 0 },
          "end": { "row": 139, "column": 0 }
        }
      }
    }
  ],
  [
    "812",
    {
      "pageContent": "class StableDiffusionOnnxInpaintLegacyPipelineIntegrationTests(unittest.TestCase):\n    @property\n    def gpu_provider(self):\n        return (\n            \"CUDAExecutionProvider\",\n            {\n                \"gpu_mem_limit\": \"15000000000\",  # 15GB\n                \"arena_extend_strategy\": \"kSameAsRequested\",\n            },\n        )\n\n    @property\n    def gpu_options(self):\n        options = ort.SessionOptions()\n        options.enable_mem_pattern = False\n        return options\n\n    def test_inference(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/in_paint/overture-creations-5sI6fQgYIuo.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/in_paint/overture-creations-5sI6fQgYIuo_mask.png\"\n        )\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/in_paint/red_cat_sitting_on_a_park_bench_onnx.npy\"\n        )\n\n        # using the PNDM scheduler by default\n        pipe = OnnxStableDiffusionInpaintPipelineLegacy.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\",\n            revision=\"onnx\",\n            safety_checker=None,\n            feature_extractor=None,\n            provider=self.gpu_provider,\n            sess_options=self.gpu_options,\n        )\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A red cat sitting on a park bench\"\n\n        generato",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "813",
    {
      "pageContent": "class StableDiffusionControlNetPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionControlNetPipeline\n    params = TEXT_TO_IMAGE_PARAMS\n    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        torch.manual_seed(0)\n        controlnet = ControlNetModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            in_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            cross_attention_dim=32,\n            conditioning_embedding_out_channels=(16, 32),\n        )\n        torch.manual_seed(0)\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torc",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "814",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        torch.manual_seed(0)\n        controlnet = ControlNetModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            in_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            cross_attention_dim=32,\n            conditioning_embedding_out_channels=(16, 32),\n        )\n        torch.manual_seed(0)\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "815",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n\n        controlnet_embedder_scale_factor = 2\n        image = randn_tensor(\n            (1, 3, 32 * controlnet_embedder_scale_factor, 32 * controlnet_embedder_scale_factor),\n            generator=generator,\n            device=torch.device(device),\n        )\n\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n            \"image\": image,\n        }\n\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 107, "column": 4 },
          "end": { "row": 107, "column": 4 }
        }
      }
    }
  ],
  [
    "816",
    {
      "pageContent": "class StableDiffusionControlNetPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_canny(self):\n        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\")\n\n        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\", safety_checker=None, controlnet=controlnet\n        )\n        pipe.enable_model_cpu_offload()\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=\"cpu\").manual_seed(0)\n        prompt = \"bird\"\n        image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny.png\"\n        )\n\n        output = pipe(prompt, image, generator=generator, output_type=\"np\")\n\n        image = output.images[0]\n\n        assert image.shape == (768, 512, 3)\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny_out.npy\"\n        )\n\n        assert np.abs(expected_image - image).max() < 5e-3\n\n    def test_depth(self):\n        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-depth\")\n\n        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\", safety_checker=None, controlnet=controlnet\n        )\n        pipe.enable_model_cpu_offload()\n        pipe.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(d",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 147, "column": 0 },
          "end": { "row": 147, "column": 0 }
        }
      }
    }
  ],
  [
    "817",
    {
      "pageContent": "class CycleDiffusionPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = CycleDiffusionPipeline\n    params = TEXT_GUIDED_IMAGE_VARIATION_PARAMS - {\n        \"negative_prompt\",\n        \"height\",\n        \"width\",\n        \"negative_prompt_embeds\",\n    }\n    required_optional_params = PipelineTesterMixin.required_optional_params - {\"latents\"}\n    batch_params = TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS.union({\"source_prompt\"})\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            num_train_timesteps=1000,\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eo",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_cycle_diffusion.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "818",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            num_train_timesteps=1000,\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"s",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_cycle_diffusion.py",
        "range": {
          "start": { "row": 45, "column": 4 },
          "end": { "row": 45, "column": 4 }
        }
      }
    }
  ],
  [
    "819",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"An astronaut riding an elephant\",\n            \"source_prompt\": \"An astronaut riding a horse\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"eta\": 0.1,\n            \"strength\": 0.8,\n            \"guidance_scale\": 3,\n            \"source_guidance_scale\": 1,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_cycle_diffusion.py",
        "range": {
          "start": { "row": 100, "column": 4 },
          "end": { "row": 100, "column": 4 }
        }
      }
    }
  ],
  [
    "820",
    {
      "pageContent": "def test_stable_diffusion_cycle(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n\n        components = self.get_dummy_components()\n        pipe = CycleDiffusionPipeline(**components)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = pipe(**inputs)\n        images = output.images\n\n        image_slice = images[0, -3:, -3:, -1]\n\n        assert images.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4459, 0.4943, 0.4544, 0.6643, 0.5474, 0.4327, 0.5701, 0.5959, 0.5179])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_cycle_diffusion.py",
        "range": {
          "start": { "row": 120, "column": 4 },
          "end": { "row": 120, "column": 4 }
        }
      }
    }
  ],
  [
    "821",
    {
      "pageContent": "class CycleDiffusionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_cycle_diffusion_pipeline_fp16(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/cycle-diffusion/black_colored_car.png\"\n        )\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/cycle-diffusion/blue_colored_car_fp16.npy\"\n        )\n        init_image = init_image.resize((512, 512))\n\n        model_id = \"CompVis/stable-diffusion-v1-4\"\n        scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n        pipe = CycleDiffusionPipeline.from_pretrained(\n            model_id, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16, revision=\"fp16\"\n        )\n\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        source_prompt = \"A black colored car\"\n        prompt = \"A blue colored car\"\n\n        generator = torch.manual_seed(0)\n        output = pipe(\n            prompt=prompt,\n            source_prompt=source_prompt,\n            image=init_image,\n            num_inference_steps=100,\n            eta=0.1,\n            strength=0.85,\n            guidance_scale=3,\n            source_guidance_scale=1,\n            generator=generator,\n            output_type=\"np\",\n        )\n     ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_cycle_diffusion.py",
        "range": {
          "start": { "row": 183, "column": 0 },
          "end": { "row": 183, "column": 0 }
        }
      }
    }
  ],
  [
    "822",
    {
      "pageContent": "class StableDiffusionInstructPix2PixPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionInstructPix2PixPipeline\n    params = TEXT_GUIDED_IMAGE_VARIATION_PARAMS - {\"height\", \"width\", \"cross_attention_kwargs\"}\n    batch_params = TEXT_GUIDED_IMAGE_INPAINTING_BATCH_PARAMS\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=8,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.f",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 43, "column": 0 },
          "end": { "row": 43, "column": 0 }
        }
      }
    }
  ],
  [
    "823",
    {
      "pageContent": "def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=8,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae\": vae,\n            \"text_encoder\": text_encoder,\n            \"tokenizer\": tokenizer,\n            \"safety_checker\": None,\n            \"feature_extractor\": None,",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "824",
    {
      "pageContent": "def get_dummy_inputs(self, device, seed=0):\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"image_guidance_scale\": 1,\n            \"output_type\": \"numpy\",\n        }\n        return inputs",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 96, "column": 4 },
          "end": { "row": 96, "column": 4 }
        }
      }
    }
  ],
  [
    "825",
    {
      "pageContent": "def test_stable_diffusion_pix2pix_default_case(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInstructPix2PixPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.7318, 0.3723, 0.4662, 0.623, 0.5770, 0.5014, 0.4281, 0.5550, 0.4813])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 115, "column": 4 },
          "end": { "row": 115, "column": 4 }
        }
      }
    }
  ],
  [
    "826",
    {
      "pageContent": "def test_stable_diffusion_pix2pix_negative_prompt(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInstructPix2PixPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        negative_prompt = \"french fries\"\n        output = sd_pipe(**inputs, negative_prompt=negative_prompt)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.7323, 0.3688, 0.4611, 0.6255, 0.5746, 0.5017, 0.433, 0.5553, 0.4827])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 130, "column": 4 },
          "end": { "row": 130, "column": 4 }
        }
      }
    }
  ],
  [
    "827",
    {
      "pageContent": "def test_stable_diffusion_pix2pix_multiple_init_images(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInstructPix2PixPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * 2\n\n        image = np.array(inputs[\"image\"]).astype(np.float32) / 255.0\n        image = torch.from_numpy(image).unsqueeze(0).to(device)\n        image = image.permute(0, 3, 1, 2)\n        inputs[\"image\"] = image.repeat(2, 1, 1, 1)\n\n        image = sd_pipe(**inputs).images\n        image_slice = image[-1, -3:, -3:, -1]\n\n        assert image.shape == (2, 32, 32, 3)\n        expected_slice = np.array([0.606, 0.5712, 0.5099, 0.598, 0.5805, 0.7205, 0.6793, 0.554, 0.5607])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 148, "column": 4 },
          "end": { "row": 148, "column": 4 }
        }
      }
    }
  ],
  [
    "828",
    {
      "pageContent": "def test_stable_diffusion_pix2pix_euler(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = EulerAncestralDiscreteScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\"\n        )\n        sd_pipe = StableDiffusionInstructPix2PixPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        slice = [round(x, 4) for x in image_slice.flatten().tolist()]\n        print(\",\".join([str(x) for x in slice]))\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.726, 0.3902, 0.4868, 0.585, 0.5672, 0.511, 0.3906, 0.551, 0.4846])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 171, "column": 4 },
          "end": { "row": 171, "column": 4 }
        }
      }
    }
  ],
  [
    "829",
    {
      "pageContent": "class StableDiffusionInstructPix2PixPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, seed=0):\n        generator = torch.manual_seed(seed)\n        image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_pix2pix/example.jpg\"\n        )\n        inputs = {\n            \"prompt\": \"turn him into a cyborg\",\n            \"image\": image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"image_guidance_scale\": 1.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_pix2pix_default(self):\n        pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n            \"timbrooks/instruct-pix2pix\", safety_checker=None\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1].flatten()\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.5902, 0.6015, 0.6027, 0.5983, 0.6092, 0.6061, 0.5765, 0.5785, 0.5555])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-3\n\n    def test_stable_diffusion_pix2pix_k_lms(self):\n        pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n            \"timbrooks/instruct-pix2pix\", safety_checker=None\n        )\n        p",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py",
        "range": {
          "start": { "row": 196, "column": 0 },
          "end": { "row": 196, "column": 0 }
        }
      }
    }
  ],
  [
    "830",
    {
      "pageContent": "class OnnxStableDiffusionInpaintPipelineIntegrationTests(unittest.TestCase):\n    @property\n    def gpu_provider(self):\n        return (\n            \"CUDAExecutionProvider\",\n            {\n                \"gpu_mem_limit\": \"15000000000\",  # 15GB\n                \"arena_extend_strategy\": \"kSameAsRequested\",\n            },\n        )\n\n    @property\n    def gpu_options(self):\n        options = ort.SessionOptions()\n        options.enable_mem_pattern = False\n        return options\n\n    def test_inference_default_pndm(self):\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/in_paint/overture-creations-5sI6fQgYIuo.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n            \"/in_paint/overture-creations-5sI6fQgYIuo_mask.png\"\n        )\n        pipe = OnnxStableDiffusionInpaintPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-inpainting\",\n            revision=\"onnx\",\n            safety_checker=None,\n            feature_extractor=None,\n            provider=self.gpu_provider,\n            sess_options=self.gpu_options,\n        )\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A red cat sitting on a park bench\"\n\n        generator = np.random.RandomState(0)\n        output = pipe(\n            prompt=prompt,\n            image=init_image,\n            mask_image=mask_image,\n            guidance_scale=7.5,\n            num_inference_steps=10,\n            generator=generator,\n  ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_onnx_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 43, "column": 0 },
          "end": { "row": 43, "column": 0 }
        }
      }
    }
  ],
  [
    "831",
    {
      "pageContent": "class StableDiffusionInpaintLegacyPipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    @property\n    def dummy_uncond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    @property\n    def dummy_cond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        return model\n\n    @property\n    def dummy_cond_unet_inpaint(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "832",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 43, "column": 4 },
          "end": { "row": 43, "column": 4 }
        }
      }
    }
  ],
  [
    "833",
    {
      "pageContent": "def test_stable_diffusion_inpaint_legacy(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        image = self.dummy_image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((32, 32))\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionInpaintPipelineLegacy(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            image=init_image,\n            mask_image=mask_image,\n        )\n\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            generato",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 159, "column": 4 },
          "end": { "row": 159, "column": 4 }
        }
      }
    }
  ],
  [
    "834",
    {
      "pageContent": "def test_stable_diffusion_inpaint_legacy_negative_prompt(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        image = self.dummy_image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((32, 32))\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionInpaintPipelineLegacy(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        negative_prompt = \"french fries\"\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe(\n            prompt,\n            negative_prompt=negative_prompt,\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            image=init_image,\n            mask_image=mask_image,\n        )\n\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 219, "column": 4 },
          "end": { "row": 219, "column": 4 }
        }
      }
    }
  ],
  [
    "835",
    {
      "pageContent": "def test_stable_diffusion_inpaint_legacy_num_images_per_prompt(self):\n        device = \"cpu\"\n        unet = self.dummy_cond_unet\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        image = self.dummy_image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((32, 32))\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionInpaintPipelineLegacy(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n\n        # test num_images_per_prompt=1 (default)\n        images = sd_pipe(\n            prompt,\n            num_inference_steps=2,\n            output_type=\"np\",\n            image=init_image,\n            mask_image=mask_image,\n        ).images\n\n        assert images.shape == (1, 32, 32, 3)\n\n        # test num_images_per_prompt=1 (default) for batch of prompts\n        batch_size = 2\n        images = sd_pipe(\n            [prompt] * batch_size,\n            num_inference_steps=2,\n            output_type=\"np\",\n            image=init_ima",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 266, "column": 4 },
          "end": { "row": 266, "column": 4 }
        }
      }
    }
  ],
  [
    "836",
    {
      "pageContent": "class StableDiffusionInpaintLegacyPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_image.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_mask.png\"\n        )\n        inputs = {\n            \"prompt\": \"A red cat sitting on a park bench\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_inpaint_legacy_pndm(self):\n        pipe = StableDiffusionInpaintPipelineLegacy.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\", safety_checker=None\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs(torch_device)\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 512, 512, 3)\n        e",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 345, "column": 0 },
          "end": { "row": 345, "column": 0 }
        }
      }
    }
  ],
  [
    "837",
    {
      "pageContent": "class StableDiffusionInpaintLegacyPipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device, generator_device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=generator_device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_image.png\"\n        )\n        mask_image = load_image(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint/input_bench_mask.png\"\n        )\n        inputs = {\n            \"prompt\": \"A red cat sitting on a park bench\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_inpaint_pndm(self):\n        sd_pipe = StableDiffusionInpaintPipelineLegacy.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        sd_pipe.to(torch_device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(torch_device)\n        image = sd_pipe(**inputs).images[0]\n\n        expected_image = load_numpy(\n            \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main\"\n            \"/stable_diffusion_inpaint_legacy/stable_diffusion_1_5_pndm.npy\"\n        )\n ",
      "metadata": {
        "source": "tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 447, "column": 0 },
          "end": { "row": 447, "column": 0 }
        }
      }
    }
  ],
  [
    "838",
    {
      "pageContent": "class SafeDiffusionPipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    @property\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    @property\n    def dummy_cond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        return model\n\n    @property\n    def dummy_vae(self):\n        torch.manual_seed(0)\n        model = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        return model\n\n    @property\n    def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_a",
      "metadata": {
        "source": "tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "839",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()",
      "metadata": {
        "source": "tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py",
        "range": {
          "start": { "row": 34, "column": 4 },
          "end": { "row": 34, "column": 4 }
        }
      }
    }
  ],
  [
    "840",
    {
      "pageContent": "def test_semantic_diffusion_ddim(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = DDIMScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from",
      "metadata": {
        "source": "tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "841",
    {
      "pageContent": "def test_semantic_diffusion_pndm(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        unet = self.dummy_cond_unet\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        # make sure here that pndm scheduler skips prk\n        sd_pipe = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = sd_pipe([prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type=\"np\")\n\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = sd_pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=6.0,\n            num_inference_steps=2,\n            output_type=\"np\",\n            return_dict=False,\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.5095, 0.5674, 0.4668, 0.5126,",
      "metadata": {
        "source": "tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py",
        "range": {
          "start": { "row": 161, "column": 4 },
          "end": { "row": 161, "column": 4 }
        }
      }
    }
  ],
  [
    "842",
    {
      "pageContent": "def test_semantic_diffusion_no_safety_checker(self):\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-lms-pipe\", safety_checker=None\n        )\n        assert isinstance(pipe, StableDiffusionPipeline)\n        assert isinstance(pipe.scheduler, LMSDiscreteScheduler)\n        assert pipe.safety_checker is None\n\n        image = pipe(\"example prompt\", num_inference_steps=2).images[0]\n        assert image is not None\n\n        # check that there's no error when saving a pipeline with one of the models being None\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe = StableDiffusionPipeline.from_pretrained(tmpdirname)\n\n        # sanity check that the pipeline still works\n        assert pipe.safety_checker is None\n        image = pipe(\"example prompt\", num_inference_steps=2).images[0]\n        assert image is not None",
      "metadata": {
        "source": "tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py",
        "range": {
          "start": { "row": 207, "column": 4 },
          "end": { "row": 207, "column": 4 }
        }
      }
    }
  ],
  [
    "843",
    {
      "pageContent": "class SemanticDiffusionPipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_positive_guidance(self):\n        torch_device = \"cuda\"\n        pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        pipe = pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"a photo of a cat\"\n        edit = {\n            \"editing_prompt\": [\"sunglasses\"],\n            \"reverse_editing_direction\": [False],\n            \"edit_warmup_steps\": 10,\n            \"edit_guidance_scale\": 6,\n            \"edit_threshold\": 0.95,\n            \"edit_momentum_scale\": 0.5,\n            \"edit_mom_beta\": 0.6,\n        }\n\n        seed = 3\n        guidance_scale = 7\n\n        # no sega enabled\n        generator = torch.Generator(torch_device)\n        generator.manual_seed(seed)\n        output = pipe(\n            [prompt],\n            generator=generator,\n            guidance_scale=guidance_scale,\n            num_inference_steps=50,\n            output_type=\"np\",\n            width=512,\n            height=512,\n        )\n\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n        expected_slice = [\n            0.34673113,\n            0.38492733,\n            0.37597352,\n            0.34086335,\n            0.35650748,\n            0.35579205,\n            0.3384763,\n            0.34340236,\n            0.3573271,\n        ]\n\n        assert image.shape == (1, 512, 512, 3)\n\n        assert",
      "metadata": {
        "source": "tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py",
        "range": {
          "start": { "row": 263, "column": 0 },
          "end": { "row": 263, "column": 0 }
        }
      }
    }
  ],
  [
    "844",
    {
      "pageContent": "class DDPMPipelineFastTests(unittest.TestCase):\n    @property\n    def dummy_uncond_unet(self):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    def test_fast_inference(self):\n        device = \"cpu\"\n        unet = self.dummy_uncond_unet\n        scheduler = DDPMScheduler()\n\n        ddpm = DDPMPipeline(unet=unet, scheduler=scheduler)\n        ddpm.to(device)\n        ddpm.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image = ddpm(generator=generator, num_inference_steps=2, output_type=\"numpy\").images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = ddpm(generator=generator, num_inference_steps=2, output_type=\"numpy\", return_dict=False)[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array(\n            [9.956e-01, 5.785e-01, 4.675e-01, 9.930e-01, 0.0, 1.000, 1.199e-03, 2.648e-04, 5.101e-04]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_inference_predict_sample(self):\n       ",
      "metadata": {
        "source": "tests/pipelines/ddpm/test_ddpm.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "845",
    {
      "pageContent": "def test_fast_inference(self):\n        device = \"cpu\"\n        unet = self.dummy_uncond_unet\n        scheduler = DDPMScheduler()\n\n        ddpm = DDPMPipeline(unet=unet, scheduler=scheduler)\n        ddpm.to(device)\n        ddpm.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image = ddpm(generator=generator, num_inference_steps=2, output_type=\"numpy\").images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = ddpm(generator=generator, num_inference_steps=2, output_type=\"numpy\", return_dict=False)[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array(\n            [9.956e-01, 5.785e-01, 4.675e-01, 9.930e-01, 0.0, 1.000, 1.199e-03, 2.648e-04, 5.101e-04]\n        )\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/ddpm/test_ddpm.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "846",
    {
      "pageContent": "def test_inference_predict_sample(self):\n        unet = self.dummy_uncond_unet\n        scheduler = DDPMScheduler(prediction_type=\"sample\")\n\n        ddpm = DDPMPipeline(unet=unet, scheduler=scheduler)\n        ddpm.to(torch_device)\n        ddpm.set_progress_bar_config(disable=None)\n\n        # Warmup pass when using mps (see #372)\n        if torch_device == \"mps\":\n            _ = ddpm(num_inference_steps=1)\n\n        generator = torch.manual_seed(0)\n        image = ddpm(generator=generator, num_inference_steps=2, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        image_eps = ddpm(generator=generator, num_inference_steps=2, output_type=\"numpy\")[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_eps_slice = image_eps[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        tolerance = 1e-2 if torch_device != \"mps\" else 3e-2\n        assert np.abs(image_slice.flatten() - image_eps_slice.flatten()).max() < tolerance",
      "metadata": {
        "source": "tests/pipelines/ddpm/test_ddpm.py",
        "range": {
          "start": { "row": 68, "column": 4 },
          "end": { "row": 68, "column": 4 }
        }
      }
    }
  ],
  [
    "847",
    {
      "pageContent": "class DDPMPipelineIntegrationTests(unittest.TestCase):\n    def test_inference_cifar10(self):\n        model_id = \"google/ddpm-cifar10-32\"\n\n        unet = UNet2DModel.from_pretrained(model_id)\n        scheduler = DDPMScheduler.from_pretrained(model_id)\n\n        ddpm = DDPMPipeline(unet=unet, scheduler=scheduler)\n        ddpm.to(torch_device)\n        ddpm.set_progress_bar_config(disable=None)\n\n        generator = torch.manual_seed(0)\n        image = ddpm(generator=generator, output_type=\"numpy\").images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4200, 0.3588, 0.1939, 0.3847, 0.3382, 0.2647, 0.4155, 0.3582, 0.3385])\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2",
      "metadata": {
        "source": "tests/pipelines/ddpm/test_ddpm.py",
        "range": {
          "start": { "row": 96, "column": 0 },
          "end": { "row": 96, "column": 0 }
        }
      }
    }
  ],
  [
    "848",
    {
      "pageContent": "def pytest_addoption(parser):\n    from diffusers.utils.testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)",
      "metadata": {
        "source": "tests/conftest.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "849",
    {
      "pageContent": "def pytest_terminal_summary(terminalreporter):\n    from diffusers.utils.testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)",
      "metadata": {
        "source": "tests/conftest.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "850",
    {
      "pageContent": "class OnnxPipelineTesterMixin:\n    \"\"\"\n    This mixin is designed to be used with unittest.TestCase classes.\n    It provides a set of common tests for each ONNXRuntime pipeline, e.g. saving and loading the pipeline,\n    equivalence of dict and tuple outputs, etc.\n    \"\"\"\n\n    pass",
      "metadata": {
        "source": "tests/test_pipelines_onnx_common.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "851",
    {
      "pageContent": "class EmbeddingsTests(unittest.TestCase):\n    def test_timestep_embeddings(self):\n        embedding_dim = 256\n        timesteps = torch.arange(16)\n\n        t1 = get_timestep_embedding(timesteps, embedding_dim)\n\n        # first vector should always be composed only of 0's and 1's\n        assert (t1[0, : embedding_dim // 2] - 0).abs().sum() < 1e-5\n        assert (t1[0, embedding_dim // 2 :] - 1).abs().sum() < 1e-5\n\n        # last element of each vector should be one\n        assert (t1[:, -1] - 1).abs().sum() < 1e-5\n\n        # For large embeddings (e.g. 128) the frequency of every vector is higher\n        # than the previous one which means that the gradients of later vectors are\n        # ALWAYS higher than the previous ones\n        grad_mean = np.abs(np.gradient(t1, axis=-1)).mean(axis=1)\n\n        prev_grad = 0.0\n        for grad in grad_mean:\n            assert grad > prev_grad\n            prev_grad = grad\n\n    def test_timestep_defaults(self):\n        embedding_dim = 16\n        timesteps = torch.arange(10)\n\n        t1 = get_timestep_embedding(timesteps, embedding_dim)\n        t2 = get_timestep_embedding(\n            timesteps, embedding_dim, flip_sin_to_cos=False, downscale_freq_shift=1, max_period=10_000\n        )\n\n        assert torch.allclose(t1.cpu(), t2.cpu(), 1e-3)\n\n    def test_timestep_flip_sin_cos(self):\n        embedding_dim = 16\n        timesteps = torch.arange(10)\n\n        t1 = get_timestep_embedding(timesteps, embedding_dim, flip_sin_to_cos=True)\n        t1 = torch.cat([t1[:, embedding_dim // 2 :], t1[:, : embedding_dim // 2]], dim=-1)\n\n        t2 = get_timest",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "852",
    {
      "pageContent": "def test_timestep_embeddings(self):\n        embedding_dim = 256\n        timesteps = torch.arange(16)\n\n        t1 = get_timestep_embedding(timesteps, embedding_dim)\n\n        # first vector should always be composed only of 0's and 1's\n        assert (t1[0, : embedding_dim // 2] - 0).abs().sum() < 1e-5\n        assert (t1[0, embedding_dim // 2 :] - 1).abs().sum() < 1e-5\n\n        # last element of each vector should be one\n        assert (t1[:, -1] - 1).abs().sum() < 1e-5\n\n        # For large embeddings (e.g. 128) the frequency of every vector is higher\n        # than the previous one which means that the gradients of later vectors are\n        # ALWAYS higher than the previous ones\n        grad_mean = np.abs(np.gradient(t1, axis=-1)).mean(axis=1)\n\n        prev_grad = 0.0\n        for grad in grad_mean:\n            assert grad > prev_grad\n            prev_grad = grad",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 33, "column": 4 },
          "end": { "row": 33, "column": 4 }
        }
      }
    }
  ],
  [
    "853",
    {
      "pageContent": "def test_timestep_defaults(self):\n        embedding_dim = 16\n        timesteps = torch.arange(10)\n\n        t1 = get_timestep_embedding(timesteps, embedding_dim)\n        t2 = get_timestep_embedding(\n            timesteps, embedding_dim, flip_sin_to_cos=False, downscale_freq_shift=1, max_period=10_000\n        )\n\n        assert torch.allclose(t1.cpu(), t2.cpu(), 1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 56, "column": 4 },
          "end": { "row": 56, "column": 4 }
        }
      }
    }
  ],
  [
    "854",
    {
      "pageContent": "def test_timestep_flip_sin_cos(self):\n        embedding_dim = 16\n        timesteps = torch.arange(10)\n\n        t1 = get_timestep_embedding(timesteps, embedding_dim, flip_sin_to_cos=True)\n        t1 = torch.cat([t1[:, embedding_dim // 2 :], t1[:, : embedding_dim // 2]], dim=-1)\n\n        t2 = get_timestep_embedding(timesteps, embedding_dim, flip_sin_to_cos=False)\n\n        assert torch.allclose(t1.cpu(), t2.cpu(), 1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 67, "column": 4 },
          "end": { "row": 67, "column": 4 }
        }
      }
    }
  ],
  [
    "855",
    {
      "pageContent": "def test_timestep_downscale_freq_shift(self):\n        embedding_dim = 16\n        timesteps = torch.arange(10)\n\n        t1 = get_timestep_embedding(timesteps, embedding_dim, downscale_freq_shift=0)\n        t2 = get_timestep_embedding(timesteps, embedding_dim, downscale_freq_shift=1)\n\n        # get cosine half (vectors that are wrapped into cosine)\n        cosine_half = (t1 - t2)[:, embedding_dim // 2 :]\n\n        # cosine needs to be negative\n        assert (np.abs((cosine_half <= 0).numpy()) - 1).sum() < 1e-5",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 78, "column": 4 },
          "end": { "row": 78, "column": 4 }
        }
      }
    }
  ],
  [
    "856",
    {
      "pageContent": "def test_sinoid_embeddings_hardcoded(self):\n        embedding_dim = 64\n        timesteps = torch.arange(128)\n\n        # standard unet, score_vde\n        t1 = get_timestep_embedding(timesteps, embedding_dim, downscale_freq_shift=1, flip_sin_to_cos=False)\n        # glide, ldm\n        t2 = get_timestep_embedding(timesteps, embedding_dim, downscale_freq_shift=0, flip_sin_to_cos=True)\n        # grad-tts\n        t3 = get_timestep_embedding(timesteps, embedding_dim, scale=1000)\n\n        assert torch.allclose(\n            t1[23:26, 47:50].flatten().cpu(),\n            torch.tensor([0.9646, 0.9804, 0.9892, 0.9615, 0.9787, 0.9882, 0.9582, 0.9769, 0.9872]),\n            1e-3,\n        )\n        assert torch.allclose(\n            t2[23:26, 47:50].flatten().cpu(),\n            torch.tensor([0.3019, 0.2280, 0.1716, 0.3146, 0.2377, 0.1790, 0.3272, 0.2474, 0.1864]),\n            1e-3,\n        )\n        assert torch.allclose(\n            t3[23:26, 47:50].flatten().cpu(),\n            torch.tensor([-0.9801, -0.9464, -0.9349, -0.3952, 0.8887, -0.9709, 0.5299, -0.2853, -0.9927]),\n            1e-3,\n        )",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 91, "column": 4 },
          "end": { "row": 91, "column": 4 }
        }
      }
    }
  ],
  [
    "857",
    {
      "pageContent": "class Upsample2DBlockTests(unittest.TestCase):\n    def test_upsample_default(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 32, 32)\n        upsample = Upsample2D(channels=32, use_conv=False)\n        with torch.no_grad():\n            upsampled = upsample(sample)\n\n        assert upsampled.shape == (1, 32, 64, 64)\n        output_slice = upsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([-0.2173, -1.2079, -1.2079, 0.2952, 1.1254, 1.1254, 0.2952, 1.1254, 1.1254])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)\n\n    def test_upsample_with_conv(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 32, 32)\n        upsample = Upsample2D(channels=32, use_conv=True)\n        with torch.no_grad():\n            upsampled = upsample(sample)\n\n        assert upsampled.shape == (1, 32, 64, 64)\n        output_slice = upsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([0.7145, 1.3773, 0.3492, 0.8448, 1.0839, -0.3341, 0.5956, 0.1250, -0.4841])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)\n\n    def test_upsample_with_conv_out_dim(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 32, 32)\n        upsample = Upsample2D(channels=32, use_conv=True, out_channels=64)\n        with torch.no_grad():\n            upsampled = upsample(sample)\n\n        assert upsampled.shape == (1, 64, 64, 64)\n        output_slice = upsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([0.2703, 0.1656, -0.2538, -0.0553, -0.2984, 0.1044, 0.1155, 0.2579, 0.77",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 119, "column": 0 },
          "end": { "row": 119, "column": 0 }
        }
      }
    }
  ],
  [
    "858",
    {
      "pageContent": "def test_upsample_default(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 32, 32)\n        upsample = Upsample2D(channels=32, use_conv=False)\n        with torch.no_grad():\n            upsampled = upsample(sample)\n\n        assert upsampled.shape == (1, 32, 64, 64)\n        output_slice = upsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([-0.2173, -1.2079, -1.2079, 0.2952, 1.1254, 1.1254, 0.2952, 1.1254, 1.1254])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 120, "column": 4 },
          "end": { "row": 120, "column": 4 }
        }
      }
    }
  ],
  [
    "859",
    {
      "pageContent": "def test_upsample_with_conv(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 32, 32)\n        upsample = Upsample2D(channels=32, use_conv=True)\n        with torch.no_grad():\n            upsampled = upsample(sample)\n\n        assert upsampled.shape == (1, 32, 64, 64)\n        output_slice = upsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([0.7145, 1.3773, 0.3492, 0.8448, 1.0839, -0.3341, 0.5956, 0.1250, -0.4841])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 132, "column": 4 },
          "end": { "row": 132, "column": 4 }
        }
      }
    }
  ],
  [
    "860",
    {
      "pageContent": "def test_upsample_with_conv_out_dim(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 32, 32)\n        upsample = Upsample2D(channels=32, use_conv=True, out_channels=64)\n        with torch.no_grad():\n            upsampled = upsample(sample)\n\n        assert upsampled.shape == (1, 64, 64, 64)\n        output_slice = upsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([0.2703, 0.1656, -0.2538, -0.0553, -0.2984, 0.1044, 0.1155, 0.2579, 0.7755])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 144, "column": 4 },
          "end": { "row": 144, "column": 4 }
        }
      }
    }
  ],
  [
    "861",
    {
      "pageContent": "def test_upsample_with_transpose(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 32, 32)\n        upsample = Upsample2D(channels=32, use_conv=False, use_conv_transpose=True)\n        with torch.no_grad():\n            upsampled = upsample(sample)\n\n        assert upsampled.shape == (1, 32, 64, 64)\n        output_slice = upsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([-0.3028, -0.1582, 0.0071, 0.0350, -0.4799, -0.1139, 0.1056, -0.1153, -0.1046])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 156, "column": 4 },
          "end": { "row": 156, "column": 4 }
        }
      }
    }
  ],
  [
    "862",
    {
      "pageContent": "class Downsample2DBlockTests(unittest.TestCase):\n    def test_downsample_default(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64)\n        downsample = Downsample2D(channels=32, use_conv=False)\n        with torch.no_grad():\n            downsampled = downsample(sample)\n\n        assert downsampled.shape == (1, 32, 32, 32)\n        output_slice = downsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([-0.0513, -0.3889, 0.0640, 0.0836, -0.5460, -0.0341, -0.0169, -0.6967, 0.1179])\n        max_diff = (output_slice.flatten() - expected_slice).abs().sum().item()\n        assert max_diff <= 1e-3\n        # assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-1)\n\n    def test_downsample_with_conv(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64)\n        downsample = Downsample2D(channels=32, use_conv=True)\n        with torch.no_grad():\n            downsampled = downsample(sample)\n\n        assert downsampled.shape == (1, 32, 32, 32)\n        output_slice = downsampled[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [0.9267, 0.5878, 0.3337, 1.2321, -0.1191, -0.3984, -0.7532, -0.0715, -0.3913],\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)\n\n    def test_downsample_with_conv_pad1(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64)\n        downsample = Downsample2D(channels=32, use_conv=True, padding=1)\n        with torch.no_grad():\n            downsampled = downsample(sample)\n\n        assert downsampled.shape ==",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 169, "column": 0 },
          "end": { "row": 169, "column": 0 }
        }
      }
    }
  ],
  [
    "863",
    {
      "pageContent": "def test_downsample_default(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64)\n        downsample = Downsample2D(channels=32, use_conv=False)\n        with torch.no_grad():\n            downsampled = downsample(sample)\n\n        assert downsampled.shape == (1, 32, 32, 32)\n        output_slice = downsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([-0.0513, -0.3889, 0.0640, 0.0836, -0.5460, -0.0341, -0.0169, -0.6967, 0.1179])\n        max_diff = (output_slice.flatten() - expected_slice).abs().sum().item()\n        assert max_diff <= 1e-3\n        # assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-1)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 170, "column": 4 },
          "end": { "row": 170, "column": 4 }
        }
      }
    }
  ],
  [
    "864",
    {
      "pageContent": "def test_downsample_with_conv(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64)\n        downsample = Downsample2D(channels=32, use_conv=True)\n        with torch.no_grad():\n            downsampled = downsample(sample)\n\n        assert downsampled.shape == (1, 32, 32, 32)\n        output_slice = downsampled[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [0.9267, 0.5878, 0.3337, 1.2321, -0.1191, -0.3984, -0.7532, -0.0715, -0.3913],\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 184, "column": 4 },
          "end": { "row": 184, "column": 4 }
        }
      }
    }
  ],
  [
    "865",
    {
      "pageContent": "def test_downsample_with_conv_pad1(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64)\n        downsample = Downsample2D(channels=32, use_conv=True, padding=1)\n        with torch.no_grad():\n            downsampled = downsample(sample)\n\n        assert downsampled.shape == (1, 32, 32, 32)\n        output_slice = downsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([0.9267, 0.5878, 0.3337, 1.2321, -0.1191, -0.3984, -0.7532, -0.0715, -0.3913])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 199, "column": 4 },
          "end": { "row": 199, "column": 4 }
        }
      }
    }
  ],
  [
    "866",
    {
      "pageContent": "def test_downsample_with_conv_out_dim(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64)\n        downsample = Downsample2D(channels=32, use_conv=True, out_channels=16)\n        with torch.no_grad():\n            downsampled = downsample(sample)\n\n        assert downsampled.shape == (1, 16, 32, 32)\n        output_slice = downsampled[0, -1, -3:, -3:]\n        expected_slice = torch.tensor([-0.6586, 0.5985, 0.0721, 0.1256, -0.1492, 0.4436, -0.2544, 0.5021, 1.1522])\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 211, "column": 4 },
          "end": { "row": 211, "column": 4 }
        }
      }
    }
  ],
  [
    "867",
    {
      "pageContent": "class ResnetBlock2DTests(unittest.TestCase):\n    def test_resnet_default(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 64, 64)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [-1.9010, -0.2974, -0.8245, -1.3533, 0.8742, -0.9645, -2.0584, 1.3387, -0.4746], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)\n\n    def test_restnet_with_use_in_shortcut(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128, use_in_shortcut=True).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 64, 64)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [0.2226, -1.0791, -0.1629, 0.3659, -0.2889, -1.2376, 0.0582, 0.9206, 0.0044], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)\n\n    def test_resnet_up(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_de",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 224, "column": 0 },
          "end": { "row": 224, "column": 0 }
        }
      }
    }
  ],
  [
    "868",
    {
      "pageContent": "def test_resnet_default(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 64, 64)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [-1.9010, -0.2974, -0.8245, -1.3533, 0.8742, -0.9645, -2.0584, 1.3387, -0.4746], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 225, "column": 4 },
          "end": { "row": 225, "column": 4 }
        }
      }
    }
  ],
  [
    "869",
    {
      "pageContent": "def test_restnet_with_use_in_shortcut(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128, use_in_shortcut=True).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 64, 64)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [0.2226, -1.0791, -0.1629, 0.3659, -0.2889, -1.2376, 0.0582, 0.9206, 0.0044], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 240, "column": 4 },
          "end": { "row": 240, "column": 4 }
        }
      }
    }
  ],
  [
    "870",
    {
      "pageContent": "def test_resnet_up(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128, up=True).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 128, 128)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [1.2130, -0.8753, -0.9027, 1.5783, -0.5362, -0.5001, 1.0726, -0.7732, -0.4182], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 255, "column": 4 },
          "end": { "row": 255, "column": 4 }
        }
      }
    }
  ],
  [
    "871",
    {
      "pageContent": "def test_resnet_down(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128, down=True).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 32, 32)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [-0.3002, -0.7135, 0.1359, 0.0561, -0.7935, 0.0113, -0.1766, -0.6714, -0.0436], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 270, "column": 4 },
          "end": { "row": 270, "column": 4 }
        }
      }
    }
  ],
  [
    "872",
    {
      "pageContent": "def test_restnet_with_kernel_fir(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128, kernel=\"fir\", down=True).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 32, 32)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [-0.0934, -0.5729, 0.0909, -0.2710, -0.5044, 0.0243, -0.0665, -0.5267, -0.3136], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "873",
    {
      "pageContent": "def test_restnet_with_kernel_sde_vp(self):\n        torch.manual_seed(0)\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        temb = torch.randn(1, 128).to(torch_device)\n        resnet_block = ResnetBlock2D(in_channels=32, temb_channels=128, kernel=\"sde_vp\", down=True).to(torch_device)\n        with torch.no_grad():\n            output_tensor = resnet_block(sample, temb)\n\n        assert output_tensor.shape == (1, 32, 32, 32)\n        output_slice = output_tensor[0, -1, -3:, -3:]\n        expected_slice = torch.tensor(\n            [-0.3002, -0.7135, 0.1359, 0.0561, -0.7935, 0.0113, -0.1766, -0.6714, -0.0436], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 300, "column": 4 },
          "end": { "row": 300, "column": 4 }
        }
      }
    }
  ],
  [
    "874",
    {
      "pageContent": "class AttentionBlockTests(unittest.TestCase):\n    @unittest.skipIf(\n        torch_device == \"mps\", \"Matmul crashes on MPS, see https://github.com/pytorch/pytorch/issues/84039\"\n    )\n    def test_attention_block_default(self):\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        attentionBlock = AttentionBlock(\n            channels=32,\n            num_head_channels=1,\n            rescale_output_factor=1.0,\n            eps=1e-6,\n            norm_num_groups=32,\n        ).to(torch_device)\n        with torch.no_grad():\n            attention_scores = attentionBlock(sample)\n\n        assert attention_scores.shape == (1, 32, 64, 64)\n        output_slice = attention_scores[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [-1.4975, -0.0038, -0.7847, -1.4567, 1.1220, -0.8962, -1.7394, 1.1319, -0.5427], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)\n\n    def test_attention_block_sd(self):\n        # This version uses SD params and is compatible with mps\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 512, 64, 64).to(torch_device)\n        attentionBlock = AttentionBlock(\n            channels=512,\n            rescale_output_factor=1.0,\n            eps=1e-6,\n            norm_num_groups=32,\n        ).to(torch_device)\n        with torch.no_grad():\n            attention_scores = attentionBlock(s",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 316, "column": 0 },
          "end": { "row": 316, "column": 0 }
        }
      }
    }
  ],
  [
    "875",
    {
      "pageContent": "def test_attention_block_sd(self):\n        # This version uses SD params and is compatible with mps\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 512, 64, 64).to(torch_device)\n        attentionBlock = AttentionBlock(\n            channels=512,\n            rescale_output_factor=1.0,\n            eps=1e-6,\n            norm_num_groups=32,\n        ).to(torch_device)\n        with torch.no_grad():\n            attention_scores = attentionBlock(sample)\n\n        assert attention_scores.shape == (1, 512, 64, 64)\n        output_slice = attention_scores[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [-0.6621, -0.0156, -3.2766, 0.8025, -0.8609, 0.2820, 0.0905, -1.1179, -3.2126], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 344, "column": 4 },
          "end": { "row": 344, "column": 4 }
        }
      }
    }
  ],
  [
    "876",
    {
      "pageContent": "class Transformer2DModelTests(unittest.TestCase):\n    def test_spatial_transformer_default(self):\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        spatial_transformer_block = Transformer2DModel(\n            in_channels=32,\n            num_attention_heads=1,\n            attention_head_dim=32,\n            dropout=0.0,\n            cross_attention_dim=None,\n        ).to(torch_device)\n        with torch.no_grad():\n            attention_scores = spatial_transformer_block(sample).sample\n\n        assert attention_scores.shape == (1, 32, 64, 64)\n        output_slice = attention_scores[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [-1.9455, -0.0066, -1.3933, -1.5878, 0.5325, -0.6486, -1.8648, 0.7515, -0.9689], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)\n\n    def test_spatial_transformer_cross_attention_dim(self):\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 64, 64, 64).to(torch_device)\n        spatial_transformer_block = Transformer2DModel(\n            in_channels=64,\n            num_attention_heads=2,\n            attention_head_dim=32,\n            dropout=0.0,\n            cross_attention_dim=64,\n        ).to(torch_device)\n        with torch.no_grad():\n            context = torch.randn(1, 4, 64).to(torch_device)\n            attention_scores = spatial_transformer_block(samp",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 369, "column": 0 },
          "end": { "row": 369, "column": 0 }
        }
      }
    }
  ],
  [
    "877",
    {
      "pageContent": "def test_spatial_transformer_default(self):\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        spatial_transformer_block = Transformer2DModel(\n            in_channels=32,\n            num_attention_heads=1,\n            attention_head_dim=32,\n            dropout=0.0,\n            cross_attention_dim=None,\n        ).to(torch_device)\n        with torch.no_grad():\n            attention_scores = spatial_transformer_block(sample).sample\n\n        assert attention_scores.shape == (1, 32, 64, 64)\n        output_slice = attention_scores[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [-1.9455, -0.0066, -1.3933, -1.5878, 0.5325, -0.6486, -1.8648, 0.7515, -0.9689], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 370, "column": 4 },
          "end": { "row": 370, "column": 4 }
        }
      }
    }
  ],
  [
    "878",
    {
      "pageContent": "def test_spatial_transformer_cross_attention_dim(self):\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 64, 64, 64).to(torch_device)\n        spatial_transformer_block = Transformer2DModel(\n            in_channels=64,\n            num_attention_heads=2,\n            attention_head_dim=32,\n            dropout=0.0,\n            cross_attention_dim=64,\n        ).to(torch_device)\n        with torch.no_grad():\n            context = torch.randn(1, 4, 64).to(torch_device)\n            attention_scores = spatial_transformer_block(sample, context).sample\n\n        assert attention_scores.shape == (1, 64, 64, 64)\n        output_slice = attention_scores[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [-0.2555, -0.8877, -2.4739, -2.2251, 1.2714, 0.0807, -0.4161, -1.6408, -0.0471], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 394, "column": 4 },
          "end": { "row": 394, "column": 4 }
        }
      }
    }
  ],
  [
    "879",
    {
      "pageContent": "def test_spatial_transformer_timestep(self):\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        num_embeds_ada_norm = 5\n\n        sample = torch.randn(1, 64, 64, 64).to(torch_device)\n        spatial_transformer_block = Transformer2DModel(\n            in_channels=64,\n            num_attention_heads=2,\n            attention_head_dim=32,\n            dropout=0.0,\n            cross_attention_dim=64,\n            num_embeds_ada_norm=num_embeds_ada_norm,\n        ).to(torch_device)\n        with torch.no_grad():\n            timestep_1 = torch.tensor(1, dtype=torch.long).to(torch_device)\n            timestep_2 = torch.tensor(2, dtype=torch.long).to(torch_device)\n            attention_scores_1 = spatial_transformer_block(sample, timestep=timestep_1).sample\n            attention_scores_2 = spatial_transformer_block(sample, timestep=timestep_2).sample\n\n        assert attention_scores_1.shape == (1, 64, 64, 64)\n        assert attention_scores_2.shape == (1, 64, 64, 64)\n\n        output_slice_1 = attention_scores_1[0, -1, -3:, -3:]\n        output_slice_2 = attention_scores_2[0, -1, -3:, -3:]\n\n        expected_slice_1 = torch.tensor(\n            [-0.1874, -0.9704, -1.4290, -1.3357, 1.5138, 0.3036, -0.0976, -1.1667, 0.1283], device=torch_device\n        )\n        expected_slice_2 = torch.tensor(\n            [-0.3493, -1.0924, -1.6161, -1.5016, 1.4245, 0.1367, -0.2526, -1.3109, -0.0547], device=torch_device\n        )\n\n        assert torch.allclose(output_slice_1.flatten(), expected_slice_1, atol=1e-3)\n        assert torch.allclos",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 419, "column": 4 },
          "end": { "row": 419, "column": 4 }
        }
      }
    }
  ],
  [
    "880",
    {
      "pageContent": "def test_spatial_transformer_dropout(self):\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        sample = torch.randn(1, 32, 64, 64).to(torch_device)\n        spatial_transformer_block = (\n            Transformer2DModel(\n                in_channels=32,\n                num_attention_heads=2,\n                attention_head_dim=16,\n                dropout=0.3,\n                cross_attention_dim=None,\n            )\n            .to(torch_device)\n            .eval()\n        )\n        with torch.no_grad():\n            attention_scores = spatial_transformer_block(sample).sample\n\n        assert attention_scores.shape == (1, 32, 64, 64)\n        output_slice = attention_scores[0, -1, -3:, -3:]\n\n        expected_slice = torch.tensor(\n            [-1.9380, -0.0083, -1.3771, -1.5819, 0.5209, -0.6441, -1.8545, 0.7563, -0.9615], device=torch_device\n        )\n        assert torch.allclose(output_slice.flatten(), expected_slice, atol=1e-3)",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 457, "column": 4 },
          "end": { "row": 457, "column": 4 }
        }
      }
    }
  ],
  [
    "881",
    {
      "pageContent": "def test_spatial_transformer_default_norm_layers(self):\n        spatial_transformer_block = Transformer2DModel(num_attention_heads=1, attention_head_dim=32, in_channels=32)\n\n        assert spatial_transformer_block.transformer_blocks[0].norm1.__class__ == nn.LayerNorm\n        assert spatial_transformer_block.transformer_blocks[0].norm3.__class__ == nn.LayerNorm",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 515, "column": 4 },
          "end": { "row": 515, "column": 4 }
        }
      }
    }
  ],
  [
    "882",
    {
      "pageContent": "def test_spatial_transformer_ada_norm_layers(self):\n        spatial_transformer_block = Transformer2DModel(\n            num_attention_heads=1,\n            attention_head_dim=32,\n            in_channels=32,\n            num_embeds_ada_norm=5,\n        )\n\n        assert spatial_transformer_block.transformer_blocks[0].norm1.__class__ == AdaLayerNorm\n        assert spatial_transformer_block.transformer_blocks[0].norm3.__class__ == nn.LayerNorm",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 521, "column": 4 },
          "end": { "row": 521, "column": 4 }
        }
      }
    }
  ],
  [
    "883",
    {
      "pageContent": "def test_spatial_transformer_default_ff_layers(self):\n        spatial_transformer_block = Transformer2DModel(\n            num_attention_heads=1,\n            attention_head_dim=32,\n            in_channels=32,\n        )\n\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[0].__class__ == GEGLU\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[1].__class__ == nn.Dropout\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[2].__class__ == nn.Linear\n\n        dim = 32\n        inner_dim = 128\n\n        # First dimension change\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[0].proj.in_features == dim\n        # NOTE: inner_dim * 2 because GEGLU\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[0].proj.out_features == inner_dim * 2\n\n        # Second dimension change\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[2].in_features == inner_dim\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[2].out_features == dim",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 532, "column": 4 },
          "end": { "row": 532, "column": 4 }
        }
      }
    }
  ],
  [
    "884",
    {
      "pageContent": "def test_spatial_transformer_geglu_approx_ff_layers(self):\n        spatial_transformer_block = Transformer2DModel(\n            num_attention_heads=1,\n            attention_head_dim=32,\n            in_channels=32,\n            activation_fn=\"geglu-approximate\",\n        )\n\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[0].__class__ == ApproximateGELU\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[1].__class__ == nn.Dropout\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[2].__class__ == nn.Linear\n\n        dim = 32\n        inner_dim = 128\n\n        # First dimension change\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[0].proj.in_features == dim\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[0].proj.out_features == inner_dim\n\n        # Second dimension change\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[2].in_features == inner_dim\n        assert spatial_transformer_block.transformer_blocks[0].ff.net[2].out_features == dim",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 555, "column": 4 },
          "end": { "row": 555, "column": 4 }
        }
      }
    }
  ],
  [
    "885",
    {
      "pageContent": "def test_spatial_transformer_attention_bias(self):\n        spatial_transformer_block = Transformer2DModel(\n            num_attention_heads=1, attention_head_dim=32, in_channels=32, attention_bias=True\n        )\n\n        assert spatial_transformer_block.transformer_blocks[0].attn1.to_q.bias is not None\n        assert spatial_transformer_block.transformer_blocks[0].attn1.to_k.bias is not None\n        assert spatial_transformer_block.transformer_blocks[0].attn1.to_v.bias is not None",
      "metadata": {
        "source": "tests/test_layers_utils.py",
        "range": {
          "start": { "row": 578, "column": 4 },
          "end": { "row": 578, "column": 4 }
        }
      }
    }
  ],
  [
    "886",
    {
      "pageContent": "class DownloadTests(unittest.TestCase):\n    def test_one_request_upon_cached(self):\n        # TODO: For some reason this test fails on MPS where no HEAD call is made.\n        if torch_device == \"mps\":\n            return\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            with requests_mock.mock(real_http=True) as m:\n                DiffusionPipeline.download(\n                    \"hf-internal-testing/tiny-stable-diffusion-pipe\", safety_checker=None, cache_dir=tmpdirname\n                )\n\n            download_requests = [r.method for r in m.request_history]\n            assert download_requests.count(\"HEAD\") == 15, \"15 calls to files\"\n            assert download_requests.count(\"GET\") == 17, \"15 calls to files + model_info + model_index.json\"\n            assert (\n                len(download_requests) == 32\n            ), \"2 calls per file (15 files) + send_telemetry, model_info and model_index.json\"\n\n            with requests_mock.mock(real_http=True) as m:\n                DiffusionPipeline.download(\n                    \"hf-internal-testing/tiny-stable-diffusion-pipe\", safety_checker=None, cache_dir=tmpdirname\n                )\n\n            cache_requests = [r.method for r in m.request_history]\n            assert cache_requests.count(\"HEAD\") == 1, \"model_index.json is only HEAD\"\n            assert cache_requests.count(\"GET\") == 1, \"model info is only GET\"\n            assert (\n                len(cache_requests) == 2\n            ), \"We should call only `model_info` to check for _commit hash and `send_telemetry`\"\n\n    def test_download_only_pytorch(self):\n   ",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 63, "column": 0 },
          "end": { "row": 63, "column": 0 }
        }
      }
    }
  ],
  [
    "887",
    {
      "pageContent": "def test_one_request_upon_cached(self):\n        # TODO: For some reason this test fails on MPS where no HEAD call is made.\n        if torch_device == \"mps\":\n            return\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            with requests_mock.mock(real_http=True) as m:\n                DiffusionPipeline.download(\n                    \"hf-internal-testing/tiny-stable-diffusion-pipe\", safety_checker=None, cache_dir=tmpdirname\n                )\n\n            download_requests = [r.method for r in m.request_history]\n            assert download_requests.count(\"HEAD\") == 15, \"15 calls to files\"\n            assert download_requests.count(\"GET\") == 17, \"15 calls to files + model_info + model_index.json\"\n            assert (\n                len(download_requests) == 32\n            ), \"2 calls per file (15 files) + send_telemetry, model_info and model_index.json\"\n\n            with requests_mock.mock(real_http=True) as m:\n                DiffusionPipeline.download(\n                    \"hf-internal-testing/tiny-stable-diffusion-pipe\", safety_checker=None, cache_dir=tmpdirname\n                )\n\n            cache_requests = [r.method for r in m.request_history]\n            assert cache_requests.count(\"HEAD\") == 1, \"model_index.json is only HEAD\"\n            assert cache_requests.count(\"GET\") == 1, \"model info is only GET\"\n            assert (\n                len(cache_requests) == 2\n            ), \"We should call only `model_info` to check for _commit hash and `send_telemetry`\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 64, "column": 4 },
          "end": { "row": 64, "column": 4 }
        }
      }
    }
  ],
  [
    "888",
    {
      "pageContent": "def test_download_only_pytorch(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            # pipeline has Flax weights\n            tmpdirname = DiffusionPipeline.download(\n                \"hf-internal-testing/tiny-stable-diffusion-pipe\", safety_checker=None, cache_dir=tmpdirname\n            )\n\n            all_root_files = [t[-1] for t in os.walk(os.path.join(tmpdirname))]\n            files = [item for sublist in all_root_files for item in sublist]\n\n            # None of the downloaded files should be a flax file even if we have some here:\n            # https://huggingface.co/hf-internal-testing/tiny-stable-diffusion-pipe/blob/main/unet/diffusion_flax_model.msgpack\n            assert not any(f.endswith(\".msgpack\") for f in files)\n            # We need to never convert this tiny model to safetensors for this test to pass\n            assert not any(f.endswith(\".safetensors\") for f in files)",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 94, "column": 4 },
          "end": { "row": 94, "column": 4 }
        }
      }
    }
  ],
  [
    "889",
    {
      "pageContent": "def test_returned_cached_folder(self):\n        prompt = \"hello\"\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None\n        )\n        _, local_path = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None, return_cached_folder=True\n        )\n        pipe_2 = StableDiffusionPipeline.from_pretrained(local_path)\n\n        pipe = pipe.to(torch_device)\n        pipe_2 = pipe_2.to(torch_device)\n\n        generator = torch.manual_seed(0)\n        out = pipe(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        out_2 = pipe_2(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        assert np.max(np.abs(out - out_2)) < 1e-3",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 110, "column": 4 },
          "end": { "row": 110, "column": 4 }
        }
      }
    }
  ],
  [
    "890",
    {
      "pageContent": "def test_download_safetensors(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            # pipeline has Flax weights\n            tmpdirname = DiffusionPipeline.download(\n                \"hf-internal-testing/tiny-stable-diffusion-pipe-safetensors\",\n                safety_checker=None,\n                cache_dir=tmpdirname,\n            )\n\n            all_root_files = [t[-1] for t in os.walk(os.path.join(tmpdirname))]\n            files = [item for sublist in all_root_files for item in sublist]\n\n            # None of the downloaded files should be a pytorch file even if we have some here:\n            # https://huggingface.co/hf-internal-testing/tiny-stable-diffusion-pipe/blob/main/unet/diffusion_flax_model.msgpack\n            assert not any(f.endswith(\".bin\") for f in files)",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 131, "column": 4 },
          "end": { "row": 131, "column": 4 }
        }
      }
    }
  ],
  [
    "891",
    {
      "pageContent": "def test_download_no_safety_checker(self):\n        prompt = \"hello\"\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None\n        )\n        pipe = pipe.to(torch_device)\n        generator = torch.manual_seed(0)\n        out = pipe(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        pipe_2 = StableDiffusionPipeline.from_pretrained(\"hf-internal-testing/tiny-stable-diffusion-torch\")\n        pipe_2 = pipe_2.to(torch_device)\n        generator = torch.manual_seed(0)\n        out_2 = pipe_2(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        assert np.max(np.abs(out - out_2)) < 1e-3",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "892",
    {
      "pageContent": "def test_load_no_safety_checker_explicit_locally(self):\n        prompt = \"hello\"\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None\n        )\n        pipe = pipe.to(torch_device)\n        generator = torch.manual_seed(0)\n        out = pipe(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe_2 = StableDiffusionPipeline.from_pretrained(tmpdirname, safety_checker=None)\n            pipe_2 = pipe_2.to(torch_device)\n\n            generator = torch.manual_seed(0)\n\n            out_2 = pipe_2(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        assert np.max(np.abs(out - out_2)) < 1e-3",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 163, "column": 4 },
          "end": { "row": 163, "column": 4 }
        }
      }
    }
  ],
  [
    "893",
    {
      "pageContent": "def test_load_no_safety_checker_default_locally(self):\n        prompt = \"hello\"\n        pipe = StableDiffusionPipeline.from_pretrained(\"hf-internal-testing/tiny-stable-diffusion-torch\")\n        pipe = pipe.to(torch_device)\n\n        generator = torch.manual_seed(0)\n        out = pipe(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe_2 = StableDiffusionPipeline.from_pretrained(tmpdirname)\n            pipe_2 = pipe_2.to(torch_device)\n\n            generator = torch.manual_seed(0)\n\n            out_2 = pipe_2(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        assert np.max(np.abs(out - out_2)) < 1e-3",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "894",
    {
      "pageContent": "def test_cached_files_are_used_when_no_internet(self):\n        # A mock response for an HTTP head request to emulate server down\n        response_mock = mock.Mock()\n        response_mock.status_code = 500\n        response_mock.headers = {}\n        response_mock.raise_for_status.side_effect = HTTPError\n        response_mock.json.return_value = {}\n\n        # Download this model to make sure it's in the cache.\n        orig_pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None\n        )\n        orig_comps = {k: v for k, v in orig_pipe.components.items() if hasattr(v, \"parameters\")}\n\n        # Under the mock environment we get a 500 error when trying to reach the model.\n        with mock.patch(\"requests.request\", return_value=response_mock):\n            # Download this model to make sure it's in the cache.\n            pipe = StableDiffusionPipeline.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None, local_files_only=True\n            )\n            comps = {k: v for k, v in pipe.components.items() if hasattr(v, \"parameters\")}\n\n        for m1, m2 in zip(orig_comps.values(), comps.values()):\n            for p1, p2 in zip(m1.parameters(), m2.parameters()):\n                if p1.data.ne(p2.data).sum() > 0:\n                    assert False, \"Parameters not the same!\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 202, "column": 4 },
          "end": { "row": 202, "column": 4 }
        }
      }
    }
  ],
  [
    "895",
    {
      "pageContent": "def test_download_from_variant_folder(self):\n        for safe_avail in [False, True]:\n            import diffusers\n\n            diffusers.utils.import_utils._safetensors_available = safe_avail\n\n            other_format = \".bin\" if safe_avail else \".safetensors\"\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                tmpdirname = StableDiffusionPipeline.download(\n                    \"hf-internal-testing/stable-diffusion-all-variants\", cache_dir=tmpdirname\n                )\n                all_root_files = [t[-1] for t in os.walk(tmpdirname)]\n                files = [item for sublist in all_root_files for item in sublist]\n\n                # None of the downloaded files should be a variant file even if we have some here:\n                # https://huggingface.co/hf-internal-testing/stable-diffusion-all-variants/tree/main/unet\n                assert len(files) == 15, f\"We should only download 15 files, not {len(files)}\"\n                assert not any(f.endswith(other_format) for f in files)\n                # no variants\n                assert not any(len(f.split(\".\")) == 3 for f in files)\n\n        diffusers.utils.import_utils._safetensors_available = True",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 229, "column": 4 },
          "end": { "row": 229, "column": 4 }
        }
      }
    }
  ],
  [
    "896",
    {
      "pageContent": "def test_download_variant_all(self):\n        for safe_avail in [False, True]:\n            import diffusers\n\n            diffusers.utils.import_utils._safetensors_available = safe_avail\n\n            other_format = \".bin\" if safe_avail else \".safetensors\"\n            this_format = \".safetensors\" if safe_avail else \".bin\"\n            variant = \"fp16\"\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                tmpdirname = StableDiffusionPipeline.download(\n                    \"hf-internal-testing/stable-diffusion-all-variants\", cache_dir=tmpdirname, variant=variant\n                )\n                all_root_files = [t[-1] for t in os.walk(tmpdirname)]\n                files = [item for sublist in all_root_files for item in sublist]\n\n                # None of the downloaded files should be a non-variant file even if we have some here:\n                # https://huggingface.co/hf-internal-testing/stable-diffusion-all-variants/tree/main/unet\n                assert len(files) == 15, f\"We should only download 15 files, not {len(files)}\"\n                # unet, vae, text_encoder, safety_checker\n                assert len([f for f in files if f.endswith(f\"{variant}{this_format}\")]) == 4\n                # all checkpoints should have variant ending\n                assert not any(f.endswith(this_format) and not f.endswith(f\"{variant}{this_format}\") for f in files)\n                assert not any(f.endswith(other_format) for f in files)\n\n        diffusers.utils.import_utils._safetensors_available = True",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 252, "column": 4 },
          "end": { "row": 252, "column": 4 }
        }
      }
    }
  ],
  [
    "897",
    {
      "pageContent": "def test_download_variant_partly(self):\n        for safe_avail in [False, True]:\n            import diffusers\n\n            diffusers.utils.import_utils._safetensors_available = safe_avail\n\n            other_format = \".bin\" if safe_avail else \".safetensors\"\n            this_format = \".safetensors\" if safe_avail else \".bin\"\n            variant = \"no_ema\"\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                tmpdirname = StableDiffusionPipeline.download(\n                    \"hf-internal-testing/stable-diffusion-all-variants\", cache_dir=tmpdirname, variant=variant\n                )\n                all_root_files = [t[-1] for t in os.walk(tmpdirname)]\n                files = [item for sublist in all_root_files for item in sublist]\n\n                unet_files = os.listdir(os.path.join(tmpdirname, \"unet\"))\n\n                # Some of the downloaded files should be a non-variant file, check:\n                # https://huggingface.co/hf-internal-testing/stable-diffusion-all-variants/tree/main/unet\n                assert len(files) == 15, f\"We should only download 15 files, not {len(files)}\"\n                # only unet has \"no_ema\" variant\n                assert f\"diffusion_pytorch_model.{variant}{this_format}\" in unet_files\n                assert len([f for f in files if f.endswith(f\"{variant}{this_format}\")]) == 1\n                # vae, safety_checker and text_encoder should have no variant\n                assert sum(f.endswith(this_format) and not f.endswith(f\"{variant}{this_format}\") for f in files) == 3\n                assert not any(f.endswith(other_forma",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 280, "column": 4 },
          "end": { "row": 280, "column": 4 }
        }
      }
    }
  ],
  [
    "898",
    {
      "pageContent": "def test_download_broken_variant(self):\n        for safe_avail in [False, True]:\n            import diffusers\n\n            diffusers.utils.import_utils._safetensors_available = safe_avail\n            # text encoder is missing no variant and \"no_ema\" variant weights, so the following can't work\n            for variant in [None, \"no_ema\"]:\n                with self.assertRaises(OSError) as error_context:\n                    with tempfile.TemporaryDirectory() as tmpdirname:\n                        tmpdirname = StableDiffusionPipeline.from_pretrained(\n                            \"hf-internal-testing/stable-diffusion-broken-variants\",\n                            cache_dir=tmpdirname,\n                            variant=variant,\n                        )\n\n                assert \"Error no file name\" in str(error_context.exception)\n\n            # text encoder has fp16 variants so we can load it\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                tmpdirname = StableDiffusionPipeline.download(\n                    \"hf-internal-testing/stable-diffusion-broken-variants\", cache_dir=tmpdirname, variant=\"fp16\"\n                )\n\n                all_root_files = [t[-1] for t in os.walk(tmpdirname)]\n                files = [item for sublist in all_root_files for item in sublist]\n\n                # None of the downloaded files should be a non-variant file even if we have some here:\n                # https://huggingface.co/hf-internal-testing/stable-diffusion-broken-variants/tree/main/unet\n                assert len(files) == 15, f\"We should only download 15 files, no",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 311, "column": 4 },
          "end": { "row": 311, "column": 4 }
        }
      }
    }
  ],
  [
    "899",
    {
      "pageContent": "class CustomPipelineTests(unittest.TestCase):\n    def test_load_custom_pipeline(self):\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\"\n        )\n        pipeline = pipeline.to(torch_device)\n        # NOTE that `\"CustomPipeline\"` is not a class that is defined in this library, but solely on the Hub\n        # under https://huggingface.co/hf-internal-testing/diffusers-dummy-pipeline/blob/main/pipeline.py#L24\n        assert pipeline.__class__.__name__ == \"CustomPipeline\"\n\n    def test_load_custom_github(self):\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=\"one_step_unet\", custom_revision=\"main\"\n        )\n\n        # make sure that on \"main\" pipeline gives only ones because of: https://github.com/huggingface/diffusers/pull/1690\n        with torch.no_grad():\n            output = pipeline()\n\n        assert output.numel() == output.sum()\n\n        # hack since Python doesn't like overwriting modules: https://stackoverflow.com/questions/3105801/unload-a-module-in-python\n        # Could in the future work with hashes instead.\n        del sys.modules[\"diffusers_modules.git.one_step_unet\"]\n\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=\"one_step_unet\", custom_revision=\"0.10.2\"\n        )\n        with torch.no_grad():\n            output = pipeline()\n\n        assert output.numel() != output.sum()\n\n        assert pipeline.__class__.__name__ == \"UnetSchedulerOneForw",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 345, "column": 0 },
          "end": { "row": 345, "column": 0 }
        }
      }
    }
  ],
  [
    "900",
    {
      "pageContent": "def test_load_custom_pipeline(self):\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\"\n        )\n        pipeline = pipeline.to(torch_device)\n        # NOTE that `\"CustomPipeline\"` is not a class that is defined in this library, but solely on the Hub\n        # under https://huggingface.co/hf-internal-testing/diffusers-dummy-pipeline/blob/main/pipeline.py#L24\n        assert pipeline.__class__.__name__ == \"CustomPipeline\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 346, "column": 4 },
          "end": { "row": 346, "column": 4 }
        }
      }
    }
  ],
  [
    "901",
    {
      "pageContent": "def test_load_custom_github(self):\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=\"one_step_unet\", custom_revision=\"main\"\n        )\n\n        # make sure that on \"main\" pipeline gives only ones because of: https://github.com/huggingface/diffusers/pull/1690\n        with torch.no_grad():\n            output = pipeline()\n\n        assert output.numel() == output.sum()\n\n        # hack since Python doesn't like overwriting modules: https://stackoverflow.com/questions/3105801/unload-a-module-in-python\n        # Could in the future work with hashes instead.\n        del sys.modules[\"diffusers_modules.git.one_step_unet\"]\n\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=\"one_step_unet\", custom_revision=\"0.10.2\"\n        )\n        with torch.no_grad():\n            output = pipeline()\n\n        assert output.numel() != output.sum()\n\n        assert pipeline.__class__.__name__ == \"UnetSchedulerOneForwardPipeline\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 355, "column": 4 },
          "end": { "row": 355, "column": 4 }
        }
      }
    }
  ],
  [
    "902",
    {
      "pageContent": "def test_run_custom_pipeline(self):\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\"\n        )\n        pipeline = pipeline.to(torch_device)\n        images, output_str = pipeline(num_inference_steps=2, output_type=\"np\")\n\n        assert images[0].shape == (1, 32, 32, 3)\n\n        # compare output to https://huggingface.co/hf-internal-testing/diffusers-dummy-pipeline/blob/main/pipeline.py#L102\n        assert output_str == \"This is a test\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 380, "column": 4 },
          "end": { "row": 380, "column": 4 }
        }
      }
    }
  ],
  [
    "903",
    {
      "pageContent": "def test_local_custom_pipeline_repo(self):\n        local_custom_pipeline_path = get_tests_dir(\"fixtures/custom_pipeline\")\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=local_custom_pipeline_path\n        )\n        pipeline = pipeline.to(torch_device)\n        images, output_str = pipeline(num_inference_steps=2, output_type=\"np\")\n\n        assert pipeline.__class__.__name__ == \"CustomLocalPipeline\"\n        assert images[0].shape == (1, 32, 32, 3)\n        # compare to https://github.com/huggingface/diffusers/blob/main/tests/fixtures/custom_pipeline/pipeline.py#L102\n        assert output_str == \"This is a local test\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 392, "column": 4 },
          "end": { "row": 392, "column": 4 }
        }
      }
    }
  ],
  [
    "904",
    {
      "pageContent": "def test_local_custom_pipeline_file(self):\n        local_custom_pipeline_path = get_tests_dir(\"fixtures/custom_pipeline\")\n        local_custom_pipeline_path = os.path.join(local_custom_pipeline_path, \"what_ever.py\")\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"google/ddpm-cifar10-32\", custom_pipeline=local_custom_pipeline_path\n        )\n        pipeline = pipeline.to(torch_device)\n        images, output_str = pipeline(num_inference_steps=2, output_type=\"np\")\n\n        assert pipeline.__class__.__name__ == \"CustomLocalPipeline\"\n        assert images[0].shape == (1, 32, 32, 3)\n        # compare to https://github.com/huggingface/diffusers/blob/main/tests/fixtures/custom_pipeline/pipeline.py#L102\n        assert output_str == \"This is a local test\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 405, "column": 4 },
          "end": { "row": 405, "column": 4 }
        }
      }
    }
  ],
  [
    "905",
    {
      "pageContent": "class PipelineFastTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        import diffusers\n\n        diffusers.utils.import_utils._safetensors_available = True\n\n    def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image\n\n    def dummy_uncond_unet(self, sample_size=32):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=sample_size,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model\n\n    def dummy_cond_unet(self, sample_size=32):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=sample_size,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        return model\n\n    @property\n    def dummy_vae(self):\n        torch.manual_seed(0)\n        model = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n        ",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 445, "column": 0 },
          "end": { "row": 445, "column": 0 }
        }
      }
    }
  ],
  [
    "906",
    {
      "pageContent": "def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        import diffusers\n\n        diffusers.utils.import_utils._safetensors_available = True",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 446, "column": 4 },
          "end": { "row": 446, "column": 4 }
        }
      }
    }
  ],
  [
    "907",
    {
      "pageContent": "def dummy_image(self):\n        batch_size = 1\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes, rng=random.Random(0)).to(torch_device)\n        return image",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 456, "column": 4 },
          "end": { "row": 456, "column": 4 }
        }
      }
    }
  ],
  [
    "908",
    {
      "pageContent": "def dummy_uncond_unet(self, sample_size=32):\n        torch.manual_seed(0)\n        model = UNet2DModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=sample_size,\n            in_channels=3,\n            out_channels=3,\n            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n        )\n        return model",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 464, "column": 4 },
          "end": { "row": 464, "column": 4 }
        }
      }
    }
  ],
  [
    "909",
    {
      "pageContent": "def dummy_cond_unet(self, sample_size=32):\n        torch.manual_seed(0)\n        model = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=sample_size,\n            in_channels=4,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        return model",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 477, "column": 4 },
          "end": { "row": 477, "column": 4 }
        }
      }
    }
  ],
  [
    "910",
    {
      "pageContent": "def test_stable_diffusion_components(self):\n        \"\"\"Test that components property works correctly\"\"\"\n        unet = self.dummy_cond_unet()\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        image = self.dummy_image().cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((32, 32))\n\n        # make sure here that pndm scheduler skips prk\n        inpaint = StableDiffusionInpaintPipelineLegacy(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        ).to(torch_device)\n        img2img = StableDiffusionImg2ImgPipeline(**inpaint.components).to(torch_device)\n        text2img = StableDiffusionPipeline(**inpaint.components).to(torch_device)\n\n        prompt = \"A painting of a squirrel eating a burger\"\n\n        generator = torch.manual_seed(0)\n        image_inpaint = inpaint(\n            [prompt],\n            generator=generator,\n            num_inference_steps=2,\n            output_type=\"np\",\n            image=init_image,\n            mask_image=mask_image,\n        ).images\n        image_img2img = img2img(\n            [prompt],\n            generator=generator,\n            num_inference_steps=2,\n            output_type=\"",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 557, "column": 4 },
          "end": { "row": 557, "column": 4 }
        }
      }
    }
  ],
  [
    "911",
    {
      "pageContent": "def test_set_scheduler(self):\n        unet = self.dummy_cond_unet()\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        sd = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n\n        sd.scheduler = DDIMScheduler.from_config(sd.scheduler.config)\n        assert isinstance(sd.scheduler, DDIMScheduler)\n        sd.scheduler = DDPMScheduler.from_config(sd.scheduler.config)\n        assert isinstance(sd.scheduler, DDPMScheduler)\n        sd.scheduler = PNDMScheduler.from_config(sd.scheduler.config)\n        assert isinstance(sd.scheduler, PNDMScheduler)\n        sd.scheduler = LMSDiscreteScheduler.from_config(sd.scheduler.config)\n        assert isinstance(sd.scheduler, LMSDiscreteScheduler)\n        sd.scheduler = EulerDiscreteScheduler.from_config(sd.scheduler.config)\n        assert isinstance(sd.scheduler, EulerDiscreteScheduler)\n        sd.scheduler = EulerAncestralDiscreteScheduler.from_config(sd.scheduler.config)\n        assert isinstance(sd.scheduler, EulerAncestralDiscreteScheduler)\n        sd.scheduler = DPMSolverMultistepScheduler.from_config(sd.scheduler.config)\n        assert isinstance(sd.scheduler, DPMSolverMultistepScheduler)",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 647, "column": 4 },
          "end": { "row": 647, "column": 4 }
        }
      }
    }
  ],
  [
    "912",
    {
      "pageContent": "def test_set_scheduler_consistency(self):\n        unet = self.dummy_cond_unet()\n        pndm = PNDMScheduler.from_config(\"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\")\n        ddim = DDIMScheduler.from_config(\"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\")\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        sd = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=pndm,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n\n        pndm_config = sd.scheduler.config\n        sd.scheduler = DDPMScheduler.from_config(pndm_config)\n        sd.scheduler = PNDMScheduler.from_config(sd.scheduler.config)\n        pndm_config_2 = sd.scheduler.config\n        pndm_config_2 = {k: v for k, v in pndm_config_2.items() if k in pndm_config}\n\n        assert dict(pndm_config) == dict(pndm_config_2)\n\n        sd = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=ddim,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=None,\n            feature_extractor=self.dummy_extractor,\n        )\n\n        ddim_config = sd.scheduler.config\n        sd.scheduler = LMSDiscreteScheduler.from_config(ddim_config)\n        sd.scheduler = DDIMScheduler.from_config(sd.scheduler.config)\n        ddim_config_2 = sd.scheduler.config\n     ",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 679, "column": 4 },
          "end": { "row": 679, "column": 4 }
        }
      }
    }
  ],
  [
    "913",
    {
      "pageContent": "def test_save_safe_serialization(self):\n        pipeline = StableDiffusionPipeline.from_pretrained(\"hf-internal-testing/tiny-stable-diffusion-torch\")\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipeline.save_pretrained(tmpdirname, safe_serialization=True)\n\n            # Validate that the VAE safetensor exists and are of the correct format\n            vae_path = os.path.join(tmpdirname, \"vae\", \"diffusion_pytorch_model.safetensors\")\n            assert os.path.exists(vae_path), f\"Could not find {vae_path}\"\n            _ = safetensors.torch.load_file(vae_path)\n\n            # Validate that the UNet safetensor exists and are of the correct format\n            unet_path = os.path.join(tmpdirname, \"unet\", \"diffusion_pytorch_model.safetensors\")\n            assert os.path.exists(unet_path), f\"Could not find {unet_path}\"\n            _ = safetensors.torch.load_file(unet_path)\n\n            # Validate that the text encoder safetensor exists and are of the correct format\n            text_encoder_path = os.path.join(tmpdirname, \"text_encoder\", \"model.safetensors\")\n            assert os.path.exists(text_encoder_path), f\"Could not find {text_encoder_path}\"\n            _ = safetensors.torch.load_file(text_encoder_path)\n\n            pipeline = StableDiffusionPipeline.from_pretrained(tmpdirname)\n            assert pipeline.unet is not None\n            assert pipeline.vae is not None\n            assert pipeline.text_encoder is not None\n            assert pipeline.scheduler is not None\n            assert pipeline.feature_extractor is not None",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 723, "column": 4 },
          "end": { "row": 723, "column": 4 }
        }
      }
    }
  ],
  [
    "914",
    {
      "pageContent": "def test_no_pytorch_download_when_doing_safetensors(self):\n        # by default we don't download\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            _ = StableDiffusionPipeline.from_pretrained(\n                \"hf-internal-testing/diffusers-stable-diffusion-tiny-all\", cache_dir=tmpdirname\n            )\n\n            path = os.path.join(\n                tmpdirname,\n                \"models--hf-internal-testing--diffusers-stable-diffusion-tiny-all\",\n                \"snapshots\",\n                \"07838d72e12f9bcec1375b0482b80c1d399be843\",\n                \"unet\",\n            )\n            # safetensors exists\n            assert os.path.exists(os.path.join(path, \"diffusion_pytorch_model.safetensors\"))\n            # pytorch does not\n            assert not os.path.exists(os.path.join(path, \"diffusion_pytorch_model.bin\"))",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 750, "column": 4 },
          "end": { "row": 750, "column": 4 }
        }
      }
    }
  ],
  [
    "915",
    {
      "pageContent": "def test_no_safetensors_download_when_doing_pytorch(self):\n        # mock diffusers safetensors not available\n        import diffusers\n\n        diffusers.utils.import_utils._safetensors_available = False\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            _ = StableDiffusionPipeline.from_pretrained(\n                \"hf-internal-testing/diffusers-stable-diffusion-tiny-all\", cache_dir=tmpdirname\n            )\n\n            path = os.path.join(\n                tmpdirname,\n                \"models--hf-internal-testing--diffusers-stable-diffusion-tiny-all\",\n                \"snapshots\",\n                \"07838d72e12f9bcec1375b0482b80c1d399be843\",\n                \"unet\",\n            )\n            # safetensors does not exists\n            assert not os.path.exists(os.path.join(path, \"diffusion_pytorch_model.safetensors\"))\n            # pytorch does\n            assert os.path.exists(os.path.join(path, \"diffusion_pytorch_model.bin\"))\n\n        diffusers.utils.import_utils._safetensors_available = True",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 769, "column": 4 },
          "end": { "row": 769, "column": 4 }
        }
      }
    }
  ],
  [
    "916",
    {
      "pageContent": "def test_optional_components(self):\n        unet = self.dummy_cond_unet()\n        pndm = PNDMScheduler.from_config(\"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\")\n        vae = self.dummy_vae\n        bert = self.dummy_text_encoder\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        orig_sd = StableDiffusionPipeline(\n            unet=unet,\n            scheduler=pndm,\n            vae=vae,\n            text_encoder=bert,\n            tokenizer=tokenizer,\n            safety_checker=unet,\n            feature_extractor=self.dummy_extractor,\n        )\n        sd = orig_sd\n\n        assert sd.config.requires_safety_checker is True\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            sd.save_pretrained(tmpdirname)\n\n            # Test that passing None works\n            sd = StableDiffusionPipeline.from_pretrained(\n                tmpdirname, feature_extractor=None, safety_checker=None, requires_safety_checker=False\n            )\n\n            assert sd.config.requires_safety_checker is False\n            assert sd.config.safety_checker == (None, None)\n            assert sd.config.feature_extractor == (None, None)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            sd.save_pretrained(tmpdirname)\n\n            # Test that loading previous None works\n            sd = StableDiffusionPipeline.from_pretrained(tmpdirname)\n\n            assert sd.config.requires_safety_checker is False\n            assert sd.config.safety_checker == (None, None)\n            assert sd.config.feature_extracto",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 794, "column": 4 },
          "end": { "row": 794, "column": 4 }
        }
      }
    }
  ],
  [
    "917",
    {
      "pageContent": "class PipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_smart_download(self):\n        model_id = \"hf-internal-testing/unet-pipeline-dummy\"\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            _ = DiffusionPipeline.from_pretrained(model_id, cache_dir=tmpdirname, force_download=True)\n            local_repo_name = \"--\".join([\"models\"] + model_id.split(\"/\"))\n            snapshot_dir = os.path.join(tmpdirname, local_repo_name, \"snapshots\")\n            snapshot_dir = os.path.join(snapshot_dir, os.listdir(snapshot_dir)[0])\n\n            # inspect all downloaded files to make sure that everything is included\n            assert os.path.isfile(os.path.join(snapshot_dir, DiffusionPipeline.config_name))\n            assert os.path.isfile(os.path.join(snapshot_dir, CONFIG_NAME))\n            assert os.path.isfile(os.path.join(snapshot_dir, SCHEDULER_CONFIG_NAME))\n            assert os.path.isfile(os.path.join(snapshot_dir, WEIGHTS_NAME))\n            assert os.path.isfile(os.path.join(snapshot_dir, \"scheduler\", SCHEDULER_CONFIG_NAME))\n            assert os.path.isfile(os.path.join(snapshot_dir, \"unet\", WEIGHTS_NAME))\n            assert os.path.isfile(os.path.join(snapshot_dir, \"unet\", WEIGHTS_NAME))\n            # let's make sure the super large numpy file:\n            # https://huggingface.co/hf-internal-testing/unet-pipeline-dummy/blob/main/big_array.npy\n            # is not downloaded, but all the expected ones\n            ",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 901, "column": 0 },
          "end": { "row": 901, "column": 0 }
        }
      }
    }
  ],
  [
    "918",
    {
      "pageContent": "class PipelineNightlyTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def test_ddpm_ddim_equality_batched(self):\n        seed = 0\n        model_id = \"google/ddpm-cifar10-32\"\n\n        unet = UNet2DModel.from_pretrained(model_id)\n        ddpm_scheduler = DDPMScheduler()\n        ddim_scheduler = DDIMScheduler()\n\n        ddpm = DDPMPipeline(unet=unet, scheduler=ddpm_scheduler)\n        ddpm.to(torch_device)\n        ddpm.set_progress_bar_config(disable=None)\n\n        ddim = DDIMPipeline(unet=unet, scheduler=ddim_scheduler)\n        ddim.to(torch_device)\n        ddim.set_progress_bar_config(disable=None)\n\n        generator = torch.Generator(device=torch_device).manual_seed(seed)\n        ddpm_images = ddpm(batch_size=2, generator=generator, output_type=\"numpy\").images\n\n        generator = torch.Generator(device=torch_device).manual_seed(seed)\n        ddim_images = ddim(\n            batch_size=2,\n            generator=generator,\n            num_inference_steps=1000,\n            eta=1.0,\n            output_type=\"numpy\",\n            use_clipped_model_output=True,  # Need this to make DDIM match DDPM\n        ).images\n\n        # the values aren't exactly equal, but the images look the same visually\n        assert np.abs(ddpm_images - ddim_images).max() < 1e-1",
      "metadata": {
        "source": "tests/test_pipelines.py",
        "range": {
          "start": { "row": 1119, "column": 0 },
          "end": { "row": 1119, "column": 0 }
        }
      }
    }
  ],
  [
    "919",
    {
      "pageContent": "class PipelineTesterMixin:\n    \"\"\"\n    This mixin is designed to be used with unittest.TestCase classes.\n    It provides a set of common tests for each PyTorch pipeline, e.g. saving and loading the pipeline,\n    equivalence of dict and tuple outputs, etc.\n    \"\"\"\n\n    # Canonical parameters that are passed to `__call__` regardless\n    # of the type of pipeline. They are always optional and have common\n    # sense default values.\n    required_optional_params = frozenset(\n        [\n            \"num_inference_steps\",\n            \"num_images_per_prompt\",\n            \"generator\",\n            \"latents\",\n            \"output_type\",\n            \"return_dict\",\n            \"callback\",\n            \"callback_steps\",\n        ]\n    )\n\n    # set these parameters to False in the child class if the pipeline does not support the corresponding functionality\n    test_attention_slicing = True\n    test_cpu_offload = True\n    test_xformers_attention = True\n\n    def get_generator(self, seed):\n        device = torch_device if torch_device != \"mps\" else \"cpu\"\n        generator = torch.Generator(device).manual_seed(seed)\n        return generator\n\n    @property\n    def pipeline_class(self) -> Union[Callable, DiffusionPipeline]:\n        raise NotImplementedError(\n            \"You need to set the attribute `pipeline_class = ClassNameOfPipeline` in the child test class. \"\n            \"See existing pipeline tests for reference.\"\n        )\n\n    def get_dummy_components(self):\n        raise NotImplementedError(\n            \"You need to implement `get_dummy_components(self)` in the child test class. \"\n       ",
      "metadata": {
        "source": "tests/test_pipelines_common.py",
        "range": {
          "start": { "row": 23, "column": 0 },
          "end": { "row": 23, "column": 0 }
        }
      }
    }
  ],
  [
    "920",
    {
      "pageContent": "def assert_mean_pixel_difference(image, expected_image):\n    image = np.asarray(DiffusionPipeline.numpy_to_pil(image)[0], dtype=np.float32)\n    expected_image = np.asarray(DiffusionPipeline.numpy_to_pil(expected_image)[0], dtype=np.float32)\n    avg_diff = np.abs(image - expected_image).mean()\n    assert avg_diff < 10, f\"Error image deviates {avg_diff} pixels on average\"",
      "metadata": {
        "source": "tests/test_pipelines_common.py",
        "range": {
          "start": { "row": 585, "column": 0 },
          "end": { "row": 585, "column": 0 }
        }
      }
    }
  ],
  [
    "921",
    {
      "pageContent": "class FlaxModelTesterMixin:\n    def test_output(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        variables = model.init(inputs_dict[\"prng_key\"], inputs_dict[\"sample\"])\n        jax.lax.stop_gradient(variables)\n\n        output = model.apply(variables, inputs_dict[\"sample\"])\n\n        if isinstance(output, dict):\n            output = output.sample\n\n        self.assertIsNotNone(output)\n        expected_shape = inputs_dict[\"sample\"].shape\n        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")\n\n    def test_forward_with_norm_groups(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"norm_num_groups\"] = 16\n        init_dict[\"block_out_channels\"] = (16, 32)\n\n        model = self.model_class(**init_dict)\n        variables = model.init(inputs_dict[\"prng_key\"], inputs_dict[\"sample\"])\n        jax.lax.stop_gradient(variables)\n\n        output = model.apply(variables, inputs_dict[\"sample\"])\n\n        if isinstance(output, dict):\n            output = output.sample\n\n        self.assertIsNotNone(output)\n        expected_shape = inputs_dict[\"sample\"].shape\n        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")\n\n    def test_deprecated_kwargs(self):\n        has_kwarg_in_model_class = \"kwargs\" in inspect.signature(self.model_class.__init__).parameters\n        has_deprecated_kwarg = len(self.model_class._deprecated_kwargs) > 0\n\n        if has_kwarg_in_model_class and not ha",
      "metadata": {
        "source": "tests/test_modeling_common_flax.py",
        "range": {
          "start": { "row": 11, "column": 0 },
          "end": { "row": 11, "column": 0 }
        }
      }
    }
  ],
  [
    "922",
    {
      "pageContent": "class DeprecateTester(unittest.TestCase):\n    higher_version = \".\".join([str(int(__version__.split(\".\")[0]) + 1)] + __version__.split(\".\")[1:])\n    lower_version = \"0.0.1\"\n\n    def test_deprecate_function_arg(self):\n        kwargs = {\"deprecated_arg\": 4}\n\n        with self.assertWarns(FutureWarning) as warning:\n            output = deprecate(\"deprecated_arg\", self.higher_version, \"message\", take_from=kwargs)\n\n        assert output == 4\n        assert (\n            str(warning.warning)\n            == f\"The `deprecated_arg` argument is deprecated and will be removed in version {self.higher_version}.\"\n            \" message\"\n        )\n\n    def test_deprecate_function_arg_tuple(self):\n        kwargs = {\"deprecated_arg\": 4}\n\n        with self.assertWarns(FutureWarning) as warning:\n            output = deprecate((\"deprecated_arg\", self.higher_version, \"message\"), take_from=kwargs)\n\n        assert output == 4\n        assert (\n            str(warning.warning)\n            == f\"The `deprecated_arg` argument is deprecated and will be removed in version {self.higher_version}.\"\n            \" message\"\n        )\n\n    def test_deprecate_function_args(self):\n        kwargs = {\"deprecated_arg_1\": 4, \"deprecated_arg_2\": 8}\n        with self.assertWarns(FutureWarning) as warning:\n            output_1, output_2 = deprecate(\n                (\"deprecated_arg_1\", self.higher_version, \"Hey\"),\n                (\"deprecated_arg_2\", self.higher_version, \"Hey\"),\n                take_from=kwargs,\n            )\n        assert output_1 == 4\n        assert output_2 == 8\n        assert (\n            str(warni",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "923",
    {
      "pageContent": "def test_deprecate_function_arg(self):\n        kwargs = {\"deprecated_arg\": 4}\n\n        with self.assertWarns(FutureWarning) as warning:\n            output = deprecate(\"deprecated_arg\", self.higher_version, \"message\", take_from=kwargs)\n\n        assert output == 4\n        assert (\n            str(warning.warning)\n            == f\"The `deprecated_arg` argument is deprecated and will be removed in version {self.higher_version}.\"\n            \" message\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 25, "column": 4 },
          "end": { "row": 25, "column": 4 }
        }
      }
    }
  ],
  [
    "924",
    {
      "pageContent": "def test_deprecate_function_arg_tuple(self):\n        kwargs = {\"deprecated_arg\": 4}\n\n        with self.assertWarns(FutureWarning) as warning:\n            output = deprecate((\"deprecated_arg\", self.higher_version, \"message\"), take_from=kwargs)\n\n        assert output == 4\n        assert (\n            str(warning.warning)\n            == f\"The `deprecated_arg` argument is deprecated and will be removed in version {self.higher_version}.\"\n            \" message\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 38, "column": 4 },
          "end": { "row": 38, "column": 4 }
        }
      }
    }
  ],
  [
    "925",
    {
      "pageContent": "def test_deprecate_function_args(self):\n        kwargs = {\"deprecated_arg_1\": 4, \"deprecated_arg_2\": 8}\n        with self.assertWarns(FutureWarning) as warning:\n            output_1, output_2 = deprecate(\n                (\"deprecated_arg_1\", self.higher_version, \"Hey\"),\n                (\"deprecated_arg_2\", self.higher_version, \"Hey\"),\n                take_from=kwargs,\n            )\n        assert output_1 == 4\n        assert output_2 == 8\n        assert (\n            str(warning.warnings[0].message)\n            == \"The `deprecated_arg_1` argument is deprecated and will be removed in version\"\n            f\" {self.higher_version}. Hey\"\n        )\n        assert (\n            str(warning.warnings[1].message)\n            == \"The `deprecated_arg_2` argument is deprecated and will be removed in version\"\n            f\" {self.higher_version}. Hey\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 51, "column": 4 },
          "end": { "row": 51, "column": 4 }
        }
      }
    }
  ],
  [
    "926",
    {
      "pageContent": "def test_deprecate_function_incorrect_arg(self):\n        kwargs = {\"deprecated_arg\": 4}\n\n        with self.assertRaises(TypeError) as error:\n            deprecate((\"wrong_arg\", self.higher_version, \"message\"), take_from=kwargs)\n\n        assert \"test_deprecate_function_incorrect_arg in\" in str(error.exception)\n        assert \"line\" in str(error.exception)\n        assert \"got an unexpected keyword argument `deprecated_arg`\" in str(error.exception)",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 72, "column": 4 },
          "end": { "row": 72, "column": 4 }
        }
      }
    }
  ],
  [
    "927",
    {
      "pageContent": "def test_deprecate_arg_no_kwarg(self):\n        with self.assertWarns(FutureWarning) as warning:\n            deprecate((\"deprecated_arg\", self.higher_version, \"message\"))\n\n        assert (\n            str(warning.warning)\n            == f\"`deprecated_arg` is deprecated and will be removed in version {self.higher_version}. message\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 82, "column": 4 },
          "end": { "row": 82, "column": 4 }
        }
      }
    }
  ],
  [
    "928",
    {
      "pageContent": "def test_deprecate_args_no_kwarg(self):\n        with self.assertWarns(FutureWarning) as warning:\n            deprecate(\n                (\"deprecated_arg_1\", self.higher_version, \"Hey\"),\n                (\"deprecated_arg_2\", self.higher_version, \"Hey\"),\n            )\n        assert (\n            str(warning.warnings[0].message)\n            == f\"`deprecated_arg_1` is deprecated and will be removed in version {self.higher_version}. Hey\"\n        )\n        assert (\n            str(warning.warnings[1].message)\n            == f\"`deprecated_arg_2` is deprecated and will be removed in version {self.higher_version}. Hey\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 91, "column": 4 },
          "end": { "row": 91, "column": 4 }
        }
      }
    }
  ],
  [
    "929",
    {
      "pageContent": "def test_deprecate_class_obj(self):\n        class Args:\n            arg = 5\n\n        with self.assertWarns(FutureWarning) as warning:\n            arg = deprecate((\"arg\", self.higher_version, \"message\"), take_from=Args())\n\n        assert arg == 5\n        assert (\n            str(warning.warning)\n            == f\"The `arg` attribute is deprecated and will be removed in version {self.higher_version}. message\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 106, "column": 4 },
          "end": { "row": 106, "column": 4 }
        }
      }
    }
  ],
  [
    "930",
    {
      "pageContent": "def test_deprecate_class_objs(self):\n        class Args:\n            arg = 5\n            foo = 7\n\n        with self.assertWarns(FutureWarning) as warning:\n            arg_1, arg_2 = deprecate(\n                (\"arg\", self.higher_version, \"message\"),\n                (\"foo\", self.higher_version, \"message\"),\n                (\"does not exist\", self.higher_version, \"message\"),\n                take_from=Args(),\n            )\n\n        assert arg_1 == 5\n        assert arg_2 == 7\n        assert (\n            str(warning.warning)\n            == f\"The `arg` attribute is deprecated and will be removed in version {self.higher_version}. message\"\n        )\n        assert (\n            str(warning.warnings[0].message)\n            == f\"The `arg` attribute is deprecated and will be removed in version {self.higher_version}. message\"\n        )\n        assert (\n            str(warning.warnings[1].message)\n            == f\"The `foo` attribute is deprecated and will be removed in version {self.higher_version}. message\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 119, "column": 4 },
          "end": { "row": 119, "column": 4 }
        }
      }
    }
  ],
  [
    "931",
    {
      "pageContent": "def test_deprecate_incorrect_version(self):\n        kwargs = {\"deprecated_arg\": 4}\n\n        with self.assertRaises(ValueError) as error:\n            deprecate((\"wrong_arg\", self.lower_version, \"message\"), take_from=kwargs)\n\n        assert (\n            str(error.exception)\n            == \"The deprecation tuple ('wrong_arg', '0.0.1', 'message') should be removed since diffusers' version\"\n            f\" {__version__} is >= {self.lower_version}\"\n        )",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "932",
    {
      "pageContent": "def test_deprecate_incorrect_no_standard_warn(self):\n        with self.assertWarns(FutureWarning) as warning:\n            deprecate((\"deprecated_arg\", self.higher_version, \"This message is better!!!\"), standard_warn=False)\n\n        assert str(warning.warning) == \"This message is better!!!\"",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 159, "column": 4 },
          "end": { "row": 159, "column": 4 }
        }
      }
    }
  ],
  [
    "933",
    {
      "pageContent": "def test_deprecate_stacklevel(self):\n        with self.assertWarns(FutureWarning) as warning:\n            deprecate((\"deprecated_arg\", self.higher_version, \"This message is better!!!\"), standard_warn=False)\n        assert str(warning.warning) == \"This message is better!!!\"\n        assert \"diffusers/tests/test_utils.py\" in warning.filename",
      "metadata": {
        "source": "tests/test_utils.py",
        "range": {
          "start": { "row": 165, "column": 4 },
          "end": { "row": 165, "column": 4 }
        }
      }
    }
  ],
  [
    "934",
    {
      "pageContent": "class CustomLocalPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n            [`DDPMScheduler`], or [`DDIMScheduler`].\n    \"\"\"\n\n    def __init__(self, unet, scheduler):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        generator: Optional[torch.Generator] = None,\n        num_inference_steps: int = 50,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            eta (`float`, *optional*, defaults to 0.0):\n                The eta parameter which controls the scale of the variance (0 is DDIM and 1 is one type of DDPM).\n            num_inference_steps (`int`, *optional*, d",
      "metadata": {
        "source": "tests/fixtures/custom_pipeline/what_ever.py",
        "range": {
          "start": { "row": 23, "column": 0 },
          "end": { "row": 23, "column": 0 }
        }
      }
    }
  ],
  [
    "935",
    {
      "pageContent": "class CustomLocalPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n            [`DDPMScheduler`], or [`DDIMScheduler`].\n    \"\"\"\n\n    def __init__(self, unet, scheduler):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        generator: Optional[torch.Generator] = None,\n        num_inference_steps: int = 50,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            eta (`float`, *optional*, defaults to 0.0):\n                The eta parameter which controls the scale of the variance (0 is DDIM and 1 is one type of DDPM).\n            num_inference_steps (`int`, *optional*, d",
      "metadata": {
        "source": "tests/fixtures/custom_pipeline/pipeline.py",
        "range": {
          "start": { "row": 23, "column": 0 },
          "end": { "row": 23, "column": 0 }
        }
      }
    }
  ],
  [
    "936",
    {
      "pageContent": "class SampleObject(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n    ):\n        pass",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "937",
    {
      "pageContent": "class SampleObject2(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        f=[1, 3],\n    ):\n        pass",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 46, "column": 0 },
          "end": { "row": 46, "column": 0 }
        }
      }
    }
  ],
  [
    "938",
    {
      "pageContent": "class SampleObject3(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n        f=[1, 3],\n    ):\n        pass",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 61, "column": 0 },
          "end": { "row": 61, "column": 0 }
        }
      }
    }
  ],
  [
    "939",
    {
      "pageContent": "class ConfigTester(unittest.TestCase):\n    def test_load_not_from_mixin(self):\n        with self.assertRaises(ValueError):\n            ConfigMixin.load_config(\"dummy_path\")\n\n    def test_register_to_config(self):\n        obj = SampleObject()\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # init ignore private arguments\n        obj = SampleObject(_name_or_path=\"lalala\")\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can override default\n        obj = SampleObject(c=6)\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can use positional arguments.\n        obj = SampleObject(1, c=6)\n        config = obj.config\n        assert config[\"a\"] == 1\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n    def test_save_load(self):\n        obj = SampleObject()\n        config = obj.config\n\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        with t",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 77, "column": 0 },
          "end": { "row": 77, "column": 0 }
        }
      }
    }
  ],
  [
    "940",
    {
      "pageContent": "def test_register_to_config(self):\n        obj = SampleObject()\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # init ignore private arguments\n        obj = SampleObject(_name_or_path=\"lalala\")\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can override default\n        obj = SampleObject(c=6)\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can use positional arguments.\n        obj = SampleObject(1, c=6)\n        config = obj.config\n        assert config[\"a\"] == 1\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 82, "column": 4 },
          "end": { "row": 82, "column": 4 }
        }
      }
    }
  ],
  [
    "941",
    {
      "pageContent": "def test_save_load(self):\n        obj = SampleObject()\n        config = obj.config\n\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n            new_obj = SampleObject.from_config(SampleObject.load_config(tmpdirname))\n            new_config = new_obj.config\n\n        # unfreeze configs\n        config = dict(config)\n        new_config = dict(new_config)\n\n        assert config.pop(\"c\") == (2, 5)  # instantiated as tuple\n        assert new_config.pop(\"c\") == [2, 5]  # saved & loaded as list because of json\n        assert config == new_config",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 118, "column": 4 },
          "end": { "row": 118, "column": 4 }
        }
      }
    }
  ],
  [
    "942",
    {
      "pageContent": "def test_load_ddim_from_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            ddim = DDIMScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert ddim.__class__ == DDIMScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "943",
    {
      "pageContent": "def test_load_euler_from_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            euler = EulerDiscreteScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert euler.__class__ == EulerDiscreteScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 153, "column": 4 },
          "end": { "row": 153, "column": 4 }
        }
      }
    }
  ],
  [
    "944",
    {
      "pageContent": "def test_load_euler_ancestral_from_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            euler = EulerAncestralDiscreteScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert euler.__class__ == EulerAncestralDiscreteScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 165, "column": 4 },
          "end": { "row": 165, "column": 4 }
        }
      }
    }
  ],
  [
    "945",
    {
      "pageContent": "def test_load_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            pndm = PNDMScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert pndm.__class__ == PNDMScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 177, "column": 4 },
          "end": { "row": 177, "column": 4 }
        }
      }
    }
  ],
  [
    "946",
    {
      "pageContent": "def test_overwrite_config_on_load(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            ddpm = DDPMScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\",\n                subfolder=\"scheduler\",\n                prediction_type=\"sample\",\n                beta_end=8,\n            )\n\n        with CaptureLogger(logger) as cap_logger_2:\n            ddpm_2 = DDPMScheduler.from_pretrained(\"google/ddpm-celebahq-256\", beta_start=88)\n\n        assert ddpm.__class__ == DDPMScheduler\n        assert ddpm.config.prediction_type == \"sample\"\n        assert ddpm.config.beta_end == 8\n        assert ddpm_2.config.beta_start == 88\n\n        # no warning should be thrown\n        assert cap_logger.out == \"\"\n        assert cap_logger_2.out == \"\"",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 189, "column": 4 },
          "end": { "row": 189, "column": 4 }
        }
      }
    }
  ],
  [
    "947",
    {
      "pageContent": "def test_load_dpmsolver(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            dpm = DPMSolverMultistepScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert dpm.__class__ == DPMSolverMultistepScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"",
      "metadata": {
        "source": "tests/test_config.py",
        "range": {
          "start": { "row": 212, "column": 4 },
          "end": { "row": 212, "column": 4 }
        }
      }
    }
  ],
  [
    "948",
    {
      "pageContent": "class CheckDummiesTester(unittest.TestCase):\n    def test_find_backend(self):\n        simple_backend = find_backend(\"    if not is_torch_available():\")\n        self.assertEqual(simple_backend, \"torch\")\n\n        # backend_with_underscore = find_backend(\"    if not is_tensorflow_text_available():\")\n        # self.assertEqual(backend_with_underscore, \"tensorflow_text\")\n\n        double_backend = find_backend(\"    if not (is_torch_available() and is_transformers_available()):\")\n        self.assertEqual(double_backend, \"torch_and_transformers\")\n\n        # double_backend_with_underscore = find_backend(\n        #    \"    if not (is_sentencepiece_available() and is_tensorflow_text_available()):\"\n        # )\n        # self.assertEqual(double_backend_with_underscore, \"sentencepiece_and_tensorflow_text\")\n\n        triple_backend = find_backend(\n            \"    if not (is_torch_available() and is_transformers_available() and is_onnx_available()):\"\n        )\n        self.assertEqual(triple_backend, \"torch_and_transformers_and_onnx\")\n\n    def test_read_init(self):\n        objects = read_init()\n        # We don't assert on the exact list of keys to allow for smooth grow of backend-specific objects\n        self.assertIn(\"torch\", objects)\n        self.assertIn(\"torch_and_transformers\", objects)\n        self.assertIn(\"flax_and_transformers\", objects)\n        self.assertIn(\"torch_and_transformers_and_onnx\", objects)\n\n        # Likewise, we can't assert on the exact content of a key\n        self.assertIn(\"UNet2DModel\", objects[\"torch\"])\n        self.assertIn(\"FlaxUNet2DConditionModel\", objects[",
      "metadata": {
        "source": "tests/repo_utils/test_check_dummies.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "949",
    {
      "pageContent": "def test_find_backend(self):\n        simple_backend = find_backend(\"    if not is_torch_available():\")\n        self.assertEqual(simple_backend, \"torch\")\n\n        # backend_with_underscore = find_backend(\"    if not is_tensorflow_text_available():\")\n        # self.assertEqual(backend_with_underscore, \"tensorflow_text\")\n\n        double_backend = find_backend(\"    if not (is_torch_available() and is_transformers_available()):\")\n        self.assertEqual(double_backend, \"torch_and_transformers\")\n\n        # double_backend_with_underscore = find_backend(\n        #    \"    if not (is_sentencepiece_available() and is_tensorflow_text_available()):\"\n        # )\n        # self.assertEqual(double_backend_with_underscore, \"sentencepiece_and_tensorflow_text\")\n\n        triple_backend = find_backend(\n            \"    if not (is_torch_available() and is_transformers_available() and is_onnx_available()):\"\n        )\n        self.assertEqual(triple_backend, \"torch_and_transformers_and_onnx\")",
      "metadata": {
        "source": "tests/repo_utils/test_check_dummies.py",
        "range": {
          "start": { "row": 31, "column": 4 },
          "end": { "row": 31, "column": 4 }
        }
      }
    }
  ],
  [
    "950",
    {
      "pageContent": "def test_read_init(self):\n        objects = read_init()\n        # We don't assert on the exact list of keys to allow for smooth grow of backend-specific objects\n        self.assertIn(\"torch\", objects)\n        self.assertIn(\"torch_and_transformers\", objects)\n        self.assertIn(\"flax_and_transformers\", objects)\n        self.assertIn(\"torch_and_transformers_and_onnx\", objects)\n\n        # Likewise, we can't assert on the exact content of a key\n        self.assertIn(\"UNet2DModel\", objects[\"torch\"])\n        self.assertIn(\"FlaxUNet2DConditionModel\", objects[\"flax\"])\n        self.assertIn(\"StableDiffusionPipeline\", objects[\"torch_and_transformers\"])\n        self.assertIn(\"FlaxStableDiffusionPipeline\", objects[\"flax_and_transformers\"])\n        self.assertIn(\"LMSDiscreteScheduler\", objects[\"torch_and_scipy\"])\n        self.assertIn(\"OnnxStableDiffusionPipeline\", objects[\"torch_and_transformers_and_onnx\"])",
      "metadata": {
        "source": "tests/repo_utils/test_check_dummies.py",
        "range": {
          "start": { "row": 51, "column": 4 },
          "end": { "row": 51, "column": 4 }
        }
      }
    }
  ],
  [
    "951",
    {
      "pageContent": "def test_create_dummy_object(self):\n        dummy_constant = create_dummy_object(\"CONSTANT\", \"'torch'\")\n        self.assertEqual(dummy_constant, \"\\nCONSTANT = None\\n\")\n\n        dummy_function = create_dummy_object(\"function\", \"'torch'\")\n        self.assertEqual(\n            dummy_function, \"\\ndef function(*args, **kwargs):\\n    requires_backends(function, 'torch')\\n\"\n        )\n\n        expected_dummy_class = \"\"\"\nclass FakeClass(metaclass=DummyObject):\n    _backends = 'torch'\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, 'torch')\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, 'torch')\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, 'torch')\n\"\"\"\n        dummy_class = create_dummy_object(\"FakeClass\", \"'torch'\")\n        self.assertEqual(dummy_class, expected_dummy_class)",
      "metadata": {
        "source": "tests/repo_utils/test_check_dummies.py",
        "range": {
          "start": { "row": 67, "column": 4 },
          "end": { "row": 67, "column": 4 }
        }
      }
    }
  ],
  [
    "952",
    {
      "pageContent": "def test_create_dummy_files(self):\n        expected_dummy_pytorch_file = \"\"\"# This file is autogenerated by the command `make fix-copies`, do not edit.\nfrom ..utils import DummyObject, requires_backends\n\n\nCONSTANT = None\n\n\ndef function(*args, **kwargs):\n    requires_backends(function, [\"torch\"])\n\n\nclass FakeClass(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\"\"\"\n        dummy_files = create_dummy_files({\"torch\": [\"CONSTANT\", \"function\", \"FakeClass\"]})\n        self.assertEqual(dummy_files[\"torch\"], expected_dummy_pytorch_file)",
      "metadata": {
        "source": "tests/repo_utils/test_check_dummies.py",
        "range": {
          "start": { "row": 94, "column": 4 },
          "end": { "row": 94, "column": 4 }
        }
      }
    }
  ],
  [
    "953",
    {
      "pageContent": "class CopyCheckTester(unittest.TestCase):\n    def setUp(self):\n        self.diffusers_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(self.diffusers_dir, \"schedulers/\"))\n        check_copies.DIFFUSERS_PATH = self.diffusers_dir\n        shutil.copy(\n            os.path.join(git_repo_path, \"src/diffusers/schedulers/scheduling_ddpm.py\"),\n            os.path.join(self.diffusers_dir, \"schedulers/scheduling_ddpm.py\"),\n        )\n\n    def tearDown(self):\n        check_copies.DIFFUSERS_PATH = \"src/diffusers\"\n        shutil.rmtree(self.diffusers_dir)\n\n    def check_copy_consistency(self, comment, class_name, class_code, overwrite_result=None):\n        code = comment + f\"\\nclass {class_name}(nn.Module):\\n\" + class_code\n        if overwrite_result is not None:\n            expected = comment + f\"\\nclass {class_name}(nn.Module):\\n\" + overwrite_result\n        mode = black.Mode(target_versions={black.TargetVersion.PY35}, line_length=119)\n        code = black.format_str(code, mode=mode)\n        fname = os.path.join(self.diffusers_dir, \"new_code.py\")\n        with open(fname, \"w\", newline=\"\\n\") as f:\n            f.write(code)\n        if overwrite_result is None:\n            self.assertTrue(len(check_copies.is_copy_consistent(fname)) == 0)\n        else:\n            check_copies.is_copy_consistent(f.name, overwrite=True)\n            with open(fname, \"r\") as f:\n                self.assertTrue(f.read(), expected)\n\n    def test_find_code_in_diffusers(self):\n        code = check_copies.find_code_in_diffusers(\"schedulers.scheduling_ddpm.DDPMSchedulerOutput\")\n        self.assertEqual(code, R",
      "metadata": {
        "source": "tests/repo_utils/test_check_copies.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "954",
    {
      "pageContent": "def setUp(self):\n        self.diffusers_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(self.diffusers_dir, \"schedulers/\"))\n        check_copies.DIFFUSERS_PATH = self.diffusers_dir\n        shutil.copy(\n            os.path.join(git_repo_path, \"src/diffusers/schedulers/scheduling_ddpm.py\"),\n            os.path.join(self.diffusers_dir, \"schedulers/scheduling_ddpm.py\"),\n        )",
      "metadata": {
        "source": "tests/repo_utils/test_check_copies.py",
        "range": {
          "start": { "row": 50, "column": 4 },
          "end": { "row": 50, "column": 4 }
        }
      }
    }
  ],
  [
    "955",
    {
      "pageContent": "def check_copy_consistency(self, comment, class_name, class_code, overwrite_result=None):\n        code = comment + f\"\\nclass {class_name}(nn.Module):\\n\" + class_code\n        if overwrite_result is not None:\n            expected = comment + f\"\\nclass {class_name}(nn.Module):\\n\" + overwrite_result\n        mode = black.Mode(target_versions={black.TargetVersion.PY35}, line_length=119)\n        code = black.format_str(code, mode=mode)\n        fname = os.path.join(self.diffusers_dir, \"new_code.py\")\n        with open(fname, \"w\", newline=\"\\n\") as f:\n            f.write(code)\n        if overwrite_result is None:\n            self.assertTrue(len(check_copies.is_copy_consistent(fname)) == 0)\n        else:\n            check_copies.is_copy_consistent(f.name, overwrite=True)\n            with open(fname, \"r\") as f:\n                self.assertTrue(f.read(), expected)",
      "metadata": {
        "source": "tests/repo_utils/test_check_copies.py",
        "range": {
          "start": { "row": 63, "column": 4 },
          "end": { "row": 63, "column": 4 }
        }
      }
    }
  ],
  [
    "956",
    {
      "pageContent": "def test_is_copy_consistent(self):\n        # Base copy consistency\n        self.check_copy_consistency(\n            \"# Copied from diffusers.schedulers.scheduling_ddpm.DDPMSchedulerOutput\",\n            \"DDPMSchedulerOutput\",\n            REFERENCE_CODE + \"\\n\",\n        )\n\n        # With no empty line at the end\n        self.check_copy_consistency(\n            \"# Copied from diffusers.schedulers.scheduling_ddpm.DDPMSchedulerOutput\",\n            \"DDPMSchedulerOutput\",\n            REFERENCE_CODE,\n        )\n\n        # Copy consistency with rename\n        self.check_copy_consistency(\n            \"# Copied from diffusers.schedulers.scheduling_ddpm.DDPMSchedulerOutput with DDPM->Test\",\n            \"TestSchedulerOutput\",\n            re.sub(\"DDPM\", \"Test\", REFERENCE_CODE),\n        )\n\n        # Copy consistency with a really long name\n        long_class_name = \"TestClassWithAReallyLongNameBecauseSomePeopleLikeThatForSomeReason\"\n        self.check_copy_consistency(\n            f\"# Copied from diffusers.schedulers.scheduling_ddpm.DDPMSchedulerOutput with DDPM->{long_class_name}\",\n            f\"{long_class_name}SchedulerOutput\",\n            re.sub(\"Bert\", long_class_name, REFERENCE_CODE),\n        )\n\n        # Copy consistency with overwrite\n        self.check_copy_consistency(\n            \"# Copied from diffusers.schedulers.scheduling_ddpm.DDPMSchedulerOutput with DDPM->Test\",\n            \"TestSchedulerOutput\",\n            REFERENCE_CODE,\n            overwrite_result=re.sub(\"DDPM\", \"Test\", REFERENCE_CODE),\n        )",
      "metadata": {
        "source": "tests/repo_utils/test_check_copies.py",
        "range": {
          "start": { "row": 83, "column": 4 },
          "end": { "row": 83, "column": 4 }
        }
      }
    }
  ],
  [
    "957",
    {
      "pageContent": "class DownBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = DownBlock2D  # noqa F405\n    block_type = \"down\"\n\n    def test_output(self):\n        expected_slice = [-0.0232, -0.9869, 0.8054, -0.0637, -0.1688, -1.4264, 0.4470, -1.3394, 0.0904]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 22, "column": 0 },
          "end": { "row": 22, "column": 0 }
        }
      }
    }
  ],
  [
    "958",
    {
      "pageContent": "class ResnetDownsampleBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = ResnetDownsampleBlock2D  # noqa F405\n    block_type = \"down\"\n\n    def test_output(self):\n        expected_slice = [0.0710, 0.2410, -0.7320, -1.0757, -1.1343, 0.3540, -0.0133, -0.2576, 0.0948]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "959",
    {
      "pageContent": "class AttnDownBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = AttnDownBlock2D  # noqa F405\n    block_type = \"down\"\n\n    def test_output(self):\n        expected_slice = [0.0636, 0.8964, -0.6234, -1.0131, 0.0844, 0.4935, 0.3437, 0.0911, -0.2957]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "960",
    {
      "pageContent": "class CrossAttnDownBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = CrossAttnDownBlock2D  # noqa F405\n    block_type = \"down\"\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [0.2440, -0.6953, -0.2140, -0.3874, 0.1966, 1.2077, 0.0441, -0.7718, 0.2800]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "961",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 53, "column": 4 },
          "end": { "row": 53, "column": 4 }
        }
      }
    }
  ],
  [
    "962",
    {
      "pageContent": "class SimpleCrossAttnDownBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = SimpleCrossAttnDownBlock2D  # noqa F405\n    block_type = \"down\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_encoder_hidden_states=True)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict\n\n    @unittest.skipIf(torch_device == \"mps\", \"MPS result is not consistent\")\n    def test_output(self):\n        expected_slice = [0.7921, -0.0992, -0.1962, -0.7695, -0.4242, 0.7804, 0.4737, 0.2765, 0.3338]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 63, "column": 0 },
          "end": { "row": 63, "column": 0 }
        }
      }
    }
  ],
  [
    "963",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 71, "column": 4 },
          "end": { "row": 71, "column": 4 }
        }
      }
    }
  ],
  [
    "964",
    {
      "pageContent": "class SkipDownBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = SkipDownBlock2D  # noqa F405\n    block_type = \"down\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_skip_sample=True)\n\n    def test_output(self):\n        expected_slice = [-0.0845, -0.2087, -0.2465, 0.0971, 0.1900, -0.0484, 0.2664, 0.4179, 0.5069]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 82, "column": 0 },
          "end": { "row": 82, "column": 0 }
        }
      }
    }
  ],
  [
    "965",
    {
      "pageContent": "class AttnSkipDownBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = AttnSkipDownBlock2D  # noqa F405\n    block_type = \"down\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_skip_sample=True)\n\n    def test_output(self):\n        expected_slice = [0.5539, 0.1609, 0.4924, 0.0537, -0.1995, 0.4050, 0.0979, -0.2721, -0.0642]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 95, "column": 0 },
          "end": { "row": 95, "column": 0 }
        }
      }
    }
  ],
  [
    "966",
    {
      "pageContent": "class DownEncoderBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = DownEncoderBlock2D  # noqa F405\n    block_type = \"down\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_temb=False)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"in_channels\": 32,\n            \"out_channels\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [1.1102, 0.5302, 0.4872, -0.0023, -0.8042, 0.0483, -0.3489, -0.5632, 0.7626]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 108, "column": 0 },
          "end": { "row": 108, "column": 0 }
        }
      }
    }
  ],
  [
    "967",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"in_channels\": 32,\n            \"out_channels\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 116, "column": 4 },
          "end": { "row": 116, "column": 4 }
        }
      }
    }
  ],
  [
    "968",
    {
      "pageContent": "class AttnDownEncoderBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = AttnDownEncoderBlock2D  # noqa F405\n    block_type = \"down\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_temb=False)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"in_channels\": 32,\n            \"out_channels\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [0.8966, -0.1486, 0.8568, 0.8141, -0.9046, -0.1342, -0.0972, -0.7417, 0.1538]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 129, "column": 0 },
          "end": { "row": 129, "column": 0 }
        }
      }
    }
  ],
  [
    "969",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"in_channels\": 32,\n            \"out_channels\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 137, "column": 4 },
          "end": { "row": 137, "column": 4 }
        }
      }
    }
  ],
  [
    "970",
    {
      "pageContent": "class UNetMidBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = UNetMidBlock2D  # noqa F405\n    block_type = \"mid\"\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"in_channels\": 32,\n            \"temb_channels\": 128,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [-0.1062, 1.7248, 0.3494, 1.4569, -0.0910, -1.2421, -0.9984, 0.6736, 1.0028]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 150, "column": 0 },
          "end": { "row": 150, "column": 0 }
        }
      }
    }
  ],
  [
    "971",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"in_channels\": 32,\n            \"temb_channels\": 128,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 154, "column": 4 },
          "end": { "row": 154, "column": 4 }
        }
      }
    }
  ],
  [
    "972",
    {
      "pageContent": "class UNetMidBlock2DCrossAttnTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = UNetMidBlock2DCrossAttn  # noqa F405\n    block_type = \"mid\"\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [0.1879, 2.2653, 0.5987, 1.1568, -0.8454, -1.6109, -0.8919, 0.8306, 1.6758]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 167, "column": 0 },
          "end": { "row": 167, "column": 0 }
        }
      }
    }
  ],
  [
    "973",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 171, "column": 4 },
          "end": { "row": 171, "column": 4 }
        }
      }
    }
  ],
  [
    "974",
    {
      "pageContent": "class UNetMidBlock2DSimpleCrossAttnTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = UNetMidBlock2DSimpleCrossAttn  # noqa F405\n    block_type = \"mid\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_encoder_hidden_states=True)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [0.7143, 1.9974, 0.5448, 1.3977, 0.1282, -1.1237, -1.4238, 0.5530, 0.8880]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 181, "column": 0 },
          "end": { "row": 181, "column": 0 }
        }
      }
    }
  ],
  [
    "975",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 189, "column": 4 },
          "end": { "row": 189, "column": 4 }
        }
      }
    }
  ],
  [
    "976",
    {
      "pageContent": "class UpBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = UpBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_res_hidden_states_tuple=True)\n\n    def test_output(self):\n        expected_slice = [-0.2041, -0.4165, -0.3022, 0.0041, -0.6628, -0.7053, 0.1928, -0.0325, 0.0523]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 199, "column": 0 },
          "end": { "row": 199, "column": 0 }
        }
      }
    }
  ],
  [
    "977",
    {
      "pageContent": "class ResnetUpsampleBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = ResnetUpsampleBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_res_hidden_states_tuple=True)\n\n    def test_output(self):\n        expected_slice = [0.2287, 0.3549, -0.1346, 0.4797, -0.1715, -0.9649, 0.7305, -0.5864, -0.6244]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 212, "column": 0 },
          "end": { "row": 212, "column": 0 }
        }
      }
    }
  ],
  [
    "978",
    {
      "pageContent": "class CrossAttnUpBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = CrossAttnUpBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_res_hidden_states_tuple=True)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [-0.2796, -0.4364, -0.1067, -0.2693, 0.1894, 0.3869, -0.3470, 0.4584, 0.5091]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 225, "column": 0 },
          "end": { "row": 225, "column": 0 }
        }
      }
    }
  ],
  [
    "979",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 233, "column": 4 },
          "end": { "row": 233, "column": 4 }
        }
      }
    }
  ],
  [
    "980",
    {
      "pageContent": "class SimpleCrossAttnUpBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = SimpleCrossAttnUpBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_res_hidden_states_tuple=True, include_encoder_hidden_states=True)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        if torch_device == \"mps\":\n            expected_slice = [0.4327, 0.5538, 0.3919, 0.5682, 0.2704, 0.1573, -0.8768, -0.4615, -0.4146]\n        else:\n            expected_slice = [0.2645, 0.1480, 0.0909, 0.8044, -0.9758, -0.9083, 0.0994, -1.1453, -0.7402]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 243, "column": 0 },
          "end": { "row": 243, "column": 0 }
        }
      }
    }
  ],
  [
    "981",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict, inputs_dict = super().prepare_init_args_and_inputs_for_common()\n        init_dict[\"cross_attention_dim\"] = 32\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 251, "column": 4 },
          "end": { "row": 251, "column": 4 }
        }
      }
    }
  ],
  [
    "982",
    {
      "pageContent": "def test_output(self):\n        if torch_device == \"mps\":\n            expected_slice = [0.4327, 0.5538, 0.3919, 0.5682, 0.2704, 0.1573, -0.8768, -0.4615, -0.4146]\n        else:\n            expected_slice = [0.2645, 0.1480, 0.0909, 0.8044, -0.9758, -0.9083, 0.0994, -1.1453, -0.7402]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 256, "column": 4 },
          "end": { "row": 256, "column": 4 }
        }
      }
    }
  ],
  [
    "983",
    {
      "pageContent": "class AttnUpBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = AttnUpBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_res_hidden_states_tuple=True)\n\n    @unittest.skipIf(torch_device == \"mps\", \"MPS result is not consistent\")\n    def test_output(self):\n        expected_slice = [0.0979, 0.1326, 0.0021, 0.0659, 0.2249, 0.0059, 0.1132, 0.5952, 0.1033]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 264, "column": 0 },
          "end": { "row": 264, "column": 0 }
        }
      }
    }
  ],
  [
    "984",
    {
      "pageContent": "class SkipUpBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = SkipUpBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_res_hidden_states_tuple=True)\n\n    def test_output(self):\n        expected_slice = [-0.0893, -0.1234, -0.1506, -0.0332, 0.0123, -0.0211, 0.0566, 0.0143, 0.0362]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 278, "column": 0 },
          "end": { "row": 278, "column": 0 }
        }
      }
    }
  ],
  [
    "985",
    {
      "pageContent": "class AttnSkipUpBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = AttnSkipUpBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_res_hidden_states_tuple=True)\n\n    def test_output(self):\n        expected_slice = [0.0361, 0.0617, 0.2787, -0.0350, 0.0342, 0.3421, -0.0843, 0.0913, 0.3015]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 291, "column": 0 },
          "end": { "row": 291, "column": 0 }
        }
      }
    }
  ],
  [
    "986",
    {
      "pageContent": "class UpDecoderBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = UpDecoderBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_temb=False)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\"in_channels\": 32, \"out_channels\": 32}\n\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        expected_slice = [0.4404, 0.1998, -0.9886, -0.3320, -0.3128, -0.7034, -0.6955, -0.2338, -0.3137]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 304, "column": 0 },
          "end": { "row": 304, "column": 0 }
        }
      }
    }
  ],
  [
    "987",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\"in_channels\": 32, \"out_channels\": 32}\n\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 312, "column": 4 },
          "end": { "row": 312, "column": 4 }
        }
      }
    }
  ],
  [
    "988",
    {
      "pageContent": "class AttnUpDecoderBlock2DTests(UNetBlockTesterMixin, unittest.TestCase):\n    block_class = AttnUpDecoderBlock2D  # noqa F405\n    block_type = \"up\"\n\n    @property\n    def dummy_input(self):\n        return super().get_dummy_input(include_temb=False)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\"in_channels\": 32, \"out_channels\": 32}\n\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_output(self):\n        if torch_device == \"mps\":\n            expected_slice = [-0.3669, -0.3387, 0.1029, -0.6564, 0.2728, -0.3233, 0.5977, -0.1784, 0.5482]\n        else:\n            expected_slice = [0.6738, 0.4491, 0.1055, 1.0710, 0.7316, 0.3339, 0.3352, 0.1023, 0.3568]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 323, "column": 0 },
          "end": { "row": 323, "column": 0 }
        }
      }
    }
  ],
  [
    "989",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\"in_channels\": 32, \"out_channels\": 32}\n\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 331, "column": 4 },
          "end": { "row": 331, "column": 4 }
        }
      }
    }
  ],
  [
    "990",
    {
      "pageContent": "def test_output(self):\n        if torch_device == \"mps\":\n            expected_slice = [-0.3669, -0.3387, 0.1029, -0.6564, 0.2728, -0.3233, 0.5977, -0.1784, 0.5482]\n        else:\n            expected_slice = [0.6738, 0.4491, 0.1055, 1.0710, 0.7316, 0.3339, 0.3352, 0.1023, 0.3568]\n        super().test_output(expected_slice)",
      "metadata": {
        "source": "tests/test_unet_2d_blocks.py",
        "range": {
          "start": { "row": 337, "column": 4 },
          "end": { "row": 337, "column": 4 }
        }
      }
    }
  ],
  [
    "991",
    {
      "pageContent": "class ModelUtilsTest(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n\n        import diffusers\n\n        diffusers.utils.import_utils._safetensors_available = True\n\n    def test_accelerate_loading_error_message(self):\n        with self.assertRaises(ValueError) as error_context:\n            UNet2DConditionModel.from_pretrained(\"hf-internal-testing/stable-diffusion-broken\", subfolder=\"unet\")\n\n        # make sure that error message states what keys are missing\n        assert \"conv_out.bias\" in str(error_context.exception)\n\n    def test_cached_files_are_used_when_no_internet(self):\n        # A mock response for an HTTP head request to emulate server down\n        response_mock = mock.Mock()\n        response_mock.status_code = 500\n        response_mock.headers = {}\n        response_mock.raise_for_status.side_effect = HTTPError\n        response_mock.json.return_value = {}\n\n        # Download this model to make sure it's in the cache.\n        orig_model = UNet2DConditionModel.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"unet\"\n        )\n\n        # Under the mock environment we get a 500 error when trying to reach the model.\n        with mock.patch(\"requests.request\", return_value=response_mock):\n            # Download this model to make sure it's in the cache.\n            model = UNet2DConditionModel.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"unet\", local_files_only=True\n            )\n\n        for p1, p2 in zip(orig_model.parameters(), model.parameters()):\n        ",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "992",
    {
      "pageContent": "def tearDown(self):\n        super().tearDown()\n\n        import diffusers\n\n        diffusers.utils.import_utils._safetensors_available = True",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 32, "column": 4 },
          "end": { "row": 32, "column": 4 }
        }
      }
    }
  ],
  [
    "993",
    {
      "pageContent": "def test_accelerate_loading_error_message(self):\n        with self.assertRaises(ValueError) as error_context:\n            UNet2DConditionModel.from_pretrained(\"hf-internal-testing/stable-diffusion-broken\", subfolder=\"unet\")\n\n        # make sure that error message states what keys are missing\n        assert \"conv_out.bias\" in str(error_context.exception)",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "994",
    {
      "pageContent": "def test_cached_files_are_used_when_no_internet(self):\n        # A mock response for an HTTP head request to emulate server down\n        response_mock = mock.Mock()\n        response_mock.status_code = 500\n        response_mock.headers = {}\n        response_mock.raise_for_status.side_effect = HTTPError\n        response_mock.json.return_value = {}\n\n        # Download this model to make sure it's in the cache.\n        orig_model = UNet2DConditionModel.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"unet\"\n        )\n\n        # Under the mock environment we get a 500 error when trying to reach the model.\n        with mock.patch(\"requests.request\", return_value=response_mock):\n            # Download this model to make sure it's in the cache.\n            model = UNet2DConditionModel.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"unet\", local_files_only=True\n            )\n\n        for p1, p2 in zip(orig_model.parameters(), model.parameters()):\n            if p1.data.ne(p2.data).sum() > 0:\n                assert False, \"Parameters not the same!\"",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 46, "column": 4 },
          "end": { "row": 46, "column": 4 }
        }
      }
    }
  ],
  [
    "995",
    {
      "pageContent": "def test_one_request_upon_cached(self):\n        # TODO: For some reason this test fails on MPS where no HEAD call is made.\n        if torch_device == \"mps\":\n            return\n\n        import diffusers\n\n        diffusers.utils.import_utils._safetensors_available = False\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            with requests_mock.mock(real_http=True) as m:\n                UNet2DConditionModel.from_pretrained(\n                    \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"unet\", cache_dir=tmpdirname\n                )\n\n            download_requests = [r.method for r in m.request_history]\n            assert download_requests.count(\"HEAD\") == 2, \"2 HEAD requests one for config, one for model\"\n            assert download_requests.count(\"GET\") == 2, \"2 GET requests one for config, one for model\"\n\n            with requests_mock.mock(real_http=True) as m:\n                UNet2DConditionModel.from_pretrained(\n                    \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"unet\", cache_dir=tmpdirname\n                )\n\n            cache_requests = [r.method for r in m.request_history]\n            assert (\n                \"HEAD\" == cache_requests[0] and len(cache_requests) == 1\n            ), \"We should call only `model_info` to check for _commit hash and `send_telemetry`\"\n\n        diffusers.utils.import_utils._safetensors_available = True",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 70, "column": 4 },
          "end": { "row": 70, "column": 4 }
        }
      }
    }
  ],
  [
    "996",
    {
      "pageContent": "class ModelTesterMixin:\n    def test_from_save_pretrained(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_pretrained(tmpdirname)\n            new_model = self.model_class.from_pretrained(tmpdirname)\n            new_model.to(torch_device)\n\n        with torch.no_grad():\n            # Warmup pass when using mps (see #372)\n            if torch_device == \"mps\" and isinstance(model, ModelMixin):\n                _ = model(**self.dummy_input)\n                _ = new_model(**self.dummy_input)\n\n            image = model(**inputs_dict)\n            if isinstance(image, dict):\n                image = image.sample\n\n            new_image = new_model(**inputs_dict)\n\n            if isinstance(new_image, dict):\n                new_image = new_image.sample\n\n        max_diff = (image - new_image).abs().sum().item()\n        self.assertLessEqual(max_diff, 5e-5, \"Models give different forward passes\")\n\n    def test_from_save_pretrained_variant(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_pretrained(tmpdirname, variant=\"fp16\")\n            new_model = self.model_class.from_pretrained(tmpdirname, variant=\"fp16\")\n\n            # non-variant cannot be loaded\n          ",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 102, "column": 0 },
          "end": { "row": 102, "column": 0 }
        }
      }
    }
  ],
  [
    "997",
    {
      "pageContent": "def test_from_save_pretrained(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_pretrained(tmpdirname)\n            new_model = self.model_class.from_pretrained(tmpdirname)\n            new_model.to(torch_device)\n\n        with torch.no_grad():\n            # Warmup pass when using mps (see #372)\n            if torch_device == \"mps\" and isinstance(model, ModelMixin):\n                _ = model(**self.dummy_input)\n                _ = new_model(**self.dummy_input)\n\n            image = model(**inputs_dict)\n            if isinstance(image, dict):\n                image = image.sample\n\n            new_image = new_model(**inputs_dict)\n\n            if isinstance(new_image, dict):\n                new_image = new_image.sample\n\n        max_diff = (image - new_image).abs().sum().item()\n        self.assertLessEqual(max_diff, 5e-5, \"Models give different forward passes\")",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 103, "column": 4 },
          "end": { "row": 103, "column": 4 }
        }
      }
    }
  ],
  [
    "998",
    {
      "pageContent": "def test_from_save_pretrained_variant(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_pretrained(tmpdirname, variant=\"fp16\")\n            new_model = self.model_class.from_pretrained(tmpdirname, variant=\"fp16\")\n\n            # non-variant cannot be loaded\n            with self.assertRaises(OSError) as error_context:\n                self.model_class.from_pretrained(tmpdirname)\n\n            # make sure that error message states what keys are missing\n            assert \"Error no file named diffusion_pytorch_model.bin found in directory\" in str(error_context.exception)\n\n            new_model.to(torch_device)\n\n        with torch.no_grad():\n            # Warmup pass when using mps (see #372)\n            if torch_device == \"mps\" and isinstance(model, ModelMixin):\n                _ = model(**self.dummy_input)\n                _ = new_model(**self.dummy_input)\n\n            image = model(**inputs_dict)\n            if isinstance(image, dict):\n                image = image.sample\n\n            new_image = new_model(**inputs_dict)\n\n            if isinstance(new_image, dict):\n                new_image = new_image.sample\n\n        max_diff = (image - new_image).abs().sum().item()\n        self.assertLessEqual(max_diff, 5e-5, \"Models give different forward passes\")",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 133, "column": 4 },
          "end": { "row": 133, "column": 4 }
        }
      }
    }
  ],
  [
    "999",
    {
      "pageContent": "def test_from_save_pretrained_dtype(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        for dtype in [torch.float32, torch.float16, torch.bfloat16]:\n            if torch_device == \"mps\" and dtype == torch.bfloat16:\n                continue\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                model.to(dtype)\n                model.save_pretrained(tmpdirname)\n                new_model = self.model_class.from_pretrained(tmpdirname, low_cpu_mem_usage=True, torch_dtype=dtype)\n                assert new_model.dtype == dtype\n                new_model = self.model_class.from_pretrained(tmpdirname, low_cpu_mem_usage=False, torch_dtype=dtype)\n                assert new_model.dtype == dtype",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 171, "column": 4 },
          "end": { "row": 171, "column": 4 }
        }
      }
    }
  ],
  [
    "1000",
    {
      "pageContent": "def test_determinism(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with torch.no_grad():\n            # Warmup pass when using mps (see #372)\n            if torch_device == \"mps\" and isinstance(model, ModelMixin):\n                model(**self.dummy_input)\n\n            first = model(**inputs_dict)\n            if isinstance(first, dict):\n                first = first.sample\n\n            second = model(**inputs_dict)\n            if isinstance(second, dict):\n                second = second.sample\n\n        out_1 = first.cpu().numpy()\n        out_2 = second.cpu().numpy()\n        out_1 = out_1[~np.isnan(out_1)]\n        out_2 = out_2[~np.isnan(out_2)]\n        max_diff = np.amax(np.abs(out_1 - out_2))\n        self.assertLessEqual(max_diff, 1e-5)",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 189, "column": 4 },
          "end": { "row": 189, "column": 4 }
        }
      }
    }
  ],
  [
    "1001",
    {
      "pageContent": "def test_output(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with torch.no_grad():\n            output = model(**inputs_dict)\n\n            if isinstance(output, dict):\n                output = output.sample\n\n        self.assertIsNotNone(output)\n        expected_shape = inputs_dict[\"sample\"].shape\n        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 215, "column": 4 },
          "end": { "row": 215, "column": 4 }
        }
      }
    }
  ],
  [
    "1002",
    {
      "pageContent": "def test_forward_with_norm_groups(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"norm_num_groups\"] = 16\n        init_dict[\"block_out_channels\"] = (16, 32)\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with torch.no_grad():\n            output = model(**inputs_dict)\n\n            if isinstance(output, dict):\n                output = output.sample\n\n        self.assertIsNotNone(output)\n        expected_shape = inputs_dict[\"sample\"].shape\n        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 231, "column": 4 },
          "end": { "row": 231, "column": 4 }
        }
      }
    }
  ],
  [
    "1003",
    {
      "pageContent": "def test_forward_signature(self):\n        init_dict, _ = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        signature = inspect.signature(model.forward)\n        # signature.parameters is an OrderedDict => so arg_names order is deterministic\n        arg_names = [*signature.parameters.keys()]\n\n        expected_arg_names = [\"sample\", \"timestep\"]\n        self.assertListEqual(arg_names[:2], expected_arg_names)",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 251, "column": 4 },
          "end": { "row": 251, "column": 4 }
        }
      }
    }
  ],
  [
    "1004",
    {
      "pageContent": "def test_model_from_pretrained(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        # test if the model can be loaded from the config\n        # and has all the expected shape\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_pretrained(tmpdirname)\n            new_model = self.model_class.from_pretrained(tmpdirname)\n            new_model.to(torch_device)\n            new_model.eval()\n\n        # check if all parameters shape are the same\n        for param_name in model.state_dict().keys():\n            param_1 = model.state_dict()[param_name]\n            param_2 = new_model.state_dict()[param_name]\n            self.assertEqual(param_1.shape, param_2.shape)\n\n        with torch.no_grad():\n            output_1 = model(**inputs_dict)\n\n            if isinstance(output_1, dict):\n                output_1 = output_1.sample\n\n            output_2 = new_model(**inputs_dict)\n\n            if isinstance(output_2, dict):\n                output_2 = output_2.sample\n\n        self.assertEqual(output_1.shape, output_2.shape)",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 262, "column": 4 },
          "end": { "row": 262, "column": 4 }
        }
      }
    }
  ],
  [
    "1005",
    {
      "pageContent": "def test_outputs_equivalence(self):\n        def set_nan_tensor_to_zero(t):\n            # Temporary fallback until `aten::_index_put_impl_` is implemented in mps\n            # Track progress in https://github.com/pytorch/pytorch/issues/77764\n            device = t.device\n            if device.type == \"mps\":\n                t = t.to(\"cpu\")\n            t[t != t] = 0\n            return t.to(device)\n\n        def recursive_check(tuple_object, dict_object):\n            if isinstance(tuple_object, (List, Tuple)):\n                for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object.values()):\n                    recursive_check(tuple_iterable_value, dict_iterable_value)\n            elif isinstance(tuple_object, Dict):\n                for tuple_iterable_value, dict_iterable_value in zip(tuple_object.values(), dict_object.values()):\n                    recursive_check(tuple_iterable_value, dict_iterable_value)\n            elif tuple_object is None:\n                return\n            else:\n                self.assertTrue(\n                    torch.allclose(\n                        set_nan_tensor_to_zero(tuple_object), set_nan_tensor_to_zero(dict_object), atol=1e-5\n                    ),\n                    msg=(\n                        \"Tuple and dict output are not equal. Difference:\"\n                        f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"\n                        f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"\n                        f\" `nan`: {torch.isnan(dict_object).any()} and ",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 331, "column": 4 },
          "end": { "row": 331, "column": 4 }
        }
      }
    }
  ],
  [
    "1006",
    {
      "pageContent": "def test_deprecated_kwargs(self):\n        has_kwarg_in_model_class = \"kwargs\" in inspect.signature(self.model_class.__init__).parameters\n        has_deprecated_kwarg = len(self.model_class._deprecated_kwargs) > 0\n\n        if has_kwarg_in_model_class and not has_deprecated_kwarg:\n            raise ValueError(\n                f\"{self.model_class} has `**kwargs` in its __init__ method but has not defined any deprecated kwargs\"\n                \" under the `_deprecated_kwargs` class attribute. Make sure to either remove `**kwargs` if there are\"\n                \" no deprecated arguments or add the deprecated argument with `_deprecated_kwargs =\"\n                \" [<deprecated_argument>]`\"\n            )\n\n        if not has_kwarg_in_model_class and has_deprecated_kwarg:\n            raise ValueError(\n                f\"{self.model_class} doesn't have `**kwargs` in its __init__ method but has defined deprecated kwargs\"\n                \" under the `_deprecated_kwargs` class attribute. Make sure to either add the `**kwargs` argument to\"\n                f\" {self.model_class}.__init__ if there are deprecated arguments or remove the deprecated argument\"\n                \" from `_deprecated_kwargs = [<deprecated_argument>]`\"\n            )",
      "metadata": {
        "source": "tests/test_modeling_common.py",
        "range": {
          "start": { "row": 398, "column": 4 },
          "end": { "row": 398, "column": 4 }
        }
      }
    }
  ],
  [
    "1007",
    {
      "pageContent": "class TrainingTests(unittest.TestCase):\n    def get_model_optimizer(self, resolution=32):\n        set_seed(0)\n        model = UNet2DModel(sample_size=resolution, in_channels=3, out_channels=3)\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n        return model, optimizer\n\n    @slow\n    def test_training_step_equality(self):\n        device = \"cpu\"  # ensure full determinism without setting the CUBLAS_WORKSPACE_CONFIG env variable\n        ddpm_scheduler = DDPMScheduler(\n            num_train_timesteps=1000,\n            beta_start=0.0001,\n            beta_end=0.02,\n            beta_schedule=\"linear\",\n            clip_sample=True,\n        )\n        ddim_scheduler = DDIMScheduler(\n            num_train_timesteps=1000,\n            beta_start=0.0001,\n            beta_end=0.02,\n            beta_schedule=\"linear\",\n            clip_sample=True,\n        )\n\n        assert ddpm_scheduler.config.num_train_timesteps == ddim_scheduler.config.num_train_timesteps\n\n        # shared batches for DDPM and DDIM\n        set_seed(0)\n        clean_images = [torch.randn((4, 3, 32, 32)).clip(-1, 1).to(device) for _ in range(4)]\n        noise = [torch.randn((4, 3, 32, 32)).to(device) for _ in range(4)]\n        timesteps = [torch.randint(0, 1000, (4,)).long().to(device) for _ in range(4)]\n\n        # train with a DDPM scheduler\n        model, optimizer = self.get_model_optimizer(resolution=32)\n        model.train().to(device)\n        for i in range(4):\n            optimizer.zero_grad()\n            ddpm_noisy_images = ddpm_scheduler.add_noise(clean_images[i], noise[i], timesteps[i])\n  ",
      "metadata": {
        "source": "tests/test_training.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1008",
    {
      "pageContent": "def get_model_optimizer(self, resolution=32):\n        set_seed(0)\n        model = UNet2DModel(sample_size=resolution, in_channels=3, out_channels=3)\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n        return model, optimizer",
      "metadata": {
        "source": "tests/test_training.py",
        "range": {
          "start": { "row": 28, "column": 4 },
          "end": { "row": 28, "column": 4 }
        }
      }
    }
  ],
  [
    "1009",
    {
      "pageContent": "class DownloadTests(unittest.TestCase):\n    def test_download_only_pytorch(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            # pipeline has Flax weights\n            _ = FlaxDiffusionPipeline.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-pipe\", safety_checker=None, cache_dir=tmpdirname\n            )\n\n            all_root_files = [t[-1] for t in os.walk(os.path.join(tmpdirname, os.listdir(tmpdirname)[0], \"snapshots\"))]\n            files = [item for sublist in all_root_files for item in sublist]\n\n            # None of the downloaded files should be a PyTorch file even if we have some here:\n            # https://huggingface.co/hf-internal-testing/tiny-stable-diffusion-pipe/blob/main/unet/diffusion_pytorch_model.bin\n            assert not any(f.endswith(\".bin\") for f in files)",
      "metadata": {
        "source": "tests/test_pipelines_flax.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "1010",
    {
      "pageContent": "class FlaxPipelineTests(unittest.TestCase):\n    def test_dummy_all_tpus(self):\n        pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-pipe\", safety_checker=None\n        )\n\n        prompt = (\n            \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of\"\n            \" field, close up, split lighting, cinematic\"\n        )\n\n        prng_seed = jax.random.PRNGKey(0)\n        num_inference_steps = 4\n\n        num_samples = jax.device_count()\n        prompt = num_samples * [prompt]\n        prompt_ids = pipeline.prepare_inputs(prompt)\n\n        p_sample = pmap(pipeline.__call__, static_broadcasted_argnums=(3,))\n\n        # shard inputs and rng\n        params = replicate(params)\n        prng_seed = jax.random.split(prng_seed, num_samples)\n        prompt_ids = shard(prompt_ids)\n\n        images = p_sample(prompt_ids, params, prng_seed, num_inference_steps).images\n\n        assert images.shape == (num_samples, 1, 64, 64, 3)\n        if jax.device_count() == 8:\n            assert np.abs(np.abs(images[0, 0, :2, :2, -2:], dtype=np.float32).sum() - 3.1111548) < 1e-3\n            assert np.abs(np.abs(images, dtype=np.float32).sum() - 199746.95) < 5e-1\n\n        images_pil = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n\n        assert len(images_pil) == num_samples\n\n    def test_stable_diffusion_v1_4(self):\n        pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\", revision=\"",
      "metadata": {
        "source": "tests/test_pipelines_flax.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "1011",
    {
      "pageContent": "class FlaxUNet2DConditionModelIntegrationTests(unittest.TestCase):\n    def get_file_format(self, seed, shape):\n        return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n\n    def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n        dtype = jnp.bfloat16 if fp16 else jnp.float32\n        image = jnp.array(load_hf_numpy(self.get_file_format(seed, shape)), dtype=dtype)\n        return image\n\n    def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n        dtype = jnp.bfloat16 if fp16 else jnp.float32\n        revision = \"bf16\" if fp16 else None\n\n        model, params = FlaxUNet2DConditionModel.from_pretrained(\n            model_id, subfolder=\"unet\", dtype=dtype, revision=revision\n        )\n        return model, params\n\n    def get_encoder_hidden_states(self, seed=0, shape=(4, 77, 768), fp16=False):\n        dtype = jnp.bfloat16 if fp16 else jnp.float32\n        hidden_states = jnp.array(load_hf_numpy(self.get_file_format(seed, shape)), dtype=dtype)\n        return hidden_states\n\n    @parameterized.expand(\n        [\n            # fmt: off\n            [83, 4, [-0.2323, -0.1304, 0.0813, -0.3093, -0.0919, -0.1571, -0.1125, -0.5806]],\n            [17, 0.55, [-0.0831, -0.2443, 0.0901, -0.0919, 0.3396, 0.0103, -0.3743, 0.0701]],\n            [8, 0.89, [-0.4863, 0.0859, 0.0875, -0.1658, 0.9199, -0.0114, 0.4839, 0.4639]],\n            [3, 1000, [-0.5649, 0.2402, -0.5518, 0.1248, 1.1328, -0.2443, -0.0325, -1.0",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_flax.py",
        "range": {
          "start": { "row": 17, "column": 0 },
          "end": { "row": 17, "column": 0 }
        }
      }
    }
  ],
  [
    "1012",
    {
      "pageContent": "class FlaxAutoencoderKLTests(FlaxModelTesterMixin, unittest.TestCase):\n    model_class = FlaxAutoencoderKL\n\n    @property\n    def dummy_input(self):\n        batch_size = 4\n        num_channels = 3\n        sizes = (32, 32)\n\n        prng_key = jax.random.PRNGKey(0)\n        image = jax.random.uniform(prng_key, ((batch_size, num_channels) + sizes))\n\n        return {\"sample\": image, \"prng_key\": prng_key}\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": [32, 64],\n            \"in_channels\": 3,\n            \"out_channels\": 3,\n            \"down_block_types\": [\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            \"up_block_types\": [\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            \"latent_channels\": 4,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_vae_flax.py",
        "range": {
          "start": { "row": 14, "column": 0 },
          "end": { "row": 14, "column": 0 }
        }
      }
    }
  ],
  [
    "1013",
    {
      "pageContent": "class AutoencoderKLTests(ModelTesterMixin, unittest.TestCase):\n    model_class = AutoencoderKL\n\n    @property\n    def dummy_input(self):\n        batch_size = 4\n        num_channels = 3\n        sizes = (32, 32)\n\n        image = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)\n\n        return {\"sample\": image}\n\n    @property\n    def input_shape(self):\n        return (3, 32, 32)\n\n    @property\n    def output_shape(self):\n        return (3, 32, 32)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": [32, 64],\n            \"in_channels\": 3,\n            \"out_channels\": 3,\n            \"down_block_types\": [\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            \"up_block_types\": [\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            \"latent_channels\": 4,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_forward_signature(self):\n        pass\n\n    def test_training(self):\n        pass\n\n    def test_from_pretrained_hub(self):\n        model, loading_info = AutoencoderKL.from_pretrained(\"fusing/autoencoder-kl-dummy\", output_loading_info=True)\n        self.assertIsNotNone(model)\n        self.assertEqual(len(loading_info[\"missing_keys\"]), 0)\n\n        model.to(torch_device)\n        image = model(**self.dummy_input)\n\n        assert image is not None, \"Make sure output is not None\"\n\n    def test_output_pretrained(self):\n        model = AutoencoderKL.from_pretrained(\"fusing/autoencoder-kl-dummy\")\n        model = model.to(torch_device)\n        model.eval()\n\n        # ",
      "metadata": {
        "source": "tests/models/test_models_vae.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1014",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": [32, 64],\n            \"in_channels\": 3,\n            \"out_channels\": 3,\n            \"down_block_types\": [\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            \"up_block_types\": [\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            \"latent_channels\": 4,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_vae.py",
        "range": {
          "start": { "row": 52, "column": 4 },
          "end": { "row": 52, "column": 4 }
        }
      }
    }
  ],
  [
    "1015",
    {
      "pageContent": "def test_from_pretrained_hub(self):\n        model, loading_info = AutoencoderKL.from_pretrained(\"fusing/autoencoder-kl-dummy\", output_loading_info=True)\n        self.assertIsNotNone(model)\n        self.assertEqual(len(loading_info[\"missing_keys\"]), 0)\n\n        model.to(torch_device)\n        image = model(**self.dummy_input)\n\n        assert image is not None, \"Make sure output is not None\"",
      "metadata": {
        "source": "tests/models/test_models_vae.py",
        "range": {
          "start": { "row": 70, "column": 4 },
          "end": { "row": 70, "column": 4 }
        }
      }
    }
  ],
  [
    "1016",
    {
      "pageContent": "def test_output_pretrained(self):\n        model = AutoencoderKL.from_pretrained(\"fusing/autoencoder-kl-dummy\")\n        model = model.to(torch_device)\n        model.eval()\n\n        # One-time warmup pass (see #372)\n        if torch_device == \"mps\" and isinstance(model, ModelMixin):\n            image = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)\n            image = image.to(torch_device)\n            with torch.no_grad():\n                _ = model(image, sample_posterior=True).sample\n            generator = torch.manual_seed(0)\n        else:\n            generator = torch.Generator(device=torch_device).manual_seed(0)\n\n        image = torch.randn(\n            1,\n            model.config.in_channels,\n            model.config.sample_size,\n            model.config.sample_size,\n            generator=torch.manual_seed(0),\n        )\n        image = image.to(torch_device)\n        with torch.no_grad():\n            output = model(image, sample_posterior=True, generator=generator).sample\n\n        output_slice = output[0, -1, -3:, -3:].flatten().cpu()\n\n        # Since the VAE Gaussian prior's generator is seeded on the appropriate device,\n        # the expected output slices are not the same for CPU and GPU.\n        if torch_device == \"mps\":\n            expected_output_slice = torch.tensor(\n                [\n                    -4.0078e-01,\n                    -3.8323e-04,\n                    -1.2681e-01,\n                    -1.1462e-01,\n                    2.0095e-01,\n                    1.0893e-01,\n                    -8.8247e-02,\n        ",
      "metadata": {
        "source": "tests/models/test_models_vae.py",
        "range": {
          "start": { "row": 80, "column": 4 },
          "end": { "row": 80, "column": 4 }
        }
      }
    }
  ],
  [
    "1017",
    {
      "pageContent": "class AutoencoderKLIntegrationTests(unittest.TestCase):\n    def get_file_format(self, seed, shape):\n        return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_sd_image(self, seed=0, shape=(4, 3, 512, 512), fp16=False):\n        dtype = torch.float16 if fp16 else torch.float32\n        image = torch.from_numpy(load_hf_numpy(self.get_file_format(seed, shape))).to(torch_device).to(dtype)\n        return image\n\n    def get_sd_vae_model(self, model_id=\"CompVis/stable-diffusion-v1-4\", fp16=False):\n        revision = \"fp16\" if fp16 else None\n        torch_dtype = torch.float16 if fp16 else torch.float32\n\n        model = AutoencoderKL.from_pretrained(\n            model_id,\n            subfolder=\"vae\",\n            torch_dtype=torch_dtype,\n            revision=revision,\n        )\n        model.to(torch_device).eval()\n\n        return model\n\n    def get_generator(self, seed=0):\n        if torch_device == \"mps\":\n            return torch.manual_seed(seed)\n        return torch.Generator(device=torch_device).manual_seed(seed)\n\n    @parameterized.expand(\n        [\n            # fmt: off\n            [33, [-0.1603, 0.9878, -0.0495, -0.0790, -0.2709, 0.8375, -0.2060, -0.0824], [-0.2395, 0.0098, 0.0102, -0.0709, -0.2840, -0.0274, -0.0718, -0.1824]],\n            [47, [-0.2376, 0.1168, 0.1332, -0.4840, -0.2508, -0.0791, -0.0493, -0.4089], [0.0350, 0.0847, 0.0467, 0.0344, -0.0842, -0.0547, -0.0633, -0.1131]],\n  ",
      "metadata": {
        "source": "tests/models/test_models_vae.py",
        "range": {
          "start": { "row": 137, "column": 0 },
          "end": { "row": 137, "column": 0 }
        }
      }
    }
  ],
  [
    "1018",
    {
      "pageContent": "class UNet1DModelTests(ModelTesterMixin, unittest.TestCase):\n    model_class = UNet1DModel\n\n    @property\n    def dummy_input(self):\n        batch_size = 4\n        num_features = 14\n        seq_len = 16\n\n        noise = floats_tensor((batch_size, num_features, seq_len)).to(torch_device)\n        time_step = torch.tensor([10] * batch_size).to(torch_device)\n\n        return {\"sample\": noise, \"timestep\": time_step}\n\n    @property\n    def input_shape(self):\n        return (4, 14, 16)\n\n    @property\n    def output_shape(self):\n        return (4, 14, 16)\n\n    def test_ema_training(self):\n        pass\n\n    def test_training(self):\n        pass\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_determinism(self):\n        super().test_determinism()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_outputs_equivalence(self):\n        super().test_outputs_equivalence()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_from_save_pretrained(self):\n        super().test_from_save_pretrained()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_from_save_pretrained_variant(self):\n        super().test_from_save_pretrained_variant()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_model_from_pretrained(self):\n        super().test_model_from_pretrained()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_output(self):\n        super().test_output()\n\n    def prepar",
      "metadata": {
        "source": "tests/models/test_models_unet_1d.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1019",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": (32, 64, 128, 256),\n            \"in_channels\": 14,\n            \"out_channels\": 14,\n            \"time_embedding_type\": \"positional\",\n            \"use_timestep_embedding\": True,\n            \"flip_sin_to_cos\": False,\n            \"freq_shift\": 1.0,\n            \"out_block_type\": \"OutConv1DBlock\",\n            \"mid_block_type\": \"MidResTemporalBlock1D\",\n            \"down_block_types\": (\"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\"),\n            \"up_block_types\": (\"UpResnetBlock1D\", \"UpResnetBlock1D\", \"UpResnetBlock1D\"),\n            \"act_fn\": \"mish\",\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_unet_1d.py",
        "range": {
          "start": { "row": 80, "column": 4 },
          "end": { "row": 80, "column": 4 }
        }
      }
    }
  ],
  [
    "1020",
    {
      "pageContent": "class UNetRLModelTests(ModelTesterMixin, unittest.TestCase):\n    model_class = UNet1DModel\n\n    @property\n    def dummy_input(self):\n        batch_size = 4\n        num_features = 14\n        seq_len = 16\n\n        noise = floats_tensor((batch_size, num_features, seq_len)).to(torch_device)\n        time_step = torch.tensor([10] * batch_size).to(torch_device)\n\n        return {\"sample\": noise, \"timestep\": time_step}\n\n    @property\n    def input_shape(self):\n        return (4, 14, 16)\n\n    @property\n    def output_shape(self):\n        return (4, 14, 1)\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_determinism(self):\n        super().test_determinism()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_outputs_equivalence(self):\n        super().test_outputs_equivalence()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_from_save_pretrained(self):\n        super().test_from_save_pretrained()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_from_save_pretrained_variant(self):\n        super().test_from_save_pretrained_variant()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_model_from_pretrained(self):\n        super().test_model_from_pretrained()\n\n    @unittest.skipIf(torch_device == \"mps\", \"mish op not supported in MPS\")\n    def test_output(self):\n        # UNetRL is a value-function is different output shape\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_co",
      "metadata": {
        "source": "tests/models/test_models_unet_1d.py",
        "range": {
          "start": { "row": 158, "column": 0 },
          "end": { "row": 158, "column": 0 }
        }
      }
    }
  ],
  [
    "1021",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"in_channels\": 14,\n            \"out_channels\": 14,\n            \"down_block_types\": [\"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\"],\n            \"up_block_types\": [],\n            \"out_block_type\": \"ValueFunction\",\n            \"mid_block_type\": \"ValueFunctionMidBlock1D\",\n            \"block_out_channels\": [32, 64, 128, 256],\n            \"layers_per_block\": 1,\n            \"downsample_each_block\": True,\n            \"use_timestep_embedding\": True,\n            \"freq_shift\": 1.0,\n            \"flip_sin_to_cos\": False,\n            \"time_embedding_type\": \"positional\",\n            \"act_fn\": \"mish\",\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_unet_1d.py",
        "range": {
          "start": { "row": 224, "column": 4 },
          "end": { "row": 224, "column": 4 }
        }
      }
    }
  ],
  [
    "1022",
    {
      "pageContent": "def create_lora_layers(model):\n    lora_attn_procs = {}\n    for name in model.attn_processors.keys():\n        cross_attention_dim = None if name.endswith(\"attn1.processor\") else model.config.cross_attention_dim\n        if name.startswith(\"mid_block\"):\n            hidden_size = model.config.block_out_channels[-1]\n        elif name.startswith(\"up_blocks\"):\n            block_id = int(name[len(\"up_blocks.\")])\n            hidden_size = list(reversed(model.config.block_out_channels))[block_id]\n        elif name.startswith(\"down_blocks\"):\n            block_id = int(name[len(\"down_blocks.\")])\n            hidden_size = model.config.block_out_channels[block_id]\n\n        lora_attn_procs[name] = LoRACrossAttnProcessor(\n            hidden_size=hidden_size, cross_attention_dim=cross_attention_dim\n        )\n        lora_attn_procs[name] = lora_attn_procs[name].to(model.device)\n\n        # add 1 to weights to mock trained weights\n        with torch.no_grad():\n            lora_attn_procs[name].to_q_lora.up.weight += 1\n            lora_attn_procs[name].to_k_lora.up.weight += 1\n            lora_attn_procs[name].to_v_lora.up.weight += 1\n            lora_attn_procs[name].to_out_lora.up.weight += 1\n\n    return lora_attn_procs",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 43, "column": 0 },
          "end": { "row": 43, "column": 0 }
        }
      }
    }
  ],
  [
    "1023",
    {
      "pageContent": "class UNet2DConditionModelTests(ModelTesterMixin, unittest.TestCase):\n    model_class = UNet2DConditionModel\n\n    @property\n    def dummy_input(self):\n        batch_size = 4\n        num_channels = 4\n        sizes = (32, 32)\n\n        noise = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)\n        time_step = torch.tensor([10]).to(torch_device)\n        encoder_hidden_states = floats_tensor((batch_size, 4, 32)).to(torch_device)\n\n        return {\"sample\": noise, \"timestep\": time_step, \"encoder_hidden_states\": encoder_hidden_states}\n\n    @property\n    def input_shape(self):\n        return (4, 32, 32)\n\n    @property\n    def output_shape(self):\n        return (4, 32, 32)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": (32, 64),\n            \"down_block_types\": (\"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n            \"up_block_types\": (\"UpBlock2D\", \"CrossAttnUpBlock2D\"),\n            \"cross_attention_dim\": 32,\n            \"attention_head_dim\": 8,\n            \"out_channels\": 4,\n            \"in_channels\": 4,\n            \"layers_per_block\": 2,\n            \"sample_size\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    @unittest.skipIf(\n        torch_device != \"cuda\" or not is_xformers_available(),\n        reason=\"XFormers attention is only available with CUDA and `xformers` installed\",\n    )\n    def test_xformers_enable_works(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n        model = self.model_class(**init_dict)\n\n     ",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 71, "column": 0 },
          "end": { "row": 71, "column": 0 }
        }
      }
    }
  ],
  [
    "1024",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": (32, 64),\n            \"down_block_types\": (\"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n            \"up_block_types\": (\"UpBlock2D\", \"CrossAttnUpBlock2D\"),\n            \"cross_attention_dim\": 32,\n            \"attention_head_dim\": 8,\n            \"out_channels\": 4,\n            \"in_channels\": 4,\n            \"layers_per_block\": 2,\n            \"sample_size\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 94, "column": 4 },
          "end": { "row": 94, "column": 4 }
        }
      }
    }
  ],
  [
    "1025",
    {
      "pageContent": "def test_model_with_attention_head_dim_tuple(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with torch.no_grad():\n            output = model(**inputs_dict)\n\n            if isinstance(output, dict):\n                output = output.sample\n\n        self.assertIsNotNone(output)\n        expected_shape = inputs_dict[\"sample\"].shape\n        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 164, "column": 4 },
          "end": { "row": 164, "column": 4 }
        }
      }
    }
  ],
  [
    "1026",
    {
      "pageContent": "def test_model_with_use_linear_projection(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"use_linear_projection\"] = True\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with torch.no_grad():\n            output = model(**inputs_dict)\n\n            if isinstance(output, dict):\n                output = output.sample\n\n        self.assertIsNotNone(output)\n        expected_shape = inputs_dict[\"sample\"].shape\n        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "1027",
    {
      "pageContent": "def test_model_attention_slicing(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        model.set_attention_slice(\"auto\")\n        with torch.no_grad():\n            output = model(**inputs_dict)\n        assert output is not None\n\n        model.set_attention_slice(\"max\")\n        with torch.no_grad():\n            output = model(**inputs_dict)\n        assert output is not None\n\n        model.set_attention_slice(2)\n        with torch.no_grad():\n            output = model(**inputs_dict)\n        assert output is not None",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 202, "column": 4 },
          "end": { "row": 202, "column": 4 }
        }
      }
    }
  ],
  [
    "1028",
    {
      "pageContent": "def test_model_slicable_head_dim(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        model = self.model_class(**init_dict)\n\n        def check_slicable_dim_attr(module: torch.nn.Module):\n            if hasattr(module, \"set_attention_slice\"):\n                assert isinstance(module.sliceable_head_dim, int)\n\n            for child in module.children():\n                check_slicable_dim_attr(child)\n\n        # retrieve number of attention layers\n        for module in model.children():\n            check_slicable_dim_attr(module)",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "1029",
    {
      "pageContent": "def test_special_attn_proc(self):\n        class AttnEasyProc(torch.nn.Module):\n            def __init__(self, num):\n                super().__init__()\n                self.weight = torch.nn.Parameter(torch.tensor(num))\n                self.is_run = False\n                self.number = 0\n                self.counter = 0\n\n            def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, number=None):\n                batch_size, sequence_length, _ = hidden_states.shape\n                attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n                query = attn.to_q(hidden_states)\n\n                encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n                key = attn.to_k(encoder_hidden_states)\n                value = attn.to_v(encoder_hidden_states)\n\n                query = attn.head_to_batch_dim(query)\n                key = attn.head_to_batch_dim(key)\n                value = attn.head_to_batch_dim(value)\n\n                attention_probs = attn.get_attention_scores(query, key, attention_mask)\n                hidden_states = torch.bmm(attention_probs, value)\n                hidden_states = attn.batch_to_head_dim(hidden_states)\n\n                # linear proj\n                hidden_states = attn.to_out[0](hidden_states)\n                # dropout\n                hidden_states = attn.to_out[1](hidden_states)\n\n                hidden_states += self.weight\n\n                self.is_run = True\n                self.counter += 1\n                self.number",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 244, "column": 4 },
          "end": { "row": 244, "column": 4 }
        }
      }
    }
  ],
  [
    "1030",
    {
      "pageContent": "def test_lora_processors(self):\n        # enable deterministic behavior for gradient checkpointing\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n\n        with torch.no_grad():\n            sample1 = model(**inputs_dict).sample\n\n        lora_attn_procs = {}\n        for name in model.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else model.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = model.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(model.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = model.config.block_out_channels[block_id]\n\n            lora_attn_procs[name] = LoRACrossAttnProcessor(\n                hidden_size=hidden_size, cross_attention_dim=cross_attention_dim\n            )\n\n            # add 1 to weights to mock trained weights\n            with torch.no_grad():\n                lora_attn_procs[name].to_q_lora.up.weight += 1\n                lora_attn_procs[name].to_k_lora.up.weight += 1\n                lora_attn_procs[name].to_v_lora.up.weight += 1\n                lora_attn_procs[name].to_out_lora.up.weight += 1\n\n        # make sure we can set a lis",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 301, "column": 4 },
          "end": { "row": 301, "column": 4 }
        }
      }
    }
  ],
  [
    "1031",
    {
      "pageContent": "def test_lora_save_load(self):\n        # enable deterministic behavior for gradient checkpointing\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        torch.manual_seed(0)\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n\n        with torch.no_grad():\n            old_sample = model(**inputs_dict).sample\n\n        lora_attn_procs = create_lora_layers(model)\n        model.set_attn_processor(lora_attn_procs)\n\n        with torch.no_grad():\n            sample = model(**inputs_dict, cross_attention_kwargs={\"scale\": 0.5}).sample\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_attn_procs(tmpdirname)\n            self.assertTrue(os.path.isfile(os.path.join(tmpdirname, \"pytorch_lora_weights.bin\")))\n            torch.manual_seed(0)\n            new_model = self.model_class(**init_dict)\n            new_model.to(torch_device)\n            new_model.load_attn_procs(tmpdirname)\n\n        with torch.no_grad():\n            new_sample = new_model(**inputs_dict, cross_attention_kwargs={\"scale\": 0.5}).sample\n\n        assert (sample - new_sample).abs().max() < 1e-4\n\n        # LoRA and no LoRA should NOT be the same\n        assert (sample - old_sample).abs().max() > 1e-4",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 354, "column": 4 },
          "end": { "row": 354, "column": 4 }
        }
      }
    }
  ],
  [
    "1032",
    {
      "pageContent": "def test_lora_save_load_safetensors(self):\n        # enable deterministic behavior for gradient checkpointing\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        torch.manual_seed(0)\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n\n        with torch.no_grad():\n            old_sample = model(**inputs_dict).sample\n\n        lora_attn_procs = {}\n        for name in model.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else model.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = model.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(model.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = model.config.block_out_channels[block_id]\n\n            lora_attn_procs[name] = LoRACrossAttnProcessor(\n                hidden_size=hidden_size, cross_attention_dim=cross_attention_dim\n            )\n            lora_attn_procs[name] = lora_attn_procs[name].to(model.device)\n\n            # add 1 to weights to mock trained weights\n            with torch.no_grad():\n                lora_attn_procs[name].to_q_lora.up.weight += 1\n                lora_attn_procs[name].to_k_lora.up.weight += 1\n                lora_attn_procs[name].to_v_lora",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 389, "column": 4 },
          "end": { "row": 389, "column": 4 }
        }
      }
    }
  ],
  [
    "1033",
    {
      "pageContent": "def test_lora_save_load_safetensors_load_torch(self):\n        # enable deterministic behavior for gradient checkpointing\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        torch.manual_seed(0)\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n\n        lora_attn_procs = {}\n        for name in model.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else model.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = model.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(model.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = model.config.block_out_channels[block_id]\n\n            lora_attn_procs[name] = LoRACrossAttnProcessor(\n                hidden_size=hidden_size, cross_attention_dim=cross_attention_dim\n            )\n            lora_attn_procs[name] = lora_attn_procs[name].to(model.device)\n\n        model.set_attn_processor(lora_attn_procs)\n        # Saving as torch, properly reloads with directly filename\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_attn_procs(tmpdirname)\n            self.assertTrue(os.path.isfile(os.path.join(tmpdirname, \"pytorch_lora_weights.bin\")))\n            torc",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 447, "column": 4 },
          "end": { "row": 447, "column": 4 }
        }
      }
    }
  ],
  [
    "1034",
    {
      "pageContent": "def test_lora_on_off(self):\n        # enable deterministic behavior for gradient checkpointing\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        init_dict[\"attention_head_dim\"] = (8, 16)\n\n        torch.manual_seed(0)\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n\n        with torch.no_grad():\n            old_sample = model(**inputs_dict).sample\n\n        lora_attn_procs = create_lora_layers(model)\n        model.set_attn_processor(lora_attn_procs)\n\n        with torch.no_grad():\n            sample = model(**inputs_dict, cross_attention_kwargs={\"scale\": 0.0}).sample\n\n        model.set_attn_processor(CrossAttnProcessor())\n\n        with torch.no_grad():\n            new_sample = model(**inputs_dict).sample\n\n        assert (sample - new_sample).abs().max() < 1e-4\n        assert (sample - old_sample).abs().max() < 1e-4",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 484, "column": 4 },
          "end": { "row": 484, "column": 4 }
        }
      }
    }
  ],
  [
    "1035",
    {
      "pageContent": "class UNet2DConditionModelIntegrationTests(unittest.TestCase):\n    def get_file_format(self, seed, shape):\n        return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n        dtype = torch.float16 if fp16 else torch.float32\n        image = torch.from_numpy(load_hf_numpy(self.get_file_format(seed, shape))).to(torch_device).to(dtype)\n        return image\n\n    def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n        revision = \"fp16\" if fp16 else None\n        torch_dtype = torch.float16 if fp16 else torch.float32\n\n        model = UNet2DConditionModel.from_pretrained(\n            model_id, subfolder=\"unet\", torch_dtype=torch_dtype, revision=revision\n        )\n        model.to(torch_device).eval()\n\n        return model\n\n    def test_set_attention_slice_auto(self):\n        torch.cuda.empty_cache()\n        torch.cuda.reset_max_memory_allocated()\n        torch.cuda.reset_peak_memory_stats()\n\n        unet = self.get_unet_model()\n        unet.set_attention_slice(\"auto\")\n\n        latents = self.get_latents(33)\n        encoder_hidden_states = self.get_encoder_hidden_states(33)\n        timestep = 1\n\n        with torch.no_grad():\n            _ = unet(latents, timestep=timestep, encoder_hidden_states=encoder_hidden_states).sample\n\n        mem_bytes = torch.cuda.max_memory_allocated()\n\n        assert mem_bytes < 5 *",
      "metadata": {
        "source": "tests/models/test_models_unet_2d_condition.py",
        "range": {
          "start": { "row": 542, "column": 0 },
          "end": { "row": 542, "column": 0 }
        }
      }
    }
  ],
  [
    "1036",
    {
      "pageContent": "class Unet2DModelTests(ModelTesterMixin, unittest.TestCase):\n    model_class = UNet2DModel\n\n    @property\n    def dummy_input(self):\n        batch_size = 4\n        num_channels = 3\n        sizes = (32, 32)\n\n        noise = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)\n        time_step = torch.tensor([10]).to(torch_device)\n\n        return {\"sample\": noise, \"timestep\": time_step}\n\n    @property\n    def input_shape(self):\n        return (3, 32, 32)\n\n    @property\n    def output_shape(self):\n        return (3, 32, 32)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": (32, 64),\n            \"down_block_types\": (\"DownBlock2D\", \"AttnDownBlock2D\"),\n            \"up_block_types\": (\"AttnUpBlock2D\", \"UpBlock2D\"),\n            \"attention_head_dim\": None,\n            \"out_channels\": 3,\n            \"in_channels\": 3,\n            \"layers_per_block\": 2,\n            \"sample_size\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "1037",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": (32, 64),\n            \"down_block_types\": (\"DownBlock2D\", \"AttnDownBlock2D\"),\n            \"up_block_types\": (\"AttnUpBlock2D\", \"UpBlock2D\"),\n            \"attention_head_dim\": None,\n            \"out_channels\": 3,\n            \"in_channels\": 3,\n            \"layers_per_block\": 2,\n            \"sample_size\": 32,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 54, "column": 4 },
          "end": { "row": 54, "column": 4 }
        }
      }
    }
  ],
  [
    "1038",
    {
      "pageContent": "class UNetLDMModelTests(ModelTesterMixin, unittest.TestCase):\n    model_class = UNet2DModel\n\n    @property\n    def dummy_input(self):\n        batch_size = 4\n        num_channels = 4\n        sizes = (32, 32)\n\n        noise = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)\n        time_step = torch.tensor([10]).to(torch_device)\n\n        return {\"sample\": noise, \"timestep\": time_step}\n\n    @property\n    def input_shape(self):\n        return (4, 32, 32)\n\n    @property\n    def output_shape(self):\n        return (4, 32, 32)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"sample_size\": 32,\n            \"in_channels\": 4,\n            \"out_channels\": 4,\n            \"layers_per_block\": 2,\n            \"block_out_channels\": (32, 64),\n            \"attention_head_dim\": 32,\n            \"down_block_types\": (\"DownBlock2D\", \"DownBlock2D\"),\n            \"up_block_types\": (\"UpBlock2D\", \"UpBlock2D\"),\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_from_pretrained_hub(self):\n        model, loading_info = UNet2DModel.from_pretrained(\"fusing/unet-ldm-dummy-update\", output_loading_info=True)\n\n        self.assertIsNotNone(model)\n        self.assertEqual(len(loading_info[\"missing_keys\"]), 0)\n\n        model.to(torch_device)\n        image = model(**self.dummy_input).sample\n\n        assert image is not None, \"Make sure output is not None\"\n\n    @unittest.skipIf(torch_device != \"cuda\", \"This test is supposed to run on GPU\")\n    def test_from_pretrained_accelerate(self):\n        model, _ = UNet2D",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 69, "column": 0 },
          "end": { "row": 69, "column": 0 }
        }
      }
    }
  ],
  [
    "1039",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"sample_size\": 32,\n            \"in_channels\": 4,\n            \"out_channels\": 4,\n            \"layers_per_block\": 2,\n            \"block_out_channels\": (32, 64),\n            \"attention_head_dim\": 32,\n            \"down_block_types\": (\"DownBlock2D\", \"DownBlock2D\"),\n            \"up_block_types\": (\"UpBlock2D\", \"UpBlock2D\"),\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 91, "column": 4 },
          "end": { "row": 91, "column": 4 }
        }
      }
    }
  ],
  [
    "1040",
    {
      "pageContent": "def test_from_pretrained_hub(self):\n        model, loading_info = UNet2DModel.from_pretrained(\"fusing/unet-ldm-dummy-update\", output_loading_info=True)\n\n        self.assertIsNotNone(model)\n        self.assertEqual(len(loading_info[\"missing_keys\"]), 0)\n\n        model.to(torch_device)\n        image = model(**self.dummy_input).sample\n\n        assert image is not None, \"Make sure output is not None\"",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 105, "column": 4 },
          "end": { "row": 105, "column": 4 }
        }
      }
    }
  ],
  [
    "1041",
    {
      "pageContent": "def test_output_pretrained(self):\n        model = UNet2DModel.from_pretrained(\"fusing/unet-ldm-dummy-update\")\n        model.eval()\n        model.to(torch_device)\n\n        noise = torch.randn(\n            1,\n            model.config.in_channels,\n            model.config.sample_size,\n            model.config.sample_size,\n            generator=torch.manual_seed(0),\n        )\n        noise = noise.to(torch_device)\n        time_step = torch.tensor([10] * noise.shape[0]).to(torch_device)\n\n        with torch.no_grad():\n            output = model(noise, time_step).sample\n\n        output_slice = output[0, -1, -3:, -3:].flatten().cpu()\n        # fmt: off\n        expected_output_slice = torch.tensor([-13.3258, -20.1100, -15.9873, -17.6617, -23.0596, -17.9419, -13.3675, -16.1889, -12.3800])\n        # fmt: on\n\n        self.assertTrue(torch_all_close(output_slice, expected_output_slice, rtol=1e-3))",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 184, "column": 4 },
          "end": { "row": 184, "column": 4 }
        }
      }
    }
  ],
  [
    "1042",
    {
      "pageContent": "class NCSNppModelTests(ModelTesterMixin, unittest.TestCase):\n    model_class = UNet2DModel\n\n    @property\n    def dummy_input(self, sizes=(32, 32)):\n        batch_size = 4\n        num_channels = 3\n\n        noise = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)\n        time_step = torch.tensor(batch_size * [10]).to(dtype=torch.int32, device=torch_device)\n\n        return {\"sample\": noise, \"timestep\": time_step}\n\n    @property\n    def input_shape(self):\n        return (3, 32, 32)\n\n    @property\n    def output_shape(self):\n        return (3, 32, 32)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": [32, 64, 64, 64],\n            \"in_channels\": 3,\n            \"layers_per_block\": 1,\n            \"out_channels\": 3,\n            \"time_embedding_type\": \"fourier\",\n            \"norm_eps\": 1e-6,\n            \"mid_block_scale_factor\": math.sqrt(2.0),\n            \"norm_num_groups\": None,\n            \"down_block_types\": [\n                \"SkipDownBlock2D\",\n                \"AttnSkipDownBlock2D\",\n                \"SkipDownBlock2D\",\n                \"SkipDownBlock2D\",\n            ],\n            \"up_block_types\": [\n                \"SkipUpBlock2D\",\n                \"SkipUpBlock2D\",\n                \"AttnSkipUpBlock2D\",\n                \"SkipUpBlock2D\",\n            ],\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    @slow\n    def test_from_pretrained_hub(self):\n        model, loading_info = UNet2DModel.from_pretrained(\"google/ncsnpp-celebahq-256\", output_loading_info=True)\n       ",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 210, "column": 0 },
          "end": { "row": 210, "column": 0 }
        }
      }
    }
  ],
  [
    "1043",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": [32, 64, 64, 64],\n            \"in_channels\": 3,\n            \"layers_per_block\": 1,\n            \"out_channels\": 3,\n            \"time_embedding_type\": \"fourier\",\n            \"norm_eps\": 1e-6,\n            \"mid_block_scale_factor\": math.sqrt(2.0),\n            \"norm_num_groups\": None,\n            \"down_block_types\": [\n                \"SkipDownBlock2D\",\n                \"AttnSkipDownBlock2D\",\n                \"SkipDownBlock2D\",\n                \"SkipDownBlock2D\",\n            ],\n            \"up_block_types\": [\n                \"SkipUpBlock2D\",\n                \"SkipUpBlock2D\",\n                \"AttnSkipUpBlock2D\",\n                \"SkipUpBlock2D\",\n            ],\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 231, "column": 4 },
          "end": { "row": 231, "column": 4 }
        }
      }
    }
  ],
  [
    "1044",
    {
      "pageContent": "def test_output_pretrained_ve_large(self):\n        model = UNet2DModel.from_pretrained(\"fusing/ncsnpp-ffhq-ve-dummy-update\")\n        model.to(torch_device)\n\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        batch_size = 4\n        num_channels = 3\n        sizes = (32, 32)\n\n        noise = torch.ones((batch_size, num_channels) + sizes).to(torch_device)\n        time_step = torch.tensor(batch_size * [1e-4]).to(torch_device)\n\n        with torch.no_grad():\n            output = model(noise, time_step).sample\n\n        output_slice = output[0, -3:, -3:, -1].flatten().cpu()\n        # fmt: off\n        expected_output_slice = torch.tensor([-0.0325, -0.0900, -0.0869, -0.0332, -0.0725, -0.0270, -0.0101, 0.0227, 0.0256])\n        # fmt: on\n\n        self.assertTrue(torch_all_close(output_slice, expected_output_slice, rtol=1e-2))",
      "metadata": {
        "source": "tests/models/test_models_unet_2d.py",
        "range": {
          "start": { "row": 297, "column": 4 },
          "end": { "row": 297, "column": 4 }
        }
      }
    }
  ],
  [
    "1045",
    {
      "pageContent": "class VQModelTests(ModelTesterMixin, unittest.TestCase):\n    model_class = VQModel\n\n    @property\n    def dummy_input(self, sizes=(32, 32)):\n        batch_size = 4\n        num_channels = 3\n\n        image = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)\n\n        return {\"sample\": image}\n\n    @property\n    def input_shape(self):\n        return (3, 32, 32)\n\n    @property\n    def output_shape(self):\n        return (3, 32, 32)\n\n    def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": [32, 64],\n            \"in_channels\": 3,\n            \"out_channels\": 3,\n            \"down_block_types\": [\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            \"up_block_types\": [\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            \"latent_channels\": 3,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict\n\n    def test_forward_signature(self):\n        pass\n\n    def test_training(self):\n        pass\n\n    def test_from_pretrained_hub(self):\n        model, loading_info = VQModel.from_pretrained(\"fusing/vqgan-dummy\", output_loading_info=True)\n        self.assertIsNotNone(model)\n        self.assertEqual(len(loading_info[\"missing_keys\"]), 0)\n\n        model.to(torch_device)\n        image = model(**self.dummy_input)\n\n        assert image is not None, \"Make sure output is not None\"\n\n    def test_output_pretrained(self):\n        model = VQModel.from_pretrained(\"fusing/vqgan-dummy\")\n        model.to(torch_device).eval()\n\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torc",
      "metadata": {
        "source": "tests/models/test_models_vq.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1046",
    {
      "pageContent": "def prepare_init_args_and_inputs_for_common(self):\n        init_dict = {\n            \"block_out_channels\": [32, 64],\n            \"in_channels\": 3,\n            \"out_channels\": 3,\n            \"down_block_types\": [\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            \"up_block_types\": [\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            \"latent_channels\": 3,\n        }\n        inputs_dict = self.dummy_input\n        return init_dict, inputs_dict",
      "metadata": {
        "source": "tests/models/test_models_vq.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "1047",
    {
      "pageContent": "def test_from_pretrained_hub(self):\n        model, loading_info = VQModel.from_pretrained(\"fusing/vqgan-dummy\", output_loading_info=True)\n        self.assertIsNotNone(model)\n        self.assertEqual(len(loading_info[\"missing_keys\"]), 0)\n\n        model.to(torch_device)\n        image = model(**self.dummy_input)\n\n        assert image is not None, \"Make sure output is not None\"",
      "metadata": {
        "source": "tests/models/test_models_vq.py",
        "range": {
          "start": { "row": 66, "column": 4 },
          "end": { "row": 66, "column": 4 }
        }
      }
    }
  ],
  [
    "1048",
    {
      "pageContent": "def test_output_pretrained(self):\n        model = VQModel.from_pretrained(\"fusing/vqgan-dummy\")\n        model.to(torch_device).eval()\n\n        torch.manual_seed(0)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(0)\n\n        image = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)\n        image = image.to(torch_device)\n        with torch.no_grad():\n            # Warmup pass when using mps (see #372)\n            if torch_device == \"mps\":\n                _ = model(image)\n            output = model(image).sample\n\n        output_slice = output[0, -1, -3:, -3:].flatten().cpu()\n        # fmt: off\n        expected_output_slice = torch.tensor([-0.0153, -0.4044, -0.1880, -0.5161, -0.2418, -0.4072, -0.1612, -0.0633, -0.0143])\n        # fmt: on\n        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))",
      "metadata": {
        "source": "tests/models/test_models_vq.py",
        "range": {
          "start": { "row": 76, "column": 4 },
          "end": { "row": 76, "column": 4 }
        }
      }
    }
  ],
  [
    "1049",
    {
      "pageContent": "class ConfigTester(unittest.TestCase):\n    def test_outputs_single_attribute(self):\n        outputs = CustomOutput(images=np.random.rand(1, 3, 4, 4))\n\n        # check every way of getting the attribute\n        assert isinstance(outputs.images, np.ndarray)\n        assert outputs.images.shape == (1, 3, 4, 4)\n        assert isinstance(outputs[\"images\"], np.ndarray)\n        assert outputs[\"images\"].shape == (1, 3, 4, 4)\n        assert isinstance(outputs[0], np.ndarray)\n        assert outputs[0].shape == (1, 3, 4, 4)\n\n        # test with a non-tensor attribute\n        outputs = CustomOutput(images=[PIL.Image.new(\"RGB\", (4, 4))])\n\n        # check every way of getting the attribute\n        assert isinstance(outputs.images, list)\n        assert isinstance(outputs.images[0], PIL.Image.Image)\n        assert isinstance(outputs[\"images\"], list)\n        assert isinstance(outputs[\"images\"][0], PIL.Image.Image)\n        assert isinstance(outputs[0], list)\n        assert isinstance(outputs[0][0], PIL.Image.Image)\n\n    def test_outputs_dict_init(self):\n        # test output reinitialization with a `dict` for compatibility with `accelerate`\n        outputs = CustomOutput({\"images\": np.random.rand(1, 3, 4, 4)})\n\n        # check every way of getting the attribute\n        assert isinstance(outputs.images, np.ndarray)\n        assert outputs.images.shape == (1, 3, 4, 4)\n        assert isinstance(outputs[\"images\"], np.ndarray)\n        assert outputs[\"images\"].shape == (1, 3, 4, 4)\n        assert isinstance(outputs[0], np.ndarray)\n        assert outputs[0].shape == (1, 3, 4, 4)\n\n        # test with ",
      "metadata": {
        "source": "tests/test_outputs.py",
        "range": {
          "start": { "row": 15, "column": 0 },
          "end": { "row": 15, "column": 0 }
        }
      }
    }
  ],
  [
    "1050",
    {
      "pageContent": "def test_outputs_single_attribute(self):\n        outputs = CustomOutput(images=np.random.rand(1, 3, 4, 4))\n\n        # check every way of getting the attribute\n        assert isinstance(outputs.images, np.ndarray)\n        assert outputs.images.shape == (1, 3, 4, 4)\n        assert isinstance(outputs[\"images\"], np.ndarray)\n        assert outputs[\"images\"].shape == (1, 3, 4, 4)\n        assert isinstance(outputs[0], np.ndarray)\n        assert outputs[0].shape == (1, 3, 4, 4)\n\n        # test with a non-tensor attribute\n        outputs = CustomOutput(images=[PIL.Image.new(\"RGB\", (4, 4))])\n\n        # check every way of getting the attribute\n        assert isinstance(outputs.images, list)\n        assert isinstance(outputs.images[0], PIL.Image.Image)\n        assert isinstance(outputs[\"images\"], list)\n        assert isinstance(outputs[\"images\"][0], PIL.Image.Image)\n        assert isinstance(outputs[0], list)\n        assert isinstance(outputs[0][0], PIL.Image.Image)",
      "metadata": {
        "source": "tests/test_outputs.py",
        "range": {
          "start": { "row": 16, "column": 4 },
          "end": { "row": 16, "column": 4 }
        }
      }
    }
  ],
  [
    "1051",
    {
      "pageContent": "def test_outputs_dict_init(self):\n        # test output reinitialization with a `dict` for compatibility with `accelerate`\n        outputs = CustomOutput({\"images\": np.random.rand(1, 3, 4, 4)})\n\n        # check every way of getting the attribute\n        assert isinstance(outputs.images, np.ndarray)\n        assert outputs.images.shape == (1, 3, 4, 4)\n        assert isinstance(outputs[\"images\"], np.ndarray)\n        assert outputs[\"images\"].shape == (1, 3, 4, 4)\n        assert isinstance(outputs[0], np.ndarray)\n        assert outputs[0].shape == (1, 3, 4, 4)\n\n        # test with a non-tensor attribute\n        outputs = CustomOutput({\"images\": [PIL.Image.new(\"RGB\", (4, 4))]})\n\n        # check every way of getting the attribute\n        assert isinstance(outputs.images, list)\n        assert isinstance(outputs.images[0], PIL.Image.Image)\n        assert isinstance(outputs[\"images\"], list)\n        assert isinstance(outputs[\"images\"][0], PIL.Image.Image)\n        assert isinstance(outputs[0], list)\n        assert isinstance(outputs[0][0], PIL.Image.Image)",
      "metadata": {
        "source": "tests/test_outputs.py",
        "range": {
          "start": { "row": 38, "column": 4 },
          "end": { "row": 38, "column": 4 }
        }
      }
    }
  ],
  [
    "1052",
    {
      "pageContent": "class UNetBlockTesterMixin:\n    @property\n    def dummy_input(self):\n        return self.get_dummy_input()\n\n    @property\n    def output_shape(self):\n        if self.block_type == \"down\":\n            return (4, 32, 16, 16)\n        elif self.block_type == \"mid\":\n            return (4, 32, 32, 32)\n        elif self.block_type == \"up\":\n            return (4, 32, 64, 64)\n\n        raise ValueError(f\"'{self.block_type}' is not a supported block_type. Set it to 'up', 'mid', or 'down'.\")\n\n    def get_dummy_input(\n        self,\n        include_temb=True,\n        include_res_hidden_states_tuple=False,\n        include_encoder_hidden_states=False,\n        include_skip_sample=False,\n    ):\n        batch_size = 4\n        num_channels = 32\n        sizes = (32, 32)\n\n        generator = torch.manual_seed(0)\n        device = torch.device(torch_device)\n        shape = (batch_size, num_channels) + sizes\n        hidden_states = randn_tensor(shape, generator=generator, device=device)\n        dummy_input = {\"hidden_states\": hidden_states}\n\n        if include_temb:\n            temb_channels = 128\n            dummy_input[\"temb\"] = randn_tensor((batch_size, temb_channels), generator=generator, device=device)\n\n        if include_res_hidden_states_tuple:\n            generator_1 = torch.manual_seed(1)\n            dummy_input[\"res_hidden_states_tuple\"] = (randn_tensor(shape, generator=generator_1, device=device),)\n\n        if include_encoder_hidden_states:\n            dummy_input[\"encoder_hidden_states\"] = floats_tensor((batch_size, 32, 32)).to(torch_device)\n\n        if include_skip_sample:\n            ",
      "metadata": {
        "source": "tests/test_unet_blocks_common.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1053",
    {
      "pageContent": "def update_version_in_file(fname, version, pattern):\n    \"\"\"Update the version in one file using a specific pattern.\"\"\"\n    with open(fname, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        code = f.read()\n    re_pattern, replace = REPLACE_PATTERNS[pattern]\n    replace = replace.replace(\"VERSION\", version)\n    code = re_pattern.sub(replace, code)\n    with open(fname, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        f.write(code)",
      "metadata": {
        "source": "utils/release.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "1054",
    {
      "pageContent": "def update_version_in_examples(version):\n    \"\"\"Update the version in all examples files.\"\"\"\n    for folder, directories, fnames in os.walk(PATH_TO_EXAMPLES):\n        # Removing some of the folders with non-actively maintained examples from the walk\n        if \"research_projects\" in directories:\n            directories.remove(\"research_projects\")\n        if \"legacy\" in directories:\n            directories.remove(\"legacy\")\n        for fname in fnames:\n            if fname.endswith(\".py\"):\n                update_version_in_file(os.path.join(folder, fname), version, pattern=\"examples\")",
      "metadata": {
        "source": "utils/release.py",
        "range": {
          "start": { "row": 47, "column": 0 },
          "end": { "row": 47, "column": 0 }
        }
      }
    }
  ],
  [
    "1055",
    {
      "pageContent": "def global_version_update(version, patch=False):\n    \"\"\"Update the version in all needed files.\"\"\"\n    for pattern, fname in REPLACE_FILES.items():\n        update_version_in_file(fname, version, pattern)\n    if not patch:\n        update_version_in_examples(version)",
      "metadata": {
        "source": "utils/release.py",
        "range": {
          "start": { "row": 60, "column": 0 },
          "end": { "row": 60, "column": 0 }
        }
      }
    }
  ],
  [
    "1056",
    {
      "pageContent": "def clean_main_ref_in_model_list():\n    \"\"\"Replace the links from main doc tp stable doc in the model list of the README.\"\"\"\n    # If the introduction or the conclusion of the list change, the prompts may need to be updated.\n    _start_prompt = \" Transformers currently provides the following architectures\"\n    _end_prompt = \"1. Want to contribute a new model?\"\n    with open(README_FILE, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        lines = f.readlines()\n\n    # Find the start of the list.\n    start_index = 0\n    while not lines[start_index].startswith(_start_prompt):\n        start_index += 1\n    start_index += 1\n\n    index = start_index\n    # Update the lines in the model list.\n    while not lines[index].startswith(_end_prompt):\n        if lines[index].startswith(\"1.\"):\n            lines[index] = lines[index].replace(\n                \"https://huggingface.co/docs/diffusers/main/model_doc\",\n                \"https://huggingface.co/docs/diffusers/model_doc\",\n            )\n        index += 1\n\n    with open(README_FILE, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        f.writelines(lines)",
      "metadata": {
        "source": "utils/release.py",
        "range": {
          "start": { "row": 68, "column": 0 },
          "end": { "row": 68, "column": 0 }
        }
      }
    }
  ],
  [
    "1057",
    {
      "pageContent": "def get_version():\n    \"\"\"Reads the current version in the __init__.\"\"\"\n    with open(REPLACE_FILES[\"init\"], \"r\") as f:\n        code = f.read()\n    default_version = REPLACE_PATTERNS[\"init\"][0].search(code).groups()[0]\n    return packaging.version.parse(default_version)",
      "metadata": {
        "source": "utils/release.py",
        "range": {
          "start": { "row": 96, "column": 0 },
          "end": { "row": 96, "column": 0 }
        }
      }
    }
  ],
  [
    "1058",
    {
      "pageContent": "def pre_release_work(patch=False):\n    \"\"\"Do all the necessary pre-release steps.\"\"\"\n    # First let's get the default version: base version if we are in dev, bump minor otherwise.\n    default_version = get_version()\n    if patch and default_version.is_devrelease:\n        raise ValueError(\"Can't create a patch version from the dev branch, checkout a released version!\")\n    if default_version.is_devrelease:\n        default_version = default_version.base_version\n    elif patch:\n        default_version = f\"{default_version.major}.{default_version.minor}.{default_version.micro + 1}\"\n    else:\n        default_version = f\"{default_version.major}.{default_version.minor + 1}.0\"\n\n    # Now let's ask nicely if that's the right one.\n    version = input(f\"Which version are you releasing? [{default_version}]\")\n    if len(version) == 0:\n        version = default_version\n\n    print(f\"Updating version to {version}.\")\n    global_version_update(version, patch=patch)",
      "metadata": {
        "source": "utils/release.py",
        "range": {
          "start": { "row": 104, "column": 0 },
          "end": { "row": 104, "column": 0 }
        }
      }
    }
  ],
  [
    "1059",
    {
      "pageContent": "def post_release_work():\n    \"\"\"Do all the necesarry post-release steps.\"\"\"\n    # First let's get the current version\n    current_version = get_version()\n    dev_version = f\"{current_version.major}.{current_version.minor + 1}.0.dev0\"\n    current_version = current_version.base_version\n\n    # Check with the user we got that right.\n    version = input(f\"Which version are we developing now? [{dev_version}]\")\n    if len(version) == 0:\n        version = dev_version\n\n    print(f\"Updating version to {version}.\")\n    global_version_update(version)",
      "metadata": {
        "source": "utils/release.py",
        "range": {
          "start": { "row": 131, "column": 0 },
          "end": { "row": 131, "column": 0 }
        }
      }
    }
  ],
  [
    "1060",
    {
      "pageContent": "def _find_text_in_file(filename, start_prompt, end_prompt):\n    \"\"\"\n    Find the text in `filename` between a line beginning with `start_prompt` and before `end_prompt`, removing empty\n    lines.\n    \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        lines = f.readlines()\n    # Find the start prompt.\n    start_index = 0\n    while not lines[start_index].startswith(start_prompt):\n        start_index += 1\n    start_index += 1\n\n    end_index = start_index\n    while not lines[end_index].startswith(end_prompt):\n        end_index += 1\n    end_index -= 1\n\n    while len(lines[start_index]) <= 1:\n        start_index += 1\n    while len(lines[end_index]) <= 1:\n        end_index -= 1\n    end_index += 1\n    return \"\".join(lines[start_index:end_index]), start_index, end_index, lines",
      "metadata": {
        "source": "utils/check_table.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "1061",
    {
      "pageContent": "def camel_case_split(identifier):\n    \"Split a camelcased `identifier` into words.\"\n    matches = re.finditer(\".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\", identifier)\n    return [m.group(0) for m in matches]",
      "metadata": {
        "source": "utils/check_table.py",
        "range": {
          "start": { "row": 74, "column": 0 },
          "end": { "row": 74, "column": 0 }
        }
      }
    }
  ],
  [
    "1062",
    {
      "pageContent": "def _center_text(text, width):\n    text_length = 2 if text == \"\" or text == \"\" else len(text)\n    left_indent = (width - text_length) // 2\n    right_indent = width - text_length - left_indent\n    return \" \" * left_indent + text + \" \" * right_indent",
      "metadata": {
        "source": "utils/check_table.py",
        "range": {
          "start": { "row": 80, "column": 0 },
          "end": { "row": 80, "column": 0 }
        }
      }
    }
  ],
  [
    "1063",
    {
      "pageContent": "def get_model_table_from_auto_modules():\n    \"\"\"Generates an up-to-date model table from the content of the auto modules.\"\"\"\n    # Dictionary model names to config.\n    config_mapping_names = diffusers_module.models.auto.configuration_auto.CONFIG_MAPPING_NAMES\n    model_name_to_config = {\n        name: config_mapping_names[code]\n        for code, name in diffusers_module.MODEL_NAMES_MAPPING.items()\n        if code in config_mapping_names\n    }\n    model_name_to_prefix = {name: config.replace(\"ConfigMixin\", \"\") for name, config in model_name_to_config.items()}\n\n    # Dictionaries flagging if each model prefix has a slow/fast tokenizer, backend in PT/TF/Flax.\n    slow_tokenizers = collections.defaultdict(bool)\n    fast_tokenizers = collections.defaultdict(bool)\n    pt_models = collections.defaultdict(bool)\n    tf_models = collections.defaultdict(bool)\n    flax_models = collections.defaultdict(bool)\n\n    # Let's lookup through all diffusers object (once).\n    for attr_name in dir(diffusers_module):\n        lookup_dict = None\n        if attr_name.endswith(\"Tokenizer\"):\n            lookup_dict = slow_tokenizers\n            attr_name = attr_name[:-9]\n        elif attr_name.endswith(\"TokenizerFast\"):\n            lookup_dict = fast_tokenizers\n            attr_name = attr_name[:-13]\n        elif _re_tf_models.match(attr_name) is not None:\n            lookup_dict = tf_models\n            attr_name = _re_tf_models.match(attr_name).groups()[0]\n        elif _re_flax_models.match(attr_name) is not None:\n            lookup_dict = flax_models\n            attr_name = _re_flax_models.match(at",
      "metadata": {
        "source": "utils/check_table.py",
        "range": {
          "start": { "row": 87, "column": 0 },
          "end": { "row": 87, "column": 0 }
        }
      }
    }
  ],
  [
    "1064",
    {
      "pageContent": "def check_model_table(overwrite=False):\n    \"\"\"Check the model table in the index.rst is consistent with the state of the lib and maybe `overwrite`.\"\"\"\n    current_table, start_index, end_index, lines = _find_text_in_file(\n        filename=os.path.join(PATH_TO_DOCS, \"index.mdx\"),\n        start_prompt=\"<!--This table is updated automatically from the auto modules\",\n        end_prompt=\"<!-- End table-->\",\n    )\n    new_table = get_model_table_from_auto_modules()\n\n    if current_table != new_table:\n        if overwrite:\n            with open(os.path.join(PATH_TO_DOCS, \"index.mdx\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n                f.writelines(lines[:start_index] + [new_table] + lines[end_index:])\n        else:\n            raise ValueError(\n                \"The model table in the `index.mdx` has not been updated. Run `make fix-copies` to fix this.\"\n            )",
      "metadata": {
        "source": "utils/check_table.py",
        "range": {
          "start": { "row": 160, "column": 0 },
          "end": { "row": 160, "column": 0 }
        }
      }
    }
  ],
  [
    "1065",
    {
      "pageContent": "def check_model_list():\n    \"\"\"Check the model list inside the transformers library.\"\"\"\n    # Get the models from the directory structure of `src/diffusers/models/`\n    models_dir = os.path.join(PATH_TO_DIFFUSERS, \"models\")\n    _models = []\n    for model in os.listdir(models_dir):\n        model_dir = os.path.join(models_dir, model)\n        if os.path.isdir(model_dir) and \"__init__.py\" in os.listdir(model_dir):\n            _models.append(model)\n\n    # Get the models from the directory structure of `src/transformers/models/`\n    models = [model for model in dir(diffusers.models) if not model.startswith(\"__\")]\n\n    missing_models = sorted(list(set(_models).difference(models)))\n    if missing_models:\n        raise Exception(\n            f\"The following models should be included in {models_dir}/__init__.py: {','.join(missing_models)}.\"\n        )",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 208, "column": 0 },
          "end": { "row": 208, "column": 0 }
        }
      }
    }
  ],
  [
    "1066",
    {
      "pageContent": "def get_model_modules():\n    \"\"\"Get the model modules inside the transformers library.\"\"\"\n    _ignore_modules = [\n        \"modeling_auto\",\n        \"modeling_encoder_decoder\",\n        \"modeling_marian\",\n        \"modeling_mmbt\",\n        \"modeling_outputs\",\n        \"modeling_retribert\",\n        \"modeling_utils\",\n        \"modeling_flax_auto\",\n        \"modeling_flax_encoder_decoder\",\n        \"modeling_flax_utils\",\n        \"modeling_speech_encoder_decoder\",\n        \"modeling_flax_speech_encoder_decoder\",\n        \"modeling_flax_vision_encoder_decoder\",\n        \"modeling_transfo_xl_utilities\",\n        \"modeling_tf_auto\",\n        \"modeling_tf_encoder_decoder\",\n        \"modeling_tf_outputs\",\n        \"modeling_tf_pytorch_utils\",\n        \"modeling_tf_utils\",\n        \"modeling_tf_transfo_xl_utilities\",\n        \"modeling_tf_vision_encoder_decoder\",\n        \"modeling_vision_encoder_decoder\",\n    ]\n    modules = []\n    for model in dir(diffusers.models):\n        # There are some magic dunder attributes in the dir, we ignore them\n        if not model.startswith(\"__\"):\n            model_module = getattr(diffusers.models, model)\n            for submodule in dir(model_module):\n                if submodule.startswith(\"modeling\") and submodule not in _ignore_modules:\n                    modeling_module = getattr(model_module, submodule)\n                    if inspect.ismodule(modeling_module):\n                        modules.append(modeling_module)\n    return modules",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 230, "column": 0 },
          "end": { "row": 230, "column": 0 }
        }
      }
    }
  ],
  [
    "1067",
    {
      "pageContent": "def get_models(module, include_pretrained=False):\n    \"\"\"Get the objects in module that are models.\"\"\"\n    models = []\n    model_classes = (diffusers.ModelMixin, diffusers.TFModelMixin, diffusers.FlaxModelMixin)\n    for attr_name in dir(module):\n        if not include_pretrained and (\"Pretrained\" in attr_name or \"PreTrained\" in attr_name):\n            continue\n        attr = getattr(module, attr_name)\n        if isinstance(attr, type) and issubclass(attr, model_classes) and attr.__module__ == module.__name__:\n            models.append((attr_name, attr))\n    return models",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 269, "column": 0 },
          "end": { "row": 269, "column": 0 }
        }
      }
    }
  ],
  [
    "1068",
    {
      "pageContent": "def is_a_private_model(model):\n    \"\"\"Returns True if the model should not be in the main init.\"\"\"\n    if model in PRIVATE_MODELS:\n        return True\n\n    # Wrapper, Encoder and Decoder are all privates\n    if model.endswith(\"Wrapper\"):\n        return True\n    if model.endswith(\"Encoder\"):\n        return True\n    if model.endswith(\"Decoder\"):\n        return True\n    return False",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 282, "column": 0 },
          "end": { "row": 282, "column": 0 }
        }
      }
    }
  ],
  [
    "1069",
    {
      "pageContent": "def check_models_are_in_init():\n    \"\"\"Checks all models defined in the library are in the main init.\"\"\"\n    models_not_in_init = []\n    dir_transformers = dir(diffusers)\n    for module in get_model_modules():\n        models_not_in_init += [\n            model[0] for model in get_models(module, include_pretrained=True) if model[0] not in dir_transformers\n        ]\n\n    # Remove private models\n    models_not_in_init = [model for model in models_not_in_init if not is_a_private_model(model)]\n    if len(models_not_in_init) > 0:\n        raise Exception(f\"The following models should be in the main init: {','.join(models_not_in_init)}.\")",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 297, "column": 0 },
          "end": { "row": 297, "column": 0 }
        }
      }
    }
  ],
  [
    "1070",
    {
      "pageContent": "def get_model_test_files():\n    \"\"\"Get the model test files.\n\n    The returned files should NOT contain the `tests` (i.e. `PATH_TO_TESTS` defined in this script). They will be\n    considered as paths relative to `tests`. A caller has to use `os.path.join(PATH_TO_TESTS, ...)` to access the files.\n    \"\"\"\n\n    _ignore_files = [\n        \"test_modeling_common\",\n        \"test_modeling_encoder_decoder\",\n        \"test_modeling_flax_encoder_decoder\",\n        \"test_modeling_flax_speech_encoder_decoder\",\n        \"test_modeling_marian\",\n        \"test_modeling_tf_common\",\n        \"test_modeling_tf_encoder_decoder\",\n    ]\n    test_files = []\n    # Check both `PATH_TO_TESTS` and `PATH_TO_TESTS/models`\n    model_test_root = os.path.join(PATH_TO_TESTS, \"models\")\n    model_test_dirs = []\n    for x in os.listdir(model_test_root):\n        x = os.path.join(model_test_root, x)\n        if os.path.isdir(x):\n            model_test_dirs.append(x)\n\n    for target_dir in [PATH_TO_TESTS] + model_test_dirs:\n        for file_or_dir in os.listdir(target_dir):\n            path = os.path.join(target_dir, file_or_dir)\n            if os.path.isfile(path):\n                filename = os.path.split(path)[-1]\n                if \"test_modeling\" in filename and os.path.splitext(filename)[0] not in _ignore_files:\n                    file = os.path.join(*path.split(os.sep)[1:])\n                    test_files.append(file)\n\n    return test_files",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 314, "column": 0 },
          "end": { "row": 314, "column": 0 }
        }
      }
    }
  ],
  [
    "1071",
    {
      "pageContent": "def find_tested_models(test_file):\n    \"\"\"Parse the content of test_file to detect what's in all_model_classes\"\"\"\n    # This is a bit hacky but I didn't find a way to import the test_file as a module and read inside the class\n    with open(os.path.join(PATH_TO_TESTS, test_file), \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        content = f.read()\n    all_models = re.findall(r\"all_model_classes\\s+=\\s+\\(\\s*\\(([^\\)]*)\\)\", content)\n    # Check with one less parenthesis as well\n    all_models += re.findall(r\"all_model_classes\\s+=\\s+\\(([^\\)]*)\\)\", content)\n    if len(all_models) > 0:\n        model_tested = []\n        for entry in all_models:\n            for line in entry.split(\",\"):\n                name = line.strip()\n                if len(name) > 0:\n                    model_tested.append(name)\n        return model_tested",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 353, "column": 0 },
          "end": { "row": 353, "column": 0 }
        }
      }
    }
  ],
  [
    "1072",
    {
      "pageContent": "def check_models_are_tested(module, test_file):\n    \"\"\"Check models defined in module are tested in test_file.\"\"\"\n    # XxxModelMixin are not tested\n    defined_models = get_models(module)\n    tested_models = find_tested_models(test_file)\n    if tested_models is None:\n        if test_file.replace(os.path.sep, \"/\") in TEST_FILES_WITH_NO_COMMON_TESTS:\n            return\n        return [\n            f\"{test_file} should define `all_model_classes` to apply common tests to the models it tests. \"\n            + \"If this intentional, add the test filename to `TEST_FILES_WITH_NO_COMMON_TESTS` in the file \"\n            + \"`utils/check_repo.py`.\"\n        ]\n    failures = []\n    for model_name, _ in defined_models:\n        if model_name not in tested_models and model_name not in IGNORE_NON_TESTED:\n            failures.append(\n                f\"{model_name} is defined in {module.__name__} but is not tested in \"\n                + f\"{os.path.join(PATH_TO_TESTS, test_file)}. Add it to the all_model_classes in that file.\"\n                + \"If common tests should not applied to that model, add its name to `IGNORE_NON_TESTED`\"\n                + \"in the file `utils/check_repo.py`.\"\n            )\n    return failures",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 371, "column": 0 },
          "end": { "row": 371, "column": 0 }
        }
      }
    }
  ],
  [
    "1073",
    {
      "pageContent": "def check_all_models_are_tested():\n    \"\"\"Check all models are properly tested.\"\"\"\n    modules = get_model_modules()\n    test_files = get_model_test_files()\n    failures = []\n    for module in modules:\n        test_file = [file for file in test_files if f\"test_{module.__name__.split('.')[-1]}.py\" in file]\n        if len(test_file) == 0:\n            failures.append(f\"{module.__name__} does not have its corresponding test file {test_file}.\")\n        elif len(test_file) > 1:\n            failures.append(f\"{module.__name__} has several test files: {test_file}.\")\n        else:\n            test_file = test_file[0]\n            new_failures = check_models_are_tested(module, test_file)\n            if new_failures is not None:\n                failures += new_failures\n    if len(failures) > 0:\n        raise Exception(f\"There were {len(failures)} failures:\\n\" + \"\\n\".join(failures))",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 396, "column": 0 },
          "end": { "row": 396, "column": 0 }
        }
      }
    }
  ],
  [
    "1074",
    {
      "pageContent": "def get_all_auto_configured_models():\n    \"\"\"Return the list of all models in at least one auto class.\"\"\"\n    result = set()  # To avoid duplicates we concatenate all model classes in a set.\n    if is_torch_available():\n        for attr_name in dir(diffusers.models.auto.modeling_auto):\n            if attr_name.startswith(\"MODEL_\") and attr_name.endswith(\"MAPPING_NAMES\"):\n                result = result | set(get_values(getattr(diffusers.models.auto.modeling_auto, attr_name)))\n    if is_tf_available():\n        for attr_name in dir(diffusers.models.auto.modeling_tf_auto):\n            if attr_name.startswith(\"TF_MODEL_\") and attr_name.endswith(\"MAPPING_NAMES\"):\n                result = result | set(get_values(getattr(diffusers.models.auto.modeling_tf_auto, attr_name)))\n    if is_flax_available():\n        for attr_name in dir(diffusers.models.auto.modeling_flax_auto):\n            if attr_name.startswith(\"FLAX_MODEL_\") and attr_name.endswith(\"MAPPING_NAMES\"):\n                result = result | set(get_values(getattr(diffusers.models.auto.modeling_flax_auto, attr_name)))\n    return [cls for cls in result]",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 416, "column": 0 },
          "end": { "row": 416, "column": 0 }
        }
      }
    }
  ],
  [
    "1075",
    {
      "pageContent": "def ignore_unautoclassed(model_name):\n    \"\"\"Rules to determine if `name` should be in an auto class.\"\"\"\n    # Special white list\n    if model_name in IGNORE_NON_AUTO_CONFIGURED:\n        return True\n    # Encoder and Decoder should be ignored\n    if \"Encoder\" in model_name or \"Decoder\" in model_name:\n        return True\n    return False",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 434, "column": 0 },
          "end": { "row": 434, "column": 0 }
        }
      }
    }
  ],
  [
    "1076",
    {
      "pageContent": "def check_models_are_auto_configured(module, all_auto_models):\n    \"\"\"Check models defined in module are each in an auto class.\"\"\"\n    defined_models = get_models(module)\n    failures = []\n    for model_name, _ in defined_models:\n        if model_name not in all_auto_models and not ignore_unautoclassed(model_name):\n            failures.append(\n                f\"{model_name} is defined in {module.__name__} but is not present in any of the auto mapping. \"\n                \"If that is intended behavior, add its name to `IGNORE_NON_AUTO_CONFIGURED` in the file \"\n                \"`utils/check_repo.py`.\"\n            )\n    return failures",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 445, "column": 0 },
          "end": { "row": 445, "column": 0 }
        }
      }
    }
  ],
  [
    "1077",
    {
      "pageContent": "def check_all_models_are_auto_configured():\n    \"\"\"Check all models are each in an auto class.\"\"\"\n    missing_backends = []\n    if not is_torch_available():\n        missing_backends.append(\"PyTorch\")\n    if not is_tf_available():\n        missing_backends.append(\"TensorFlow\")\n    if not is_flax_available():\n        missing_backends.append(\"Flax\")\n    if len(missing_backends) > 0:\n        missing = \", \".join(missing_backends)\n        if os.getenv(\"TRANSFORMERS_IS_CI\", \"\").upper() in ENV_VARS_TRUE_VALUES:\n            raise Exception(\n                \"Full quality checks require all backends to be installed (with `pip install -e .[dev]` in the \"\n                f\"Transformers repo, the following are missing: {missing}.\"\n            )\n        else:\n            warnings.warn(\n                \"Full quality checks require all backends to be installed (with `pip install -e .[dev]` in the \"\n                f\"Transformers repo, the following are missing: {missing}. While it's probably fine as long as you \"\n                \"didn't make any change in one of those backends modeling files, you should probably execute the \"\n                \"command above to be on the safe side.\"\n            )\n    modules = get_model_modules()\n    all_auto_models = get_all_auto_configured_models()\n    failures = []\n    for module in modules:\n        new_failures = check_models_are_auto_configured(module, all_auto_models)\n        if new_failures is not None:\n            failures += new_failures\n    if len(failures) > 0:\n        raise Exception(f\"There were {len(failures)} failures:\\n\" + \"\\n\".join(failures))",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 459, "column": 0 },
          "end": { "row": 459, "column": 0 }
        }
      }
    }
  ],
  [
    "1078",
    {
      "pageContent": "def check_decorator_order(filename):\n    \"\"\"Check that in the test file `filename` the slow decorator is always last.\"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        lines = f.readlines()\n    decorator_before = None\n    errors = []\n    for i, line in enumerate(lines):\n        search = _re_decorator.search(line)\n        if search is not None:\n            decorator_name = search.groups()[0]\n            if decorator_before is not None and decorator_name.startswith(\"parameterized\"):\n                errors.append(i)\n            decorator_before = decorator_name\n        elif decorator_before is not None:\n            decorator_before = None\n    return errors",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 496, "column": 0 },
          "end": { "row": 496, "column": 0 }
        }
      }
    }
  ],
  [
    "1079",
    {
      "pageContent": "def check_all_decorator_order():\n    \"\"\"Check that in all test files, the slow decorator is always last.\"\"\"\n    errors = []\n    for fname in os.listdir(PATH_TO_TESTS):\n        if fname.endswith(\".py\"):\n            filename = os.path.join(PATH_TO_TESTS, fname)\n            new_errors = check_decorator_order(filename)\n            errors += [f\"- {filename}, line {i}\" for i in new_errors]\n    if len(errors) > 0:\n        msg = \"\\n\".join(errors)\n        raise ValueError(\n            \"The parameterized decorator (and its variants) should always be first, but this is not the case in the\"\n            f\" following files:\\n{msg}\"\n        )",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 514, "column": 0 },
          "end": { "row": 514, "column": 0 }
        }
      }
    }
  ],
  [
    "1080",
    {
      "pageContent": "def find_all_documented_objects():\n    \"\"\"Parse the content of all doc files to detect which classes and functions it documents\"\"\"\n    documented_obj = []\n    for doc_file in Path(PATH_TO_DOC).glob(\"**/*.rst\"):\n        with open(doc_file, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            content = f.read()\n        raw_doc_objs = re.findall(r\"(?:autoclass|autofunction):: transformers.(\\S+)\\s+\", content)\n        documented_obj += [obj.split(\".\")[-1] for obj in raw_doc_objs]\n    for doc_file in Path(PATH_TO_DOC).glob(\"**/*.mdx\"):\n        with open(doc_file, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            content = f.read()\n        raw_doc_objs = re.findall(\"\\[\\[autodoc\\]\\]\\s+(\\S+)\\s+\", content)\n        documented_obj += [obj.split(\".\")[-1] for obj in raw_doc_objs]\n    return documented_obj",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 530, "column": 0 },
          "end": { "row": 530, "column": 0 }
        }
      }
    }
  ],
  [
    "1081",
    {
      "pageContent": "def ignore_undocumented(name):\n    \"\"\"Rules to determine if `name` should be undocumented.\"\"\"\n    # NOT DOCUMENTED ON PURPOSE.\n    # Constants uppercase are not documented.\n    if name.isupper():\n        return True\n    # ModelMixins / Encoders / Decoders / Layers / Embeddings / Attention are not documented.\n    if (\n        name.endswith(\"ModelMixin\")\n        or name.endswith(\"Decoder\")\n        or name.endswith(\"Encoder\")\n        or name.endswith(\"Layer\")\n        or name.endswith(\"Embeddings\")\n        or name.endswith(\"Attention\")\n    ):\n        return True\n    # Submodules are not documented.\n    if os.path.isdir(os.path.join(PATH_TO_DIFFUSERS, name)) or os.path.isfile(\n        os.path.join(PATH_TO_DIFFUSERS, f\"{name}.py\")\n    ):\n        return True\n    # All load functions are not documented.\n    if name.startswith(\"load_tf\") or name.startswith(\"load_pytorch\"):\n        return True\n    # is_xxx_available functions are not documented.\n    if name.startswith(\"is_\") and name.endswith(\"_available\"):\n        return True\n    # Deprecated objects are not documented.\n    if name in DEPRECATED_OBJECTS or name in UNDOCUMENTED_OBJECTS:\n        return True\n    # MMBT model does not really work.\n    if name.startswith(\"MMBT\"):\n        return True\n    if name in SHOULD_HAVE_THEIR_OWN_PAGE:\n        return True\n    return False",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 620, "column": 0 },
          "end": { "row": 620, "column": 0 }
        }
      }
    }
  ],
  [
    "1082",
    {
      "pageContent": "def check_all_objects_are_documented():\n    \"\"\"Check all models are properly documented.\"\"\"\n    documented_objs = find_all_documented_objects()\n    modules = diffusers._modules\n    objects = [c for c in dir(diffusers) if c not in modules and not c.startswith(\"_\")]\n    undocumented_objs = [c for c in objects if c not in documented_objs and not ignore_undocumented(c)]\n    if len(undocumented_objs) > 0:\n        raise Exception(\n            \"The following objects are in the public init so should be documented:\\n - \"\n            + \"\\n - \".join(undocumented_objs)\n        )\n    check_docstrings_are_in_md()\n    check_model_type_doc_match()",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 658, "column": 0 },
          "end": { "row": 658, "column": 0 }
        }
      }
    }
  ],
  [
    "1083",
    {
      "pageContent": "def check_model_type_doc_match():\n    \"\"\"Check all doc pages have a corresponding model type.\"\"\"\n    model_doc_folder = Path(PATH_TO_DOC) / \"model_doc\"\n    model_docs = [m.stem for m in model_doc_folder.glob(\"*.mdx\")]\n\n    model_types = list(diffusers.models.auto.configuration_auto.MODEL_NAMES_MAPPING.keys())\n    model_types = [MODEL_TYPE_TO_DOC_MAPPING[m] if m in MODEL_TYPE_TO_DOC_MAPPING else m for m in model_types]\n\n    errors = []\n    for m in model_docs:\n        if m not in model_types and m != \"auto\":\n            close_matches = get_close_matches(m, model_types)\n            error_message = f\"{m} is not a proper model identifier.\"\n            if len(close_matches) > 0:\n                close_matches = \"/\".join(close_matches)\n                error_message += f\" Did you mean {close_matches}?\"\n            errors.append(error_message)\n\n    if len(errors) > 0:\n        raise ValueError(\n            \"Some model doc pages do not match any existing model type:\\n\"\n            + \"\\n\".join(errors)\n            + \"\\nYou can add any missing model type to the `MODEL_NAMES_MAPPING` constant in \"\n            \"models/auto/configuration_auto.py.\"\n        )",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 673, "column": 0 },
          "end": { "row": 673, "column": 0 }
        }
      }
    }
  ],
  [
    "1084",
    {
      "pageContent": "def is_rst_docstring(docstring):\n    \"\"\"\n    Returns `True` if `docstring` is written in rst.\n    \"\"\"\n    if _re_rst_special_words.search(docstring) is not None:\n        return True\n    if _re_double_backquotes.search(docstring) is not None:\n        return True\n    if _re_rst_example.search(docstring) is not None:\n        return True\n    return False",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 708, "column": 0 },
          "end": { "row": 708, "column": 0 }
        }
      }
    }
  ],
  [
    "1085",
    {
      "pageContent": "def check_docstrings_are_in_md():\n    \"\"\"Check all docstrings are in md\"\"\"\n    files_with_rst = []\n    for file in Path(PATH_TO_DIFFUSERS).glob(\"**/*.py\"):\n        with open(file, \"r\") as f:\n            code = f.read()\n        docstrings = code.split('\"\"\"')\n\n        for idx, docstring in enumerate(docstrings):\n            if idx % 2 == 0 or not is_rst_docstring(docstring):\n                continue\n            files_with_rst.append(file)\n            break\n\n    if len(files_with_rst) > 0:\n        raise ValueError(\n            \"The following files have docstrings written in rst:\\n\"\n            + \"\\n\".join([f\"- {f}\" for f in files_with_rst])\n            + \"\\nTo fix this run `doc-builder convert path_to_py_file` after installing `doc-builder`\\n\"\n            \"(`pip install git+https://github.com/huggingface/doc-builder`)\"\n        )",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 721, "column": 0 },
          "end": { "row": 721, "column": 0 }
        }
      }
    }
  ],
  [
    "1086",
    {
      "pageContent": "def check_repo_quality():\n    \"\"\"Check all models are properly tested and documented.\"\"\"\n    print(\"Checking all models are included.\")\n    check_model_list()\n    print(\"Checking all models are public.\")\n    check_models_are_in_init()\n    print(\"Checking all models are properly tested.\")\n    check_all_decorator_order()\n    check_all_models_are_tested()\n    print(\"Checking all objects are properly documented.\")\n    check_all_objects_are_documented()\n    print(\"Checking all models are in at least one auto class.\")\n    check_all_models_are_auto_configured()",
      "metadata": {
        "source": "utils/check_repo.py",
        "range": {
          "start": { "row": 744, "column": 0 },
          "end": { "row": 744, "column": 0 }
        }
      }
    }
  ],
  [
    "1087",
    {
      "pageContent": "def main():\n    g = Github(os.environ[\"GITHUB_TOKEN\"])\n    repo = g.get_repo(\"huggingface/diffusers\")\n    open_issues = repo.get_issues(state=\"open\")\n\n    for issue in open_issues:\n        comments = sorted([comment for comment in issue.get_comments()], key=lambda i: i.created_at, reverse=True)\n        last_comment = comments[0] if len(comments) > 0 else None\n        if (\n            last_comment is not None\n            and last_comment.user.login == \"github-actions[bot]\"\n            and (dt.utcnow() - issue.updated_at).days > 7\n            and (dt.utcnow() - issue.created_at).days >= 30\n            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())\n        ):\n            # Closes the issue after 7 days of inactivity since the Stalebot notification.\n            issue.edit(state=\"closed\")\n        elif (\n            \"stale\" in issue.get_labels()\n            and last_comment is not None\n            and last_comment.user.login != \"github-actions[bot]\"\n        ):\n            # Opens the issue if someone other than Stalebot commented.\n            issue.edit(state=\"open\")\n            issue.remove_from_labels(\"stale\")\n        elif (\n            (dt.utcnow() - issue.updated_at).days > 23\n            and (dt.utcnow() - issue.created_at).days >= 30\n            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())\n        ):\n            # Post a Stalebot notification after 23 days of inactivity.\n            issue.create_comment(\n                \"This issue has been automatically marked as stale because it has not had \"\n    ",
      "metadata": {
        "source": "utils/stale.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1088",
    {
      "pageContent": "def overwrite_file(file, class_name, test_name, correct_line, done_test):\n    _id = f\"{file}_{class_name}_{test_name}\"\n    done_test[_id] += 1\n\n    with open(file, \"r\") as f:\n        lines = f.readlines()\n\n    class_regex = f\"class {class_name}(\"\n    test_regex = f\"{4 * ' '}def {test_name}(\"\n    line_begin_regex = f\"{8 * ' '}{correct_line.split()[0]}\"\n    another_line_begin_regex = f\"{16 * ' '}{correct_line.split()[0]}\"\n    in_class = False\n    in_func = False\n    in_line = False\n    insert_line = False\n    count = 0\n    spaces = 0\n\n    new_lines = []\n    for line in lines:\n        if line.startswith(class_regex):\n            in_class = True\n        elif in_class and line.startswith(test_regex):\n            in_func = True\n        elif in_class and in_func and (line.startswith(line_begin_regex) or line.startswith(another_line_begin_regex)):\n            spaces = len(line.split(correct_line.split()[0])[0])\n            count += 1\n\n            if count == done_test[_id]:\n                in_line = True\n\n        if in_class and in_func and in_line:\n            if \")\" not in line:\n                continue\n            else:\n                insert_line = True\n\n        if in_class and in_func and in_line and insert_line:\n            new_lines.append(f\"{spaces * ' '}{correct_line}\")\n            in_class = in_func = in_line = insert_line = False\n        else:\n            new_lines.append(line)\n\n    with open(file, \"w\") as f:\n        for line in new_lines:\n            f.write(line)",
      "metadata": {
        "source": "utils/overwrite_expected_slice.py",
        "range": {
          "start": { "row": 18, "column": 0 },
          "end": { "row": 18, "column": 0 }
        }
      }
    }
  ],
  [
    "1089",
    {
      "pageContent": "def main(correct, fail=None):\n    if fail is not None:\n        with open(fail, \"r\") as f:\n            test_failures = set([l.strip() for l in f.readlines()])\n    else:\n        test_failures = None\n\n    with open(correct, \"r\") as f:\n        correct_lines = f.readlines()\n\n    done_tests = defaultdict(int)\n    for line in correct_lines:\n        file, class_name, test_name, correct_line = line.split(\";\")\n        if test_failures is None or \"::\".join([file, class_name, test_name]) in test_failures:\n            overwrite_file(file, class_name, test_name, correct_line, done_tests)",
      "metadata": {
        "source": "utils/overwrite_expected_slice.py",
        "range": {
          "start": { "row": 66, "column": 0 },
          "end": { "row": 66, "column": 0 }
        }
      }
    }
  ],
  [
    "1090",
    {
      "pageContent": "def check_config_docstrings_have_checkpoints():\n    configs_without_checkpoint = []\n\n    for config_class in list(CONFIG_MAPPING.values()):\n        checkpoint_found = False\n\n        # source code of `config_class`\n        config_source = inspect.getsource(config_class)\n        checkpoints = _re_checkpoint.findall(config_source)\n\n        for checkpoint in checkpoints:\n            # Each `checkpoint` is a tuple of a checkpoint name and a checkpoint link.\n            # For example, `('bert-base-uncased', 'https://huggingface.co/bert-base-uncased')`\n            ckpt_name, ckpt_link = checkpoint\n\n            # verify the checkpoint name corresponds to the checkpoint link\n            ckpt_link_from_name = f\"https://huggingface.co/{ckpt_name}\"\n            if ckpt_link == ckpt_link_from_name:\n                checkpoint_found = True\n                break\n\n        name = config_class.__name__\n        if not checkpoint_found and name not in CONFIG_CLASSES_TO_IGNORE_FOR_DOCSTRING_CHECKPOINT_CHECK:\n            configs_without_checkpoint.append(name)\n\n    if len(configs_without_checkpoint) > 0:\n        message = \"\\n\".join(sorted(configs_without_checkpoint))\n        raise ValueError(f\"The following configurations don't contain any valid checkpoint:\\n{message}\")",
      "metadata": {
        "source": "utils/check_config_docstrings.py",
        "range": {
          "start": { "row": 52, "column": 0 },
          "end": { "row": 52, "column": 0 }
        }
      }
    }
  ],
  [
    "1091",
    {
      "pageContent": "def find_code_in_diffusers(object_name):\n    \"\"\"Find and return the code source code of `object_name`.\"\"\"\n    parts = object_name.split(\".\")\n    i = 0\n\n    # First let's find the module where our object lives.\n    module = parts[i]\n    while i < len(parts) and not os.path.isfile(os.path.join(DIFFUSERS_PATH, f\"{module}.py\")):\n        i += 1\n        if i < len(parts):\n            module = os.path.join(module, parts[i])\n    if i >= len(parts):\n        raise ValueError(f\"`object_name` should begin with the name of a module of diffusers but got {object_name}.\")\n\n    with open(os.path.join(DIFFUSERS_PATH, f\"{module}.py\"), \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        lines = f.readlines()\n\n    # Now let's find the class / func in the code!\n    indent = \"\"\n    line_index = 0\n    for name in parts[i + 1 :]:\n        while (\n            line_index < len(lines) and re.search(rf\"^{indent}(class|def)\\s+{name}(\\(|\\:)\", lines[line_index]) is None\n        ):\n            line_index += 1\n        indent += \"    \"\n        line_index += 1\n\n    if line_index >= len(lines):\n        raise ValueError(f\" {object_name} does not match any function or class in {module}.\")\n\n    # We found the beginning of the class / func, now let's find the end (when the indent diminishes).\n    start_index = line_index\n    while line_index < len(lines) and _should_continue(lines[line_index], indent):\n        line_index += 1\n    # Clean up empty lines at the end (if any).\n    while len(lines[line_index - 1]) <= 1:\n        line_index -= 1\n\n    code_lines = lines[start_index:line_index]\n    return \"\".join(code_lines)",
      "metadata": {
        "source": "utils/check_copies.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "1092",
    {
      "pageContent": "def get_indent(code):\n    lines = code.split(\"\\n\")\n    idx = 0\n    while idx < len(lines) and len(lines[idx]) == 0:\n        idx += 1\n    if idx < len(lines):\n        return re.search(r\"^(\\s*)\\S\", lines[idx]).groups()[0]\n    return \"\"",
      "metadata": {
        "source": "utils/check_copies.py",
        "range": {
          "start": { "row": 92, "column": 0 },
          "end": { "row": 92, "column": 0 }
        }
      }
    }
  ],
  [
    "1093",
    {
      "pageContent": "def blackify(code):\n    \"\"\"\n    Applies the black part of our `make style` command to `code`.\n    \"\"\"\n    has_indent = len(get_indent(code)) > 0\n    if has_indent:\n        code = f\"class Bla:\\n{code}\"\n    mode = black.Mode(target_versions={black.TargetVersion.PY37}, line_length=119, preview=True)\n    result = black.format_str(code, mode=mode)\n    result, _ = style_docstrings_in_code(result)\n    return result[len(\"class Bla:\\n\") :] if has_indent else result",
      "metadata": {
        "source": "utils/check_copies.py",
        "range": {
          "start": { "row": 102, "column": 0 },
          "end": { "row": 102, "column": 0 }
        }
      }
    }
  ],
  [
    "1094",
    {
      "pageContent": "def is_copy_consistent(filename, overwrite=False):\n    \"\"\"\n    Check if the code commented as a copy in `filename` matches the original.\n    Return the differences or overwrites the content depending on `overwrite`.\n    \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        lines = f.readlines()\n    diffs = []\n    line_index = 0\n    # Not a for loop cause `lines` is going to change (if `overwrite=True`).\n    while line_index < len(lines):\n        search = _re_copy_warning.search(lines[line_index])\n        if search is None:\n            line_index += 1\n            continue\n\n        # There is some copied code here, let's retrieve the original.\n        indent, object_name, replace_pattern = search.groups()\n        theoretical_code = find_code_in_diffusers(object_name)\n        theoretical_indent = get_indent(theoretical_code)\n\n        start_index = line_index + 1 if indent == theoretical_indent else line_index + 2\n        indent = theoretical_indent\n        line_index = start_index\n\n        # Loop to check the observed code, stop when indentation diminishes or if we see a End copy comment.\n        should_continue = True\n        while line_index < len(lines) and should_continue:\n            line_index += 1\n            if line_index >= len(lines):\n                break\n            line = lines[line_index]\n            should_continue = _should_continue(line, indent) and re.search(f\"^{indent}# End copy\", line) is None\n        # Clean up empty lines at the end (if any).\n        while len(lines[line_index - 1]) <= 1:\n            line_index -= 1\n\n        obser",
      "metadata": {
        "source": "utils/check_copies.py",
        "range": {
          "start": { "row": 115, "column": 0 },
          "end": { "row": 115, "column": 0 }
        }
      }
    }
  ],
  [
    "1095",
    {
      "pageContent": "def check_copies(overwrite: bool = False):\n    all_files = glob.glob(os.path.join(DIFFUSERS_PATH, \"**/*.py\"), recursive=True)\n    diffs = []\n    for filename in all_files:\n        new_diffs = is_copy_consistent(filename, overwrite)\n        diffs += [f\"- {filename}: copy does not match {d[0]} at line {d[1]}\" for d in new_diffs]\n    if not overwrite and len(diffs) > 0:\n        diff = \"\\n\".join(diffs)\n        raise Exception(\n            \"Found the following copy inconsistencies:\\n\"\n            + diff\n            + \"\\nRun `make fix-copies` or `python utils/check_copies.py --fix_and_overwrite` to fix them.\"\n        )",
      "metadata": {
        "source": "utils/check_copies.py",
        "range": {
          "start": { "row": 192, "column": 0 },
          "end": { "row": 192, "column": 0 }
        }
      }
    }
  ],
  [
    "1096",
    {
      "pageContent": "def find_backend(line):\n    \"\"\"Find one (or multiple) backend in a code line of the init.\"\"\"\n    if _re_test_backend.search(line) is None:\n        return None\n    backends = [b[0] for b in _re_backend.findall(line)]\n    backends.sort()\n    return \"_and_\".join(backends)",
      "metadata": {
        "source": "utils/check_inits.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "1097",
    {
      "pageContent": "def parse_init(init_file):\n    \"\"\"\n    Read an init_file and parse (per backend) the _import_structure objects defined and the TYPE_CHECKING objects\n    defined\n    \"\"\"\n    with open(init_file, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        lines = f.readlines()\n\n    line_index = 0\n    while line_index < len(lines) and not lines[line_index].startswith(\"_import_structure = {\"):\n        line_index += 1\n\n    # If this is a traditional init, just return.\n    if line_index >= len(lines):\n        return None\n\n    # First grab the objects without a specific backend in _import_structure\n    objects = []\n    while not lines[line_index].startswith(\"if TYPE_CHECKING\") and find_backend(lines[line_index]) is None:\n        line = lines[line_index]\n        # If we have everything on a single line, let's deal with it.\n        if _re_one_line_import_struct.search(line):\n            content = _re_one_line_import_struct.search(line).groups()[0]\n            imports = re.findall(\"\\[([^\\]]+)\\]\", content)\n            for imp in imports:\n                objects.extend([obj[1:-1] for obj in imp.split(\", \")])\n            line_index += 1\n            continue\n        single_line_import_search = _re_import_struct_key_value.search(line)\n        if single_line_import_search is not None:\n            imports = [obj[1:-1] for obj in single_line_import_search.groups()[0].split(\", \") if len(obj) > 0]\n            objects.extend(imports)\n        elif line.startswith(\" \" * 8 + '\"'):\n            objects.append(line[9:-3])\n        line_index += 1\n\n    import_dict_objects = {\"none\": objects}\n    # Let's continu",
      "metadata": {
        "source": "utils/check_inits.py",
        "range": {
          "start": { "row": 58, "column": 0 },
          "end": { "row": 58, "column": 0 }
        }
      }
    }
  ],
  [
    "1098",
    {
      "pageContent": "def analyze_results(import_dict_objects, type_hint_objects):\n    \"\"\"\n    Analyze the differences between _import_structure objects and TYPE_CHECKING objects found in an init.\n    \"\"\"\n\n    def find_duplicates(seq):\n        return [k for k, v in collections.Counter(seq).items() if v > 1]\n\n    if list(import_dict_objects.keys()) != list(type_hint_objects.keys()):\n        return [\"Both sides of the init do not have the same backends!\"]\n\n    errors = []\n    for key in import_dict_objects.keys():\n        duplicate_imports = find_duplicates(import_dict_objects[key])\n        if duplicate_imports:\n            errors.append(f\"Duplicate _import_structure definitions for: {duplicate_imports}\")\n        duplicate_type_hints = find_duplicates(type_hint_objects[key])\n        if duplicate_type_hints:\n            errors.append(f\"Duplicate TYPE_CHECKING objects for: {duplicate_type_hints}\")\n\n        if sorted(set(import_dict_objects[key])) != sorted(set(type_hint_objects[key])):\n            name = \"base imports\" if key == \"none\" else f\"{key} backend\"\n            errors.append(f\"Differences for {name}:\")\n            for a in type_hint_objects[key]:\n                if a not in import_dict_objects[key]:\n                    errors.append(f\"  {a} in TYPE_HINT but not in _import_structure.\")\n            for a in import_dict_objects[key]:\n                if a not in type_hint_objects[key]:\n                    errors.append(f\"  {a} in _import_structure but not in TYPE_HINT.\")\n    return errors",
      "metadata": {
        "source": "utils/check_inits.py",
        "range": {
          "start": { "row": 189, "column": 0 },
          "end": { "row": 189, "column": 0 }
        }
      }
    }
  ],
  [
    "1099",
    {
      "pageContent": "def check_all_inits():\n    \"\"\"\n    Check all inits in the transformers repo and raise an error if at least one does not define the same objects in\n    both halves.\n    \"\"\"\n    failures = []\n    for root, _, files in os.walk(PATH_TO_TRANSFORMERS):\n        if \"__init__.py\" in files:\n            fname = os.path.join(root, \"__init__.py\")\n            objects = parse_init(fname)\n            if objects is not None:\n                errors = analyze_results(*objects)\n                if len(errors) > 0:\n                    errors[0] = f\"Problem in {fname}, both halves do not define the same objects.\\n{errors[0]}\"\n                    failures.append(\"\\n\".join(errors))\n    if len(failures) > 0:\n        raise ValueError(\"\\n\\n\".join(failures))",
      "metadata": {
        "source": "utils/check_inits.py",
        "range": {
          "start": { "row": 221, "column": 0 },
          "end": { "row": 221, "column": 0 }
        }
      }
    }
  ],
  [
    "1100",
    {
      "pageContent": "def get_transformers_submodules():\n    \"\"\"\n    Returns the list of Transformers submodules.\n    \"\"\"\n    submodules = []\n    for path, directories, files in os.walk(PATH_TO_TRANSFORMERS):\n        for folder in directories:\n            # Ignore private modules\n            if folder.startswith(\"_\"):\n                directories.remove(folder)\n                continue\n            # Ignore leftovers from branches (empty folders apart from pycache)\n            if len(list((Path(path) / folder).glob(\"*.py\"))) == 0:\n                continue\n            short_path = str((Path(path) / folder).relative_to(PATH_TO_TRANSFORMERS))\n            submodule = short_path.replace(os.path.sep, \".\")\n            submodules.append(submodule)\n        for fname in files:\n            if fname == \"__init__.py\":\n                continue\n            short_path = str((Path(path) / fname).relative_to(PATH_TO_TRANSFORMERS))\n            submodule = short_path.replace(\".py\", \"\").replace(os.path.sep, \".\")\n            if len(submodule.split(\".\")) == 1:\n                submodules.append(submodule)\n    return submodules",
      "metadata": {
        "source": "utils/check_inits.py",
        "range": {
          "start": { "row": 240, "column": 0 },
          "end": { "row": 240, "column": 0 }
        }
      }
    }
  ],
  [
    "1101",
    {
      "pageContent": "def check_submodules():\n    # This is to make sure the transformers module imported is the one in the repo.\n    spec = importlib.util.spec_from_file_location(\n        \"transformers\",\n        os.path.join(PATH_TO_TRANSFORMERS, \"__init__.py\"),\n        submodule_search_locations=[PATH_TO_TRANSFORMERS],\n    )\n    transformers = spec.loader.load_module()\n\n    module_not_registered = [\n        module\n        for module in get_transformers_submodules()\n        if module not in IGNORE_SUBMODULES and module not in transformers._import_structure.keys()\n    ]\n    if len(module_not_registered) > 0:\n        list_of_modules = \"\\n\".join(f\"- {module}\" for module in module_not_registered)\n        raise ValueError(\n            \"The following submodules are not properly registered in the main init of Transformers:\\n\"\n            f\"{list_of_modules}\\n\"\n            \"Make sure they appear somewhere in the keys of `_import_structure` with an empty list as value.\"\n        )",
      "metadata": {
        "source": "utils/check_inits.py",
        "range": {
          "start": { "row": 273, "column": 0 },
          "end": { "row": 273, "column": 0 }
        }
      }
    }
  ],
  [
    "1102",
    {
      "pageContent": "def get_indent(line):\n    \"\"\"Returns the indent in `line`.\"\"\"\n    search = _re_indent.search(line)\n    return \"\" if search is None else search.groups()[0]",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1103",
    {
      "pageContent": "def split_code_in_indented_blocks(code, indent_level=\"\", start_prompt=None, end_prompt=None):\n    \"\"\"\n    Split `code` into its indented blocks, starting at `indent_level`. If provided, begins splitting after\n    `start_prompt` and stops at `end_prompt` (but returns what's before `start_prompt` as a first block and what's\n    after `end_prompt` as a last block, so `code` is always the same as joining the result of this function).\n    \"\"\"\n    # Let's split the code into lines and move to start_index.\n    index = 0\n    lines = code.split(\"\\n\")\n    if start_prompt is not None:\n        while not lines[index].startswith(start_prompt):\n            index += 1\n        blocks = [\"\\n\".join(lines[:index])]\n    else:\n        blocks = []\n\n    # We split into blocks until we get to the `end_prompt` (or the end of the block).\n    current_block = [lines[index]]\n    index += 1\n    while index < len(lines) and (end_prompt is None or not lines[index].startswith(end_prompt)):\n        if len(lines[index]) > 0 and get_indent(lines[index]) == indent_level:\n            if len(current_block) > 0 and get_indent(current_block[-1]).startswith(indent_level + \" \"):\n                current_block.append(lines[index])\n                blocks.append(\"\\n\".join(current_block))\n                if index < len(lines) - 1:\n                    current_block = [lines[index + 1]]\n                    index += 1\n                else:\n                    current_block = []\n            else:\n                blocks.append(\"\\n\".join(current_block))\n                current_block = [lines[index]]\n        else:\n            cu",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "1104",
    {
      "pageContent": "def ignore_underscore(key):\n    \"Wraps a `key` (that maps an object to string) to lower case and remove underscores.\"\n\n    def _inner(x):\n        return key(x).lower().replace(\"_\", \"\")\n\n    return _inner",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 87, "column": 0 },
          "end": { "row": 87, "column": 0 }
        }
      }
    }
  ],
  [
    "1105",
    {
      "pageContent": "def sort_objects(objects, key=None):\n    \"Sort a list of `objects` following the rules of isort. `key` optionally maps an object to a str.\"\n\n    # If no key is provided, we use a noop.\n    def noop(x):\n        return x\n\n    if key is None:\n        key = noop\n    # Constants are all uppercase, they go first.\n    constants = [obj for obj in objects if key(obj).isupper()]\n    # Classes are not all uppercase but start with a capital, they go second.\n    classes = [obj for obj in objects if key(obj)[0].isupper() and not key(obj).isupper()]\n    # Functions begin with a lowercase, they go last.\n    functions = [obj for obj in objects if not key(obj)[0].isupper()]\n\n    key1 = ignore_underscore(key)\n    return sorted(constants, key=key1) + sorted(classes, key=key1) + sorted(functions, key=key1)",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 96, "column": 0 },
          "end": { "row": 96, "column": 0 }
        }
      }
    }
  ],
  [
    "1106",
    {
      "pageContent": "def sort_objects_in_import(import_statement):\n    \"\"\"\n    Return the same `import_statement` but with objects properly sorted.\n    \"\"\"\n\n    # This inner function sort imports between [ ].\n    def _replace(match):\n        imports = match.groups()[0]\n        if \",\" not in imports:\n            return f\"[{imports}]\"\n        keys = [part.strip().replace('\"', \"\") for part in imports.split(\",\")]\n        # We will have a final empty element if the line finished with a comma.\n        if len(keys[-1]) == 0:\n            keys = keys[:-1]\n        return \"[\" + \", \".join([f'\"{k}\"' for k in sort_objects(keys)]) + \"]\"\n\n    lines = import_statement.split(\"\\n\")\n    if len(lines) > 3:\n        # Here we have to sort internal imports that are on several lines (one per name):\n        # key: [\n        #     \"object1\",\n        #     \"object2\",\n        #     ...\n        # ]\n\n        # We may have to ignore one or two lines on each side.\n        idx = 2 if lines[1].strip() == \"[\" else 1\n        keys_to_sort = [(i, _re_strip_line.search(line).groups()[0]) for i, line in enumerate(lines[idx:-idx])]\n        sorted_indices = sort_objects(keys_to_sort, key=lambda x: x[1])\n        sorted_lines = [lines[x[0] + idx] for x in sorted_indices]\n        return \"\\n\".join(lines[:idx] + sorted_lines + lines[-idx:])\n    elif len(lines) == 3:\n        # Here we have to sort internal imports that are on one separate line:\n        # key: [\n        #     \"object1\", \"object2\", ...\n        # ]\n        if _re_bracket_content.search(lines[1]) is not None:\n            lines[1] = _re_bracket_content.sub(_replace, lines[1])\n    ",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 116, "column": 0 },
          "end": { "row": 116, "column": 0 }
        }
      }
    }
  ],
  [
    "1107",
    {
      "pageContent": "def _replace(match):\n        imports = match.groups()[0]\n        if \",\" not in imports:\n            return f\"[{imports}]\"\n        keys = [part.strip().replace('\"', \"\") for part in imports.split(\",\")]\n        # We will have a final empty element if the line finished with a comma.\n        if len(keys[-1]) == 0:\n            keys = keys[:-1]\n        return \"[\" + \", \".join([f'\"{k}\"' for k in sort_objects(keys)]) + \"]\"",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "1108",
    {
      "pageContent": "def sort_imports(file, check_only=True):\n    \"\"\"\n    Sort `_import_structure` imports in `file`, `check_only` determines if we only check or overwrite.\n    \"\"\"\n    with open(file, \"r\") as f:\n        code = f.read()\n\n    if \"_import_structure\" not in code:\n        return\n\n    # Blocks of indent level 0\n    main_blocks = split_code_in_indented_blocks(\n        code, start_prompt=\"_import_structure = {\", end_prompt=\"if TYPE_CHECKING:\"\n    )\n\n    # We ignore block 0 (everything until start_prompt) and the last block (everything after end_prompt).\n    for block_idx in range(1, len(main_blocks) - 1):\n        # Check if the block contains some `_import_structure`s thingy to sort.\n        block = main_blocks[block_idx]\n        block_lines = block.split(\"\\n\")\n\n        # Get to the start of the imports.\n        line_idx = 0\n        while line_idx < len(block_lines) and \"_import_structure\" not in block_lines[line_idx]:\n            # Skip dummy import blocks\n            if \"import dummy\" in block_lines[line_idx]:\n                line_idx = len(block_lines)\n            else:\n                line_idx += 1\n        if line_idx >= len(block_lines):\n            continue\n\n        # Ignore beginning and last line: they don't contain anything.\n        internal_block_code = \"\\n\".join(block_lines[line_idx:-1])\n        indent = get_indent(block_lines[1])\n        # Slit the internal block into blocks of indent level 1.\n        internal_blocks = split_code_in_indented_blocks(internal_block_code, indent_level=indent)\n        # We have two categories of import key: list or _import_structure[key].append",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 167, "column": 0 },
          "end": { "row": 167, "column": 0 }
        }
      }
    }
  ],
  [
    "1109",
    {
      "pageContent": "def sort_imports_in_all_inits(check_only=True):\n    failures = []\n    for root, _, files in os.walk(PATH_TO_TRANSFORMERS):\n        if \"__init__.py\" in files:\n            result = sort_imports(os.path.join(root, \"__init__.py\"), check_only=check_only)\n            if result:\n                failures = [os.path.join(root, \"__init__.py\")]\n    if len(failures) > 0:\n        raise ValueError(f\"Would overwrite {len(failures)} files, run `make style`.\")",
      "metadata": {
        "source": "utils/custom_init_isort.py",
        "range": {
          "start": { "row": 235, "column": 0 },
          "end": { "row": 235, "column": 0 }
        }
      }
    }
  ],
  [
    "1110",
    {
      "pageContent": "def find_backend(line):\n    \"\"\"Find one (or multiple) backend in a code line of the init.\"\"\"\n    backends = _re_backend.findall(line)\n    if len(backends) == 0:\n        return None\n\n    return \"_and_\".join(backends)",
      "metadata": {
        "source": "utils/check_dummies.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "1111",
    {
      "pageContent": "def read_init():\n    \"\"\"Read the init and extracts PyTorch, TensorFlow, SentencePiece and Tokenizers objects.\"\"\"\n    with open(os.path.join(PATH_TO_DIFFUSERS, \"__init__.py\"), \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        lines = f.readlines()\n\n    # Get to the point we do the actual imports for type checking\n    line_index = 0\n    backend_specific_objects = {}\n    # Go through the end of the file\n    while line_index < len(lines):\n        # If the line contains is_backend_available, we grab all objects associated with the `else` block\n        backend = find_backend(lines[line_index])\n        if backend is not None:\n            while not lines[line_index].startswith(\"else:\"):\n                line_index += 1\n            line_index += 1\n            objects = []\n            # Until we unindent, add backend objects to the list\n            while line_index < len(lines) and len(lines[line_index]) > 1:\n                line = lines[line_index]\n                single_line_import_search = _re_single_line_import.search(line)\n                if single_line_import_search is not None:\n                    objects.extend(single_line_import_search.groups()[0].split(\", \"))\n                elif line.startswith(\" \" * 8):\n                    objects.append(line[8:-2])\n                line_index += 1\n\n            if len(objects) > 0:\n                backend_specific_objects[backend] = objects\n        else:\n            line_index += 1\n\n    return backend_specific_objects",
      "metadata": {
        "source": "utils/check_dummies.py",
        "range": {
          "start": { "row": 66, "column": 0 },
          "end": { "row": 66, "column": 0 }
        }
      }
    }
  ],
  [
    "1112",
    {
      "pageContent": "def create_dummy_object(name, backend_name):\n    \"\"\"Create the code for the dummy object corresponding to `name`.\"\"\"\n    if name.isupper():\n        return DUMMY_CONSTANT.format(name)\n    elif name.islower():\n        return DUMMY_FUNCTION.format(name, backend_name)\n    else:\n        return DUMMY_CLASS.format(name, backend_name)",
      "metadata": {
        "source": "utils/check_dummies.py",
        "range": {
          "start": { "row": 101, "column": 0 },
          "end": { "row": 101, "column": 0 }
        }
      }
    }
  ],
  [
    "1113",
    {
      "pageContent": "def create_dummy_files(backend_specific_objects=None):\n    \"\"\"Create the content of the dummy files.\"\"\"\n    if backend_specific_objects is None:\n        backend_specific_objects = read_init()\n    # For special correspondence backend to module name as used in the function requires_modulename\n    dummy_files = {}\n\n    for backend, objects in backend_specific_objects.items():\n        backend_name = \"[\" + \", \".join(f'\"{b}\"' for b in backend.split(\"_and_\")) + \"]\"\n        dummy_file = \"# This file is autogenerated by the command `make fix-copies`, do not edit.\\n\"\n        dummy_file += \"from ..utils import DummyObject, requires_backends\\n\\n\"\n        dummy_file += \"\\n\".join([create_dummy_object(o, backend_name) for o in objects])\n        dummy_files[backend] = dummy_file\n\n    return dummy_files",
      "metadata": {
        "source": "utils/check_dummies.py",
        "range": {
          "start": { "row": 111, "column": 0 },
          "end": { "row": 111, "column": 0 }
        }
      }
    }
  ],
  [
    "1114",
    {
      "pageContent": "def check_dummies(overwrite=False):\n    \"\"\"Check if the dummy files are up to date and maybe `overwrite` with the right content.\"\"\"\n    dummy_files = create_dummy_files()\n    # For special correspondence backend to shortcut as used in utils/dummy_xxx_objects.py\n    short_names = {\"torch\": \"pt\"}\n\n    # Locate actual dummy modules and read their content.\n    path = os.path.join(PATH_TO_DIFFUSERS, \"utils\")\n    dummy_file_paths = {\n        backend: os.path.join(path, f\"dummy_{short_names.get(backend, backend)}_objects.py\")\n        for backend in dummy_files.keys()\n    }\n\n    actual_dummies = {}\n    for backend, file_path in dummy_file_paths.items():\n        if os.path.isfile(file_path):\n            with open(file_path, \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n                actual_dummies[backend] = f.read()\n        else:\n            actual_dummies[backend] = \"\"\n\n    for backend in dummy_files.keys():\n        if dummy_files[backend] != actual_dummies[backend]:\n            if overwrite:\n                print(\n                    f\"Updating diffusers.utils.dummy_{short_names.get(backend, backend)}_objects.py as the main \"\n                    \"__init__ has new objects.\"\n                )\n                with open(dummy_file_paths[backend], \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n                    f.write(dummy_files[backend])\n            else:\n                raise ValueError(\n                    \"The main __init__ has objects that are not present in \"\n                    f\"diffusers.utils.dummy_{short_names.get(backend, backend)}_objects.py. Run `make fix-copies` \"\n      ",
      "metadata": {
        "source": "utils/check_dummies.py",
        "range": {
          "start": { "row": 128, "column": 0 },
          "end": { "row": 128, "column": 0 }
        }
      }
    }
  ],
  [
    "1115",
    {
      "pageContent": "def clean_doc_toc(doc_list):\n    \"\"\"\n    Cleans the table of content of the model documentation by removing duplicates and sorting models alphabetically.\n    \"\"\"\n    counts = defaultdict(int)\n    overview_doc = []\n    new_doc_list = []\n    for doc in doc_list:\n        if \"local\" in doc:\n            counts[doc[\"local\"]] += 1\n\n        if doc[\"title\"].lower() == \"overview\":\n            overview_doc.append({\"local\": doc[\"local\"], \"title\": doc[\"title\"]})\n        else:\n            new_doc_list.append(doc)\n\n    doc_list = new_doc_list\n    duplicates = [key for key, value in counts.items() if value > 1]\n\n    new_doc = []\n    for duplicate_key in duplicates:\n        titles = list(set(doc[\"title\"] for doc in doc_list if doc[\"local\"] == duplicate_key))\n        if len(titles) > 1:\n            raise ValueError(\n                f\"{duplicate_key} is present several times in the documentation table of content at \"\n                \"`docs/source/en/_toctree.yml` with different *Title* values. Choose one of those and remove the \"\n                \"others.\"\n            )\n        # Only add this once\n        new_doc.append({\"local\": duplicate_key, \"title\": titles[0]})\n\n    # Add none duplicate-keys\n    new_doc.extend([doc for doc in doc_list if \"local\" not in counts or counts[doc[\"local\"]] == 1])\n    new_doc = sorted(new_doc, key=lambda s: s[\"title\"].lower())\n\n    # \"overview\" gets special treatment and is always first\n    if len(overview_doc) > 1:\n        raise ValueError(\"{doc_list} has two 'overview' docs which is not allowed.\")\n\n    overview_doc.extend(new_doc)\n\n    # Sort\n    return overvie",
      "metadata": {
        "source": "utils/check_doc_toc.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1116",
    {
      "pageContent": "def check_scheduler_doc(overwrite=False):\n    with open(PATH_TO_TOC, encoding=\"utf-8\") as f:\n        content = yaml.safe_load(f.read())\n\n    # Get to the API doc\n    api_idx = 0\n    while content[api_idx][\"title\"] != \"API\":\n        api_idx += 1\n    api_doc = content[api_idx][\"sections\"]\n\n    # Then to the model doc\n    scheduler_idx = 0\n    while api_doc[scheduler_idx][\"title\"] != \"Schedulers\":\n        scheduler_idx += 1\n\n    scheduler_doc = api_doc[scheduler_idx][\"sections\"]\n    new_scheduler_doc = clean_doc_toc(scheduler_doc)\n\n    diff = False\n    if new_scheduler_doc != scheduler_doc:\n        diff = True\n        if overwrite:\n            api_doc[scheduler_idx][\"sections\"] = new_scheduler_doc\n\n    if diff:\n        if overwrite:\n            content[api_idx][\"sections\"] = api_doc\n            with open(PATH_TO_TOC, \"w\", encoding=\"utf-8\") as f:\n                f.write(yaml.dump(content, allow_unicode=True))\n        else:\n            raise ValueError(\n                \"The model doc part of the table of content is not properly sorted, run `make style` to fix this.\"\n            )",
      "metadata": {
        "source": "utils/check_doc_toc.py",
        "range": {
          "start": { "row": 69, "column": 0 },
          "end": { "row": 69, "column": 0 }
        }
      }
    }
  ],
  [
    "1117",
    {
      "pageContent": "def check_pipeline_doc(overwrite=False):\n    with open(PATH_TO_TOC, encoding=\"utf-8\") as f:\n        content = yaml.safe_load(f.read())\n\n    # Get to the API doc\n    api_idx = 0\n    while content[api_idx][\"title\"] != \"API\":\n        api_idx += 1\n    api_doc = content[api_idx][\"sections\"]\n\n    # Then to the model doc\n    pipeline_idx = 0\n    while api_doc[pipeline_idx][\"title\"] != \"Pipelines\":\n        pipeline_idx += 1\n\n    diff = False\n    pipeline_docs = api_doc[pipeline_idx][\"sections\"]\n    new_pipeline_docs = []\n\n    # sort sub pipeline docs\n    for pipeline_doc in pipeline_docs:\n        if \"section\" in pipeline_doc:\n            sub_pipeline_doc = pipeline_doc[\"section\"]\n            new_sub_pipeline_doc = clean_doc_toc(sub_pipeline_doc)\n            if overwrite:\n                pipeline_doc[\"section\"] = new_sub_pipeline_doc\n        new_pipeline_docs.append(pipeline_doc)\n\n    # sort overall pipeline doc\n    new_pipeline_docs = clean_doc_toc(new_pipeline_docs)\n\n    if new_pipeline_docs != pipeline_docs:\n        diff = True\n        if overwrite:\n            api_doc[pipeline_idx][\"sections\"] = new_pipeline_docs\n\n    if diff:\n        if overwrite:\n            content[api_idx][\"sections\"] = api_doc\n            with open(PATH_TO_TOC, \"w\", encoding=\"utf-8\") as f:\n                f.write(yaml.dump(content, allow_unicode=True))\n        else:\n            raise ValueError(\n                \"The model doc part of the table of content is not properly sorted, run `make style` to fix this.\"\n            )",
      "metadata": {
        "source": "utils/check_doc_toc.py",
        "range": {
          "start": { "row": 104, "column": 0 },
          "end": { "row": 104, "column": 0 }
        }
      }
    }
  ],
  [
    "1118",
    {
      "pageContent": "def download_model(model_name):\n    \"\"\"\n    Downloads a pre-trained DiT model from the web.\n    \"\"\"\n    local_path = f\"pretrained_models/{model_name}\"\n    if not os.path.isfile(local_path):\n        os.makedirs(\"pretrained_models\", exist_ok=True)\n        web_path = f\"https://dl.fbaipublicfiles.com/DiT/models/{model_name}\"\n        download_url(web_path, \"pretrained_models\")\n    model = torch.load(local_path, map_location=lambda storage, loc: storage)\n    return model",
      "metadata": {
        "source": "scripts/convert_dit_to_diffusers.py",
        "range": {
          "start": { "row": 12, "column": 0 },
          "end": { "row": 12, "column": 0 }
        }
      }
    }
  ],
  [
    "1119",
    {
      "pageContent": "def main(args):\n    state_dict = download_model(pretrained_models[args.image_size])\n\n    state_dict[\"pos_embed.proj.weight\"] = state_dict[\"x_embedder.proj.weight\"]\n    state_dict[\"pos_embed.proj.bias\"] = state_dict[\"x_embedder.proj.bias\"]\n    state_dict.pop(\"x_embedder.proj.weight\")\n    state_dict.pop(\"x_embedder.proj.bias\")\n\n    for depth in range(28):\n        state_dict[f\"transformer_blocks.{depth}.norm1.emb.timestep_embedder.linear_1.weight\"] = state_dict[\n            \"t_embedder.mlp.0.weight\"\n        ]\n        state_dict[f\"transformer_blocks.{depth}.norm1.emb.timestep_embedder.linear_1.bias\"] = state_dict[\n            \"t_embedder.mlp.0.bias\"\n        ]\n        state_dict[f\"transformer_blocks.{depth}.norm1.emb.timestep_embedder.linear_2.weight\"] = state_dict[\n            \"t_embedder.mlp.2.weight\"\n        ]\n        state_dict[f\"transformer_blocks.{depth}.norm1.emb.timestep_embedder.linear_2.bias\"] = state_dict[\n            \"t_embedder.mlp.2.bias\"\n        ]\n        state_dict[f\"transformer_blocks.{depth}.norm1.emb.class_embedder.embedding_table.weight\"] = state_dict[\n            \"y_embedder.embedding_table.weight\"\n        ]\n\n        state_dict[f\"transformer_blocks.{depth}.norm1.linear.weight\"] = state_dict[\n            f\"blocks.{depth}.adaLN_modulation.1.weight\"\n        ]\n        state_dict[f\"transformer_blocks.{depth}.norm1.linear.bias\"] = state_dict[\n            f\"blocks.{depth}.adaLN_modulation.1.bias\"\n        ]\n\n        q, k, v = torch.chunk(state_dict[f\"blocks.{depth}.attn.qkv.weight\"], 3, dim=0)\n        q_bias, k_bias, v_bias = torch.chunk(state_dict[f\"blocks.{depth}.",
      "metadata": {
        "source": "scripts/convert_dit_to_diffusers.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1120",
    {
      "pageContent": "def convert(base_model_path, checkpoint_path, LORA_PREFIX_UNET, LORA_PREFIX_TEXT_ENCODER, alpha):\n    # load base model\n    pipeline = StableDiffusionPipeline.from_pretrained(base_model_path, torch_dtype=torch.float32)\n\n    # load LoRA weight from .safetensors\n    state_dict = load_file(checkpoint_path)\n\n    visited = []\n\n    # directly update weight in diffusers model\n    for key in state_dict:\n        # it is suggested to print out the key, it usually will be something like below\n        # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n\n        # as we have set the alpha beforehand, so just skip\n        if \".alpha\" in key or key in visited:\n            continue\n\n        if \"text\" in key:\n            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n            curr_layer = pipeline.text_encoder\n        else:\n            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n            curr_layer = pipeline.unet\n\n        # find the target layer\n        temp_name = layer_infos.pop(0)\n        while len(layer_infos) > -1:\n            try:\n                curr_layer = curr_layer.__getattr__(temp_name)\n                if len(layer_infos) > 0:\n                    temp_name = layer_infos.pop(0)\n                elif len(layer_infos) == 0:\n                    break\n            except Exception:\n                if len(temp_name) > 0:\n                    temp_name += \"_\" + layer_infos.pop(0)\n                else:\n                    temp_name = layer_infos.pop(0)\n\n        pair_keys = []\n        if \"lor",
      "metadata": {
        "source": "scripts/convert_lora_safetensor_to_diffusers.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1121",
    {
      "pageContent": "def shave_segments(path, n_shave_prefix_segments=1):\n    \"\"\"\n    Removes segments. Positive values shave the first segments, negative shave the last segments.\n    \"\"\"\n    if n_shave_prefix_segments >= 0:\n        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n    else:\n        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 95, "column": 0 },
          "end": { "row": 95, "column": 0 }
        }
      }
    }
  ],
  [
    "1122",
    {
      "pageContent": "def renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item.replace(\"in_layers.0\", \"norm1\")\n        new_item = new_item.replace(\"in_layers.2\", \"conv1\")\n\n        new_item = new_item.replace(\"out_layers.0\", \"norm2\")\n        new_item = new_item.replace(\"out_layers.3\", \"conv2\")\n\n        new_item = new_item.replace(\"emb_layers.1\", \"time_emb_proj\")\n        new_item = new_item.replace(\"skip_connection\", \"conv_shortcut\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 105, "column": 0 },
          "end": { "row": 105, "column": 0 }
        }
      }
    }
  ],
  [
    "1123",
    {
      "pageContent": "def renew_vae_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"nin_shortcut\", \"conv_shortcut\")\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 127, "column": 0 },
          "end": { "row": 127, "column": 0 }
        }
      }
    }
  ],
  [
    "1124",
    {
      "pageContent": "def renew_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        #         new_item = new_item.replace('norm.weight', 'group_norm.weight')\n        #         new_item = new_item.replace('norm.bias', 'group_norm.bias')\n\n        #         new_item = new_item.replace('proj_out.weight', 'proj_attn.weight')\n        #         new_item = new_item.replace('proj_out.bias', 'proj_attn.bias')\n\n        #         new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 143, "column": 0 },
          "end": { "row": 143, "column": 0 }
        }
      }
    }
  ],
  [
    "1125",
    {
      "pageContent": "def renew_vae_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"norm.weight\", \"group_norm.weight\")\n        new_item = new_item.replace(\"norm.bias\", \"group_norm.bias\")\n\n        new_item = new_item.replace(\"q.weight\", \"query.weight\")\n        new_item = new_item.replace(\"q.bias\", \"query.bias\")\n\n        new_item = new_item.replace(\"k.weight\", \"key.weight\")\n        new_item = new_item.replace(\"k.bias\", \"key.bias\")\n\n        new_item = new_item.replace(\"v.weight\", \"value.weight\")\n        new_item = new_item.replace(\"v.bias\", \"value.bias\")\n\n        new_item = new_item.replace(\"proj_out.weight\", \"proj_attn.weight\")\n        new_item = new_item.replace(\"proj_out.bias\", \"proj_attn.bias\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 164, "column": 0 },
          "end": { "row": 164, "column": 0 }
        }
      }
    }
  ],
  [
    "1126",
    {
      "pageContent": "def assign_to_checkpoint(\n    paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None\n):\n    \"\"\"\n    This does the final conversion step: take locally converted weights and apply a global renaming\n    to them. It splits attention layers, and takes into account additional replacements\n    that may arise.\n\n    Assigns the weights to the new checkpoint.\n    \"\"\"\n    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n\n    # Splits the attention layers into three variables.\n    if attention_paths_to_split is not None:\n        for path, path_map in attention_paths_to_split.items():\n            old_tensor = old_checkpoint[path]\n            channels = old_tensor.shape[0] // 3\n\n            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else (-1)\n\n            num_heads = old_tensor.shape[0] // config[\"num_head_channels\"] // 3\n\n            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])\n            query, key, value = old_tensor.split(channels // num_heads, dim=1)\n\n            checkpoint[path_map[\"query\"]] = query.reshape(target_shape)\n            checkpoint[path_map[\"key\"]] = key.reshape(target_shape)\n            checkpoint[path_map[\"value\"]] = value.reshape(target_shape)\n\n    for path in paths:\n        new_path = path[\"new\"]\n\n        # These have already been assigned\n        if attention_paths_to_split is not None and new_path in attention_paths_to_split:\n            continue\n\n        # Global renaming happens here\n        ne",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 194, "column": 0 },
          "end": { "row": 194, "column": 0 }
        }
      }
    }
  ],
  [
    "1127",
    {
      "pageContent": "def conv_attn_to_linear(checkpoint):\n    keys = list(checkpoint.keys())\n    attn_keys = [\"query.weight\", \"key.weight\", \"value.weight\"]\n    for key in keys:\n        if \".\".join(key.split(\".\")[-2:]) in attn_keys:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0, 0]\n        elif \"proj_attn.weight\" in key:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0]",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 246, "column": 0 },
          "end": { "row": 246, "column": 0 }
        }
      }
    }
  ],
  [
    "1128",
    {
      "pageContent": "def create_image_unet_diffusers_config(unet_params):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the VD model.\n    \"\"\"\n\n    block_out_channels = [unet_params.model_channels * mult for mult in unet_params.channel_mult]\n\n    down_block_types = []\n    resolution = 1\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnDownBlock2D\" if unet_params.with_attn[i] else \"DownBlock2D\"\n        down_block_types.append(block_type)\n        if i != len(block_out_channels) - 1:\n            resolution *= 2\n\n    up_block_types = []\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnUpBlock2D\" if unet_params.with_attn[-i - 1] else \"UpBlock2D\"\n        up_block_types.append(block_type)\n        resolution //= 2\n\n    if not all(n == unet_params.num_noattn_blocks[0] for n in unet_params.num_noattn_blocks):\n        raise ValueError(\"Not all num_res_blocks are equal, which is not supported in this script.\")\n\n    config = dict(\n        sample_size=None,\n        in_channels=unet_params.input_channels,\n        out_channels=unet_params.output_channels,\n        down_block_types=tuple(down_block_types),\n        up_block_types=tuple(up_block_types),\n        block_out_channels=tuple(block_out_channels),\n        layers_per_block=unet_params.num_noattn_blocks[0],\n        cross_attention_dim=unet_params.context_dim,\n        attention_head_dim=unet_params.num_heads,\n    )\n\n    return config",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 258, "column": 0 },
          "end": { "row": 258, "column": 0 }
        }
      }
    }
  ],
  [
    "1129",
    {
      "pageContent": "def create_text_unet_diffusers_config(unet_params):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the VD model.\n    \"\"\"\n\n    block_out_channels = [unet_params.model_channels * mult for mult in unet_params.channel_mult]\n\n    down_block_types = []\n    resolution = 1\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnDownBlockFlat\" if unet_params.with_attn[i] else \"DownBlockFlat\"\n        down_block_types.append(block_type)\n        if i != len(block_out_channels) - 1:\n            resolution *= 2\n\n    up_block_types = []\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnUpBlockFlat\" if unet_params.with_attn[-i - 1] else \"UpBlockFlat\"\n        up_block_types.append(block_type)\n        resolution //= 2\n\n    if not all(n == unet_params.num_noattn_blocks[0] for n in unet_params.num_noattn_blocks):\n        raise ValueError(\"Not all num_res_blocks are equal, which is not supported in this script.\")\n\n    config = dict(\n        sample_size=None,\n        in_channels=(unet_params.input_channels, 1, 1),\n        out_channels=(unet_params.output_channels, 1, 1),\n        down_block_types=tuple(down_block_types),\n        up_block_types=tuple(up_block_types),\n        block_out_channels=tuple(block_out_channels),\n        layers_per_block=unet_params.num_noattn_blocks[0],\n        cross_attention_dim=unet_params.context_dim,\n        attention_head_dim=unet_params.num_heads,\n    )\n\n    return config",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 297, "column": 0 },
          "end": { "row": 297, "column": 0 }
        }
      }
    }
  ],
  [
    "1130",
    {
      "pageContent": "def create_vae_diffusers_config(vae_params):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the VD model.\n    \"\"\"\n\n    block_out_channels = [vae_params.ch * mult for mult in vae_params.ch_mult]\n    down_block_types = [\"DownEncoderBlock2D\"] * len(block_out_channels)\n    up_block_types = [\"UpDecoderBlock2D\"] * len(block_out_channels)\n\n    config = dict(\n        sample_size=vae_params.resolution,\n        in_channels=vae_params.in_channels,\n        out_channels=vae_params.out_ch,\n        down_block_types=tuple(down_block_types),\n        up_block_types=tuple(up_block_types),\n        block_out_channels=tuple(block_out_channels),\n        latent_channels=vae_params.z_channels,\n        layers_per_block=vae_params.num_res_blocks,\n    )\n    return config",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 336, "column": 0 },
          "end": { "row": 336, "column": 0 }
        }
      }
    }
  ],
  [
    "1131",
    {
      "pageContent": "def create_diffusers_scheduler(original_config):\n    schedular = DDIMScheduler(\n        num_train_timesteps=original_config.model.params.timesteps,\n        beta_start=original_config.model.params.linear_start,\n        beta_end=original_config.model.params.linear_end,\n        beta_schedule=\"scaled_linear\",\n    )\n    return schedular",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 358, "column": 0 },
          "end": { "row": 358, "column": 0 }
        }
      }
    }
  ],
  [
    "1132",
    {
      "pageContent": "def convert_vd_unet_checkpoint(checkpoint, config, unet_key, extract_ema=False):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n\n    # extract state_dict for UNet\n    unet_state_dict = {}\n    keys = list(checkpoint.keys())\n\n    # at least a 100 parameters have to start with `model_ema` in order for the checkpoint to be EMA\n    if sum(k.startswith(\"model_ema\") for k in keys) > 100:\n        print(\"Checkpoint has both EMA and non-EMA weights.\")\n        if extract_ema:\n            print(\n                \"In this conversion only the EMA weights are extracted. If you want to instead extract the non-EMA\"\n                \" weights (useful to continue fine-tuning), please make sure to remove the `--extract_ema` flag.\"\n            )\n            for key in keys:\n                if key.startswith(\"model.diffusion_model\"):\n                    flat_ema_key = \"model_ema.\" + \"\".join(key.split(\".\")[1:])\n                    unet_state_dict[key.replace(unet_key, \"\")] = checkpoint.pop(flat_ema_key)\n        else:\n            print(\n                \"In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA\"\n                \" weights (usually better for inference), please make sure to add the `--extract_ema` flag.\"\n            )\n\n    for key in keys:\n        if key.startswith(unet_key):\n            unet_state_dict[key.replace(unet_key, \"\")] = checkpoint.pop(key)\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"time_embedding.linear_1.weight\"] = checkpoint[\"model.diffusion_model.time_embed.0.weight\"]\n    new_checkpoint[",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 368, "column": 0 },
          "end": { "row": 368, "column": 0 }
        }
      }
    }
  ],
  [
    "1133",
    {
      "pageContent": "def convert_vd_vae_checkpoint(checkpoint, config):\n    # extract state dict for VAE\n    vae_state_dict = {}\n    keys = list(checkpoint.keys())\n    for key in keys:\n        vae_state_dict[key] = checkpoint.get(key)\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"encoder.conv_in.weight\"] = vae_state_dict[\"encoder.conv_in.weight\"]\n    new_checkpoint[\"encoder.conv_in.bias\"] = vae_state_dict[\"encoder.conv_in.bias\"]\n    new_checkpoint[\"encoder.conv_out.weight\"] = vae_state_dict[\"encoder.conv_out.weight\"]\n    new_checkpoint[\"encoder.conv_out.bias\"] = vae_state_dict[\"encoder.conv_out.bias\"]\n    new_checkpoint[\"encoder.conv_norm_out.weight\"] = vae_state_dict[\"encoder.norm_out.weight\"]\n    new_checkpoint[\"encoder.conv_norm_out.bias\"] = vae_state_dict[\"encoder.norm_out.bias\"]\n\n    new_checkpoint[\"decoder.conv_in.weight\"] = vae_state_dict[\"decoder.conv_in.weight\"]\n    new_checkpoint[\"decoder.conv_in.bias\"] = vae_state_dict[\"decoder.conv_in.bias\"]\n    new_checkpoint[\"decoder.conv_out.weight\"] = vae_state_dict[\"decoder.conv_out.weight\"]\n    new_checkpoint[\"decoder.conv_out.bias\"] = vae_state_dict[\"decoder.conv_out.bias\"]\n    new_checkpoint[\"decoder.conv_norm_out.weight\"] = vae_state_dict[\"decoder.norm_out.weight\"]\n    new_checkpoint[\"decoder.conv_norm_out.bias\"] = vae_state_dict[\"decoder.norm_out.bias\"]\n\n    new_checkpoint[\"quant_conv.weight\"] = vae_state_dict[\"quant_conv.weight\"]\n    new_checkpoint[\"quant_conv.bias\"] = vae_state_dict[\"quant_conv.bias\"]\n    new_checkpoint[\"post_quant_conv.weight\"] = vae_state_dict[\"post_quant_conv.weight\"]\n    new_checkpoint[\"post_quant_conv.bias\"] = vae_s",
      "metadata": {
        "source": "scripts/convert_versatile_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 573, "column": 0 },
          "end": { "row": 573, "column": 0 }
        }
      }
    }
  ],
  [
    "1134",
    {
      "pageContent": "def onnx_export(\n    model,\n    model_args: tuple,\n    output_path: Path,\n    ordered_input_names,\n    output_names,\n    dynamic_axes,\n    opset,\n    use_external_data_format=False,\n):\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    # PyTorch deprecated the `enable_onnx_checker` and `use_external_data_format` arguments in v1.11,\n    # so we check the torch version for backwards compatibility\n    if is_torch_less_than_1_11:\n        export(\n            model,\n            model_args,\n            f=output_path.as_posix(),\n            input_names=ordered_input_names,\n            output_names=output_names,\n            dynamic_axes=dynamic_axes,\n            do_constant_folding=True,\n            use_external_data_format=use_external_data_format,\n            enable_onnx_checker=True,\n            opset_version=opset,\n        )\n    else:\n        export(\n            model,\n            model_args,\n            f=output_path.as_posix(),\n            input_names=ordered_input_names,\n            output_names=output_names,\n            dynamic_axes=dynamic_axes,\n            do_constant_folding=True,\n            opset_version=opset,\n        )",
      "metadata": {
        "source": "scripts/convert_vae_diff_to_onnx.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1135",
    {
      "pageContent": "def convert_models(model_path: str, output_path: str, opset: int, fp16: bool = False):\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = \"cuda\"\n    elif fp16 and not torch.cuda.is_available():\n        raise ValueError(\"`float16` model export is only supported on GPUs with CUDA\")\n    else:\n        device = \"cpu\"\n    output_path = Path(output_path)\n\n    # VAE DECODER\n    vae_decoder = AutoencoderKL.from_pretrained(model_path + \"/vae\")\n    vae_latent_channels = vae_decoder.config.latent_channels\n    # forward only through the decoder part\n    vae_decoder.forward = vae_decoder.decode\n    onnx_export(\n        vae_decoder,\n        model_args=(\n            torch.randn(1, vae_latent_channels, 25, 25).to(device=device, dtype=dtype),\n            False,\n        ),\n        output_path=output_path / \"vae_decoder\" / \"model.onnx\",\n        ordered_input_names=[\"latent_sample\", \"return_dict\"],\n        output_names=[\"sample\"],\n        dynamic_axes={\n            \"latent_sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n        },\n        opset=opset,\n    )\n    del vae_decoder",
      "metadata": {
        "source": "scripts/convert_vae_diff_to_onnx.py",
        "range": {
          "start": { "row": 67, "column": 0 },
          "end": { "row": 67, "column": 0 }
        }
      }
    }
  ],
  [
    "1136",
    {
      "pageContent": "def onnx_export(\n    model,\n    model_args: tuple,\n    output_path: Path,\n    ordered_input_names,\n    output_names,\n    dynamic_axes,\n    opset,\n    use_external_data_format=False,\n):\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    # PyTorch deprecated the `enable_onnx_checker` and `use_external_data_format` arguments in v1.11,\n    # so we check the torch version for backwards compatibility\n    if is_torch_less_than_1_11:\n        export(\n            model,\n            model_args,\n            f=output_path.as_posix(),\n            input_names=ordered_input_names,\n            output_names=output_names,\n            dynamic_axes=dynamic_axes,\n            do_constant_folding=True,\n            use_external_data_format=use_external_data_format,\n            enable_onnx_checker=True,\n            opset_version=opset,\n        )\n    else:\n        export(\n            model,\n            model_args,\n            f=output_path.as_posix(),\n            input_names=ordered_input_names,\n            output_names=output_names,\n            dynamic_axes=dynamic_axes,\n            do_constant_folding=True,\n            opset_version=opset,\n        )",
      "metadata": {
        "source": "scripts/convert_stable_diffusion_checkpoint_to_onnx.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "1137",
    {
      "pageContent": "def convert_models(model_path: str, output_path: str, opset: int, fp16: bool = False):\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = \"cuda\"\n    elif fp16 and not torch.cuda.is_available():\n        raise ValueError(\"`float16` model export is only supported on GPUs with CUDA\")\n    else:\n        device = \"cpu\"\n    pipeline = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=dtype).to(device)\n    output_path = Path(output_path)\n\n    # TEXT ENCODER\n    num_tokens = pipeline.text_encoder.config.max_position_embeddings\n    text_hidden_size = pipeline.text_encoder.config.hidden_size\n    text_input = pipeline.tokenizer(\n        \"A sample prompt\",\n        padding=\"max_length\",\n        max_length=pipeline.tokenizer.model_max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    onnx_export(\n        pipeline.text_encoder,\n        # casting to torch.int32 until the CLIP fix is released: https://github.com/huggingface/transformers/pull/18515/files\n        model_args=(text_input.input_ids.to(device=device, dtype=torch.int32)),\n        output_path=output_path / \"text_encoder\" / \"model.onnx\",\n        ordered_input_names=[\"input_ids\"],\n        output_names=[\"last_hidden_state\", \"pooler_output\"],\n        dynamic_axes={\n            \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n        },\n        opset=opset,\n    )\n    del pipeline.text_encoder\n\n    # UNET\n    unet_in_channels = pipeline.unet.config.in_channels\n    unet_sample_size = pipeline.unet.config.sample_size\n    unet_path = output_path / \"unet",
      "metadata": {
        "source": "scripts/convert_stable_diffusion_checkpoint_to_onnx.py",
        "range": {
          "start": { "row": 70, "column": 0 },
          "end": { "row": 70, "column": 0 }
        }
      }
    }
  ],
  [
    "1138",
    {
      "pageContent": "def alpha_sigma_to_t(alpha, sigma):\n    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n    the noise.\"\"\"\n    return torch.atan2(sigma, alpha) / math.pi * 2",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "1139",
    {
      "pageContent": "def get_crash_schedule(t):\n    sigma = torch.sin(t * math.pi / 2) ** 2\n    alpha = (1 - sigma**2) ** 0.5\n    return alpha_sigma_to_t(alpha, sigma)",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "1140",
    {
      "pageContent": "class DiffusionUncond(nn.Module):\n    def __init__(self, global_args):\n        super().__init__()\n\n        self.diffusion = DiffusionAttnUnet1D(global_args, n_attn_layers=4)\n        self.diffusion_ema = deepcopy(self.diffusion)\n        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "1141",
    {
      "pageContent": "def __init__(self, global_args):\n        super().__init__()\n\n        self.diffusion = DiffusionAttnUnet1D(global_args, n_attn_layers=4)\n        self.diffusion_ema = deepcopy(self.diffusion)\n        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 65, "column": 4 },
          "end": { "row": 65, "column": 4 }
        }
      }
    }
  ],
  [
    "1142",
    {
      "pageContent": "def download(model_name):\n    url = MODELS_MAP[model_name][\"url\"]\n    os.system(f\"wget {url} ./\")\n\n    return f\"./{model_name}.ckpt\"",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 73, "column": 0 },
          "end": { "row": 73, "column": 0 }
        }
      }
    }
  ],
  [
    "1143",
    {
      "pageContent": "def convert_resconv_naming(name):\n    if name.startswith(\"skip\"):\n        return name.replace(\"skip\", RES_CONV_MAP[\"skip\"])\n\n    # name has to be of format main.{digit}\n    if not name.startswith(\"main.\"):\n        raise ValueError(f\"ResConvBlock error with {name}\")\n\n    return name.replace(name[:6], RES_CONV_MAP[name[:6]])",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 134, "column": 0 },
          "end": { "row": 134, "column": 0 }
        }
      }
    }
  ],
  [
    "1144",
    {
      "pageContent": "def convert_attn_naming(name):\n    for key, value in ATTN_MAP.items():\n        if name.startswith(key) and not isinstance(value, list):\n            return name.replace(key, value)\n        elif name.startswith(key):\n            return [name.replace(key, v) for v in value]\n    raise ValueError(f\"Attn error with {name}\")",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 145, "column": 0 },
          "end": { "row": 145, "column": 0 }
        }
      }
    }
  ],
  [
    "1145",
    {
      "pageContent": "def rename(input_string, max_depth=13):\n    string = input_string\n\n    if string.split(\".\")[0] == \"timestep_embed\":\n        return string.replace(\"timestep_embed\", \"time_proj\")\n\n    depth = 0\n    if string.startswith(\"net.3.\"):\n        depth += 1\n        string = string[6:]\n    elif string.startswith(\"net.\"):\n        string = string[4:]\n\n    while string.startswith(\"main.7.\"):\n        depth += 1\n        string = string[7:]\n\n    if string.startswith(\"main.\"):\n        string = string[5:]\n\n    # mid block\n    if string[:2].isdigit():\n        layer_num = string[:2]\n        string_left = string[2:]\n    else:\n        layer_num = string[0]\n        string_left = string[1:]\n\n    if depth == max_depth:\n        new_layer = MID_NUM_TO_LAYER[layer_num]\n        prefix = \"mid_block\"\n    elif depth > 0 and int(layer_num) < 7:\n        new_layer = DOWN_NUM_TO_LAYER[layer_num]\n        prefix = f\"down_blocks.{depth}\"\n    elif depth > 0 and int(layer_num) > 7:\n        new_layer = UP_NUM_TO_LAYER[layer_num]\n        prefix = f\"up_blocks.{max_depth - depth - 1}\"\n    elif depth == 0:\n        new_layer = DEPTH_0_TO_LAYER[layer_num]\n        prefix = f\"up_blocks.{max_depth - 1}\" if int(layer_num) > 3 else \"down_blocks.0\"\n\n    if not string_left.startswith(\".\"):\n        raise ValueError(f\"Naming error with {input_string} and string_left: {string_left}.\")\n\n    string_left = string_left[1:]\n\n    if \"resnets\" in new_layer:\n        string_left = convert_resconv_naming(string_left)\n    elif \"attentions\" in new_layer:\n        new_string_left = convert_attn_naming(string_left)\n        string_left = new_string",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 154, "column": 0 },
          "end": { "row": 154, "column": 0 }
        }
      }
    }
  ],
  [
    "1146",
    {
      "pageContent": "def rename_orig_weights(state_dict):\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.endswith(\"kernel\"):\n            # up- and downsample layers, don't have trainable weights\n            continue\n\n        new_k = rename(k)\n\n        # check if we need to transform from Conv => Linear for attention\n        if isinstance(new_k, list):\n            new_state_dict = transform_conv_attns(new_state_dict, new_k, v)\n        else:\n            new_state_dict[new_k] = v\n\n    return new_state_dict",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 213, "column": 0 },
          "end": { "row": 213, "column": 0 }
        }
      }
    }
  ],
  [
    "1147",
    {
      "pageContent": "def transform_conv_attns(new_state_dict, new_k, v):\n    if len(new_k) == 1:\n        if len(v.shape) == 3:\n            # weight\n            new_state_dict[new_k[0]] = v[:, :, 0]\n        else:\n            # bias\n            new_state_dict[new_k[0]] = v\n    else:\n        # qkv matrices\n        trippled_shape = v.shape[0]\n        single_shape = trippled_shape // 3\n        for i in range(3):\n            if len(v.shape) == 3:\n                new_state_dict[new_k[i]] = v[i * single_shape : (i + 1) * single_shape, :, 0]\n            else:\n                new_state_dict[new_k[i]] = v[i * single_shape : (i + 1) * single_shape]\n    return new_state_dict",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 231, "column": 0 },
          "end": { "row": 231, "column": 0 }
        }
      }
    }
  ],
  [
    "1148",
    {
      "pageContent": "def main(args):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_name = args.model_path.split(\"/\")[-1].split(\".\")[0]\n    if not os.path.isfile(args.model_path):\n        assert (\n            model_name == args.model_path\n        ), f\"Make sure to provide one of the official model names {MODELS_MAP.keys()}\"\n        args.model_path = download(model_name)\n\n    sample_rate = MODELS_MAP[model_name][\"sample_rate\"]\n    sample_size = MODELS_MAP[model_name][\"sample_size\"]\n\n    config = Object()\n    config.sample_size = sample_size\n    config.sample_rate = sample_rate\n    config.latent_dim = 0\n\n    diffusers_model = UNet1DModel(sample_size=sample_size, sample_rate=sample_rate)\n    diffusers_state_dict = diffusers_model.state_dict()\n\n    orig_model = DiffusionUncond(config)\n    orig_model.load_state_dict(torch.load(args.model_path, map_location=device)[\"state_dict\"])\n    orig_model = orig_model.diffusion_ema.eval()\n    orig_model_state_dict = orig_model.state_dict()\n    renamed_state_dict = rename_orig_weights(orig_model_state_dict)\n\n    renamed_minus_diffusers = set(renamed_state_dict.keys()) - set(diffusers_state_dict.keys())\n    diffusers_minus_renamed = set(diffusers_state_dict.keys()) - set(renamed_state_dict.keys())\n\n    assert len(renamed_minus_diffusers) == 0, f\"Problem with {renamed_minus_diffusers}\"\n    assert all(k.endswith(\"kernel\") for k in list(diffusers_minus_renamed)), f\"Problem with {diffusers_minus_renamed}\"\n\n    for key, value in renamed_state_dict.items():\n        assert (\n            diffusers_state_dict[key].squeeze().shape == v",
      "metadata": {
        "source": "scripts/convert_dance_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 251, "column": 0 },
          "end": { "row": 251, "column": 0 }
        }
      }
    }
  ],
  [
    "1149",
    {
      "pageContent": "def shave_segments(path, n_shave_prefix_segments=1):\n    \"\"\"\n    Removes segments. Positive values shave the first segments, negative shave the last segments.\n    \"\"\"\n    if n_shave_prefix_segments >= 0:\n        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n    else:\n        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])",
      "metadata": {
        "source": "scripts/convert_ddpm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "1150",
    {
      "pageContent": "def renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n        new_item = new_item.replace(\"block.\", \"resnets.\")\n        new_item = new_item.replace(\"conv_shorcut\", \"conv1\")\n        new_item = new_item.replace(\"in_shortcut\", \"conv_shortcut\")\n        new_item = new_item.replace(\"temb_proj\", \"time_emb_proj\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_ddpm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 18, "column": 0 },
          "end": { "row": 18, "column": 0 }
        }
      }
    }
  ],
  [
    "1151",
    {
      "pageContent": "def renew_attention_paths(old_list, n_shave_prefix_segments=0, in_mid=False):\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        # In `model.mid`, the layer is called `attn`.\n        if not in_mid:\n            new_item = new_item.replace(\"attn\", \"attentions\")\n        new_item = new_item.replace(\".k.\", \".key.\")\n        new_item = new_item.replace(\".v.\", \".value.\")\n        new_item = new_item.replace(\".q.\", \".query.\")\n\n        new_item = new_item.replace(\"proj_out\", \"proj_attn\")\n        new_item = new_item.replace(\"norm\", \"group_norm\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_ddpm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1152",
    {
      "pageContent": "def assign_to_checkpoint(\n    paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None\n):\n    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n\n    if attention_paths_to_split is not None:\n        if config is None:\n            raise ValueError(\"Please specify the config if setting 'attention_paths_to_split' to 'True'.\")\n\n        for path, path_map in attention_paths_to_split.items():\n            old_tensor = old_checkpoint[path]\n            channels = old_tensor.shape[0] // 3\n\n            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else (-1)\n\n            num_heads = old_tensor.shape[0] // config.get(\"num_head_channels\", 1) // 3\n\n            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])\n            query, key, value = old_tensor.split(channels // num_heads, dim=1)\n\n            checkpoint[path_map[\"query\"]] = query.reshape(target_shape).squeeze()\n            checkpoint[path_map[\"key\"]] = key.reshape(target_shape).squeeze()\n            checkpoint[path_map[\"value\"]] = value.reshape(target_shape).squeeze()\n\n    for path in paths:\n        new_path = path[\"new\"]\n\n        if attention_paths_to_split is not None and new_path in attention_paths_to_split:\n            continue\n\n        new_path = new_path.replace(\"down.\", \"down_blocks.\")\n        new_path = new_path.replace(\"up.\", \"up_blocks.\")\n\n        if additional_replacements is not None:\n            for replacement in additional_replacements:\n                new_path ",
      "metadata": {
        "source": "scripts/convert_ddpm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 55, "column": 0 },
          "end": { "row": 55, "column": 0 }
        }
      }
    }
  ],
  [
    "1153",
    {
      "pageContent": "def convert_ddpm_checkpoint(checkpoint, config):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n    new_checkpoint = {}\n\n    new_checkpoint[\"time_embedding.linear_1.weight\"] = checkpoint[\"temb.dense.0.weight\"]\n    new_checkpoint[\"time_embedding.linear_1.bias\"] = checkpoint[\"temb.dense.0.bias\"]\n    new_checkpoint[\"time_embedding.linear_2.weight\"] = checkpoint[\"temb.dense.1.weight\"]\n    new_checkpoint[\"time_embedding.linear_2.bias\"] = checkpoint[\"temb.dense.1.bias\"]\n\n    new_checkpoint[\"conv_norm_out.weight\"] = checkpoint[\"norm_out.weight\"]\n    new_checkpoint[\"conv_norm_out.bias\"] = checkpoint[\"norm_out.bias\"]\n\n    new_checkpoint[\"conv_in.weight\"] = checkpoint[\"conv_in.weight\"]\n    new_checkpoint[\"conv_in.bias\"] = checkpoint[\"conv_in.bias\"]\n    new_checkpoint[\"conv_out.weight\"] = checkpoint[\"conv_out.weight\"]\n    new_checkpoint[\"conv_out.bias\"] = checkpoint[\"conv_out.bias\"]\n\n    num_down_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in checkpoint if \"down\" in layer})\n    down_blocks = {\n        layer_id: [key for key in checkpoint if f\"down.{layer_id}\" in key] for layer_id in range(num_down_blocks)\n    }\n\n    num_up_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in checkpoint if \"up\" in layer})\n    up_blocks = {layer_id: [key for key in checkpoint if f\"up.{layer_id}\" in key] for layer_id in range(num_up_blocks)}\n\n    for i in range(num_down_blocks):\n        block_id = (i - 1) // (config[\"layers_per_block\"] + 1)\n\n        if any(\"downsample\" in layer for layer in down_blocks[i]):\n            new_checkpoint[f\"down_blocks",
      "metadata": {
        "source": "scripts/convert_ddpm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 98, "column": 0 },
          "end": { "row": 98, "column": 0 }
        }
      }
    }
  ],
  [
    "1154",
    {
      "pageContent": "def convert_vq_autoenc_checkpoint(checkpoint, config):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n    new_checkpoint = {}\n\n    new_checkpoint[\"encoder.conv_norm_out.weight\"] = checkpoint[\"encoder.norm_out.weight\"]\n    new_checkpoint[\"encoder.conv_norm_out.bias\"] = checkpoint[\"encoder.norm_out.bias\"]\n\n    new_checkpoint[\"encoder.conv_in.weight\"] = checkpoint[\"encoder.conv_in.weight\"]\n    new_checkpoint[\"encoder.conv_in.bias\"] = checkpoint[\"encoder.conv_in.bias\"]\n    new_checkpoint[\"encoder.conv_out.weight\"] = checkpoint[\"encoder.conv_out.weight\"]\n    new_checkpoint[\"encoder.conv_out.bias\"] = checkpoint[\"encoder.conv_out.bias\"]\n\n    new_checkpoint[\"decoder.conv_norm_out.weight\"] = checkpoint[\"decoder.norm_out.weight\"]\n    new_checkpoint[\"decoder.conv_norm_out.bias\"] = checkpoint[\"decoder.norm_out.bias\"]\n\n    new_checkpoint[\"decoder.conv_in.weight\"] = checkpoint[\"decoder.conv_in.weight\"]\n    new_checkpoint[\"decoder.conv_in.bias\"] = checkpoint[\"decoder.conv_in.bias\"]\n    new_checkpoint[\"decoder.conv_out.weight\"] = checkpoint[\"decoder.conv_out.weight\"]\n    new_checkpoint[\"decoder.conv_out.bias\"] = checkpoint[\"decoder.conv_out.bias\"]\n\n    num_down_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in checkpoint if \"down\" in layer})\n    down_blocks = {\n        layer_id: [key for key in checkpoint if f\"down.{layer_id}\" in key] for layer_id in range(num_down_blocks)\n    }\n\n    num_up_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in checkpoint if \"up\" in layer})\n    up_blocks = {layer_id: [key for key in checkpoint if f\"up.{l",
      "metadata": {
        "source": "scripts/convert_ddpm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 234, "column": 0 },
          "end": { "row": 234, "column": 0 }
        }
      }
    }
  ],
  [
    "1155",
    {
      "pageContent": "def vqvae_model_from_original_config(original_config):\n    assert original_config.target in PORTED_VQVAES, f\"{original_config.target} has not yet been ported to diffusers.\"\n\n    original_config = original_config.params\n\n    original_encoder_config = original_config.encoder_config.params\n    original_decoder_config = original_config.decoder_config.params\n\n    in_channels = original_encoder_config.in_channels\n    out_channels = original_decoder_config.out_ch\n\n    down_block_types = get_down_block_types(original_encoder_config)\n    up_block_types = get_up_block_types(original_decoder_config)\n\n    assert original_encoder_config.ch == original_decoder_config.ch\n    assert original_encoder_config.ch_mult == original_decoder_config.ch_mult\n    block_out_channels = tuple(\n        [original_encoder_config.ch * a_ch_mult for a_ch_mult in original_encoder_config.ch_mult]\n    )\n\n    assert original_encoder_config.num_res_blocks == original_decoder_config.num_res_blocks\n    layers_per_block = original_encoder_config.num_res_blocks\n\n    assert original_encoder_config.z_channels == original_decoder_config.z_channels\n    latent_channels = original_encoder_config.z_channels\n\n    num_vq_embeddings = original_config.n_embed\n\n    # Hard coded value for ResnetBlock.GoupNorm(num_groups) in VQ-diffusion\n    norm_num_groups = 32\n\n    e_dim = original_config.embed_dim\n\n    model = VQModel(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        down_block_types=down_block_types,\n        up_block_types=up_block_types,\n        block_out_channels=block_out_channels,\n        layers_",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 60, "column": 0 },
          "end": { "row": 60, "column": 0 }
        }
      }
    }
  ],
  [
    "1156",
    {
      "pageContent": "def get_down_block_types(original_encoder_config):\n    attn_resolutions = coerce_attn_resolutions(original_encoder_config.attn_resolutions)\n    num_resolutions = len(original_encoder_config.ch_mult)\n    resolution = coerce_resolution(original_encoder_config.resolution)\n\n    curr_res = resolution\n    down_block_types = []\n\n    for _ in range(num_resolutions):\n        if curr_res in attn_resolutions:\n            down_block_type = \"AttnDownEncoderBlock2D\"\n        else:\n            down_block_type = \"DownEncoderBlock2D\"\n\n        down_block_types.append(down_block_type)\n\n        curr_res = [r // 2 for r in curr_res]\n\n    return down_block_types",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 109, "column": 0 },
          "end": { "row": 109, "column": 0 }
        }
      }
    }
  ],
  [
    "1157",
    {
      "pageContent": "def get_up_block_types(original_decoder_config):\n    attn_resolutions = coerce_attn_resolutions(original_decoder_config.attn_resolutions)\n    num_resolutions = len(original_decoder_config.ch_mult)\n    resolution = coerce_resolution(original_decoder_config.resolution)\n\n    curr_res = [r // 2 ** (num_resolutions - 1) for r in resolution]\n    up_block_types = []\n\n    for _ in reversed(range(num_resolutions)):\n        if curr_res in attn_resolutions:\n            up_block_type = \"AttnUpDecoderBlock2D\"\n        else:\n            up_block_type = \"UpDecoderBlock2D\"\n\n        up_block_types.append(up_block_type)\n\n        curr_res = [r * 2 for r in curr_res]\n\n    return up_block_types",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 130, "column": 0 },
          "end": { "row": 130, "column": 0 }
        }
      }
    }
  ],
  [
    "1158",
    {
      "pageContent": "def coerce_attn_resolutions(attn_resolutions):\n    attn_resolutions = OmegaConf.to_object(attn_resolutions)\n    attn_resolutions_ = []\n    for ar in attn_resolutions:\n        if isinstance(ar, (list, tuple)):\n            attn_resolutions_.append(list(ar))\n        else:\n            attn_resolutions_.append([ar, ar])\n    return attn_resolutions_",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 151, "column": 0 },
          "end": { "row": 151, "column": 0 }
        }
      }
    }
  ],
  [
    "1159",
    {
      "pageContent": "def coerce_resolution(resolution):\n    resolution = OmegaConf.to_object(resolution)\n    if isinstance(resolution, int):\n        resolution = [resolution, resolution]  # H, W\n    elif isinstance(resolution, (tuple, list)):\n        resolution = list(resolution)\n    else:\n        raise ValueError(\"Unknown type of resolution:\", resolution)\n    return resolution",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 162, "column": 0 },
          "end": { "row": 162, "column": 0 }
        }
      }
    }
  ],
  [
    "1160",
    {
      "pageContent": "def vqvae_original_checkpoint_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    diffusers_checkpoint.update(vqvae_encoder_to_diffusers_checkpoint(model, checkpoint))\n\n    # quant_conv\n\n    diffusers_checkpoint.update(\n        {\n            \"quant_conv.weight\": checkpoint[\"quant_conv.weight\"],\n            \"quant_conv.bias\": checkpoint[\"quant_conv.bias\"],\n        }\n    )\n\n    # quantize\n    diffusers_checkpoint.update({\"quantize.embedding.weight\": checkpoint[\"quantize.embedding\"]})\n\n    # post_quant_conv\n    diffusers_checkpoint.update(\n        {\n            \"post_quant_conv.weight\": checkpoint[\"post_quant_conv.weight\"],\n            \"post_quant_conv.bias\": checkpoint[\"post_quant_conv.bias\"],\n        }\n    )\n\n    # decoder\n    diffusers_checkpoint.update(vqvae_decoder_to_diffusers_checkpoint(model, checkpoint))\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 178, "column": 0 },
          "end": { "row": 178, "column": 0 }
        }
      }
    }
  ],
  [
    "1161",
    {
      "pageContent": "def vqvae_encoder_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    # conv_in\n    diffusers_checkpoint.update(\n        {\n            \"encoder.conv_in.weight\": checkpoint[\"encoder.conv_in.weight\"],\n            \"encoder.conv_in.bias\": checkpoint[\"encoder.conv_in.bias\"],\n        }\n    )\n\n    # down_blocks\n    for down_block_idx, down_block in enumerate(model.encoder.down_blocks):\n        diffusers_down_block_prefix = f\"encoder.down_blocks.{down_block_idx}\"\n        down_block_prefix = f\"encoder.down.{down_block_idx}\"\n\n        # resnets\n        for resnet_idx, resnet in enumerate(down_block.resnets):\n            diffusers_resnet_prefix = f\"{diffusers_down_block_prefix}.resnets.{resnet_idx}\"\n            resnet_prefix = f\"{down_block_prefix}.block.{resnet_idx}\"\n\n            diffusers_checkpoint.update(\n                vqvae_resnet_to_diffusers_checkpoint(\n                    resnet, checkpoint, diffusers_resnet_prefix=diffusers_resnet_prefix, resnet_prefix=resnet_prefix\n                )\n            )\n\n        # downsample\n\n        # do not include the downsample when on the last down block\n        # There is no downsample on the last down block\n        if down_block_idx != len(model.encoder.down_blocks) - 1:\n            # There's a single downsample in the original checkpoint but a list of downsamples\n            # in the diffusers model.\n            diffusers_downsample_prefix = f\"{diffusers_down_block_prefix}.downsamplers.0.conv\"\n            downsample_prefix = f\"{down_block_prefix}.downsample.conv\"\n            diffusers_checkpoint.update(\n         ",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 209, "column": 0 },
          "end": { "row": 209, "column": 0 }
        }
      }
    }
  ],
  [
    "1162",
    {
      "pageContent": "def vqvae_decoder_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    # conv in\n    diffusers_checkpoint.update(\n        {\n            \"decoder.conv_in.weight\": checkpoint[\"decoder.conv_in.weight\"],\n            \"decoder.conv_in.bias\": checkpoint[\"decoder.conv_in.bias\"],\n        }\n    )\n\n    # up_blocks\n\n    for diffusers_up_block_idx, up_block in enumerate(model.decoder.up_blocks):\n        # up_blocks are stored in reverse order in the VQ-diffusion checkpoint\n        orig_up_block_idx = len(model.decoder.up_blocks) - 1 - diffusers_up_block_idx\n\n        diffusers_up_block_prefix = f\"decoder.up_blocks.{diffusers_up_block_idx}\"\n        up_block_prefix = f\"decoder.up.{orig_up_block_idx}\"\n\n        # resnets\n        for resnet_idx, resnet in enumerate(up_block.resnets):\n            diffusers_resnet_prefix = f\"{diffusers_up_block_prefix}.resnets.{resnet_idx}\"\n            resnet_prefix = f\"{up_block_prefix}.block.{resnet_idx}\"\n\n            diffusers_checkpoint.update(\n                vqvae_resnet_to_diffusers_checkpoint(\n                    resnet, checkpoint, diffusers_resnet_prefix=diffusers_resnet_prefix, resnet_prefix=resnet_prefix\n                )\n            )\n\n        # upsample\n\n        # there is no up sample on the last up block\n        if diffusers_up_block_idx != len(model.decoder.up_blocks) - 1:\n            # There's a single upsample in the VQ-diffusion checkpoint but a list of downsamples\n            # in the diffusers model.\n            diffusers_downsample_prefix = f\"{diffusers_up_block_prefix}.upsamplers.0.conv\"\n            downsample_p",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 309, "column": 0 },
          "end": { "row": 309, "column": 0 }
        }
      }
    }
  ],
  [
    "1163",
    {
      "pageContent": "def vqvae_resnet_to_diffusers_checkpoint(resnet, checkpoint, *, diffusers_resnet_prefix, resnet_prefix):\n    rv = {\n        # norm1\n        f\"{diffusers_resnet_prefix}.norm1.weight\": checkpoint[f\"{resnet_prefix}.norm1.weight\"],\n        f\"{diffusers_resnet_prefix}.norm1.bias\": checkpoint[f\"{resnet_prefix}.norm1.bias\"],\n        # conv1\n        f\"{diffusers_resnet_prefix}.conv1.weight\": checkpoint[f\"{resnet_prefix}.conv1.weight\"],\n        f\"{diffusers_resnet_prefix}.conv1.bias\": checkpoint[f\"{resnet_prefix}.conv1.bias\"],\n        # norm2\n        f\"{diffusers_resnet_prefix}.norm2.weight\": checkpoint[f\"{resnet_prefix}.norm2.weight\"],\n        f\"{diffusers_resnet_prefix}.norm2.bias\": checkpoint[f\"{resnet_prefix}.norm2.bias\"],\n        # conv2\n        f\"{diffusers_resnet_prefix}.conv2.weight\": checkpoint[f\"{resnet_prefix}.conv2.weight\"],\n        f\"{diffusers_resnet_prefix}.conv2.bias\": checkpoint[f\"{resnet_prefix}.conv2.bias\"],\n    }\n\n    if resnet.conv_shortcut is not None:\n        rv.update(\n            {\n                f\"{diffusers_resnet_prefix}.conv_shortcut.weight\": checkpoint[f\"{resnet_prefix}.nin_shortcut.weight\"],\n                f\"{diffusers_resnet_prefix}.conv_shortcut.bias\": checkpoint[f\"{resnet_prefix}.nin_shortcut.bias\"],\n            }\n        )\n\n    return rv",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 412, "column": 0 },
          "end": { "row": 412, "column": 0 }
        }
      }
    }
  ],
  [
    "1164",
    {
      "pageContent": "def vqvae_attention_to_diffusers_checkpoint(checkpoint, *, diffusers_attention_prefix, attention_prefix):\n    return {\n        # group_norm\n        f\"{diffusers_attention_prefix}.group_norm.weight\": checkpoint[f\"{attention_prefix}.norm.weight\"],\n        f\"{diffusers_attention_prefix}.group_norm.bias\": checkpoint[f\"{attention_prefix}.norm.bias\"],\n        # query\n        f\"{diffusers_attention_prefix}.query.weight\": checkpoint[f\"{attention_prefix}.q.weight\"][:, :, 0, 0],\n        f\"{diffusers_attention_prefix}.query.bias\": checkpoint[f\"{attention_prefix}.q.bias\"],\n        # key\n        f\"{diffusers_attention_prefix}.key.weight\": checkpoint[f\"{attention_prefix}.k.weight\"][:, :, 0, 0],\n        f\"{diffusers_attention_prefix}.key.bias\": checkpoint[f\"{attention_prefix}.k.bias\"],\n        # value\n        f\"{diffusers_attention_prefix}.value.weight\": checkpoint[f\"{attention_prefix}.v.weight\"][:, :, 0, 0],\n        f\"{diffusers_attention_prefix}.value.bias\": checkpoint[f\"{attention_prefix}.v.bias\"],\n        # proj_attn\n        f\"{diffusers_attention_prefix}.proj_attn.weight\": checkpoint[f\"{attention_prefix}.proj_out.weight\"][\n            :, :, 0, 0\n        ],\n        f\"{diffusers_attention_prefix}.proj_attn.bias\": checkpoint[f\"{attention_prefix}.proj_out.bias\"],\n    }",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 439, "column": 0 },
          "end": { "row": 439, "column": 0 }
        }
      }
    }
  ],
  [
    "1165",
    {
      "pageContent": "def transformer_model_from_original_config(\n    original_diffusion_config, original_transformer_config, original_content_embedding_config\n):\n    assert (\n        original_diffusion_config.target in PORTED_DIFFUSIONS\n    ), f\"{original_diffusion_config.target} has not yet been ported to diffusers.\"\n    assert (\n        original_transformer_config.target in PORTED_TRANSFORMERS\n    ), f\"{original_transformer_config.target} has not yet been ported to diffusers.\"\n    assert (\n        original_content_embedding_config.target in PORTED_CONTENT_EMBEDDINGS\n    ), f\"{original_content_embedding_config.target} has not yet been ported to diffusers.\"\n\n    original_diffusion_config = original_diffusion_config.params\n    original_transformer_config = original_transformer_config.params\n    original_content_embedding_config = original_content_embedding_config.params\n\n    inner_dim = original_transformer_config[\"n_embd\"]\n\n    n_heads = original_transformer_config[\"n_head\"]\n\n    # VQ-Diffusion gives dimension of the multi-headed attention layers as the\n    # number of attention heads times the sequence length (the dimension) of a\n    # single head. We want to specify our attention blocks with those values\n    # specified separately\n    assert inner_dim % n_heads == 0\n    d_head = inner_dim // n_heads\n\n    depth = original_transformer_config[\"n_layer\"]\n    context_dim = original_transformer_config[\"condition_dim\"]\n\n    num_embed = original_content_embedding_config[\"num_embed\"]\n    # the number of embeddings in the transformer includes the mask embedding.\n    # the content embedding (the vqvae) ",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 470, "column": 0 },
          "end": { "row": 470, "column": 0 }
        }
      }
    }
  ],
  [
    "1166",
    {
      "pageContent": "def transformer_original_checkpoint_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    transformer_prefix = \"transformer.transformer\"\n\n    diffusers_latent_image_embedding_prefix = \"latent_image_embedding\"\n    latent_image_embedding_prefix = f\"{transformer_prefix}.content_emb\"\n\n    # DalleMaskImageEmbedding\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_latent_image_embedding_prefix}.emb.weight\": checkpoint[\n                f\"{latent_image_embedding_prefix}.emb.weight\"\n            ],\n            f\"{diffusers_latent_image_embedding_prefix}.height_emb.weight\": checkpoint[\n                f\"{latent_image_embedding_prefix}.height_emb.weight\"\n            ],\n            f\"{diffusers_latent_image_embedding_prefix}.width_emb.weight\": checkpoint[\n                f\"{latent_image_embedding_prefix}.width_emb.weight\"\n            ],\n        }\n    )\n\n    # transformer blocks\n    for transformer_block_idx, transformer_block in enumerate(model.transformer_blocks):\n        diffusers_transformer_block_prefix = f\"transformer_blocks.{transformer_block_idx}\"\n        transformer_block_prefix = f\"{transformer_prefix}.blocks.{transformer_block_idx}\"\n\n        # ada norm block\n        diffusers_ada_norm_prefix = f\"{diffusers_transformer_block_prefix}.norm1\"\n        ada_norm_prefix = f\"{transformer_block_prefix}.ln1\"\n\n        diffusers_checkpoint.update(\n            transformer_ada_norm_to_diffusers_checkpoint(\n                checkpoint, diffusers_ada_norm_prefix=diffusers_ada_norm_prefix, ada_norm_prefix=ada_norm_prefix\n            )\n        )\n\n      ",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 536, "column": 0 },
          "end": { "row": 536, "column": 0 }
        }
      }
    }
  ],
  [
    "1167",
    {
      "pageContent": "def transformer_ada_norm_to_diffusers_checkpoint(checkpoint, *, diffusers_ada_norm_prefix, ada_norm_prefix):\n    return {\n        f\"{diffusers_ada_norm_prefix}.emb.weight\": checkpoint[f\"{ada_norm_prefix}.emb.weight\"],\n        f\"{diffusers_ada_norm_prefix}.linear.weight\": checkpoint[f\"{ada_norm_prefix}.linear.weight\"],\n        f\"{diffusers_ada_norm_prefix}.linear.bias\": checkpoint[f\"{ada_norm_prefix}.linear.bias\"],\n    }",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 652, "column": 0 },
          "end": { "row": 652, "column": 0 }
        }
      }
    }
  ],
  [
    "1168",
    {
      "pageContent": "def transformer_attention_to_diffusers_checkpoint(checkpoint, *, diffusers_attention_prefix, attention_prefix):\n    return {\n        # key\n        f\"{diffusers_attention_prefix}.to_k.weight\": checkpoint[f\"{attention_prefix}.key.weight\"],\n        f\"{diffusers_attention_prefix}.to_k.bias\": checkpoint[f\"{attention_prefix}.key.bias\"],\n        # query\n        f\"{diffusers_attention_prefix}.to_q.weight\": checkpoint[f\"{attention_prefix}.query.weight\"],\n        f\"{diffusers_attention_prefix}.to_q.bias\": checkpoint[f\"{attention_prefix}.query.bias\"],\n        # value\n        f\"{diffusers_attention_prefix}.to_v.weight\": checkpoint[f\"{attention_prefix}.value.weight\"],\n        f\"{diffusers_attention_prefix}.to_v.bias\": checkpoint[f\"{attention_prefix}.value.bias\"],\n        # linear out\n        f\"{diffusers_attention_prefix}.to_out.0.weight\": checkpoint[f\"{attention_prefix}.proj.weight\"],\n        f\"{diffusers_attention_prefix}.to_out.0.bias\": checkpoint[f\"{attention_prefix}.proj.bias\"],\n    }",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 660, "column": 0 },
          "end": { "row": 660, "column": 0 }
        }
      }
    }
  ],
  [
    "1169",
    {
      "pageContent": "def transformer_feedforward_to_diffusers_checkpoint(checkpoint, *, diffusers_feedforward_prefix, feedforward_prefix):\n    return {\n        f\"{diffusers_feedforward_prefix}.net.0.proj.weight\": checkpoint[f\"{feedforward_prefix}.0.weight\"],\n        f\"{diffusers_feedforward_prefix}.net.0.proj.bias\": checkpoint[f\"{feedforward_prefix}.0.bias\"],\n        f\"{diffusers_feedforward_prefix}.net.2.weight\": checkpoint[f\"{feedforward_prefix}.2.weight\"],\n        f\"{diffusers_feedforward_prefix}.net.2.bias\": checkpoint[f\"{feedforward_prefix}.2.bias\"],\n    }",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 677, "column": 0 },
          "end": { "row": 677, "column": 0 }
        }
      }
    }
  ],
  [
    "1170",
    {
      "pageContent": "def read_config_file(filename):\n    # The yaml file contains annotations that certain values should\n    # loaded as tuples. By default, OmegaConf will panic when reading\n    # these. Instead, we can manually read the yaml with the FullLoader and then\n    # construct the OmegaConf object.\n    with open(filename) as f:\n        original_config = yaml.load(f, FullLoader)\n\n    return OmegaConf.create(original_config)",
      "metadata": {
        "source": "scripts/convert_vq_diffusion_to_diffusers.py",
        "range": {
          "start": { "row": 689, "column": 0 },
          "end": { "row": 689, "column": 0 }
        }
      }
    }
  ],
  [
    "1171",
    {
      "pageContent": "def prior_model_from_original_config():\n    model = PriorTransformer(**PRIOR_CONFIG)\n\n    return model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "1172",
    {
      "pageContent": "def prior_original_checkpoint_to_diffusers_checkpoint(model, checkpoint, clip_stats_checkpoint):\n    diffusers_checkpoint = {}\n\n    # <original>.time_embed.0 -> <diffusers>.time_embedding.linear_1\n    diffusers_checkpoint.update(\n        {\n            \"time_embedding.linear_1.weight\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.time_embed.0.weight\"],\n            \"time_embedding.linear_1.bias\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.time_embed.0.bias\"],\n        }\n    )\n\n    # <original>.clip_img_proj -> <diffusers>.proj_in\n    diffusers_checkpoint.update(\n        {\n            \"proj_in.weight\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.clip_img_proj.weight\"],\n            \"proj_in.bias\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.clip_img_proj.bias\"],\n        }\n    )\n\n    # <original>.text_emb_proj -> <diffusers>.embedding_proj\n    diffusers_checkpoint.update(\n        {\n            \"embedding_proj.weight\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.text_emb_proj.weight\"],\n            \"embedding_proj.bias\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.text_emb_proj.bias\"],\n        }\n    )\n\n    # <original>.text_enc_proj -> <diffusers>.encoder_hidden_states_proj\n    diffusers_checkpoint.update(\n        {\n            \"encoder_hidden_states_proj.weight\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.text_enc_proj.weight\"],\n            \"encoder_hidden_states_proj.bias\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.text_enc_proj.bias\"],\n        }\n    )\n\n    # <original>.positional_embedding -> <diffusers>.positional_embedding\n    diffusers_checkpoint.update({\"positional_embedding\": checkpoint[f\"{PRIOR_ORIGINAL_PREFIX}.positional_em",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 50, "column": 0 },
          "end": { "row": 50, "column": 0 }
        }
      }
    }
  ],
  [
    "1173",
    {
      "pageContent": "def prior_attention_to_diffusers(\n    checkpoint, *, diffusers_attention_prefix, original_attention_prefix, attention_head_dim\n):\n    diffusers_checkpoint = {}\n\n    # <original>.c_qkv -> <diffusers>.{to_q, to_k, to_v}\n    [q_weight, k_weight, v_weight], [q_bias, k_bias, v_bias] = split_attentions(\n        weight=checkpoint[f\"{original_attention_prefix}.c_qkv.weight\"],\n        bias=checkpoint[f\"{original_attention_prefix}.c_qkv.bias\"],\n        split=3,\n        chunk_size=attention_head_dim,\n    )\n\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n            f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n            f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n            f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n            f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n            f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n        }\n    )\n\n    # <original>.c_proj -> <diffusers>.to_out.0\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.to_out.0.weight\": checkpoint[f\"{original_attention_prefix}.c_proj.weight\"],\n            f\"{diffusers_attention_prefix}.to_out.0.bias\": checkpoint[f\"{original_attention_prefix}.c_proj.bias\"],\n        }\n    )\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 171, "column": 0 },
          "end": { "row": 171, "column": 0 }
        }
      }
    }
  ],
  [
    "1174",
    {
      "pageContent": "def prior_ff_to_diffusers(checkpoint, *, diffusers_ff_prefix, original_ff_prefix):\n    diffusers_checkpoint = {\n        # <original>.c_fc -> <diffusers>.net.0.proj\n        f\"{diffusers_ff_prefix}.net.{0}.proj.weight\": checkpoint[f\"{original_ff_prefix}.c_fc.weight\"],\n        f\"{diffusers_ff_prefix}.net.{0}.proj.bias\": checkpoint[f\"{original_ff_prefix}.c_fc.bias\"],\n        # <original>.c_proj -> <diffusers>.net.2\n        f\"{diffusers_ff_prefix}.net.{2}.weight\": checkpoint[f\"{original_ff_prefix}.c_proj.weight\"],\n        f\"{diffusers_ff_prefix}.net.{2}.bias\": checkpoint[f\"{original_ff_prefix}.c_proj.bias\"],\n    }\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 206, "column": 0 },
          "end": { "row": 206, "column": 0 }
        }
      }
    }
  ],
  [
    "1175",
    {
      "pageContent": "def decoder_model_from_original_config():\n    model = UNet2DConditionModel(**DECODER_CONFIG)\n\n    return model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 254, "column": 0 },
          "end": { "row": 254, "column": 0 }
        }
      }
    }
  ],
  [
    "1176",
    {
      "pageContent": "def decoder_original_checkpoint_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    original_unet_prefix = DECODER_ORIGINAL_PREFIX\n    num_head_channels = DECODER_CONFIG[\"attention_head_dim\"]\n\n    diffusers_checkpoint.update(unet_time_embeddings(checkpoint, original_unet_prefix))\n    diffusers_checkpoint.update(unet_conv_in(checkpoint, original_unet_prefix))\n\n    # <original>.input_blocks -> <diffusers>.down_blocks\n\n    original_down_block_idx = 1\n\n    for diffusers_down_block_idx in range(len(model.down_blocks)):\n        checkpoint_update, num_original_down_blocks = unet_downblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            diffusers_down_block_idx=diffusers_down_block_idx,\n            original_down_block_idx=original_down_block_idx,\n            original_unet_prefix=original_unet_prefix,\n            num_head_channels=num_head_channels,\n        )\n\n        original_down_block_idx += num_original_down_blocks\n\n        diffusers_checkpoint.update(checkpoint_update)\n\n    # done <original>.input_blocks -> <diffusers>.down_blocks\n\n    diffusers_checkpoint.update(\n        unet_midblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            original_unet_prefix=original_unet_prefix,\n            num_head_channels=num_head_channels,\n        )\n    )\n\n    # <original>.output_blocks -> <diffusers>.up_blocks\n\n    original_up_block_idx = 0\n\n    for diffusers_up_block_idx in range(len(model.up_blocks)):\n        checkpoint_update, num_original_up_blocks = unet_upblock_to_diffusers_checkpoint(\n            ",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 260, "column": 0 },
          "end": { "row": 260, "column": 0 }
        }
      }
    }
  ],
  [
    "1177",
    {
      "pageContent": "def text_proj_from_original_config():\n    # From the conditional unet constructor where the dimension of the projected time embeddings is\n    # constructed\n    time_embed_dim = DECODER_CONFIG[\"block_out_channels\"][0] * 4\n\n    cross_attention_dim = DECODER_CONFIG[\"cross_attention_dim\"]\n\n    model = UnCLIPTextProjModel(time_embed_dim=time_embed_dim, cross_attention_dim=cross_attention_dim)\n\n    return model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 329, "column": 0 },
          "end": { "row": 329, "column": 0 }
        }
      }
    }
  ],
  [
    "1178",
    {
      "pageContent": "def text_proj_original_checkpoint_to_diffusers_checkpoint(checkpoint):\n    diffusers_checkpoint = {\n        # <original>.text_seq_proj.0 -> <diffusers>.encoder_hidden_states_proj\n        \"encoder_hidden_states_proj.weight\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.text_seq_proj.0.weight\"],\n        \"encoder_hidden_states_proj.bias\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.text_seq_proj.0.bias\"],\n        # <original>.text_seq_proj.1 -> <diffusers>.text_encoder_hidden_states_norm\n        \"text_encoder_hidden_states_norm.weight\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.text_seq_proj.1.weight\"],\n        \"text_encoder_hidden_states_norm.bias\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.text_seq_proj.1.bias\"],\n        # <original>.clip_tok_proj -> <diffusers>.clip_extra_context_tokens_proj\n        \"clip_extra_context_tokens_proj.weight\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.clip_tok_proj.weight\"],\n        \"clip_extra_context_tokens_proj.bias\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.clip_tok_proj.bias\"],\n        # <original>.text_feat_proj -> <diffusers>.embedding_proj\n        \"embedding_proj.weight\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.text_feat_proj.weight\"],\n        \"embedding_proj.bias\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.text_feat_proj.bias\"],\n        # <original>.cf_param -> <diffusers>.learned_classifier_free_guidance_embeddings\n        \"learned_classifier_free_guidance_embeddings\": checkpoint[f\"{DECODER_ORIGINAL_PREFIX}.cf_param\"],\n        # <original>.clip_emb -> <diffusers>.clip_image_embeddings_project_to_time_embeddings\n        \"clip_image_embeddings_project_to_time_em",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 342, "column": 0 },
          "end": { "row": 342, "column": 0 }
        }
      }
    }
  ],
  [
    "1179",
    {
      "pageContent": "def super_res_unet_first_steps_model_from_original_config():\n    model = UNet2DModel(**SUPER_RES_UNET_FIRST_STEPS_CONFIG)\n\n    return model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 398, "column": 0 },
          "end": { "row": 398, "column": 0 }
        }
      }
    }
  ],
  [
    "1180",
    {
      "pageContent": "def super_res_unet_first_steps_original_checkpoint_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    original_unet_prefix = SUPER_RES_UNET_FIRST_STEPS_PREFIX\n\n    diffusers_checkpoint.update(unet_time_embeddings(checkpoint, original_unet_prefix))\n    diffusers_checkpoint.update(unet_conv_in(checkpoint, original_unet_prefix))\n\n    # <original>.input_blocks -> <diffusers>.down_blocks\n\n    original_down_block_idx = 1\n\n    for diffusers_down_block_idx in range(len(model.down_blocks)):\n        checkpoint_update, num_original_down_blocks = unet_downblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            diffusers_down_block_idx=diffusers_down_block_idx,\n            original_down_block_idx=original_down_block_idx,\n            original_unet_prefix=original_unet_prefix,\n            num_head_channels=None,\n        )\n\n        original_down_block_idx += num_original_down_blocks\n\n        diffusers_checkpoint.update(checkpoint_update)\n\n    diffusers_checkpoint.update(\n        unet_midblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            original_unet_prefix=original_unet_prefix,\n            num_head_channels=None,\n        )\n    )\n\n    # <original>.output_blocks -> <diffusers>.up_blocks\n\n    original_up_block_idx = 0\n\n    for diffusers_up_block_idx in range(len(model.up_blocks)):\n        checkpoint_update, num_original_up_blocks = unet_upblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            diffusers_up_block_idx=diffusers_up_block_idx,\n            original_up_block_i",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 404, "column": 0 },
          "end": { "row": 404, "column": 0 }
        }
      }
    }
  ],
  [
    "1181",
    {
      "pageContent": "def super_res_unet_last_step_model_from_original_config():\n    model = UNet2DModel(**SUPER_RES_UNET_LAST_STEP_CONFIG)\n\n    return model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 493, "column": 0 },
          "end": { "row": 493, "column": 0 }
        }
      }
    }
  ],
  [
    "1182",
    {
      "pageContent": "def super_res_unet_last_step_original_checkpoint_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    original_unet_prefix = SUPER_RES_UNET_LAST_STEP_PREFIX\n\n    diffusers_checkpoint.update(unet_time_embeddings(checkpoint, original_unet_prefix))\n    diffusers_checkpoint.update(unet_conv_in(checkpoint, original_unet_prefix))\n\n    # <original>.input_blocks -> <diffusers>.down_blocks\n\n    original_down_block_idx = 1\n\n    for diffusers_down_block_idx in range(len(model.down_blocks)):\n        checkpoint_update, num_original_down_blocks = unet_downblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            diffusers_down_block_idx=diffusers_down_block_idx,\n            original_down_block_idx=original_down_block_idx,\n            original_unet_prefix=original_unet_prefix,\n            num_head_channels=None,\n        )\n\n        original_down_block_idx += num_original_down_blocks\n\n        diffusers_checkpoint.update(checkpoint_update)\n\n    diffusers_checkpoint.update(\n        unet_midblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            original_unet_prefix=original_unet_prefix,\n            num_head_channels=None,\n        )\n    )\n\n    # <original>.output_blocks -> <diffusers>.up_blocks\n\n    original_up_block_idx = 0\n\n    for diffusers_up_block_idx in range(len(model.up_blocks)):\n        checkpoint_update, num_original_up_blocks = unet_upblock_to_diffusers_checkpoint(\n            model,\n            checkpoint,\n            diffusers_up_block_idx=diffusers_up_block_idx,\n            original_up_block_idx=o",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 499, "column": 0 },
          "end": { "row": 499, "column": 0 }
        }
      }
    }
  ],
  [
    "1183",
    {
      "pageContent": "def unet_time_embeddings(checkpoint, original_unet_prefix):\n    diffusers_checkpoint = {}\n\n    diffusers_checkpoint.update(\n        {\n            \"time_embedding.linear_1.weight\": checkpoint[f\"{original_unet_prefix}.time_embed.0.weight\"],\n            \"time_embedding.linear_1.bias\": checkpoint[f\"{original_unet_prefix}.time_embed.0.bias\"],\n            \"time_embedding.linear_2.weight\": checkpoint[f\"{original_unet_prefix}.time_embed.2.weight\"],\n            \"time_embedding.linear_2.bias\": checkpoint[f\"{original_unet_prefix}.time_embed.2.bias\"],\n        }\n    )\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 567, "column": 0 },
          "end": { "row": 567, "column": 0 }
        }
      }
    }
  ],
  [
    "1184",
    {
      "pageContent": "def unet_conv_in(checkpoint, original_unet_prefix):\n    diffusers_checkpoint = {}\n\n    diffusers_checkpoint.update(\n        {\n            \"conv_in.weight\": checkpoint[f\"{original_unet_prefix}.input_blocks.0.0.weight\"],\n            \"conv_in.bias\": checkpoint[f\"{original_unet_prefix}.input_blocks.0.0.bias\"],\n        }\n    )\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 583, "column": 0 },
          "end": { "row": 583, "column": 0 }
        }
      }
    }
  ],
  [
    "1185",
    {
      "pageContent": "def unet_conv_norm_out(checkpoint, original_unet_prefix):\n    diffusers_checkpoint = {}\n\n    diffusers_checkpoint.update(\n        {\n            \"conv_norm_out.weight\": checkpoint[f\"{original_unet_prefix}.out.0.weight\"],\n            \"conv_norm_out.bias\": checkpoint[f\"{original_unet_prefix}.out.0.bias\"],\n        }\n    )\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 597, "column": 0 },
          "end": { "row": 597, "column": 0 }
        }
      }
    }
  ],
  [
    "1186",
    {
      "pageContent": "def unet_conv_out(checkpoint, original_unet_prefix):\n    diffusers_checkpoint = {}\n\n    diffusers_checkpoint.update(\n        {\n            \"conv_out.weight\": checkpoint[f\"{original_unet_prefix}.out.2.weight\"],\n            \"conv_out.bias\": checkpoint[f\"{original_unet_prefix}.out.2.bias\"],\n        }\n    )\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 611, "column": 0 },
          "end": { "row": 611, "column": 0 }
        }
      }
    }
  ],
  [
    "1187",
    {
      "pageContent": "def unet_downblock_to_diffusers_checkpoint(\n    model, checkpoint, *, diffusers_down_block_idx, original_down_block_idx, original_unet_prefix, num_head_channels\n):\n    diffusers_checkpoint = {}\n\n    diffusers_resnet_prefix = f\"down_blocks.{diffusers_down_block_idx}.resnets\"\n    original_down_block_prefix = f\"{original_unet_prefix}.input_blocks\"\n\n    down_block = model.down_blocks[diffusers_down_block_idx]\n\n    num_resnets = len(down_block.resnets)\n\n    if down_block.downsamplers is None:\n        downsampler = False\n    else:\n        assert len(down_block.downsamplers) == 1\n        downsampler = True\n        # The downsample block is also a resnet\n        num_resnets += 1\n\n    for resnet_idx_inc in range(num_resnets):\n        full_resnet_prefix = f\"{original_down_block_prefix}.{original_down_block_idx + resnet_idx_inc}.0\"\n\n        if downsampler and resnet_idx_inc == num_resnets - 1:\n            # this is a downsample block\n            full_diffusers_resnet_prefix = f\"down_blocks.{diffusers_down_block_idx}.downsamplers.0\"\n        else:\n            # this is a regular resnet block\n            full_diffusers_resnet_prefix = f\"{diffusers_resnet_prefix}.{resnet_idx_inc}\"\n\n        diffusers_checkpoint.update(\n            resnet_to_diffusers_checkpoint(\n                checkpoint, resnet_prefix=full_resnet_prefix, diffusers_resnet_prefix=full_diffusers_resnet_prefix\n            )\n        )\n\n    if hasattr(down_block, \"attentions\"):\n        num_attentions = len(down_block.attentions)\n        diffusers_attention_prefix = f\"down_blocks.{diffusers_down_block_idx}.attentions\"\n\n        ",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 625, "column": 0 },
          "end": { "row": 625, "column": 0 }
        }
      }
    }
  ],
  [
    "1188",
    {
      "pageContent": "def unet_midblock_to_diffusers_checkpoint(model, checkpoint, *, original_unet_prefix, num_head_channels):\n    diffusers_checkpoint = {}\n\n    # block 0\n\n    original_block_idx = 0\n\n    diffusers_checkpoint.update(\n        resnet_to_diffusers_checkpoint(\n            checkpoint,\n            diffusers_resnet_prefix=\"mid_block.resnets.0\",\n            resnet_prefix=f\"{original_unet_prefix}.middle_block.{original_block_idx}\",\n        )\n    )\n\n    original_block_idx += 1\n\n    # optional block 1\n\n    if hasattr(model.mid_block, \"attentions\") and model.mid_block.attentions[0] is not None:\n        diffusers_checkpoint.update(\n            attention_to_diffusers_checkpoint(\n                checkpoint,\n                diffusers_attention_prefix=\"mid_block.attentions.0\",\n                attention_prefix=f\"{original_unet_prefix}.middle_block.{original_block_idx}\",\n                num_head_channels=num_head_channels,\n            )\n        )\n        original_block_idx += 1\n\n    # block 1 or block 2\n\n    diffusers_checkpoint.update(\n        resnet_to_diffusers_checkpoint(\n            checkpoint,\n            diffusers_resnet_prefix=\"mid_block.resnets.1\",\n            resnet_prefix=f\"{original_unet_prefix}.middle_block.{original_block_idx}\",\n        )\n    )\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 684, "column": 0 },
          "end": { "row": 684, "column": 0 }
        }
      }
    }
  ],
  [
    "1189",
    {
      "pageContent": "def unet_upblock_to_diffusers_checkpoint(\n    model, checkpoint, *, diffusers_up_block_idx, original_up_block_idx, original_unet_prefix, num_head_channels\n):\n    diffusers_checkpoint = {}\n\n    diffusers_resnet_prefix = f\"up_blocks.{diffusers_up_block_idx}.resnets\"\n    original_up_block_prefix = f\"{original_unet_prefix}.output_blocks\"\n\n    up_block = model.up_blocks[diffusers_up_block_idx]\n\n    num_resnets = len(up_block.resnets)\n\n    if up_block.upsamplers is None:\n        upsampler = False\n    else:\n        assert len(up_block.upsamplers) == 1\n        upsampler = True\n        # The upsample block is also a resnet\n        num_resnets += 1\n\n    has_attentions = hasattr(up_block, \"attentions\")\n\n    for resnet_idx_inc in range(num_resnets):\n        if upsampler and resnet_idx_inc == num_resnets - 1:\n            # this is an upsample block\n            if has_attentions:\n                # There is a middle attention block that we skip\n                original_resnet_block_idx = 2\n            else:\n                original_resnet_block_idx = 1\n\n            # we add the `minus 1` because the last two resnets are stuck together in the same output block\n            full_resnet_prefix = (\n                f\"{original_up_block_prefix}.{original_up_block_idx + resnet_idx_inc - 1}.{original_resnet_block_idx}\"\n            )\n\n            full_diffusers_resnet_prefix = f\"up_blocks.{diffusers_up_block_idx}.upsamplers.0\"\n        else:\n            # this is a regular resnet block\n            full_resnet_prefix = f\"{original_up_block_prefix}.{original_up_block_idx + resnet_idx_inc}.0\"\n         ",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 728, "column": 0 },
          "end": { "row": 728, "column": 0 }
        }
      }
    }
  ],
  [
    "1190",
    {
      "pageContent": "def resnet_to_diffusers_checkpoint(checkpoint, *, diffusers_resnet_prefix, resnet_prefix):\n    diffusers_checkpoint = {\n        f\"{diffusers_resnet_prefix}.norm1.weight\": checkpoint[f\"{resnet_prefix}.in_layers.0.weight\"],\n        f\"{diffusers_resnet_prefix}.norm1.bias\": checkpoint[f\"{resnet_prefix}.in_layers.0.bias\"],\n        f\"{diffusers_resnet_prefix}.conv1.weight\": checkpoint[f\"{resnet_prefix}.in_layers.2.weight\"],\n        f\"{diffusers_resnet_prefix}.conv1.bias\": checkpoint[f\"{resnet_prefix}.in_layers.2.bias\"],\n        f\"{diffusers_resnet_prefix}.time_emb_proj.weight\": checkpoint[f\"{resnet_prefix}.emb_layers.1.weight\"],\n        f\"{diffusers_resnet_prefix}.time_emb_proj.bias\": checkpoint[f\"{resnet_prefix}.emb_layers.1.bias\"],\n        f\"{diffusers_resnet_prefix}.norm2.weight\": checkpoint[f\"{resnet_prefix}.out_layers.0.weight\"],\n        f\"{diffusers_resnet_prefix}.norm2.bias\": checkpoint[f\"{resnet_prefix}.out_layers.0.bias\"],\n        f\"{diffusers_resnet_prefix}.conv2.weight\": checkpoint[f\"{resnet_prefix}.out_layers.3.weight\"],\n        f\"{diffusers_resnet_prefix}.conv2.bias\": checkpoint[f\"{resnet_prefix}.out_layers.3.bias\"],\n    }\n\n    skip_connection_prefix = f\"{resnet_prefix}.skip_connection\"\n\n    if f\"{skip_connection_prefix}.weight\" in checkpoint:\n        diffusers_checkpoint.update(\n            {\n                f\"{diffusers_resnet_prefix}.conv_shortcut.weight\": checkpoint[f\"{skip_connection_prefix}.weight\"],\n                f\"{diffusers_resnet_prefix}.conv_shortcut.bias\": checkpoint[f\"{skip_connection_prefix}.bias\"],\n            }\n        )\n\n    return diffusers_checkp",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 798, "column": 0 },
          "end": { "row": 798, "column": 0 }
        }
      }
    }
  ],
  [
    "1191",
    {
      "pageContent": "def attention_to_diffusers_checkpoint(checkpoint, *, diffusers_attention_prefix, attention_prefix, num_head_channels):\n    diffusers_checkpoint = {}\n\n    # <original>.norm -> <diffusers>.group_norm\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.group_norm.weight\": checkpoint[f\"{attention_prefix}.norm.weight\"],\n            f\"{diffusers_attention_prefix}.group_norm.bias\": checkpoint[f\"{attention_prefix}.norm.bias\"],\n        }\n    )\n\n    # <original>.qkv -> <diffusers>.{query, key, value}\n    [q_weight, k_weight, v_weight], [q_bias, k_bias, v_bias] = split_attentions(\n        weight=checkpoint[f\"{attention_prefix}.qkv.weight\"][:, :, 0],\n        bias=checkpoint[f\"{attention_prefix}.qkv.bias\"],\n        split=3,\n        chunk_size=num_head_channels,\n    )\n\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n            f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n            f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n            f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n            f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n            f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n        }\n    )\n\n    # <original>.encoder_kv -> <diffusers>.{context_key, context_value}\n    [encoder_k_weight, encoder_v_weight], [encoder_k_bias, encoder_v_bias] = split_attentions(\n        weight=checkpoint[f\"{attention_prefix}.encoder_kv.weight\"][:, :, 0],\n        bias=checkpoint[f\"{attention_prefix}.encoder_kv.bias\"],\n        split=2,\n        chunk_size=num_he",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 825, "column": 0 },
          "end": { "row": 825, "column": 0 }
        }
      }
    }
  ],
  [
    "1192",
    {
      "pageContent": "def split_attentions(*, weight, bias, split, chunk_size):\n    weights = [None] * split\n    biases = [None] * split\n\n    weights_biases_idx = 0\n\n    for starting_row_index in range(0, weight.shape[0], chunk_size):\n        row_indices = torch.arange(starting_row_index, starting_row_index + chunk_size)\n\n        weight_rows = weight[row_indices, :]\n        bias_rows = bias[row_indices]\n\n        if weights[weights_biases_idx] is None:\n            assert weights[weights_biases_idx] is None\n            weights[weights_biases_idx] = weight_rows\n            biases[weights_biases_idx] = bias_rows\n        else:\n            assert weights[weights_biases_idx] is not None\n            weights[weights_biases_idx] = torch.concat([weights[weights_biases_idx], weight_rows])\n            biases[weights_biases_idx] = torch.concat([biases[weights_biases_idx], bias_rows])\n\n        weights_biases_idx = (weights_biases_idx + 1) % split\n\n    return weights, biases",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 886, "column": 0 },
          "end": { "row": 886, "column": 0 }
        }
      }
    }
  ],
  [
    "1193",
    {
      "pageContent": "def text_encoder():\n    print(\"loading CLIP text encoder\")\n\n    clip_name = \"openai/clip-vit-large-patch14\"\n\n    # sets pad_value to 0\n    pad_token = \"!\"\n\n    tokenizer_model = CLIPTokenizer.from_pretrained(clip_name, pad_token=pad_token, device_map=\"auto\")\n\n    assert tokenizer_model.convert_tokens_to_ids(pad_token) == 0\n\n    text_encoder_model = CLIPTextModelWithProjection.from_pretrained(\n        clip_name,\n        # `CLIPTextModel` does not support device_map=\"auto\"\n        # device_map=\"auto\"\n    )\n\n    print(\"done loading CLIP text encoder\")\n\n    return text_encoder_model, tokenizer_model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 918, "column": 0 },
          "end": { "row": 918, "column": 0 }
        }
      }
    }
  ],
  [
    "1194",
    {
      "pageContent": "def prior(*, args, checkpoint_map_location):\n    print(\"loading prior\")\n\n    prior_checkpoint = torch.load(args.prior_checkpoint_path, map_location=checkpoint_map_location)\n    prior_checkpoint = prior_checkpoint[\"state_dict\"]\n\n    clip_stats_checkpoint = torch.load(args.clip_stat_path, map_location=checkpoint_map_location)\n\n    prior_model = prior_model_from_original_config()\n\n    prior_diffusers_checkpoint = prior_original_checkpoint_to_diffusers_checkpoint(\n        prior_model, prior_checkpoint, clip_stats_checkpoint\n    )\n\n    del prior_checkpoint\n    del clip_stats_checkpoint\n\n    load_checkpoint_to_model(prior_diffusers_checkpoint, prior_model, strict=True)\n\n    print(\"done loading prior\")\n\n    return prior_model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 941, "column": 0 },
          "end": { "row": 941, "column": 0 }
        }
      }
    }
  ],
  [
    "1195",
    {
      "pageContent": "def decoder(*, args, checkpoint_map_location):\n    print(\"loading decoder\")\n\n    decoder_checkpoint = torch.load(args.decoder_checkpoint_path, map_location=checkpoint_map_location)\n    decoder_checkpoint = decoder_checkpoint[\"state_dict\"]\n\n    decoder_model = decoder_model_from_original_config()\n\n    decoder_diffusers_checkpoint = decoder_original_checkpoint_to_diffusers_checkpoint(\n        decoder_model, decoder_checkpoint\n    )\n\n    # text proj interlude\n\n    # The original decoder implementation includes a set of parameters that are used\n    # for creating the `encoder_hidden_states` which are what the U-net is conditioned\n    # on. The diffusers conditional unet directly takes the encoder_hidden_states. We pull\n    # the parameters into the UnCLIPTextProjModel class\n    text_proj_model = text_proj_from_original_config()\n\n    text_proj_checkpoint = text_proj_original_checkpoint_to_diffusers_checkpoint(decoder_checkpoint)\n\n    load_checkpoint_to_model(text_proj_checkpoint, text_proj_model, strict=True)\n\n    # done text proj interlude\n\n    del decoder_checkpoint\n\n    load_checkpoint_to_model(decoder_diffusers_checkpoint, decoder_model, strict=True)\n\n    print(\"done loading decoder\")\n\n    return decoder_model, text_proj_model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 965, "column": 0 },
          "end": { "row": 965, "column": 0 }
        }
      }
    }
  ],
  [
    "1196",
    {
      "pageContent": "def super_res_unet(*, args, checkpoint_map_location):\n    print(\"loading super resolution unet\")\n\n    super_res_checkpoint = torch.load(args.super_res_unet_checkpoint_path, map_location=checkpoint_map_location)\n    super_res_checkpoint = super_res_checkpoint[\"state_dict\"]\n\n    # model_first_steps\n\n    super_res_first_model = super_res_unet_first_steps_model_from_original_config()\n\n    super_res_first_steps_checkpoint = super_res_unet_first_steps_original_checkpoint_to_diffusers_checkpoint(\n        super_res_first_model, super_res_checkpoint\n    )\n\n    # model_last_step\n    super_res_last_model = super_res_unet_last_step_model_from_original_config()\n\n    super_res_last_step_checkpoint = super_res_unet_last_step_original_checkpoint_to_diffusers_checkpoint(\n        super_res_last_model, super_res_checkpoint\n    )\n\n    del super_res_checkpoint\n\n    load_checkpoint_to_model(super_res_first_steps_checkpoint, super_res_first_model, strict=True)\n\n    load_checkpoint_to_model(super_res_last_step_checkpoint, super_res_last_model, strict=True)\n\n    print(\"done loading super resolution unet\")\n\n    return super_res_first_model, super_res_last_model",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 1000, "column": 0 },
          "end": { "row": 1000, "column": 0 }
        }
      }
    }
  ],
  [
    "1197",
    {
      "pageContent": "def load_checkpoint_to_model(checkpoint, model, strict=False):\n    with tempfile.NamedTemporaryFile() as file:\n        torch.save(checkpoint, file.name)\n        del checkpoint\n        if strict:\n            model.load_state_dict(torch.load(file.name), strict=True)\n        else:\n            load_checkpoint_and_dispatch(model, file.name, device_map=\"auto\")",
      "metadata": {
        "source": "scripts/convert_kakao_brain_unclip_to_diffusers.py",
        "range": {
          "start": { "row": 1032, "column": 0 },
          "end": { "row": 1032, "column": 0 }
        }
      }
    }
  ],
  [
    "1198",
    {
      "pageContent": "def convert_ldm_original(checkpoint_path, config_path, output_path):\n    config = OmegaConf.load(config_path)\n    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")[\"model\"]\n    keys = list(state_dict.keys())\n\n    # extract state_dict for VQVAE\n    first_stage_dict = {}\n    first_stage_key = \"first_stage_model.\"\n    for key in keys:\n        if key.startswith(first_stage_key):\n            first_stage_dict[key.replace(first_stage_key, \"\")] = state_dict[key]\n\n    # extract state_dict for UNetLDM\n    unet_state_dict = {}\n    unet_key = \"model.diffusion_model.\"\n    for key in keys:\n        if key.startswith(unet_key):\n            unet_state_dict[key.replace(unet_key, \"\")] = state_dict[key]\n\n    vqvae_init_args = config.model.params.first_stage_config.params\n    unet_init_args = config.model.params.unet_config.params\n\n    vqvae = VQModel(**vqvae_init_args).eval()\n    vqvae.load_state_dict(first_stage_dict)\n\n    unet = UNetLDMModel(**unet_init_args).eval()\n    unet.load_state_dict(unet_state_dict)\n\n    noise_scheduler = DDIMScheduler(\n        timesteps=config.model.params.timesteps,\n        beta_schedule=\"scaled_linear\",\n        beta_start=config.model.params.linear_start,\n        beta_end=config.model.params.linear_end,\n        clip_sample=False,\n    )\n\n    pipeline = LDMPipeline(vqvae, unet, noise_scheduler)\n    pipeline.save_pretrained(output_path)",
      "metadata": {
        "source": "scripts/conversion_ldm_uncond.py",
        "range": {
          "start": { "row": 8, "column": 0 },
          "end": { "row": 8, "column": 0 }
        }
      }
    }
  ],
  [
    "1199",
    {
      "pageContent": "def unet(hor):\n    if hor == 128:\n        down_block_types = (\"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\")\n        block_out_channels = (32, 128, 256)\n        up_block_types = (\"UpResnetBlock1D\", \"UpResnetBlock1D\")\n\n    elif hor == 32:\n        down_block_types = (\"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\")\n        block_out_channels = (32, 64, 128, 256)\n        up_block_types = (\"UpResnetBlock1D\", \"UpResnetBlock1D\", \"UpResnetBlock1D\")\n    model = torch.load(f\"/Users/bglickenhaus/Documents/diffuser/temporal_unet-hopper-mediumv2-hor{hor}.torch\")\n    state_dict = model.state_dict()\n    config = dict(\n        down_block_types=down_block_types,\n        block_out_channels=block_out_channels,\n        up_block_types=up_block_types,\n        layers_per_block=1,\n        use_timestep_embedding=True,\n        out_block_type=\"OutConv1DBlock\",\n        norm_num_groups=8,\n        downsample_each_block=False,\n        in_channels=14,\n        out_channels=14,\n        extra_in_channels=0,\n        time_embedding_type=\"positional\",\n        flip_sin_to_cos=False,\n        freq_shift=1,\n        sample_size=65536,\n        mid_block_type=\"MidResTemporalBlock1D\",\n        act_fn=\"mish\",\n    )\n    hf_value_function = UNet1DModel(**config)\n    print(f\"length of state dict: {len(state_dict.keys())}\")\n    print(f\"length of value function dict: {len(hf_value_function.state_dict().keys())}\")\n    mapping = dict((k, hfk) for k, hfk in zip(model.state_dict().keys(), hf_value_function.state_dict().keys()))\n    for k, v in mapping.items():\n        state_dict",
      "metadata": {
        "source": "scripts/convert_models_diffuser_to_diffusers.py",
        "range": {
          "start": { "row": 14, "column": 0 },
          "end": { "row": 14, "column": 0 }
        }
      }
    }
  ],
  [
    "1200",
    {
      "pageContent": "def value_function():\n    config = dict(\n        in_channels=14,\n        down_block_types=(\"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\", \"DownResnetBlock1D\"),\n        up_block_types=(),\n        out_block_type=\"ValueFunction\",\n        mid_block_type=\"ValueFunctionMidBlock1D\",\n        block_out_channels=(32, 64, 128, 256),\n        layers_per_block=1,\n        downsample_each_block=True,\n        sample_size=65536,\n        out_channels=14,\n        extra_in_channels=0,\n        time_embedding_type=\"positional\",\n        use_timestep_embedding=True,\n        flip_sin_to_cos=False,\n        freq_shift=1,\n        norm_num_groups=8,\n        act_fn=\"mish\",\n    )\n\n    model = torch.load(\"/Users/bglickenhaus/Documents/diffuser/value_function-hopper-mediumv2-hor32.torch\")\n    state_dict = model\n    hf_value_function = UNet1DModel(**config)\n    print(f\"length of state dict: {len(state_dict.keys())}\")\n    print(f\"length of value function dict: {len(hf_value_function.state_dict().keys())}\")\n\n    mapping = dict((k, hfk) for k, hfk in zip(state_dict.keys(), hf_value_function.state_dict().keys()))\n    for k, v in mapping.items():\n        state_dict[v] = state_dict.pop(k)\n\n    hf_value_function.load_state_dict(state_dict)\n\n    torch.save(hf_value_function.state_dict(), \"hub/hopper-medium-v2/value_function/diffusion_pytorch_model.bin\")\n    with open(\"hub/hopper-medium-v2/value_function/config.json\", \"w\") as f:\n        json.dump(config, f)",
      "metadata": {
        "source": "scripts/convert_models_diffuser_to_diffusers.py",
        "range": {
          "start": { "row": 58, "column": 0 },
          "end": { "row": 58, "column": 0 }
        }
      }
    }
  ],
  [
    "1201",
    {
      "pageContent": "def custom_convert_ldm_vae_checkpoint(checkpoint, config):\n    vae_state_dict = checkpoint\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"encoder.conv_in.weight\"] = vae_state_dict[\"encoder.conv_in.weight\"]\n    new_checkpoint[\"encoder.conv_in.bias\"] = vae_state_dict[\"encoder.conv_in.bias\"]\n    new_checkpoint[\"encoder.conv_out.weight\"] = vae_state_dict[\"encoder.conv_out.weight\"]\n    new_checkpoint[\"encoder.conv_out.bias\"] = vae_state_dict[\"encoder.conv_out.bias\"]\n    new_checkpoint[\"encoder.conv_norm_out.weight\"] = vae_state_dict[\"encoder.norm_out.weight\"]\n    new_checkpoint[\"encoder.conv_norm_out.bias\"] = vae_state_dict[\"encoder.norm_out.bias\"]\n\n    new_checkpoint[\"decoder.conv_in.weight\"] = vae_state_dict[\"decoder.conv_in.weight\"]\n    new_checkpoint[\"decoder.conv_in.bias\"] = vae_state_dict[\"decoder.conv_in.bias\"]\n    new_checkpoint[\"decoder.conv_out.weight\"] = vae_state_dict[\"decoder.conv_out.weight\"]\n    new_checkpoint[\"decoder.conv_out.bias\"] = vae_state_dict[\"decoder.conv_out.bias\"]\n    new_checkpoint[\"decoder.conv_norm_out.weight\"] = vae_state_dict[\"decoder.norm_out.weight\"]\n    new_checkpoint[\"decoder.conv_norm_out.bias\"] = vae_state_dict[\"decoder.norm_out.bias\"]\n\n    new_checkpoint[\"quant_conv.weight\"] = vae_state_dict[\"quant_conv.weight\"]\n    new_checkpoint[\"quant_conv.bias\"] = vae_state_dict[\"quant_conv.bias\"]\n    new_checkpoint[\"post_quant_conv.weight\"] = vae_state_dict[\"post_quant_conv.weight\"]\n    new_checkpoint[\"post_quant_conv.bias\"] = vae_state_dict[\"post_quant_conv.bias\"]\n\n    # Retrieves the keys for the encoder down blocks only\n    num_down_blocks = len({\".\"",
      "metadata": {
        "source": "scripts/convert_vae_pt_to_diffusers.py",
        "range": {
          "start": { "row": 17, "column": 0 },
          "end": { "row": 17, "column": 0 }
        }
      }
    }
  ],
  [
    "1202",
    {
      "pageContent": "def vae_pt_to_vae_diffuser(\n    checkpoint_path: str,\n    output_path: str,\n):\n    # Only support V1\n    r = requests.get(\n        \" https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\"\n    )\n    io_obj = io.BytesIO(r.content)\n\n    original_config = OmegaConf.load(io_obj)\n    image_size = 512\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    # Convert the VAE model.\n    vae_config = create_vae_diffusers_config(original_config, image_size=image_size)\n    converted_vae_checkpoint = custom_convert_ldm_vae_checkpoint(checkpoint[\"state_dict\"], vae_config)\n\n    vae = AutoencoderKL(**vae_config)\n    vae.load_state_dict(converted_vae_checkpoint)\n    vae.save_pretrained(output_path)",
      "metadata": {
        "source": "scripts/convert_vae_pt_to_diffusers.py",
        "range": {
          "start": { "row": 118, "column": 0 },
          "end": { "row": 118, "column": 0 }
        }
      }
    }
  ],
  [
    "1203",
    {
      "pageContent": "def convert_unet_state_dict(unet_state_dict):\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in\n    # the exact order in which I have arranged them.\n    mapping = {k: k for k in unet_state_dict.keys()}\n    for sd_name, hf_name in unet_conversion_map:\n        mapping[hf_name] = sd_name\n    for k, v in mapping.items():\n        if \"resnets\" in k:\n            for sd_part, hf_part in unet_conversion_map_resnet:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    for k, v in mapping.items():\n        for sd_part, hf_part in unet_conversion_map_layer:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    new_state_dict = {v: unet_state_dict[k] for k, v in mapping.items()}\n    return new_state_dict",
      "metadata": {
        "source": "scripts/convert_diffusers_to_original_stable_diffusion.py",
        "range": {
          "start": { "row": 91, "column": 0 },
          "end": { "row": 91, "column": 0 }
        }
      }
    }
  ],
  [
    "1204",
    {
      "pageContent": "def convert_vae_state_dict(vae_state_dict):\n    mapping = {k: k for k in vae_state_dict.keys()}\n    for k, v in mapping.items():\n        for sd_part, hf_part in vae_conversion_map:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    for k, v in mapping.items():\n        if \"attentions\" in k:\n            for sd_part, hf_part in vae_conversion_map_attn:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    new_state_dict = {v: vae_state_dict[k] for k, v in mapping.items()}\n    weights_to_convert = [\"q\", \"k\", \"v\", \"proj_out\"]\n    for k, v in new_state_dict.items():\n        for weight_name in weights_to_convert:\n            if f\"mid.attn_1.{weight_name}.weight\" in k:\n                print(f\"Reshaping {k} for SD format\")\n                new_state_dict[k] = reshape_weight_for_sd(v)\n    return new_state_dict",
      "metadata": {
        "source": "scripts/convert_diffusers_to_original_stable_diffusion.py",
        "range": {
          "start": { "row": 167, "column": 0 },
          "end": { "row": 167, "column": 0 }
        }
      }
    }
  ],
  [
    "1205",
    {
      "pageContent": "def convert_text_enc_state_dict_v20(text_enc_dict):\n    new_state_dict = {}\n    capture_qkv_weight = {}\n    capture_qkv_bias = {}\n    for k, v in text_enc_dict.items():\n        if (\n            k.endswith(\".self_attn.q_proj.weight\")\n            or k.endswith(\".self_attn.k_proj.weight\")\n            or k.endswith(\".self_attn.v_proj.weight\")\n        ):\n            k_pre = k[: -len(\".q_proj.weight\")]\n            k_code = k[-len(\"q_proj.weight\")]\n            if k_pre not in capture_qkv_weight:\n                capture_qkv_weight[k_pre] = [None, None, None]\n            capture_qkv_weight[k_pre][code2idx[k_code]] = v\n            continue\n\n        if (\n            k.endswith(\".self_attn.q_proj.bias\")\n            or k.endswith(\".self_attn.k_proj.bias\")\n            or k.endswith(\".self_attn.v_proj.bias\")\n        ):\n            k_pre = k[: -len(\".q_proj.bias\")]\n            k_code = k[-len(\"q_proj.bias\")]\n            if k_pre not in capture_qkv_bias:\n                capture_qkv_bias[k_pre] = [None, None, None]\n            capture_qkv_bias[k_pre][code2idx[k_code]] = v\n            continue\n\n        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k)\n        new_state_dict[relabelled_key] = v\n\n    for k_pre, tensors in capture_qkv_weight.items():\n        if None in tensors:\n            raise Exception(\"CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing\")\n        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)\n        new_state_dict[relabelled_key + \".in_proj_weight\"] = torch.cat(tensors)\n\n    for",
      "metadata": {
        "source": "scripts/convert_diffusers_to_original_stable_diffusion.py",
        "range": {
          "start": { "row": 212, "column": 0 },
          "end": { "row": 212, "column": 0 }
        }
      }
    }
  ],
  [
    "1206",
    {
      "pageContent": "def shave_segments(path, n_shave_prefix_segments=1):\n    \"\"\"\n    Removes segments. Positive values shave the first segments, negative shave the last segments.\n    \"\"\"\n    if n_shave_prefix_segments >= 0:\n        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n    else:\n        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])",
      "metadata": {
        "source": "scripts/convert_ldm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1207",
    {
      "pageContent": "def renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item.replace(\"in_layers.0\", \"norm1\")\n        new_item = new_item.replace(\"in_layers.2\", \"conv1\")\n\n        new_item = new_item.replace(\"out_layers.0\", \"norm2\")\n        new_item = new_item.replace(\"out_layers.3\", \"conv2\")\n\n        new_item = new_item.replace(\"emb_layers.1\", \"time_emb_proj\")\n        new_item = new_item.replace(\"skip_connection\", \"conv_shortcut\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_ldm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1208",
    {
      "pageContent": "def renew_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"norm.weight\", \"group_norm.weight\")\n        new_item = new_item.replace(\"norm.bias\", \"group_norm.bias\")\n\n        new_item = new_item.replace(\"proj_out.weight\", \"proj_attn.weight\")\n        new_item = new_item.replace(\"proj_out.bias\", \"proj_attn.bias\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "scripts/convert_ldm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 56, "column": 0 },
          "end": { "row": 56, "column": 0 }
        }
      }
    }
  ],
  [
    "1209",
    {
      "pageContent": "def assign_to_checkpoint(\n    paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None\n):\n    \"\"\"\n    This does the final conversion step: take locally converted weights and apply a global renaming\n    to them. It splits attention layers, and takes into account additional replacements\n    that may arise.\n\n    Assigns the weights to the new checkpoint.\n    \"\"\"\n    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n\n    # Splits the attention layers into three variables.\n    if attention_paths_to_split is not None:\n        for path, path_map in attention_paths_to_split.items():\n            old_tensor = old_checkpoint[path]\n            channels = old_tensor.shape[0] // 3\n\n            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else (-1)\n\n            num_heads = old_tensor.shape[0] // config[\"num_head_channels\"] // 3\n\n            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])\n            query, key, value = old_tensor.split(channels // num_heads, dim=1)\n\n            checkpoint[path_map[\"query\"]] = query.reshape(target_shape)\n            checkpoint[path_map[\"key\"]] = key.reshape(target_shape)\n            checkpoint[path_map[\"value\"]] = value.reshape(target_shape)\n\n    for path in paths:\n        new_path = path[\"new\"]\n\n        # These have already been assigned\n        if attention_paths_to_split is not None and new_path in attention_paths_to_split:\n            continue\n\n        # Global renaming happens here\n        ne",
      "metadata": {
        "source": "scripts/convert_ldm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 77, "column": 0 },
          "end": { "row": 77, "column": 0 }
        }
      }
    }
  ],
  [
    "1210",
    {
      "pageContent": "def convert_ldm_checkpoint(checkpoint, config):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n    new_checkpoint = {}\n\n    new_checkpoint[\"time_embedding.linear_1.weight\"] = checkpoint[\"time_embed.0.weight\"]\n    new_checkpoint[\"time_embedding.linear_1.bias\"] = checkpoint[\"time_embed.0.bias\"]\n    new_checkpoint[\"time_embedding.linear_2.weight\"] = checkpoint[\"time_embed.2.weight\"]\n    new_checkpoint[\"time_embedding.linear_2.bias\"] = checkpoint[\"time_embed.2.bias\"]\n\n    new_checkpoint[\"conv_in.weight\"] = checkpoint[\"input_blocks.0.0.weight\"]\n    new_checkpoint[\"conv_in.bias\"] = checkpoint[\"input_blocks.0.0.bias\"]\n\n    new_checkpoint[\"conv_norm_out.weight\"] = checkpoint[\"out.0.weight\"]\n    new_checkpoint[\"conv_norm_out.bias\"] = checkpoint[\"out.0.bias\"]\n    new_checkpoint[\"conv_out.weight\"] = checkpoint[\"out.2.weight\"]\n    new_checkpoint[\"conv_out.bias\"] = checkpoint[\"out.2.bias\"]\n\n    # Retrieves the keys for the input blocks only\n    num_input_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in checkpoint if \"input_blocks\" in layer})\n    input_blocks = {\n        layer_id: [key for key in checkpoint if f\"input_blocks.{layer_id}\" in key]\n        for layer_id in range(num_input_blocks)\n    }\n\n    # Retrieves the keys for the middle blocks only\n    num_middle_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in checkpoint if \"middle_block\" in layer})\n    middle_blocks = {\n        layer_id: [key for key in checkpoint if f\"middle_block.{layer_id}\" in key]\n        for layer_id in range(num_middle_blocks)\n    }\n\n    # Retrieves the",
      "metadata": {
        "source": "scripts/convert_ldm_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 129, "column": 0 },
          "end": { "row": 129, "column": 0 }
        }
      }
    }
  ],
  [
    "1211",
    {
      "pageContent": "def resnet_to_diffusers_checkpoint(resnet, checkpoint, *, diffusers_resnet_prefix, resnet_prefix):\n    rv = {\n        # norm1\n        f\"{diffusers_resnet_prefix}.norm1.linear.weight\": checkpoint[f\"{resnet_prefix}.main.0.mapper.weight\"],\n        f\"{diffusers_resnet_prefix}.norm1.linear.bias\": checkpoint[f\"{resnet_prefix}.main.0.mapper.bias\"],\n        # conv1\n        f\"{diffusers_resnet_prefix}.conv1.weight\": checkpoint[f\"{resnet_prefix}.main.2.weight\"],\n        f\"{diffusers_resnet_prefix}.conv1.bias\": checkpoint[f\"{resnet_prefix}.main.2.bias\"],\n        # norm2\n        f\"{diffusers_resnet_prefix}.norm2.linear.weight\": checkpoint[f\"{resnet_prefix}.main.4.mapper.weight\"],\n        f\"{diffusers_resnet_prefix}.norm2.linear.bias\": checkpoint[f\"{resnet_prefix}.main.4.mapper.bias\"],\n        # conv2\n        f\"{diffusers_resnet_prefix}.conv2.weight\": checkpoint[f\"{resnet_prefix}.main.6.weight\"],\n        f\"{diffusers_resnet_prefix}.conv2.bias\": checkpoint[f\"{resnet_prefix}.main.6.bias\"],\n    }\n\n    if resnet.conv_shortcut is not None:\n        rv.update(\n            {\n                f\"{diffusers_resnet_prefix}.conv_shortcut.weight\": checkpoint[f\"{resnet_prefix}.skip.weight\"],\n            }\n        )\n\n    return rv",
      "metadata": {
        "source": "scripts/convert_k_upscaler_to_diffusers.py",
        "range": {
          "start": { "row": 12, "column": 0 },
          "end": { "row": 12, "column": 0 }
        }
      }
    }
  ],
  [
    "1212",
    {
      "pageContent": "def self_attn_to_diffusers_checkpoint(checkpoint, *, diffusers_attention_prefix, attention_prefix):\n    weight_q, weight_k, weight_v = checkpoint[f\"{attention_prefix}.qkv_proj.weight\"].chunk(3, dim=0)\n    bias_q, bias_k, bias_v = checkpoint[f\"{attention_prefix}.qkv_proj.bias\"].chunk(3, dim=0)\n    rv = {\n        # norm\n        f\"{diffusers_attention_prefix}.norm1.linear.weight\": checkpoint[f\"{attention_prefix}.norm_in.mapper.weight\"],\n        f\"{diffusers_attention_prefix}.norm1.linear.bias\": checkpoint[f\"{attention_prefix}.norm_in.mapper.bias\"],\n        # to_q\n        f\"{diffusers_attention_prefix}.attn1.to_q.weight\": weight_q.squeeze(-1).squeeze(-1),\n        f\"{diffusers_attention_prefix}.attn1.to_q.bias\": bias_q,\n        # to_k\n        f\"{diffusers_attention_prefix}.attn1.to_k.weight\": weight_k.squeeze(-1).squeeze(-1),\n        f\"{diffusers_attention_prefix}.attn1.to_k.bias\": bias_k,\n        # to_v\n        f\"{diffusers_attention_prefix}.attn1.to_v.weight\": weight_v.squeeze(-1).squeeze(-1),\n        f\"{diffusers_attention_prefix}.attn1.to_v.bias\": bias_v,\n        # to_out\n        f\"{diffusers_attention_prefix}.attn1.to_out.0.weight\": checkpoint[f\"{attention_prefix}.out_proj.weight\"]\n        .squeeze(-1)\n        .squeeze(-1),\n        f\"{diffusers_attention_prefix}.attn1.to_out.0.bias\": checkpoint[f\"{attention_prefix}.out_proj.bias\"],\n    }\n\n    return rv",
      "metadata": {
        "source": "scripts/convert_k_upscaler_to_diffusers.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "1213",
    {
      "pageContent": "def cross_attn_to_diffusers_checkpoint(\n    checkpoint, *, diffusers_attention_prefix, diffusers_attention_index, attention_prefix\n):\n    weight_k, weight_v = checkpoint[f\"{attention_prefix}.kv_proj.weight\"].chunk(2, dim=0)\n    bias_k, bias_v = checkpoint[f\"{attention_prefix}.kv_proj.bias\"].chunk(2, dim=0)\n\n    rv = {\n        # norm2 (ada groupnorm)\n        f\"{diffusers_attention_prefix}.norm{diffusers_attention_index}.linear.weight\": checkpoint[\n            f\"{attention_prefix}.norm_dec.mapper.weight\"\n        ],\n        f\"{diffusers_attention_prefix}.norm{diffusers_attention_index}.linear.bias\": checkpoint[\n            f\"{attention_prefix}.norm_dec.mapper.bias\"\n        ],\n        # layernorm on encoder_hidden_state\n        f\"{diffusers_attention_prefix}.attn{diffusers_attention_index}.norm_cross.weight\": checkpoint[\n            f\"{attention_prefix}.norm_enc.weight\"\n        ],\n        f\"{diffusers_attention_prefix}.attn{diffusers_attention_index}.norm_cross.bias\": checkpoint[\n            f\"{attention_prefix}.norm_enc.bias\"\n        ],\n        # to_q\n        f\"{diffusers_attention_prefix}.attn{diffusers_attention_index}.to_q.weight\": checkpoint[\n            f\"{attention_prefix}.q_proj.weight\"\n        ]\n        .squeeze(-1)\n        .squeeze(-1),\n        f\"{diffusers_attention_prefix}.attn{diffusers_attention_index}.to_q.bias\": checkpoint[\n            f\"{attention_prefix}.q_proj.bias\"\n        ],\n        # to_k\n        f\"{diffusers_attention_prefix}.attn{diffusers_attention_index}.to_k.weight\": weight_k.squeeze(-1).squeeze(-1),\n        f\"{diffusers_attention_prefix}.attn{diffuse",
      "metadata": {
        "source": "scripts/convert_k_upscaler_to_diffusers.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "1214",
    {
      "pageContent": "def block_to_diffusers_checkpoint(block, checkpoint, block_idx, block_type):\n    block_prefix = \"inner_model.u_net.u_blocks\" if block_type == \"up\" else \"inner_model.u_net.d_blocks\"\n    block_prefix = f\"{block_prefix}.{block_idx}\"\n\n    diffusers_checkpoint = {}\n\n    if not hasattr(block, \"attentions\"):\n        n = 1  # resnet only\n    elif not block.attentions[0].add_self_attention:\n        n = 2  # resnet -> cross-attention\n    else:\n        n = 3  # resnet -> self-attention -> cross-attention)\n\n    for resnet_idx, resnet in enumerate(block.resnets):\n        # diffusers_resnet_prefix = f\"{diffusers_up_block_prefix}.resnets.{resnet_idx}\"\n        diffusers_resnet_prefix = f\"{block_type}_blocks.{block_idx}.resnets.{resnet_idx}\"\n        idx = n * resnet_idx if block_type == \"up\" else n * resnet_idx + 1\n        resnet_prefix = f\"{block_prefix}.{idx}\" if block_type == \"up\" else f\"{block_prefix}.{idx}\"\n\n        diffusers_checkpoint.update(\n            resnet_to_diffusers_checkpoint(\n                resnet, checkpoint, diffusers_resnet_prefix=diffusers_resnet_prefix, resnet_prefix=resnet_prefix\n            )\n        )\n\n    if hasattr(block, \"attentions\"):\n        for attention_idx, attention in enumerate(block.attentions):\n            diffusers_attention_prefix = f\"{block_type}_blocks.{block_idx}.attentions.{attention_idx}\"\n            idx = n * attention_idx + 1 if block_type == \"up\" else n * attention_idx + 2\n            self_attention_prefix = f\"{block_prefix}.{idx}\"\n            cross_attention_prefix = f\"{block_prefix}.{idx }\"\n            cross_attention_index = 1 if not attent",
      "metadata": {
        "source": "scripts/convert_k_upscaler_to_diffusers.py",
        "range": {
          "start": { "row": 114, "column": 0 },
          "end": { "row": 114, "column": 0 }
        }
      }
    }
  ],
  [
    "1215",
    {
      "pageContent": "def unet_to_diffusers_checkpoint(model, checkpoint):\n    diffusers_checkpoint = {}\n\n    # pre-processing\n    diffusers_checkpoint.update(\n        {\n            \"conv_in.weight\": checkpoint[\"inner_model.proj_in.weight\"],\n            \"conv_in.bias\": checkpoint[\"inner_model.proj_in.bias\"],\n        }\n    )\n\n    # timestep and class embedding\n    diffusers_checkpoint.update(\n        {\n            \"time_proj.weight\": checkpoint[\"inner_model.timestep_embed.weight\"].squeeze(-1),\n            \"time_embedding.linear_1.weight\": checkpoint[\"inner_model.mapping.0.weight\"],\n            \"time_embedding.linear_1.bias\": checkpoint[\"inner_model.mapping.0.bias\"],\n            \"time_embedding.linear_2.weight\": checkpoint[\"inner_model.mapping.2.weight\"],\n            \"time_embedding.linear_2.bias\": checkpoint[\"inner_model.mapping.2.bias\"],\n            \"time_embedding.cond_proj.weight\": checkpoint[\"inner_model.mapping_cond.weight\"],\n        }\n    )\n\n    # down_blocks\n    for down_block_idx, down_block in enumerate(model.down_blocks):\n        diffusers_checkpoint.update(block_to_diffusers_checkpoint(down_block, checkpoint, down_block_idx, \"down\"))\n\n    # up_blocks\n    for up_block_idx, up_block in enumerate(model.up_blocks):\n        diffusers_checkpoint.update(block_to_diffusers_checkpoint(up_block, checkpoint, up_block_idx, \"up\"))\n\n    # post-processing\n    diffusers_checkpoint.update(\n        {\n            \"conv_out.weight\": checkpoint[\"inner_model.proj_out.weight\"],\n            \"conv_out.bias\": checkpoint[\"inner_model.proj_out.bias\"],\n        }\n    )\n\n    return diffusers_checkpoint",
      "metadata": {
        "source": "scripts/convert_k_upscaler_to_diffusers.py",
        "range": {
          "start": { "row": 174, "column": 0 },
          "end": { "row": 174, "column": 0 }
        }
      }
    }
  ],
  [
    "1216",
    {
      "pageContent": "def unet_model_from_original_config(original_config):\n    in_channels = original_config[\"input_channels\"] + original_config[\"unet_cond_dim\"]\n    out_channels = original_config[\"input_channels\"] + (1 if original_config[\"has_variance\"] else 0)\n\n    block_out_channels = original_config[\"channels\"]\n\n    assert (\n        len(set(original_config[\"depths\"])) == 1\n    ), \"UNet2DConditionModel currently do not support blocks with different number of layers\"\n    layers_per_block = original_config[\"depths\"][0]\n\n    class_labels_dim = original_config[\"mapping_cond_dim\"]\n    cross_attention_dim = original_config[\"cross_cond_dim\"]\n\n    attn1_types = []\n    attn2_types = []\n    for s, c in zip(original_config[\"self_attn_depths\"], original_config[\"cross_attn_depths\"]):\n        if s:\n            a1 = \"self\"\n            a2 = \"cross\" if c else None\n        elif c:\n            a1 = \"cross\"\n            a2 = None\n        else:\n            a1 = None\n            a2 = None\n        attn1_types.append(a1)\n        attn2_types.append(a2)\n\n    unet = UNet2DConditionModel(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        down_block_types=(\"KDownBlock2D\", \"KCrossAttnDownBlock2D\", \"KCrossAttnDownBlock2D\", \"KCrossAttnDownBlock2D\"),\n        mid_block_type=None,\n        up_block_types=(\"KCrossAttnUpBlock2D\", \"KCrossAttnUpBlock2D\", \"KCrossAttnUpBlock2D\", \"KUpBlock2D\"),\n        block_out_channels=block_out_channels,\n        layers_per_block=layers_per_block,\n        act_fn=\"gelu\",\n        norm_num_groups=None,\n        cross_attention_dim=cross_attention_dim,\n        attention_head_dim",
      "metadata": {
        "source": "scripts/convert_k_upscaler_to_diffusers.py",
        "range": {
          "start": { "row": 216, "column": 0 },
          "end": { "row": 216, "column": 0 }
        }
      }
    }
  ],
  [
    "1217",
    {
      "pageContent": "def main(args):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    orig_config_path = huggingface_hub.hf_hub_download(UPSCALER_REPO, \"config_laion_text_cond_latent_upscaler_2.json\")\n    orig_weights_path = huggingface_hub.hf_hub_download(\n        UPSCALER_REPO, \"laion_text_cond_latent_upscaler_2_1_00470000_slim.pth\"\n    )\n    print(f\"loading original model configuration from {orig_config_path}\")\n    print(f\"loading original model checkpoint from {orig_weights_path}\")\n\n    print(\"converting to diffusers unet\")\n    orig_config = K.config.load_config(open(orig_config_path))[\"model\"]\n    model = unet_model_from_original_config(orig_config)\n\n    orig_checkpoint = torch.load(orig_weights_path, map_location=device)[\"model_ema\"]\n    converted_checkpoint = unet_to_diffusers_checkpoint(model, orig_checkpoint)\n\n    model.load_state_dict(converted_checkpoint, strict=True)\n    model.save_pretrained(args.dump_path)\n    print(f\"saving converted unet model in {args.dump_path}\")",
      "metadata": {
        "source": "scripts/convert_k_upscaler_to_diffusers.py",
        "range": {
          "start": { "row": 268, "column": 0 },
          "end": { "row": 268, "column": 0 }
        }
      }
    }
  ],
  [
    "1218",
    {
      "pageContent": "def convert_ncsnpp_checkpoint(checkpoint, config):\n    \"\"\"\n    Takes a state dict and the path to\n    \"\"\"\n    new_model_architecture = UNet2DModel(**config)\n    new_model_architecture.time_proj.W.data = checkpoint[\"all_modules.0.W\"].data\n    new_model_architecture.time_proj.weight.data = checkpoint[\"all_modules.0.W\"].data\n    new_model_architecture.time_embedding.linear_1.weight.data = checkpoint[\"all_modules.1.weight\"].data\n    new_model_architecture.time_embedding.linear_1.bias.data = checkpoint[\"all_modules.1.bias\"].data\n\n    new_model_architecture.time_embedding.linear_2.weight.data = checkpoint[\"all_modules.2.weight\"].data\n    new_model_architecture.time_embedding.linear_2.bias.data = checkpoint[\"all_modules.2.bias\"].data\n\n    new_model_architecture.conv_in.weight.data = checkpoint[\"all_modules.3.weight\"].data\n    new_model_architecture.conv_in.bias.data = checkpoint[\"all_modules.3.bias\"].data\n\n    new_model_architecture.conv_norm_out.weight.data = checkpoint[list(checkpoint.keys())[-4]].data\n    new_model_architecture.conv_norm_out.bias.data = checkpoint[list(checkpoint.keys())[-3]].data\n    new_model_architecture.conv_out.weight.data = checkpoint[list(checkpoint.keys())[-2]].data\n    new_model_architecture.conv_out.bias.data = checkpoint[list(checkpoint.keys())[-1]].data\n\n    module_index = 4\n\n    def set_attention_weights(new_layer, old_checkpoint, index):\n        new_layer.query.weight.data = old_checkpoint[f\"all_modules.{index}.NIN_0.W\"].data.T\n        new_layer.key.weight.data = old_checkpoint[f\"all_modules.{index}.NIN_1.W\"].data.T\n        new_layer.value.weight.",
      "metadata": {
        "source": "scripts/convert_ncsnpp_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1219",
    {
      "pageContent": "def set_attention_weights(new_layer, old_checkpoint, index):\n        new_layer.query.weight.data = old_checkpoint[f\"all_modules.{index}.NIN_0.W\"].data.T\n        new_layer.key.weight.data = old_checkpoint[f\"all_modules.{index}.NIN_1.W\"].data.T\n        new_layer.value.weight.data = old_checkpoint[f\"all_modules.{index}.NIN_2.W\"].data.T\n\n        new_layer.query.bias.data = old_checkpoint[f\"all_modules.{index}.NIN_0.b\"].data\n        new_layer.key.bias.data = old_checkpoint[f\"all_modules.{index}.NIN_1.b\"].data\n        new_layer.value.bias.data = old_checkpoint[f\"all_modules.{index}.NIN_2.b\"].data\n\n        new_layer.proj_attn.weight.data = old_checkpoint[f\"all_modules.{index}.NIN_3.W\"].data.T\n        new_layer.proj_attn.bias.data = old_checkpoint[f\"all_modules.{index}.NIN_3.b\"].data\n\n        new_layer.group_norm.weight.data = old_checkpoint[f\"all_modules.{index}.GroupNorm_0.weight\"].data\n        new_layer.group_norm.bias.data = old_checkpoint[f\"all_modules.{index}.GroupNorm_0.bias\"].data",
      "metadata": {
        "source": "scripts/convert_ncsnpp_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 47, "column": 4 },
          "end": { "row": 47, "column": 4 }
        }
      }
    }
  ],
  [
    "1220",
    {
      "pageContent": "def set_resnet_weights(new_layer, old_checkpoint, index):\n        new_layer.conv1.weight.data = old_checkpoint[f\"all_modules.{index}.Conv_0.weight\"].data\n        new_layer.conv1.bias.data = old_checkpoint[f\"all_modules.{index}.Conv_0.bias\"].data\n        new_layer.norm1.weight.data = old_checkpoint[f\"all_modules.{index}.GroupNorm_0.weight\"].data\n        new_layer.norm1.bias.data = old_checkpoint[f\"all_modules.{index}.GroupNorm_0.bias\"].data\n\n        new_layer.conv2.weight.data = old_checkpoint[f\"all_modules.{index}.Conv_1.weight\"].data\n        new_layer.conv2.bias.data = old_checkpoint[f\"all_modules.{index}.Conv_1.bias\"].data\n        new_layer.norm2.weight.data = old_checkpoint[f\"all_modules.{index}.GroupNorm_1.weight\"].data\n        new_layer.norm2.bias.data = old_checkpoint[f\"all_modules.{index}.GroupNorm_1.bias\"].data\n\n        new_layer.time_emb_proj.weight.data = old_checkpoint[f\"all_modules.{index}.Dense_0.weight\"].data\n        new_layer.time_emb_proj.bias.data = old_checkpoint[f\"all_modules.{index}.Dense_0.bias\"].data\n\n        if new_layer.in_channels != new_layer.out_channels or new_layer.up or new_layer.down:\n            new_layer.conv_shortcut.weight.data = old_checkpoint[f\"all_modules.{index}.Conv_2.weight\"].data\n            new_layer.conv_shortcut.bias.data = old_checkpoint[f\"all_modules.{index}.Conv_2.bias\"].data",
      "metadata": {
        "source": "scripts/convert_ncsnpp_original_checkpoint_to_diffusers.py",
        "range": {
          "start": { "row": 62, "column": 4 },
          "end": { "row": 62, "column": 4 }
        }
      }
    }
  ],
  [
    "1221",
    {
      "pageContent": "class DepsTableUpdateCommand(Command):\n    \"\"\"\n    A custom distutils command that updates the dependency table.\n    usage: python setup.py deps_table_update\n    \"\"\"\n\n    description = \"build runtime dependency table\"\n    user_options = [\n        # format: (long option, short option, description).\n        (\"dep-table-update\", None, \"updates src/diffusers/dependency_versions_table.py\"),\n    ]\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        entries = \"\\n\".join([f'    \"{k}\": \"{v}\",' for k, v in deps.items()])\n        content = [\n            \"# THIS FILE HAS BEEN AUTOGENERATED. To update:\",\n            \"# 1. modify the `_deps` dict in setup.py\",\n            \"# 2. run `make deps_table_update``\",\n            \"deps = {\",\n            entries,\n            \"}\",\n            \"\",\n        ]\n        target = \"src/diffusers/dependency_versions_table.py\"\n        print(f\"updating {target}\")\n        with open(target, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            f.write(\"\\n\".join(content))",
      "metadata": {
        "source": "setup.py",
        "range": {
          "start": { "row": 143, "column": 0 },
          "end": { "row": 143, "column": 0 }
        }
      }
    }
  ],
  [
    "1222",
    {
      "pageContent": "def run(self):\n        entries = \"\\n\".join([f'    \"{k}\": \"{v}\",' for k, v in deps.items()])\n        content = [\n            \"# THIS FILE HAS BEEN AUTOGENERATED. To update:\",\n            \"# 1. modify the `_deps` dict in setup.py\",\n            \"# 2. run `make deps_table_update``\",\n            \"deps = {\",\n            entries,\n            \"}\",\n            \"\",\n        ]\n        target = \"src/diffusers/dependency_versions_table.py\"\n        print(f\"updating {target}\")\n        with open(target, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            f.write(\"\\n\".join(content))",
      "metadata": {
        "source": "setup.py",
        "range": {
          "start": { "row": 161, "column": 4 },
          "end": { "row": 161, "column": 4 }
        }
      }
    }
  ],
  [
    "1223",
    {
      "pageContent": "class DDIMSchedulerState:\n    common: CommonSchedulerState\n    final_alpha_cumprod: jnp.ndarray\n\n    # setable values\n    init_noise_sigma: jnp.ndarray\n    timesteps: jnp.ndarray\n    num_inference_steps: Optional[int] = None\n\n    @classmethod\n    def create(\n        cls,\n        common: CommonSchedulerState,\n        final_alpha_cumprod: jnp.ndarray,\n        init_noise_sigma: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ):\n        return cls(\n            common=common,\n            final_alpha_cumprod=final_alpha_cumprod,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "1224",
    {
      "pageContent": "class FlaxDDIMScheduler(FlaxSchedulerMixin, ConfigMixin):\n    \"\"\"\n    Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising\n    diffusion probabilistic models (DDPMs) with non-Markovian guidance.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2010.02502\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        trained_betas (`jnp.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        clip_sample (`bool`, default `True`):\n            option to clip predicted sample between -1 and 1 for numerical stability.\n        set_alpha_to_one (`bool`, default `True`):\n            each diffusion step uses the value of alphas product at that step and at the previo",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 65, "column": 0 },
          "end": { "row": 65, "column": 0 }
        }
      }
    }
  ],
  [
    "1225",
    {
      "pageContent": "def create_state(self, common: Optional[CommonSchedulerState] = None) -> DDIMSchedulerState:\n        if common is None:\n            common = CommonSchedulerState.create(self)\n\n        # At every step in ddim, we are looking into the previous alphas_cumprod\n        # For the final step, there is no previous alphas_cumprod because we are already at 0\n        # `set_alpha_to_one` decides whether we set this parameter simply to one or\n        # whether we use the final alpha of the \"non-previous\" one.\n        final_alpha_cumprod = (\n            jnp.array(1.0, dtype=self.dtype) if self.config.set_alpha_to_one else common.alphas_cumprod[0]\n        )\n\n        # standard deviation of the initial noise distribution\n        init_noise_sigma = jnp.array(1.0, dtype=self.dtype)\n\n        timesteps = jnp.arange(0, self.config.num_train_timesteps).round()[::-1]\n\n        return DDIMSchedulerState.create(\n            common=common,\n            final_alpha_cumprod=final_alpha_cumprod,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 126, "column": 4 },
          "end": { "row": 126, "column": 4 }
        }
      }
    }
  ],
  [
    "1226",
    {
      "pageContent": "def scale_model_input(\n        self, state: DDIMSchedulerState, sample: jnp.ndarray, timestep: Optional[int] = None\n    ) -> jnp.ndarray:\n        \"\"\"\n        Args:\n            state (`PNDMSchedulerState`): the `FlaxPNDMScheduler` state data class instance.\n            sample (`jnp.ndarray`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `jnp.ndarray`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 150, "column": 4 },
          "end": { "row": 150, "column": 4 }
        }
      }
    }
  ],
  [
    "1227",
    {
      "pageContent": "def set_timesteps(\n        self, state: DDIMSchedulerState, num_inference_steps: int, shape: Tuple = ()\n    ) -> DDIMSchedulerState:\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`DDIMSchedulerState`):\n                the `FlaxDDIMScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n        step_ratio = self.config.num_train_timesteps // num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # rounding to avoid issues when num_inference_step is power of 3\n        timesteps = (jnp.arange(0, num_inference_steps) * step_ratio).round()[::-1] + self.config.steps_offset\n\n        return state.replace(\n            num_inference_steps=num_inference_steps,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 164, "column": 4 },
          "end": { "row": 164, "column": 4 }
        }
      }
    }
  ],
  [
    "1228",
    {
      "pageContent": "def _get_variance(self, state: DDIMSchedulerState, timestep, prev_timestep):\n        alpha_prod_t = state.common.alphas_cumprod[timestep]\n        alpha_prod_t_prev = jnp.where(\n            prev_timestep >= 0, state.common.alphas_cumprod[prev_timestep], state.final_alpha_cumprod\n        )\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n        variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n\n        return variance",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 186, "column": 4 },
          "end": { "row": 186, "column": 4 }
        }
      }
    }
  ],
  [
    "1229",
    {
      "pageContent": "def step(\n        self,\n        state: DDIMSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n        eta: float = 0.0,\n        return_dict: bool = True,\n    ) -> Union[FlaxDDIMSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            state (`DDIMSchedulerState`): the `FlaxDDIMScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than FlaxDDIMSchedulerOutput class\n\n        Returns:\n            [`FlaxDDIMSchedulerOutput`] or `tuple`: [`FlaxDDIMSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if state.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n        # Ideally, read DDIM paper in-detail understanding\n\n        # Notation (<variable name> -> <name in paper>\n  ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 198, "column": 4 },
          "end": { "row": 198, "column": 4 }
        }
      }
    }
  ],
  [
    "1230",
    {
      "pageContent": "def add_noise(\n        self,\n        state: DDIMSchedulerState,\n        original_samples: jnp.ndarray,\n        noise: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ) -> jnp.ndarray:\n        return add_noise_common(state.common, original_samples, noise, timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "1231",
    {
      "pageContent": "def get_velocity(\n        self,\n        state: DDIMSchedulerState,\n        sample: jnp.ndarray,\n        noise: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ) -> jnp.ndarray:\n        return get_velocity_common(state.common, sample, noise, timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_flax.py",
        "range": {
          "start": { "row": 294, "column": 4 },
          "end": { "row": 294, "column": 4 }
        }
      }
    }
  ],
  [
    "1232",
    {
      "pageContent": "class DDPMSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "1233",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 46, "column": 0 },
          "end": { "row": 46, "column": 0 }
        }
      }
    }
  ],
  [
    "1234",
    {
      "pageContent": "class DDPMScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and\n    Langevin dynamics sampling.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2006.11239\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        variance_type (`str`):\n            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small`,\n            `fixed_small_log`, `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\n        clip_sample (`bool`, default `True`):\n            option to clip predicted samp",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 75, "column": 0 },
          "end": { "row": 75, "column": 0 }
        }
      }
    }
  ],
  [
    "1235",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 169, "column": 4 },
          "end": { "row": 169, "column": 4 }
        }
      }
    }
  ],
  [
    "1236",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n\n        if num_inference_steps > self.config.num_train_timesteps:\n            raise ValueError(\n                f\"`num_inference_steps`: {num_inference_steps} cannot be larger than `self.config.train_timesteps`:\"\n                f\" {self.config.num_train_timesteps} as the unet model trained with this scheduler can only handle\"\n                f\" maximal {self.config.num_train_timesteps} timesteps.\"\n            )\n\n        self.num_inference_steps = num_inference_steps\n\n        step_ratio = self.config.num_train_timesteps // self.num_inference_steps\n        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)\n        self.timesteps = torch.from_numpy(timesteps).to(device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "1237",
    {
      "pageContent": "def _get_variance(self, t, predicted_variance=None, variance_type=None):\n        num_inference_steps = self.num_inference_steps if self.num_inference_steps else self.config.num_train_timesteps\n        prev_t = t - self.config.num_train_timesteps // num_inference_steps\n        alpha_prod_t = self.alphas_cumprod[t]\n        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n        current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev\n\n        # For t > 0, compute predicted variance t (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n        # and sample from it to get previous sample\n        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t\n\n        if variance_type is None:\n            variance_type = self.config.variance_type\n\n        # hacks - were probably added for training stability\n        if variance_type == \"fixed_small\":\n            variance = torch.clamp(variance, min=1e-20)\n        # for rl-diffuser https://arxiv.org/abs/2205.09991\n        elif variance_type == \"fixed_small_log\":\n            variance = torch.log(torch.clamp(variance, min=1e-20))\n            variance = torch.exp(0.5 * variance)\n        elif variance_type == \"fixed_large\":\n            variance = current_beta_t\n        elif variance_type == \"fixed_large_log\":\n            # Glide max_log\n            variance = torch.log(current_beta_t)\n        elif variance_type == \"learned\":\n            return predicted_variance\n        elif variance_type == \"learned_ran",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 205, "column": 4 },
          "end": { "row": 205, "column": 4 }
        }
      }
    }
  ],
  [
    "1238",
    {
      "pageContent": "def _threshold_sample(self, sample: torch.FloatTensor) -> torch.FloatTensor:\n        # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n        dynamic_max_val = (\n            sample.flatten(1)\n            .abs()\n            .quantile(self.config.dynamic_thresholding_ratio, dim=1)\n            .clamp_min(self.config.sample_max_value)\n            .view(-1, *([1] * (sample.ndim - 1)))\n        )\n        return sample.clamp(-dynamic_max_val, dynamic_max_val) / dynamic_max_val",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 242, "column": 4 },
          "end": { "row": 242, "column": 4 }
        }
      }
    }
  ],
  [
    "1239",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        generator=None,\n        return_dict: bool = True,\n    ) -> Union[DDPMSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            generator: random number generator.\n            return_dict (`bool`): option for returning tuple rather than DDPMSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        t = timestep\n        num_inference_steps = self.num_inference_steps if self.num_inference_steps else self.config.num_train_timesteps\n        prev_t = timestep - self.config.num_train_timesteps // num_inference_steps\n\n        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [\"learned\", \"learned_range\"]:\n            model_output, predicted_variance = torch.split(model_output, sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 253, "column": 4 },
          "end": { "row": 253, "column": 4 }
        }
      }
    }
  ],
  [
    "1240",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 350, "column": 4 },
          "end": { "row": 350, "column": 4 }
        }
      }
    }
  ],
  [
    "1241",
    {
      "pageContent": "def get_velocity(\n        self, sample: torch.FloatTensor, noise: torch.FloatTensor, timesteps: torch.IntTensor\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as sample\n        self.alphas_cumprod = self.alphas_cumprod.to(device=sample.device, dtype=sample.dtype)\n        timesteps = timesteps.to(sample.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(sample.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(sample.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        velocity = sqrt_alpha_prod * noise - sqrt_one_minus_alpha_prod * sample\n        return velocity",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm.py",
        "range": {
          "start": { "row": 373, "column": 4 },
          "end": { "row": 373, "column": 4 }
        }
      }
    }
  ],
  [
    "1242",
    {
      "pageContent": "class SdeVeOutput(BaseOutput):\n    \"\"\"\n    Output class for the ScoreSdeVeScheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        prev_sample_mean (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Mean averaged `prev_sample`. Same as `prev_sample`, only mean-averaged over previous timesteps.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    prev_sample_mean: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1243",
    {
      "pageContent": "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    The variance exploding stochastic differential equation (SDE) scheduler.\n\n    For more information, see the original paper: https://arxiv.org/abs/2011.13456\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        snr (`float`):\n            coefficient weighting the step from the model_output sample (from the network) to the random noise.\n        sigma_min (`float`):\n                initial noise scale for sigma sequence in sampling procedure. The minimum sigma should mirror the\n                distribution of the data.\n        sigma_max (`float`): maximum value used for the range of continuous timesteps passed into the model.\n        sampling_eps (`float`): the end value of sampling, where timesteps decrease progressively from 1 to\n        epsilon.\n        correct_steps (`int`): number of correction steps performed on a produced sample.\n    \"\"\"\n\n    order = 1\n\n    @register_to_config\n    def __init__(\n        self,\n        num_train_timesteps: int = 2000,\n        snr: float = 0.15,\n        sigma_min: float = 0.01,\n        sigma_max: float = 1348.0,\n        sampling_eps: f",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "1244",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 88, "column": 4 },
          "end": { "row": 88, "column": 4 }
        }
      }
    }
  ],
  [
    "1245",
    {
      "pageContent": "def set_timesteps(\n        self, num_inference_steps: int, sampling_eps: float = None, device: Union[str, torch.device] = None\n    ):\n        \"\"\"\n        Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            sampling_eps (`float`, optional):\n                final timestep value (overrides value given at Scheduler instantiation).\n\n        \"\"\"\n        sampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps\n\n        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 102, "column": 4 },
          "end": { "row": 102, "column": 4 }
        }
      }
    }
  ],
  [
    "1246",
    {
      "pageContent": "def set_sigmas(\n        self, num_inference_steps: int, sigma_min: float = None, sigma_max: float = None, sampling_eps: float = None\n    ):\n        \"\"\"\n        Sets the noise scales used for the diffusion chain. Supporting function to be run before inference.\n\n        The sigmas control the weight of the `drift` and `diffusion` components of sample update.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            sigma_min (`float`, optional):\n                initial noise scale value (overrides value given at Scheduler instantiation).\n            sigma_max (`float`, optional):\n                final noise scale value (overrides value given at Scheduler instantiation).\n            sampling_eps (`float`, optional):\n                final timestep value (overrides value given at Scheduler instantiation).\n\n        \"\"\"\n        sigma_min = sigma_min if sigma_min is not None else self.config.sigma_min\n        sigma_max = sigma_max if sigma_max is not None else self.config.sigma_max\n        sampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps\n        if self.timesteps is None:\n            self.set_timesteps(num_inference_steps, sampling_eps)\n\n        self.sigmas = sigma_min * (sigma_max / sigma_min) ** (self.timesteps / sampling_eps)\n        self.discrete_sigmas = torch.exp(torch.linspace(math.log(sigma_min), math.log(sigma_max), num_inference_steps))\n        self.sigmas = torch.tensor([sigma_min * (sigma_max / sigma_min) ** t for t in self.",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 119, "column": 4 },
          "end": { "row": 119, "column": 4 }
        }
      }
    }
  ],
  [
    "1247",
    {
      "pageContent": "def get_adjacent_sigma(self, timesteps, t):\n        return torch.where(\n            timesteps == 0,\n            torch.zeros_like(t.to(timesteps.device)),\n            self.discrete_sigmas[timesteps - 1].to(timesteps.device),\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 148, "column": 4 },
          "end": { "row": 148, "column": 4 }
        }
      }
    }
  ],
  [
    "1248",
    {
      "pageContent": "def step_pred(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        generator: Optional[torch.Generator] = None,\n        return_dict: bool = True,\n    ) -> Union[SdeVeOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            generator: random number generator.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_sde_ve.SdeVeOutput`] or `tuple`: [`~schedulers.scheduling_sde_ve.SdeVeOutput`] if\n            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.timesteps is None:\n            raise ValueError(\n                \"`self.timesteps` is not set, you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        timestep = timestep * torch.ones(\n            sample.shape[0], device=sample.device\n        )  # torch.repeat_interleave(timestep, sample.shape[0])\n        timesteps = (timestep * (len(self.timesteps) - 1)).long()\n\n        #",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 155, "column": 4 },
          "end": { "row": 155, "column": 4 }
        }
      }
    }
  ],
  [
    "1249",
    {
      "pageContent": "def step_correct(\n        self,\n        model_output: torch.FloatTensor,\n        sample: torch.FloatTensor,\n        generator: Optional[torch.Generator] = None,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Correct the predicted sample based on the output model_output of the network. This is often run repeatedly\n        after making the prediction for the previous timestep.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            generator: random number generator.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_sde_ve.SdeVeOutput`] or `tuple`: [`~schedulers.scheduling_sde_ve.SdeVeOutput`] if\n            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.timesteps is None:\n            raise ValueError(\n                \"`self.timesteps` is not set, you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        # For small batch sizes, the paper \"suggest replacing norm(z) with sqrt(d), where d is the dim. of z\"\n        # sample noise for correction\n        noise = randn_tensor(sample.shape, layout=sample.layout, generator=generator).to(sample.device)\n\n        # compute step size from the model_output, the noise, and the snr\n        grad_norm =",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 218, "column": 4 },
          "end": { "row": 218, "column": 4 }
        }
      }
    }
  ],
  [
    "1250",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\n        timesteps = timesteps.to(original_samples.device)\n        sigmas = self.discrete_sigmas.to(original_samples.device)[timesteps]\n        noise = torch.randn_like(original_samples) * sigmas[:, None, None, None]\n        noisy_samples = noise + original_samples\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve.py",
        "range": {
          "start": { "row": 269, "column": 4 },
          "end": { "row": 269, "column": 4 }
        }
      }
    }
  ],
  [
    "1251",
    {
      "pageContent": "class DDPMSchedulerState:\n    common: CommonSchedulerState\n\n    # setable values\n    init_noise_sigma: jnp.ndarray\n    timesteps: jnp.ndarray\n    num_inference_steps: Optional[int] = None\n\n    @classmethod\n    def create(cls, common: CommonSchedulerState, init_noise_sigma: jnp.ndarray, timesteps: jnp.ndarray):\n        return cls(common=common, init_noise_sigma=init_noise_sigma, timesteps=timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "1252",
    {
      "pageContent": "class FlaxDDPMScheduler(FlaxSchedulerMixin, ConfigMixin):\n    \"\"\"\n    Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and\n    Langevin dynamics sampling.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2006.11239\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        variance_type (`str`):\n            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small`,\n            `fixed_small_log`, `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\n        clip_sample (`bool`, default `True`):\n            option to clip predic",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 53, "column": 0 },
          "end": { "row": 53, "column": 0 }
        }
      }
    }
  ],
  [
    "1253",
    {
      "pageContent": "def create_state(self, common: Optional[CommonSchedulerState] = None) -> DDPMSchedulerState:\n        if common is None:\n            common = CommonSchedulerState.create(self)\n\n        # standard deviation of the initial noise distribution\n        init_noise_sigma = jnp.array(1.0, dtype=self.dtype)\n\n        timesteps = jnp.arange(0, self.config.num_train_timesteps).round()[::-1]\n\n        return DDPMSchedulerState.create(\n            common=common,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 109, "column": 4 },
          "end": { "row": 109, "column": 4 }
        }
      }
    }
  ],
  [
    "1254",
    {
      "pageContent": "def scale_model_input(\n        self, state: DDPMSchedulerState, sample: jnp.ndarray, timestep: Optional[int] = None\n    ) -> jnp.ndarray:\n        \"\"\"\n        Args:\n            state (`PNDMSchedulerState`): the `FlaxPNDMScheduler` state data class instance.\n            sample (`jnp.ndarray`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `jnp.ndarray`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "1255",
    {
      "pageContent": "def set_timesteps(\n        self, state: DDPMSchedulerState, num_inference_steps: int, shape: Tuple = ()\n    ) -> DDPMSchedulerState:\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`DDIMSchedulerState`):\n                the `FlaxDDPMScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n\n        step_ratio = self.config.num_train_timesteps // num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # rounding to avoid issues when num_inference_step is power of 3\n        timesteps = (jnp.arange(0, num_inference_steps) * step_ratio).round()[::-1]\n\n        return state.replace(\n            num_inference_steps=num_inference_steps,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 138, "column": 4 },
          "end": { "row": 138, "column": 4 }
        }
      }
    }
  ],
  [
    "1256",
    {
      "pageContent": "def _get_variance(self, state: DDPMSchedulerState, t, predicted_variance=None, variance_type=None):\n        alpha_prod_t = state.common.alphas_cumprod[t]\n        alpha_prod_t_prev = jnp.where(t > 0, state.common.alphas_cumprod[t - 1], jnp.array(1.0, dtype=self.dtype))\n\n        # For t > 0, compute predicted variance t (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n        # and sample from it to get previous sample\n        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * state.common.betas[t]\n\n        if variance_type is None:\n            variance_type = self.config.variance_type\n\n        # hacks - were probably added for training stability\n        if variance_type == \"fixed_small\":\n            variance = jnp.clip(variance, a_min=1e-20)\n        # for rl-diffuser https://arxiv.org/abs/2205.09991\n        elif variance_type == \"fixed_small_log\":\n            variance = jnp.log(jnp.clip(variance, a_min=1e-20))\n        elif variance_type == \"fixed_large\":\n            variance = state.common.betas[t]\n        elif variance_type == \"fixed_large_log\":\n            # Glide max_log\n            variance = jnp.log(state.common.betas[t])\n        elif variance_type == \"learned\":\n            return predicted_variance\n        elif variance_type == \"learned_range\":\n            min_log = variance\n            max_log = state.common.betas[t]\n            frac = (predicted_variance + 1) / 2\n            variance = frac * max_log + (1 - frac) * min_log\n\n        return variance",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 161, "column": 4 },
          "end": { "row": 161, "column": 4 }
        }
      }
    }
  ],
  [
    "1257",
    {
      "pageContent": "def step(\n        self,\n        state: DDPMSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n        key: Optional[jax.random.KeyArray] = None,\n        return_dict: bool = True,\n    ) -> Union[FlaxDDPMSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            state (`DDPMSchedulerState`): the `FlaxDDPMScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            key (`jax.random.KeyArray`): a PRNG key.\n            return_dict (`bool`): option for returning tuple rather than FlaxDDPMSchedulerOutput class\n\n        Returns:\n            [`FlaxDDPMSchedulerOutput`] or `tuple`: [`FlaxDDPMSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        t = timestep\n\n        if key is None:\n            key = jax.random.PRNGKey(0)\n\n        if model_output.shape[1] == sample.shape[1] * 2 and self.config.variance_type in [\"learned\", \"learned_range\"]:\n            model_output, predicted_variance = jnp.split(model_output, sample.shape[1], axis=1)\n        else:\n            predic",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 194, "column": 4 },
          "end": { "row": 194, "column": 4 }
        }
      }
    }
  ],
  [
    "1258",
    {
      "pageContent": "def add_noise(\n        self,\n        state: DDPMSchedulerState,\n        original_samples: jnp.ndarray,\n        noise: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ) -> jnp.ndarray:\n        return add_noise_common(state.common, original_samples, noise, timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 279, "column": 4 },
          "end": { "row": 279, "column": 4 }
        }
      }
    }
  ],
  [
    "1259",
    {
      "pageContent": "def get_velocity(\n        self,\n        state: DDPMSchedulerState,\n        sample: jnp.ndarray,\n        noise: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ) -> jnp.ndarray:\n        return get_velocity_common(state.common, sample, noise, timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddpm_flax.py",
        "range": {
          "start": { "row": 288, "column": 4 },
          "end": { "row": 288, "column": 4 }
        }
      }
    }
  ],
  [
    "1260",
    {
      "pageContent": "class RePaintSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from\n             the current timestep. `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1261",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 45, "column": 0 },
          "end": { "row": 45, "column": 0 }
        }
      }
    }
  ],
  [
    "1262",
    {
      "pageContent": "class RePaintScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    RePaint is a schedule for DDPM inpainting inside a given mask.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/pdf/2201.09865.pdf\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        eta (`float`):\n            The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 -0.0 is DDIM and\n            1.0 is DDPM scheduler respectively.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        variance_type (`str`):\n            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small`,\n            `fixed_small_log`, `fixed_large`, `fixed_large_log`, ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 74, "column": 0 },
          "end": { "row": 74, "column": 0 }
        }
      }
    }
  ],
  [
    "1263",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 152, "column": 4 },
          "end": { "row": 152, "column": 4 }
        }
      }
    }
  ],
  [
    "1264",
    {
      "pageContent": "def set_timesteps(\n        self,\n        num_inference_steps: int,\n        jump_length: int = 10,\n        jump_n_sample: int = 10,\n        device: Union[str, torch.device] = None,\n    ):\n        num_inference_steps = min(self.config.num_train_timesteps, num_inference_steps)\n        self.num_inference_steps = num_inference_steps\n\n        timesteps = []\n\n        jumps = {}\n        for j in range(0, num_inference_steps - jump_length, jump_length):\n            jumps[j] = jump_n_sample - 1\n\n        t = num_inference_steps\n        while t >= 1:\n            t = t - 1\n            timesteps.append(t)\n\n            if jumps.get(t, 0) > 0:\n                jumps[t] = jumps[t] - 1\n                for _ in range(jump_length):\n                    t = t + 1\n                    timesteps.append(t)\n\n        timesteps = np.array(timesteps) * (self.config.num_train_timesteps // self.num_inference_steps)\n        self.timesteps = torch.from_numpy(timesteps).to(device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 166, "column": 4 },
          "end": { "row": 166, "column": 4 }
        }
      }
    }
  ],
  [
    "1265",
    {
      "pageContent": "def _get_variance(self, t):\n        prev_timestep = t - self.config.num_train_timesteps // self.num_inference_steps\n\n        alpha_prod_t = self.alphas_cumprod[t]\n        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n        # For t > 0, compute predicted variance t (see formula (6) and (7) from\n        # https://arxiv.org/pdf/2006.11239.pdf) and sample from it to get\n        # previous sample x_{t-1} ~ N(pred_prev_sample, variance) == add\n        # variance to pred_sample\n        # Is equivalent to formula (16) in https://arxiv.org/pdf/2010.02502.pdf\n        # without eta.\n        # variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * self.betas[t]\n        variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n\n        return variance",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 196, "column": 4 },
          "end": { "row": 196, "column": 4 }
        }
      }
    }
  ],
  [
    "1266",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        original_image: torch.FloatTensor,\n        mask: torch.FloatTensor,\n        generator: Optional[torch.Generator] = None,\n        return_dict: bool = True,\n    ) -> Union[RePaintSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned\n                diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            original_image (`torch.FloatTensor`):\n                the original image to inpaint on.\n            mask (`torch.FloatTensor`):\n                the mask where 0.0 values define which part of the original image to inpaint (change).\n            generator (`torch.Generator`, *optional*): random number generator.\n            return_dict (`bool`): option for returning tuple rather than\n                DDPMSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.RePaintSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.RePaintSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n       ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 215, "column": 4 },
          "end": { "row": 215, "column": 4 }
        }
      }
    }
  ],
  [
    "1267",
    {
      "pageContent": "def undo_step(self, sample, timestep, generator=None):\n        n = self.config.num_train_timesteps // self.num_inference_steps\n\n        for i in range(n):\n            beta = self.betas[timestep + i]\n            if sample.device.type == \"mps\":\n                # randn does not work reproducibly on mps\n                noise = randn_tensor(sample.shape, dtype=sample.dtype, generator=generator)\n                noise = noise.to(sample.device)\n            else:\n                noise = randn_tensor(sample.shape, generator=generator, device=sample.device, dtype=sample.dtype)\n\n            # 10. Algorithm 1 Line 10 https://arxiv.org/pdf/2201.09865.pdf\n            sample = (1 - beta) ** 0.5 * sample + beta**0.5 * noise\n\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 302, "column": 4 },
          "end": { "row": 302, "column": 4 }
        }
      }
    }
  ],
  [
    "1268",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.FloatTensor:\n        raise NotImplementedError(\"Use `DDPMScheduler.add_noise()` to train for sampling with RePaint.\")",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_repaint.py",
        "range": {
          "start": { "row": 319, "column": 4 },
          "end": { "row": 319, "column": 4 }
        }
      }
    }
  ],
  [
    "1269",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999) -> torch.Tensor:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1270",
    {
      "pageContent": "class KDPM2DiscreteScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Scheduler created by @crowsonkb in [k_diffusion](https://github.com/crowsonkb/k-diffusion), see:\n    https://github.com/crowsonkb/k-diffusion/blob/5b3af030dd83e0297272d861c19477735d0317ec/k_diffusion/sampling.py#L188\n\n    Scheduler inspired by DPM-Solver-2 and Algorthim 2 from Karras et al. (2022).\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model. beta_start (`float`): the\n        starting `beta` value of inference. beta_end (`float`): the final `beta` value. beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear` or `scaled_linear`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small`,\n            `fixed_small_log`, `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\n        prediction_type (`str`, default `epsilon`, optional):\n        ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "1271",
    {
      "pageContent": "def index_for_timestep(self, timestep):\n        indices = (self.timesteps == timestep).nonzero()\n        if self.state_in_first_order:\n            pos = -1\n        else:\n            pos = 0\n        return indices[pos].item()",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 115, "column": 4 },
          "end": { "row": 115, "column": 4 }
        }
      }
    }
  ],
  [
    "1272",
    {
      "pageContent": "def scale_model_input(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[float, torch.FloatTensor],\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Args:\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n            sample (`torch.FloatTensor`): input sample timestep (`int`, optional): current timestep\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        step_index = self.index_for_timestep(timestep)\n\n        if self.state_in_first_order:\n            sigma = self.sigmas[step_index]\n        else:\n            sigma = self.sigmas_interpol[step_index]\n\n        sample = sample / ((sigma**2 + 1) ** 0.5)\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 123, "column": 4 },
          "end": { "row": 123, "column": 4 }
        }
      }
    }
  ],
  [
    "1273",
    {
      "pageContent": "def set_timesteps(\n        self,\n        num_inference_steps: int,\n        device: Union[str, torch.device] = None,\n        num_train_timesteps: Optional[int] = None,\n    ):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n\n        num_train_timesteps = num_train_timesteps or self.config.num_train_timesteps\n\n        timesteps = np.linspace(0, num_train_timesteps - 1, num_inference_steps, dtype=float)[::-1].copy()\n\n        sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)\n        self.log_sigmas = torch.from_numpy(np.log(sigmas)).to(device)\n\n        sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)\n        sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)\n        sigmas = torch.from_numpy(sigmas).to(device=device)\n\n        # interpolate sigmas\n        sigmas_interpol = sigmas.log().lerp(sigmas.roll(1).log(), 0.5).exp()\n\n        self.sigmas = torch.cat([sigmas[:1], sigmas[1:].repeat_interleave(2), sigmas[-1:]])\n        self.sigmas_interpol = torch.cat(\n            [sigmas_interpol[:1], sigmas_interpol[1:].repeat_interleave(2), sigmas_interpol[-1:]]\n        )\n\n        # standar",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 146, "column": 4 },
          "end": { "row": 146, "column": 4 }
        }
      }
    }
  ],
  [
    "1274",
    {
      "pageContent": "def sigma_to_t(self, sigma):\n        # get log sigma\n        log_sigma = sigma.log()\n\n        # get distribution\n        dists = log_sigma - self.log_sigmas[:, None]\n\n        # get sigmas range\n        low_idx = dists.ge(0).cumsum(dim=0).argmax(dim=0).clamp(max=self.log_sigmas.shape[0] - 2)\n        high_idx = low_idx + 1\n\n        low = self.log_sigmas[low_idx]\n        high = self.log_sigmas[high_idx]\n\n        # interpolate sigmas\n        w = (low - log_sigma) / (low - high)\n        w = w.clamp(0, 1)\n\n        # transform interpolation to time range\n        t = (1 - w) * low_idx + w * high_idx\n        t = t.view(sigma.shape)\n        return t",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 199, "column": 4 },
          "end": { "row": 199, "column": 4 }
        }
      }
    }
  ],
  [
    "1275",
    {
      "pageContent": "def step(\n        self,\n        model_output: Union[torch.FloatTensor, np.ndarray],\n        timestep: Union[float, torch.FloatTensor],\n        sample: Union[torch.FloatTensor, np.ndarray],\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Args:\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n            model_output (`torch.FloatTensor` or `np.ndarray`): direct output from learned diffusion model. timestep\n            (`int`): current discrete timestep in the diffusion chain. sample (`torch.FloatTensor` or `np.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n        Returns:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        step_index = self.index_for_timestep(timestep)\n\n        if self.state_in_first_order:\n            sigma = self.sigmas[step_index]\n            sigma_interpol = self.sigmas_interpol[step_index + 1]\n            sigma_next = self.sigmas[step_index + 1]\n        else:\n            # 2nd order / KDPM2's method\n            sigma = self.sigmas[step_index - 1]\n            sigma_interpol = self.sigmas_interpol[step_index]",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "1276",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\n        self.sigmas = self.sigmas.to(device=original_samples.device, dtype=original_samples.dtype)\n        if original_samples.device.type == \"mps\" and torch.is_floating_point(timesteps):\n            # mps does not support float64\n            self.timesteps = self.timesteps.to(original_samples.device, dtype=torch.float32)\n            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)\n        else:\n            self.timesteps = self.timesteps.to(original_samples.device)\n            timesteps = timesteps.to(original_samples.device)\n\n        step_indices = [self.index_for_timestep(t) for t in timesteps]\n\n        sigma = self.sigmas[step_indices].flatten()\n        while len(sigma.shape) < len(original_samples.shape):\n            sigma = sigma.unsqueeze(-1)\n\n        noisy_samples = original_samples + noise * sigma\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py",
        "range": {
          "start": { "row": 306, "column": 4 },
          "end": { "row": 306, "column": 4 }
        }
      }
    }
  ],
  [
    "1277",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1278",
    {
      "pageContent": "class PNDMScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,\n    namely Runge-Kutta method and a linear multi-step method.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2202.09778\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        skip_prk_steps (`bool`):\n            allows the scheduler to skip the Runge-Kutta steps that are defined in the original paper as being required\n            before plms steps; defaults to `False`.\n        set_alpha_to_one (`bool`, default `False`):\n            each diffusion step uses the v",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 56, "column": 0 },
          "end": { "row": 56, "column": 0 }
        }
      }
    }
  ],
  [
    "1279",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n\n        self.num_inference_steps = num_inference_steps\n        step_ratio = self.config.num_train_timesteps // self.num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # casting to int to avoid issues when num_inference_step is power of 3\n        self._timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()\n        self._timesteps += self.config.steps_offset\n\n        if self.config.skip_prk_steps:\n            # for some models like stable diffusion the prk steps can/should be skipped to\n            # produce better results. When using PNDM with `self.config.skip_prk_steps` the implementation\n            # is based on crowsonkb's PLMS sampler implementation: https://github.com/CompVis/latent-diffusion/pull/51\n            self.prk_timesteps = np.array([])\n            self.plms_timesteps = np.concatenate([self._timesteps[:-1], self._timesteps[-2:-1], self._timesteps[-1:]])[\n                ::-1\n            ].copy()\n        else:\n            prk_timesteps = np.array(self._timesteps[-self.pndm_order :]).repeat(2) + np.tile(\n                np.array([0, self.config.num_train_timesteps // num_inference_steps // 2]), self.pndm_order\n            )\n      ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 151, "column": 4 },
          "end": { "row": 151, "column": 4 }
        }
      }
    }
  ],
  [
    "1280",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        This function calls `step_prk()` or `step_plms()` depending on the internal variable `counter`.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.counter < len(self.prk_timesteps) and not self.config.skip_prk_steps:\n            return self.step_prk(model_output=model_output, timestep=timestep, sample=sample, return_dict=return_dict)\n        else:\n            return self.step_plms(model_output=model_output, timestep=timestep, sample=sample, return_dict=return_dict)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 191, "column": 4 },
          "end": { "row": 191, "column": 4 }
        }
      }
    }
  ],
  [
    "1281",
    {
      "pageContent": "def step_prk(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the\n        solution to the differential equation.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~scheduling_utils.SchedulerOutput`] or `tuple`: [`~scheduling_utils.SchedulerOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        diff_to_prev = 0 if self.counter % 2 else self.config.num_train_timesteps // self.num_inference_steps // 2\n        prev_timestep = timestep - diff_to_prev\n        timestep = self.prk_timesteps[self.counter // 4 * 4]\n\n        if self.counter % 4 == 0:\n            self.cur_model_output += 1 / 6 * model_output\n            self.ets.append(model_output)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 222, "column": 4 },
          "end": { "row": 222, "column": 4 }
        }
      }
    }
  ],
  [
    "1282",
    {
      "pageContent": "def step_plms(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple\n        times to approximate the solution.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~scheduling_utils.SchedulerOutput`] or `tuple`: [`~scheduling_utils.SchedulerOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        if not self.config.skip_prk_steps and len(self.ets) < 3:\n            raise ValueError(\n                f\"{self.__class__} can only be run AFTER scheduler has been run \"\n                \"in 'prk' mode for at least 12 iterations \"\n                \"See: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pipeline_pndm.py \"\n       ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 277, "column": 4 },
          "end": { "row": 277, "column": 4 }
        }
      }
    }
  ],
  [
    "1283",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, *args, **kwargs) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 344, "column": 4 },
          "end": { "row": 344, "column": 4 }
        }
      }
    }
  ],
  [
    "1284",
    {
      "pageContent": "def _get_prev_sample(self, sample, timestep, prev_timestep, model_output):\n        # See formula (9) of PNDM paper https://arxiv.org/pdf/2202.09778.pdf\n        # this function computes x_(t) using the formula of (9)\n        # Note that x_t needs to be added to both sides of the equation\n\n        # Notation (<variable name> -> <name in paper>\n        # alpha_prod_t -> _t\n        # alpha_prod_t_prev -> _(t)\n        # beta_prod_t -> (1 - _t)\n        # beta_prod_t_prev -> (1 - _(t))\n        # sample -> x_t\n        # model_output -> e_(x_t, t)\n        # prev_sample -> x_(t)\n        alpha_prod_t = self.alphas_cumprod[timestep]\n        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n        if self.config.prediction_type == \"v_prediction\":\n            model_output = (alpha_prod_t**0.5) * model_output + (beta_prod_t**0.5) * sample\n        elif self.config.prediction_type != \"epsilon\":\n            raise ValueError(\n                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon` or `v_prediction`\"\n            )\n\n        # corresponds to (_(t) - _t) divided by\n        # denominator of x_t in formula (9) and plus 1\n        # Note: (_(t) - _t) / (sqrt(_t) * (sqrt(_(t)) + sqr(_t))) =\n        # sqrt(_(t)) / sqrt(_t))\n        sample_coeff = (alpha_prod_t_prev / alpha_prod_t) ** (0.5)\n\n        # corresponds to denominator of e_(x_t, t) in formula (9)\n        model_output_denom_coef",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 357, "column": 4 },
          "end": { "row": 357, "column": 4 }
        }
      }
    }
  ],
  [
    "1285",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.Tensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm.py",
        "range": {
          "start": { "row": 400, "column": 4 },
          "end": { "row": 400, "column": 4 }
        }
      }
    }
  ],
  [
    "1286",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1287",
    {
      "pageContent": "class DPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with\n    the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality\n    samples, and it can generate quite good samples even in only 10 steps.\n\n    For more details, see the original paper: https://arxiv.org/abs/2206.00927 and https://arxiv.org/abs/2211.01095\n\n    Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We\n    recommend to use `solver_order=2` for guided sampling, and `solver_order=3` for unconditional sampling.\n\n    We also support the \"dynamic thresholding\" method in Imagen (https://arxiv.org/abs/2205.11487). For pixel-space\n    diffusion models, you can set both `algorithm_type=\"dpmsolver++\"` and `thresholding=True` to use the dynamic\n    thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as\n    stable-diffusion).\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`fl",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 56, "column": 0 },
          "end": { "row": 56, "column": 0 }
        }
      }
    }
  ],
  [
    "1288",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        timesteps = (\n            np.linspace(0, self.num_train_timesteps - 1, num_inference_steps + 1)\n            .round()[::-1][:-1]\n            .copy()\n            .astype(np.int64)\n        )\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n        self.model_outputs = [\n            None,\n        ] * self.config.solver_order\n        self.lower_order_nums = 0",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 184, "column": 4 },
          "end": { "row": 184, "column": 4 }
        }
      }
    }
  ],
  [
    "1289",
    {
      "pageContent": "def _threshold_sample(self, sample: torch.FloatTensor) -> torch.FloatTensor:\n        # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n        dynamic_max_val = (\n            sample.flatten(1)\n            .abs()\n            .quantile(self.config.dynamic_thresholding_ratio, dim=1)\n            .clamp_min(self.config.sample_max_value)\n            .view(-1, *([1] * (sample.ndim - 1)))\n        )\n        return sample.clamp(-dynamic_max_val, dynamic_max_val) / dynamic_max_val",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 208, "column": 4 },
          "end": { "row": 208, "column": 4 }
        }
      }
    }
  ],
  [
    "1290",
    {
      "pageContent": "def convert_model_output(\n        self, model_output: torch.FloatTensor, timestep: int, sample: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs.\n\n        DPM-Solver is designed to discretize an integral of the noise prediction model, and DPM-Solver++ is designed to\n        discretize an integral of the data prediction model. So we need to first convert the model output to the\n        corresponding type to match the algorithm.\n\n        Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or\n        DPM-Solver++ for both noise prediction model and data prediction model.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the converted model output.\n        \"\"\"\n        # DPM-Solver++ needs to solve an integral of the data prediction model.\n        if self.config.algorithm_type == \"dpmsolver++\":\n            if self.config.prediction_type == \"epsilon\":\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                x0_pred = (sample - sigma_t * model_output) / alpha_t\n            elif self.config.prediction_type == \"sample\":\n                x0_pred = model_output\n        ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 219, "column": 4 },
          "end": { "row": 219, "column": 4 }
        }
      }
    }
  ],
  [
    "1291",
    {
      "pageContent": "def dpm_solver_first_order_update(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the first-order DPM-Solver (equivalent to DDIM).\n\n        See https://arxiv.org/abs/2206.00927 for the detailed derivation.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        lambda_t, lambda_s = self.lambda_t[prev_timestep], self.lambda_t[timestep]\n        alpha_t, alpha_s = self.alpha_t[prev_timestep], self.alpha_t[timestep]\n        sigma_t, sigma_s = self.sigma_t[prev_timestep], self.sigma_t[timestep]\n        h = lambda_t - lambda_s\n        if self.config.algorithm_type == \"dpmsolver++\":\n            x_t = (sigma_t / sigma_s) * sample - (alpha_t * (torch.exp(-h) - 1.0)) * model_output\n        elif self.config.algorithm_type == \"dpmsolver\":\n            x_t = (alpha_t / alpha_s) * sample - (sigma_t * (torch.exp(h) - 1.0)) * model_output\n        return x_t",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 282, "column": 4 },
          "end": { "row": 282, "column": 4 }
        }
      }
    }
  ],
  [
    "1292",
    {
      "pageContent": "def multistep_dpm_solver_second_order_update(\n        self,\n        model_output_list: List[torch.FloatTensor],\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the second-order multistep DPM-Solver.\n\n        Args:\n            model_output_list (`List[torch.FloatTensor]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1 = prev_timestep, timestep_list[-1], timestep_list[-2]\n        m0, m1 = model_output_list[-1], model_output_list[-2]\n        lambda_t, lambda_s0, lambda_s1 = self.lambda_t[t], self.lambda_t[s0], self.lambda_t[s1]\n        alpha_t, alpha_s0 = self.alpha_t[t], self.alpha_t[s0]\n        sigma_t, sigma_s0 = self.sigma_t[t], self.sigma_t[s0]\n        h, h_0 = lambda_t - lambda_s0, lambda_s0 - lambda_s1\n        r0 = h_0 / h\n        D0, D1 = m0, (1.0 / r0) * (m0 - m1)\n        if self.config.algorithm_type == \"dpmsolver++\":\n            # See https://arxiv.org/abs/2211.01095 for detailed derivations\n            if self.config.solver_type == \"midpoint\":\n                x_t = (\n                    (sigma_t ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 314, "column": 4 },
          "end": { "row": 314, "column": 4 }
        }
      }
    }
  ],
  [
    "1293",
    {
      "pageContent": "def multistep_dpm_solver_third_order_update(\n        self,\n        model_output_list: List[torch.FloatTensor],\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the third-order multistep DPM-Solver.\n\n        Args:\n            model_output_list (`List[torch.FloatTensor]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1, s2 = prev_timestep, timestep_list[-1], timestep_list[-2], timestep_list[-3]\n        m0, m1, m2 = model_output_list[-1], model_output_list[-2], model_output_list[-3]\n        lambda_t, lambda_s0, lambda_s1, lambda_s2 = (\n            self.lambda_t[t],\n            self.lambda_t[s0],\n            self.lambda_t[s1],\n            self.lambda_t[s2],\n        )\n        alpha_t, alpha_s0 = self.alpha_t[t], self.alpha_t[s0]\n        sigma_t, sigma_s0 = self.sigma_t[t], self.sigma_t[s0]\n        h, h_0, h_1 = lambda_t - lambda_s0, lambda_s0 - lambda_s1, lambda_s1 - lambda_s2\n        r0, r1 = h_0 / h, h_1 / h\n        D0 = m0\n        D1_0, D1_1 = (1.0 / r0) * (m0 - m1), (1.0 / r1) * (m1 - m2)\n        D1 = D1_0 +",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 373, "column": 4 },
          "end": { "row": 373, "column": 4 }
        }
      }
    }
  ],
  [
    "1294",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the multistep DPM-Solver.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~scheduling_utils.SchedulerOutput`] or `tuple`: [`~scheduling_utils.SchedulerOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.timesteps.device)\n        step_index = (self.timesteps == timestep).nonzero()\n        if len(step_index) == 0:\n            step_index = len(self.timesteps) - 1\n        else:\n            step_index = step_index.item()\n        prev_timestep = 0 if step_index == len(self.timesteps) - 1 else self.timesteps[step_index + 1]\n        lower_order_final = (\n            (st",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 428, "column": 4 },
          "end": { "row": 428, "column": 4 }
        }
      }
    }
  ],
  [
    "1295",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, *args, **kwargs) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 496, "column": 4 },
          "end": { "row": 496, "column": 4 }
        }
      }
    }
  ],
  [
    "1296",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep.py",
        "range": {
          "start": { "row": 509, "column": 4 },
          "end": { "row": 509, "column": 4 }
        }
      }
    }
  ],
  [
    "1297",
    {
      "pageContent": "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    The variance preserving stochastic differential equation (SDE) scheduler.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more information, see the original paper: https://arxiv.org/abs/2011.13456\n\n    UNDER CONSTRUCTION\n\n    \"\"\"\n\n    order = 1\n\n    @register_to_config\n    def __init__(self, num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3):\n        self.sigmas = None\n        self.discrete_sigmas = None\n        self.timesteps = None\n\n    def set_timesteps(self, num_inference_steps, device: Union[str, torch.device] = None):\n        self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)\n\n    def step_pred(self, score, x, t, generator=None):\n        if self.timesteps is None:\n            raise ValueError(\n                \"`self.timesteps` is not set, you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        # TODO(Patrick) better comments + non-PyTorch\n        # postprocess model score\n        log_mean_coeff = (\n            -0.25 * t**2 * (self.config.beta_max - self.config.beta_min) - 0.5 * t * self.config.beta_min\n        )\n        std = torch.sqrt(1.0 - torch.exp(2.0 * l",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_vp.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "1298",
    {
      "pageContent": "def step_pred(self, score, x, t, generator=None):\n        if self.timesteps is None:\n            raise ValueError(\n                \"`self.timesteps` is not set, you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        # TODO(Patrick) better comments + non-PyTorch\n        # postprocess model score\n        log_mean_coeff = (\n            -0.25 * t**2 * (self.config.beta_max - self.config.beta_min) - 0.5 * t * self.config.beta_min\n        )\n        std = torch.sqrt(1.0 - torch.exp(2.0 * log_mean_coeff))\n        std = std.flatten()\n        while len(std.shape) < len(score.shape):\n            std = std.unsqueeze(-1)\n        score = -score / std\n\n        # compute\n        dt = -1.0 / len(self.timesteps)\n\n        beta_t = self.config.beta_min + t * (self.config.beta_max - self.config.beta_min)\n        beta_t = beta_t.flatten()\n        while len(beta_t.shape) < len(x.shape):\n            beta_t = beta_t.unsqueeze(-1)\n        drift = -0.5 * beta_t * x\n\n        diffusion = torch.sqrt(beta_t)\n        drift = drift - diffusion**2 * score\n        x_mean = x + drift * dt\n\n        # add noise\n        noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)\n        x = x_mean + diffusion * math.sqrt(-dt) * noise\n\n        return x, x_mean",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_vp.py",
        "range": {
          "start": { "row": 52, "column": 4 },
          "end": { "row": 52, "column": 4 }
        }
      }
    }
  ],
  [
    "1299",
    {
      "pageContent": "class UnCLIPSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unclip.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1300",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unclip.py",
        "range": {
          "start": { "row": 46, "column": 0 },
          "end": { "row": 46, "column": 0 }
        }
      }
    }
  ],
  [
    "1301",
    {
      "pageContent": "class UnCLIPScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    This is a modified DDPM Scheduler specifically for the karlo unCLIP model.\n\n    This scheduler has some minor variations in how it calculates the learned range variance and dynamically\n    re-calculates betas based off the timesteps it is skipping.\n\n    The scheduler also uses a slightly different step ratio when computing timesteps to use for inference.\n\n    See [`~DDPMScheduler`] for more information on DDPM scheduling\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        variance_type (`str`):\n            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small_log`\n            or `learned_range`.\n        clip_sample (`bool`, default `True`):\n            option to clip predicted sample between `-clip_sample_range` and `clip_sample_range` for numerical\n            stability.\n        clip_sample_range (`float`, default `1.0`):\n            The range to clip the sample between. See `clip_sample`.\n        prediction_type (`str`, default `epsilon`, optional):\n            prediction type of the scheduler function, one of `epsilon` (predicting the noise of the diffusion process)\n            or `sample` (directly predicting the noisy sample`)\n    \"\"\"\n\n    @register_to_config\n    def __init__(\n        self,\n        num_train_timesteps: int = 1000,\n        variance_type: str = \"fixed_small_log\",\n        clip_sample: bool = True,\n        clip_sample_range: Optional[float] = 1.0,\n        prediction_type: str = \"epsilon\",\n    ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unclip.py",
        "range": {
          "start": { "row": 75, "column": 0 },
          "end": { "row": 75, "column": 0 }
        }
      }
    }
  ],
  [
    "1302",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unclip.py",
        "range": {
          "start": { "row": 129, "column": 4 },
          "end": { "row": 129, "column": 4 }
        }
      }
    }
  ],
  [
    "1303",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Note that this scheduler uses a slightly different step ratio than the other diffusers schedulers. The\n        different step ratio is to mimic the original karlo implementation and does not affect the quality or accuracy\n        of the results.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        step_ratio = (self.config.num_train_timesteps - 1) / (self.num_inference_steps - 1)\n        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)\n        self.timesteps = torch.from_numpy(timesteps).to(device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unclip.py",
        "range": {
          "start": { "row": 143, "column": 4 },
          "end": { "row": 143, "column": 4 }
        }
      }
    }
  ],
  [
    "1304",
    {
      "pageContent": "def _get_variance(self, t, prev_timestep=None, predicted_variance=None, variance_type=None):\n        if prev_timestep is None:\n            prev_timestep = t - 1\n\n        alpha_prod_t = self.alphas_cumprod[t]\n        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.one\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n        if prev_timestep == t - 1:\n            beta = self.betas[t]\n        else:\n            beta = 1 - alpha_prod_t / alpha_prod_t_prev\n\n        # For t > 0, compute predicted variance t (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n        # and sample from it to get previous sample\n        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n        variance = beta_prod_t_prev / beta_prod_t * beta\n\n        if variance_type is None:\n            variance_type = self.config.variance_type\n\n        # hacks - were probably added for training stability\n        if variance_type == \"fixed_small_log\":\n            variance = torch.log(torch.clamp(variance, min=1e-20))\n            variance = torch.exp(0.5 * variance)\n        elif variance_type == \"learned_range\":\n            # NOTE difference with DDPM scheduler\n            min_log = variance.log()\n            max_log = beta.log()\n\n            frac = (predicted_variance + 1) / 2\n            variance = frac * max_log + (1 - frac) * min_log\n\n        return variance",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unclip.py",
        "range": {
          "start": { "row": 160, "column": 4 },
          "end": { "row": 160, "column": 4 }
        }
      }
    }
  ],
  [
    "1305",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        prev_timestep: Optional[int] = None,\n        generator=None,\n        return_dict: bool = True,\n    ) -> Union[UnCLIPSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            prev_timestep (`int`, *optional*): The previous timestep to predict the previous sample at.\n                Used to dynamically compute beta. If not given, `t-1` is used and the pre-computed beta is used.\n            generator: random number generator.\n            return_dict (`bool`): option for returning tuple rather than UnCLIPSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.UnCLIPSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.UnCLIPSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        t = timestep\n\n        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type == \"learned_range\":\n            model_output,",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unclip.py",
        "range": {
          "start": { "row": 196, "column": 4 },
          "end": { "row": 196, "column": 4 }
        }
      }
    }
  ],
  [
    "1306",
    {
      "pageContent": "class DDIMSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1307",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999) -> torch.Tensor:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "1308",
    {
      "pageContent": "class DDIMScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising\n    diffusion probabilistic models (DDPMs) with non-Markovian guidance.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2010.02502\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        clip_sample (`bool`, default `True`):\n            option to clip predicted sample for numerical stability.\n        clip_sample_range (`float`, default `1.0`):\n            the maximum magnitude for sample clipping. Valid only when `clip_sample=True`.\n        set_alpha_to_one (`bo",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 78, "column": 0 },
          "end": { "row": 78, "column": 0 }
        }
      }
    }
  ],
  [
    "1309",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 177, "column": 4 },
          "end": { "row": 177, "column": 4 }
        }
      }
    }
  ],
  [
    "1310",
    {
      "pageContent": "def _get_variance(self, timestep, prev_timestep):\n        alpha_prod_t = self.alphas_cumprod[timestep]\n        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n        variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n\n        return variance",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 191, "column": 4 },
          "end": { "row": 191, "column": 4 }
        }
      }
    }
  ],
  [
    "1311",
    {
      "pageContent": "def _threshold_sample(self, sample: torch.FloatTensor) -> torch.FloatTensor:\n        # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n        dynamic_max_val = (\n            sample.flatten(1)\n            .abs()\n            .quantile(self.config.dynamic_thresholding_ratio, dim=1)\n            .clamp_min(self.config.sample_max_value)\n            .view(-1, *([1] * (sample.ndim - 1)))\n        )\n        return sample.clamp(-dynamic_max_val, dynamic_max_val) / dynamic_max_val",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 202, "column": 4 },
          "end": { "row": 202, "column": 4 }
        }
      }
    }
  ],
  [
    "1312",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n\n        if num_inference_steps > self.config.num_train_timesteps:\n            raise ValueError(\n                f\"`num_inference_steps`: {num_inference_steps} cannot be larger than `self.config.train_timesteps`:\"\n                f\" {self.config.num_train_timesteps} as the unet model trained with this scheduler can only handle\"\n                f\" maximal {self.config.num_train_timesteps} timesteps.\"\n            )\n\n        self.num_inference_steps = num_inference_steps\n        step_ratio = self.config.num_train_timesteps // self.num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # casting to int to avoid issues when num_inference_step is power of 3\n        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n        self.timesteps += self.config.steps_offset",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 213, "column": 4 },
          "end": { "row": 213, "column": 4 }
        }
      }
    }
  ],
  [
    "1313",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        eta: float = 0.0,\n        use_clipped_model_output: bool = False,\n        generator=None,\n        variance_noise: Optional[torch.FloatTensor] = None,\n        return_dict: bool = True,\n    ) -> Union[DDIMSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            eta (`float`): weight of noise for added noise in diffusion step.\n            use_clipped_model_output (`bool`): if `True`, compute \"corrected\" `model_output` from the clipped\n                predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when\n                `self.config.clip_sample` is `True`. If no clipping has happened, \"corrected\" `model_output` would\n                coincide with the one provided as input and `use_clipped_model_output` will have not effect.\n            generator: random number generator.\n            variance_noise (`torch.FloatTensor`): instead of generating noise for the variance using `generator`, we\n                can directly provi",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 237, "column": 4 },
          "end": { "row": 237, "column": 4 }
        }
      }
    }
  ],
  [
    "1314",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 360, "column": 4 },
          "end": { "row": 360, "column": 4 }
        }
      }
    }
  ],
  [
    "1315",
    {
      "pageContent": "def get_velocity(\n        self, sample: torch.FloatTensor, noise: torch.FloatTensor, timesteps: torch.IntTensor\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as sample\n        self.alphas_cumprod = self.alphas_cumprod.to(device=sample.device, dtype=sample.dtype)\n        timesteps = timesteps.to(sample.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(sample.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(sample.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        velocity = sqrt_alpha_prod * noise - sqrt_one_minus_alpha_prod * sample\n        return velocity",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim.py",
        "range": {
          "start": { "row": 383, "column": 4 },
          "end": { "row": 383, "column": 4 }
        }
      }
    }
  ],
  [
    "1316",
    {
      "pageContent": "class EulerAncestralDiscreteSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1317",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999) -> torch.Tensor:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "1318",
    {
      "pageContent": "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:\n    https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear` or `scaled_linear`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        prediction_type (`str`, default `epsilon`, optional):\n            prediction type of the scheduler function, one of `epsilon` (predicting the noise of the diffusion\n            process), `sample` (directly predicting the noisy sample`) or `v_prediction` (see section 2.4\n            https://imagen.research.google",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py",
        "range": {
          "start": { "row": 78, "column": 0 },
          "end": { "row": 78, "column": 0 }
        }
      }
    }
  ],
  [
    "1319",
    {
      "pageContent": "def scale_model_input(\n        self, sample: torch.FloatTensor, timestep: Union[float, torch.FloatTensor]\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Scales the denoising model input by `(sigma**2 + 1) ** 0.5` to match the Euler algorithm.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`float` or `torch.FloatTensor`): the current timestep in the diffusion chain\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.timesteps.device)\n        step_index = (self.timesteps == timestep).nonzero().item()\n        sigma = self.sigmas[step_index]\n        sample = sample / ((sigma**2 + 1) ** 0.5)\n        self.is_scale_input_called = True\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py",
        "range": {
          "start": { "row": 148, "column": 4 },
          "end": { "row": 148, "column": 4 }
        }
      }
    }
  ],
  [
    "1320",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n\n        timesteps = np.linspace(0, self.config.num_train_timesteps - 1, num_inference_steps, dtype=float)[::-1].copy()\n        sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)\n        sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)\n        sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)\n        self.sigmas = torch.from_numpy(sigmas).to(device=device)\n        if str(device).startswith(\"mps\"):\n            # mps does not support float64\n            self.timesteps = torch.from_numpy(timesteps).to(device, dtype=torch.float32)\n        else:\n            self.timesteps = torch.from_numpy(timesteps).to(device=device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py",
        "range": {
          "start": { "row": 169, "column": 4 },
          "end": { "row": 169, "column": 4 }
        }
      }
    }
  ],
  [
    "1321",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: Union[float, torch.FloatTensor],\n        sample: torch.FloatTensor,\n        generator: Optional[torch.Generator] = None,\n        return_dict: bool = True,\n    ) -> Union[EulerAncestralDiscreteSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`float`): current timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            generator (`torch.Generator`, optional): Random number generator.\n            return_dict (`bool`): option for returning tuple rather than EulerAncestralDiscreteSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput`] if `return_dict` is True, otherwise\n            a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n\n        if (\n            isinstance(timestep, int)\n            or isinstance(timestep, torch.IntTensor)\n            or isinstance(timestep, torch.LongTensor)\n        ):\n            raise ValueError(\n                (\n                    \"Passing integer indices (",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py",
        "range": {
          "start": { "row": 192, "column": 4 },
          "end": { "row": 192, "column": 4 }
        }
      }
    }
  ],
  [
    "1322",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\n        self.sigmas = self.sigmas.to(device=original_samples.device, dtype=original_samples.dtype)\n        if original_samples.device.type == \"mps\" and torch.is_floating_point(timesteps):\n            # mps does not support float64\n            self.timesteps = self.timesteps.to(original_samples.device, dtype=torch.float32)\n            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)\n        else:\n            self.timesteps = self.timesteps.to(original_samples.device)\n            timesteps = timesteps.to(original_samples.device)\n\n        schedule_timesteps = self.timesteps\n        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n\n        sigma = self.sigmas[step_indices].flatten()\n        while len(sigma.shape) < len(original_samples.shape):\n            sigma = sigma.unsqueeze(-1)\n\n        noisy_samples = original_samples + noise * sigma\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py",
        "range": {
          "start": { "row": 281, "column": 4 },
          "end": { "row": 281, "column": 4 }
        }
      }
    }
  ],
  [
    "1323",
    {
      "pageContent": "class KarrasVeSchedulerState:\n    # setable values\n    num_inference_steps: Optional[int] = None\n    timesteps: Optional[jnp.ndarray] = None\n    schedule: Optional[jnp.ndarray] = None  # sigma(t_i)\n\n    @classmethod\n    def create(cls):\n        return cls()",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve_flax.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1324",
    {
      "pageContent": "class FlaxKarrasVeOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        derivative (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)` for images):\n            Derivative of predicted original image sample (x_0).\n        state (`KarrasVeSchedulerState`): the `FlaxKarrasVeScheduler` state data class.\n    \"\"\"\n\n    prev_sample: jnp.ndarray\n    derivative: jnp.ndarray\n    state: KarrasVeSchedulerState",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve_flax.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "1325",
    {
      "pageContent": "class FlaxKarrasVeScheduler(FlaxSchedulerMixin, ConfigMixin):\n    \"\"\"\n    Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and\n    the VE column of Table 1 from [1] for reference.\n\n    [1] Karras, Tero, et al. \"Elucidating the Design Space of Diffusion-Based Generative Models.\"\n    https://arxiv.org/abs/2206.00364 [2] Song, Yang, et al. \"Score-based generative modeling through stochastic\n    differential equations.\" https://arxiv.org/abs/2011.13456\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details on the parameters, see the original paper's Appendix E.: \"Elucidating the Design Space of\n    Diffusion-Based Generative Models.\" https://arxiv.org/abs/2206.00364. The grid search values used to find the\n    optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.\n\n    Args:\n        sigma_min (`float`): minimum noise magnitude\n        sigma_max (`float`): maximum noise magnitude\n        s_noise (`float`): the amount of additional noise to counteract loss of detail during sampling.\n            A reasonable range is [1.000, 1.011].\n        s_churn (`float`): the parameter controlling the overall amount of stochasticity.\n",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve_flax.py",
        "range": {
          "start": { "row": 58, "column": 0 },
          "end": { "row": 58, "column": 0 }
        }
      }
    }
  ],
  [
    "1326",
    {
      "pageContent": "def set_timesteps(\n        self, state: KarrasVeSchedulerState, num_inference_steps: int, shape: Tuple = ()\n    ) -> KarrasVeSchedulerState:\n        \"\"\"\n        Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`KarrasVeSchedulerState`):\n                the `FlaxKarrasVeScheduler` state data class.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n\n        \"\"\"\n        timesteps = jnp.arange(0, num_inference_steps)[::-1].copy()\n        schedule = [\n            (\n                self.config.sigma_max**2\n                * (self.config.sigma_min**2 / self.config.sigma_max**2) ** (i / (num_inference_steps - 1))\n            )\n            for i in timesteps\n        ]\n\n        return state.replace(\n            num_inference_steps=num_inference_steps,\n            schedule=jnp.array(schedule, dtype=jnp.float32),\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve_flax.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "1327",
    {
      "pageContent": "def add_noise_to_input(\n        self,\n        state: KarrasVeSchedulerState,\n        sample: jnp.ndarray,\n        sigma: float,\n        key: random.KeyArray,\n    ) -> Tuple[jnp.ndarray, float]:\n        \"\"\"\n        Explicit Langevin-like \"churn\" step of adding noise to the sample according to a factor gamma_i  0 to reach a\n        higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.\n\n        TODO Args:\n        \"\"\"\n        if self.config.s_min <= sigma <= self.config.s_max:\n            gamma = min(self.config.s_churn / state.num_inference_steps, 2**0.5 - 1)\n        else:\n            gamma = 0\n\n        # sample eps ~ N(0, S_noise^2 * I)\n        key = random.split(key, num=1)\n        eps = self.config.s_noise * random.normal(key=key, shape=sample.shape)\n        sigma_hat = sigma + gamma * sigma\n        sample_hat = sample + ((sigma_hat**2 - sigma**2) ** 0.5 * eps)\n\n        return sample_hat, sigma_hat",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve_flax.py",
        "range": {
          "start": { "row": 136, "column": 4 },
          "end": { "row": 136, "column": 4 }
        }
      }
    }
  ],
  [
    "1328",
    {
      "pageContent": "def step(\n        self,\n        state: KarrasVeSchedulerState,\n        model_output: jnp.ndarray,\n        sigma_hat: float,\n        sigma_prev: float,\n        sample_hat: jnp.ndarray,\n        return_dict: bool = True,\n    ) -> Union[FlaxKarrasVeOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            state (`KarrasVeSchedulerState`): the `FlaxKarrasVeScheduler` state data class.\n            model_output (`torch.FloatTensor` or `np.ndarray`): direct output from learned diffusion model.\n            sigma_hat (`float`): TODO\n            sigma_prev (`float`): TODO\n            sample_hat (`torch.FloatTensor` or `np.ndarray`): TODO\n            return_dict (`bool`): option for returning tuple rather than FlaxKarrasVeOutput class\n\n        Returns:\n            [`~schedulers.scheduling_karras_ve_flax.FlaxKarrasVeOutput`] or `tuple`: Updated sample in the diffusion\n            chain and derivative. [`~schedulers.scheduling_karras_ve_flax.FlaxKarrasVeOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n        \"\"\"\n\n        pred_original_sample = sample_hat + sigma_hat * model_output\n        derivative = (sample_hat - pred_original_sample) / sigma_hat\n        sample_prev = sample_hat + (sigma_prev - sigma_hat) * derivative\n\n        if not return_dict:\n            return (sample_prev, derivative, state)\n\n        return ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve_flax.py",
        "range": {
          "start": { "row": 162, "column": 4 },
          "end": { "row": 162, "column": 4 }
        }
      }
    }
  ],
  [
    "1329",
    {
      "pageContent": "def step_correct(\n        self,\n        state: KarrasVeSchedulerState,\n        model_output: jnp.ndarray,\n        sigma_hat: float,\n        sigma_prev: float,\n        sample_hat: jnp.ndarray,\n        sample_prev: jnp.ndarray,\n        derivative: jnp.ndarray,\n        return_dict: bool = True,\n    ) -> Union[FlaxKarrasVeOutput, Tuple]:\n        \"\"\"\n        Correct the predicted sample based on the output model_output of the network. TODO complete description\n\n        Args:\n            state (`KarrasVeSchedulerState`): the `FlaxKarrasVeScheduler` state data class.\n            model_output (`torch.FloatTensor` or `np.ndarray`): direct output from learned diffusion model.\n            sigma_hat (`float`): TODO\n            sigma_prev (`float`): TODO\n            sample_hat (`torch.FloatTensor` or `np.ndarray`): TODO\n            sample_prev (`torch.FloatTensor` or `np.ndarray`): TODO\n            derivative (`torch.FloatTensor` or `np.ndarray`): TODO\n            return_dict (`bool`): option for returning tuple rather than FlaxKarrasVeOutput class\n\n        Returns:\n            prev_sample (TODO): updated sample in the diffusion chain. derivative (TODO): TODO\n\n        \"\"\"\n        pred_original_sample = sample_prev + sigma_prev * model_output\n        derivative_corr = (sample_prev - pred_original_sample) / sigma_prev\n        sample_prev = sample_hat + (sigma_prev - sigma_hat) * (0.5 * derivative + 0.5 * derivative_corr)\n\n        if not return_dict:\n            return (sample_prev, derivative, state)\n\n        return FlaxKarrasVeOutput(prev_sample=sample_prev, derivative=derivative, state=",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve_flax.py",
        "range": {
          "start": { "row": 198, "column": 4 },
          "end": { "row": 198, "column": 4 }
        }
      }
    }
  ],
  [
    "1330",
    {
      "pageContent": "class PNDMSchedulerState:\n    common: CommonSchedulerState\n    final_alpha_cumprod: jnp.ndarray\n\n    # setable values\n    init_noise_sigma: jnp.ndarray\n    timesteps: jnp.ndarray\n    num_inference_steps: Optional[int] = None\n    prk_timesteps: Optional[jnp.ndarray] = None\n    plms_timesteps: Optional[jnp.ndarray] = None\n\n    # running values\n    cur_model_output: Optional[jnp.ndarray] = None\n    counter: Optional[jnp.int32] = None\n    cur_sample: Optional[jnp.ndarray] = None\n    ets: Optional[jnp.ndarray] = None\n\n    @classmethod\n    def create(\n        cls,\n        common: CommonSchedulerState,\n        final_alpha_cumprod: jnp.ndarray,\n        init_noise_sigma: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ):\n        return cls(\n            common=common,\n            final_alpha_cumprod=final_alpha_cumprod,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1331",
    {
      "pageContent": "class FlaxPNDMScheduler(FlaxSchedulerMixin, ConfigMixin):\n    \"\"\"\n    Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,\n    namely Runge-Kutta method and a linear multi-step method.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2202.09778\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        trained_betas (`jnp.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        skip_prk_steps (`bool`):\n            allows the scheduler to skip the Runge-Kutta steps that are defined in the original paper as being required\n            before plms steps; defaults to `False`.\n        set_alpha_to_one (`bool`, default `False`):\n            each diffusion step u",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 72, "column": 0 },
          "end": { "row": 72, "column": 0 }
        }
      }
    }
  ],
  [
    "1332",
    {
      "pageContent": "def create_state(self, common: Optional[CommonSchedulerState] = None) -> PNDMSchedulerState:\n        if common is None:\n            common = CommonSchedulerState.create(self)\n\n        # At every step in ddim, we are looking into the previous alphas_cumprod\n        # For the final step, there is no previous alphas_cumprod because we are already at 0\n        # `set_alpha_to_one` decides whether we set this parameter simply to one or\n        # whether we use the final alpha of the \"non-previous\" one.\n        final_alpha_cumprod = (\n            jnp.array(1.0, dtype=self.dtype) if self.config.set_alpha_to_one else common.alphas_cumprod[0]\n        )\n\n        # standard deviation of the initial noise distribution\n        init_noise_sigma = jnp.array(1.0, dtype=self.dtype)\n\n        timesteps = jnp.arange(0, self.config.num_train_timesteps).round()[::-1]\n\n        return PNDMSchedulerState.create(\n            common=common,\n            final_alpha_cumprod=final_alpha_cumprod,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 142, "column": 4 },
          "end": { "row": 142, "column": 4 }
        }
      }
    }
  ],
  [
    "1333",
    {
      "pageContent": "def set_timesteps(self, state: PNDMSchedulerState, num_inference_steps: int, shape: Tuple) -> PNDMSchedulerState:\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`PNDMSchedulerState`):\n                the `FlaxPNDMScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            shape (`Tuple`):\n                the shape of the samples to be generated.\n        \"\"\"\n\n        step_ratio = self.config.num_train_timesteps // num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # rounding to avoid issues when num_inference_step is power of 3\n        _timesteps = (jnp.arange(0, num_inference_steps) * step_ratio).round() + self.config.steps_offset\n\n        if self.config.skip_prk_steps:\n            # for some models like stable diffusion the prk steps can/should be skipped to\n            # produce better results. When using PNDM with `self.config.skip_prk_steps` the implementation\n            # is based on crowsonkb's PLMS sampler implementation: https://github.com/CompVis/latent-diffusion/pull/51\n\n            prk_timesteps = jnp.array([], dtype=jnp.int32)\n            plms_timesteps = jnp.concatenate([_timesteps[:-1], _timesteps[-2:-1], _timesteps[-1:]])[::-1]\n\n        else:\n            prk_timesteps = _timesteps[-self.pndm_order :].repeat(2) + jnp.tile(\n                jnp.array([0, self.config.num_train_timestep",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 166, "column": 4 },
          "end": { "row": 166, "column": 4 }
        }
      }
    }
  ],
  [
    "1334",
    {
      "pageContent": "def scale_model_input(\n        self, state: PNDMSchedulerState, sample: jnp.ndarray, timestep: Optional[int] = None\n    ) -> jnp.ndarray:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            state (`PNDMSchedulerState`): the `FlaxPNDMScheduler` state data class instance.\n            sample (`jnp.ndarray`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `jnp.ndarray`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 221, "column": 4 },
          "end": { "row": 221, "column": 4 }
        }
      }
    }
  ],
  [
    "1335",
    {
      "pageContent": "def step(\n        self,\n        state: PNDMSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n        return_dict: bool = True,\n    ) -> Union[FlaxPNDMSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        This function calls `step_prk()` or `step_plms()` depending on the internal variable `counter`.\n\n        Args:\n            state (`PNDMSchedulerState`): the `FlaxPNDMScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than FlaxPNDMSchedulerOutput class\n\n        Returns:\n            [`FlaxPNDMSchedulerOutput`] or `tuple`: [`FlaxPNDMSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n\n        if state.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        if self.config.skip_prk_steps:\n            prev_sample, state = self.step_plms(state, model_output, timestep, sample)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 238, "column": 4 },
          "end": { "row": 238, "column": 4 }
        }
      }
    }
  ],
  [
    "1336",
    {
      "pageContent": "def step_prk(\n        self,\n        state: PNDMSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n    ) -> Union[FlaxPNDMSchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the\n        solution to the differential equation.\n\n        Args:\n            state (`PNDMSchedulerState`): the `FlaxPNDMScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than FlaxPNDMSchedulerOutput class\n\n        Returns:\n            [`FlaxPNDMSchedulerOutput`] or `tuple`: [`FlaxPNDMSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n\n        if state.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        diff_to_prev = jnp.where(\n            state.counter % 2, 0, self.config.num_train_timesteps // state.num_inference_steps // 2\n        )\n        prev_timestep = timestep - diff_to_prev\n        timestep = state.prk_timesteps[state.counter // 4 * 4]\n\n        model_output = jax.lax.sel",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 293, "column": 4 },
          "end": { "row": 293, "column": 4 }
        }
      }
    }
  ],
  [
    "1337",
    {
      "pageContent": "def step_plms(\n        self,\n        state: PNDMSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n    ) -> Union[FlaxPNDMSchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple\n        times to approximate the solution.\n\n        Args:\n            state (`PNDMSchedulerState`): the `FlaxPNDMScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than FlaxPNDMSchedulerOutput class\n\n        Returns:\n            [`FlaxPNDMSchedulerOutput`] or `tuple`: [`FlaxPNDMSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n\n        if state.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        # NOTE: There is no way to check in the jitted runtime if the prk mode was ran before\n\n        prev_timestep = timestep - self.config.num_train_timesteps // state.num_inference_steps\n        prev_timestep = jnp.where(prev_timestep > 0, prev_timestep, 0)\n\n        # Reference:\n        ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 361, "column": 4 },
          "end": { "row": 361, "column": 4 }
        }
      }
    }
  ],
  [
    "1338",
    {
      "pageContent": "def _get_prev_sample(self, state: PNDMSchedulerState, sample, timestep, prev_timestep, model_output):\n        # See formula (9) of PNDM paper https://arxiv.org/pdf/2202.09778.pdf\n        # this function computes x_(t) using the formula of (9)\n        # Note that x_t needs to be added to both sides of the equation\n\n        # Notation (<variable name> -> <name in paper>\n        # alpha_prod_t -> _t\n        # alpha_prod_t_prev -> _(t)\n        # beta_prod_t -> (1 - _t)\n        # beta_prod_t_prev -> (1 - _(t))\n        # sample -> x_t\n        # model_output -> e_(x_t, t)\n        # prev_sample -> x_(t)\n        alpha_prod_t = state.common.alphas_cumprod[timestep]\n        alpha_prod_t_prev = jnp.where(\n            prev_timestep >= 0, state.common.alphas_cumprod[prev_timestep], state.final_alpha_cumprod\n        )\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n        if self.config.prediction_type == \"v_prediction\":\n            model_output = (alpha_prod_t**0.5) * model_output + (beta_prod_t**0.5) * sample\n        elif self.config.prediction_type != \"epsilon\":\n            raise ValueError(\n                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon` or `v_prediction`\"\n            )\n\n        # corresponds to (_(t) - _t) divided by\n        # denominator of x_t in formula (9) and plus 1\n        # Note: (_(t) - _t) / (sqrt(_t) * (sqrt(_(t)) + sqr(_t))) =\n        # sqrt(_(t)) / sqrt(_t))\n        sample_coeff = (alpha_prod_t_prev / alpha_prod_t) ** (0.5)\n\n        # corresponds to de",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 455, "column": 4 },
          "end": { "row": 455, "column": 4 }
        }
      }
    }
  ],
  [
    "1339",
    {
      "pageContent": "def add_noise(\n        self,\n        state: PNDMSchedulerState,\n        original_samples: jnp.ndarray,\n        noise: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ) -> jnp.ndarray:\n        return add_noise_common(state.common, original_samples, noise, timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_pndm_flax.py",
        "range": {
          "start": { "row": 500, "column": 4 },
          "end": { "row": 500, "column": 4 }
        }
      }
    }
  ],
  [
    "1340",
    {
      "pageContent": "class EulerDiscreteSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_discrete.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1341",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_discrete.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "1342",
    {
      "pageContent": "class EulerDiscreteScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Euler scheduler (Algorithm 2) from Karras et al. (2022) https://arxiv.org/abs/2206.00364. . Based on the original\n    k-diffusion implementation by Katherine Crowson:\n    https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear` or `scaled_linear`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        prediction_type (`str`, default `\"epsilon\"`, optional):\n            prediction type of the scheduler function, one of `epsilon` (predicting the noise of the diffusion\n            process), `sample` (directly predicting the noisy sample`) or `v_prediction` (see section 2",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_discrete.py",
        "range": {
          "start": { "row": 78, "column": 0 },
          "end": { "row": 78, "column": 0 }
        }
      }
    }
  ],
  [
    "1343",
    {
      "pageContent": "def scale_model_input(\n        self, sample: torch.FloatTensor, timestep: Union[float, torch.FloatTensor]\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Scales the denoising model input by `(sigma**2 + 1) ** 0.5` to match the Euler algorithm.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`float` or `torch.FloatTensor`): the current timestep in the diffusion chain\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.timesteps.device)\n        step_index = (self.timesteps == timestep).nonzero().item()\n        sigma = self.sigmas[step_index]\n\n        sample = sample / ((sigma**2 + 1) ** 0.5)\n\n        self.is_scale_input_called = True\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_discrete.py",
        "range": {
          "start": { "row": 152, "column": 4 },
          "end": { "row": 152, "column": 4 }
        }
      }
    }
  ],
  [
    "1344",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n\n        timesteps = np.linspace(0, self.config.num_train_timesteps - 1, num_inference_steps, dtype=float)[::-1].copy()\n        sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)\n\n        if self.config.interpolation_type == \"linear\":\n            sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)\n        elif self.config.interpolation_type == \"log_linear\":\n            sigmas = torch.linspace(np.log(sigmas[-1]), np.log(sigmas[0]), num_inference_steps + 1).exp()\n        else:\n            raise ValueError(\n                f\"{self.config.interpolation_type} is not implemented. Please specify interpolation_type to either\"\n                \" 'linear' or 'log_linear'\"\n            )\n\n        sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)\n        self.sigmas = torch.from_numpy(sigmas).to(device=device)\n        if str(device).startswith(\"mps\"):\n            # mps does not support float64\n            self.timesteps = torch.from_numpy(timesteps).to(",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_discrete.py",
        "range": {
          "start": { "row": 175, "column": 4 },
          "end": { "row": 175, "column": 4 }
        }
      }
    }
  ],
  [
    "1345",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: Union[float, torch.FloatTensor],\n        sample: torch.FloatTensor,\n        s_churn: float = 0.0,\n        s_tmin: float = 0.0,\n        s_tmax: float = float(\"inf\"),\n        s_noise: float = 1.0,\n        generator: Optional[torch.Generator] = None,\n        return_dict: bool = True,\n    ) -> Union[EulerDiscreteSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`float`): current timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            s_churn (`float`)\n            s_tmin  (`float`)\n            s_tmax  (`float`)\n            s_noise (`float`)\n            generator (`torch.Generator`, optional): Random number generator.\n            return_dict (`bool`): option for returning tuple rather than EulerDiscreteSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n\n        if (\n            isinstance(timestep, int)\n ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_discrete.py",
        "range": {
          "start": { "row": 208, "column": 4 },
          "end": { "row": 208, "column": 4 }
        }
      }
    }
  ],
  [
    "1346",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\n        self.sigmas = self.sigmas.to(device=original_samples.device, dtype=original_samples.dtype)\n        if original_samples.device.type == \"mps\" and torch.is_floating_point(timesteps):\n            # mps does not support float64\n            self.timesteps = self.timesteps.to(original_samples.device, dtype=torch.float32)\n            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)\n        else:\n            self.timesteps = self.timesteps.to(original_samples.device)\n            timesteps = timesteps.to(original_samples.device)\n\n        schedule_timesteps = self.timesteps\n        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n\n        sigma = self.sigmas[step_indices].flatten()\n        while len(sigma.shape) < len(original_samples.shape):\n            sigma = sigma.unsqueeze(-1)\n\n        noisy_samples = original_samples + noise * sigma\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_euler_discrete.py",
        "range": {
          "start": { "row": 307, "column": 4 },
          "end": { "row": 307, "column": 4 }
        }
      }
    }
  ],
  [
    "1347",
    {
      "pageContent": "class FlaxKarrasDiffusionSchedulers(Enum):\n    FlaxDDIMScheduler = 1\n    FlaxDDPMScheduler = 2\n    FlaxPNDMScheduler = 3\n    FlaxLMSDiscreteScheduler = 4\n    FlaxDPMSolverMultistepScheduler = 5",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "1348",
    {
      "pageContent": "class FlaxSchedulerOutput(BaseOutput):\n    \"\"\"\n    Base class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n    \"\"\"\n\n    prev_sample: jnp.ndarray",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "1349",
    {
      "pageContent": "class FlaxSchedulerMixin:\n    \"\"\"\n    Mixin containing common functions for the schedulers.\n\n    Class attributes:\n        - **_compatibles** (`List[str]`) -- A list of classes that are compatible with the parent class, so that\n          `from_config` can be used from a class different than the one used to save the config (should be overridden\n          by parent class).\n    \"\"\"\n\n    config_name = SCHEDULER_CONFIG_NAME\n    ignore_for_config = [\"dtype\"]\n    _compatibles = []\n    has_compatibles = True\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        pretrained_model_name_or_path: Dict[str, Any] = None,\n        subfolder: Optional[str] = None,\n        return_unused_kwargs=False,\n        **kwargs,\n    ):\n        r\"\"\"\n        Instantiate a Scheduler class from a pre-defined JSON-file.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *model id* of a model repo on huggingface.co. Valid model ids should have an\n                      organization name, like `google/ddpm-celebahq-256`.\n                    - A path to a *directory* containing model weights saved using [`~SchedulerMixin.save_pretrained`],\n                      e.g., `./my_model_directory/`.\n            subfolder (`str`, *optional*):\n                In case the relevant files are located inside a subfolder of the model repo (either remote in\n                huggingface.co or downloaded locally), you can specify the folder name here.\n            return_unused_kwargs (`bool`, *optional*, ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 55, "column": 0 },
          "end": { "row": 55, "column": 0 }
        }
      }
    }
  ],
  [
    "1350",
    {
      "pageContent": "def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n        \"\"\"\n        Save a scheduler configuration object to the directory `save_directory`, so that it can be re-loaded using the\n        [`~FlaxSchedulerMixin.from_pretrained`] class method.\n\n        Args:\n            save_directory (`str` or `os.PathLike`):\n                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n        \"\"\"\n        self.save_config(save_directory=save_directory, push_to_hub=push_to_hub, **kwargs)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 150, "column": 4 },
          "end": { "row": 150, "column": 4 }
        }
      }
    }
  ],
  [
    "1351",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps: int, max_beta=0.999, dtype=jnp.float32) -> jnp.ndarray:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`jnp.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return jnp.array(betas, dtype=dtype)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 186, "column": 0 },
          "end": { "row": 186, "column": 0 }
        }
      }
    }
  ],
  [
    "1352",
    {
      "pageContent": "class CommonSchedulerState:\n    alphas: jnp.ndarray\n    betas: jnp.ndarray\n    alphas_cumprod: jnp.ndarray\n\n    @classmethod\n    def create(cls, scheduler):\n        config = scheduler.config\n\n        if config.trained_betas is not None:\n            betas = jnp.asarray(config.trained_betas, dtype=scheduler.dtype)\n        elif config.beta_schedule == \"linear\":\n            betas = jnp.linspace(config.beta_start, config.beta_end, config.num_train_timesteps, dtype=scheduler.dtype)\n        elif config.beta_schedule == \"scaled_linear\":\n            # this schedule is very specific to the latent diffusion model.\n            betas = (\n                jnp.linspace(\n                    config.beta_start**0.5, config.beta_end**0.5, config.num_train_timesteps, dtype=scheduler.dtype\n                )\n                ** 2\n            )\n        elif config.beta_schedule == \"squaredcos_cap_v2\":\n            # Glide cosine schedule\n            betas = betas_for_alpha_bar(config.num_train_timesteps, dtype=scheduler.dtype)\n        else:\n            raise NotImplementedError(\n                f\"beta_schedule {config.beta_schedule} is not implemented for scheduler {scheduler.__class__.__name__}\"\n            )\n\n        alphas = 1.0 - betas\n\n        alphas_cumprod = jnp.cumprod(alphas, axis=0)\n\n        return cls(\n            alphas=alphas,\n            betas=betas,\n            alphas_cumprod=alphas_cumprod,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 216, "column": 0 },
          "end": { "row": 216, "column": 0 }
        }
      }
    }
  ],
  [
    "1353",
    {
      "pageContent": "def get_sqrt_alpha_prod(\n    state: CommonSchedulerState, original_samples: jnp.ndarray, noise: jnp.ndarray, timesteps: jnp.ndarray\n):\n    alphas_cumprod = state.alphas_cumprod\n\n    sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n    sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n    sqrt_alpha_prod = broadcast_to_shape_from_left(sqrt_alpha_prod, original_samples.shape)\n\n    sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n    sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n    sqrt_one_minus_alpha_prod = broadcast_to_shape_from_left(sqrt_one_minus_alpha_prod, original_samples.shape)\n\n    return sqrt_alpha_prod, sqrt_one_minus_alpha_prod",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 256, "column": 0 },
          "end": { "row": 256, "column": 0 }
        }
      }
    }
  ],
  [
    "1354",
    {
      "pageContent": "def add_noise_common(\n    state: CommonSchedulerState, original_samples: jnp.ndarray, noise: jnp.ndarray, timesteps: jnp.ndarray\n):\n    sqrt_alpha_prod, sqrt_one_minus_alpha_prod = get_sqrt_alpha_prod(state, original_samples, noise, timesteps)\n    noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n    return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 272, "column": 0 },
          "end": { "row": 272, "column": 0 }
        }
      }
    }
  ],
  [
    "1355",
    {
      "pageContent": "def get_velocity_common(state: CommonSchedulerState, sample: jnp.ndarray, noise: jnp.ndarray, timesteps: jnp.ndarray):\n    sqrt_alpha_prod, sqrt_one_minus_alpha_prod = get_sqrt_alpha_prod(state, sample, noise, timesteps)\n    velocity = sqrt_alpha_prod * noise - sqrt_one_minus_alpha_prod * sample\n    return velocity",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils_flax.py",
        "range": {
          "start": { "row": 280, "column": 0 },
          "end": { "row": 280, "column": 0 }
        }
      }
    }
  ],
  [
    "1356",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1357",
    {
      "pageContent": "class DEISMultistepScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    DEIS (https://arxiv.org/abs/2204.13902) is a fast high order solver for diffusion ODEs. We slightly modify the\n    polynomial fitting formula in log-rho space instead of the original linear t space in DEIS paper. The modification\n    enjoys closed-form coefficients for exponential multistep update instead of replying on the numerical solver. More\n    variants of DEIS can be found in https://github.com/qsh-zh/deis.\n\n    Currently, we support the log-rho multistep DEIS. We recommend to use `solver_order=2 / 3` while `solver_order=1`\n    reduces to DDIM.\n\n    We also support the \"dynamic thresholding\" method in Imagen (https://arxiv.org/abs/2205.11487). For pixel-space\n    diffusion models, you can set `thresholding=True` to use the dynamic thresholding.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n    ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "1358",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        timesteps = (\n            np.linspace(0, self.num_train_timesteps - 1, num_inference_steps + 1)\n            .round()[::-1][:-1]\n            .copy()\n            .astype(np.int64)\n        )\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n        self.model_outputs = [\n            None,\n        ] * self.config.solver_order\n        self.lower_order_nums = 0",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 173, "column": 4 },
          "end": { "row": 173, "column": 4 }
        }
      }
    }
  ],
  [
    "1359",
    {
      "pageContent": "def _threshold_sample(self, sample: torch.FloatTensor) -> torch.FloatTensor:\n        # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n        dynamic_max_val = (\n            sample.flatten(1)\n            .abs()\n            .quantile(self.config.dynamic_thresholding_ratio, dim=1)\n            .clamp_min(self.config.sample_max_value)\n            .view(-1, *([1] * (sample.ndim - 1)))\n        )\n        return sample.clamp(-dynamic_max_val, dynamic_max_val) / dynamic_max_val",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 197, "column": 4 },
          "end": { "row": 197, "column": 4 }
        }
      }
    }
  ],
  [
    "1360",
    {
      "pageContent": "def convert_model_output(\n        self, model_output: torch.FloatTensor, timestep: int, sample: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Convert the model output to the corresponding type that the algorithm DEIS needs.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the converted model output.\n        \"\"\"\n        if self.config.prediction_type == \"epsilon\":\n            alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n            x0_pred = (sample - sigma_t * model_output) / alpha_t\n        elif self.config.prediction_type == \"sample\":\n            x0_pred = model_output\n        elif self.config.prediction_type == \"v_prediction\":\n            alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n            x0_pred = alpha_t * sample - sigma_t * model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n                \" `v_prediction` for the DEISMultistepScheduler.\"\n            )\n\n        if self.config.thresholding:\n            # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n            orig_dtype = x0_pred.dtype\n            if orig_dtype not in [torch.float, torch.double]:\n                x0_pre",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 208, "column": 4 },
          "end": { "row": 208, "column": 4 }
        }
      }
    }
  ],
  [
    "1361",
    {
      "pageContent": "def deis_first_order_update(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the first-order DEIS (equivalent to DDIM).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        lambda_t, lambda_s = self.lambda_t[prev_timestep], self.lambda_t[timestep]\n        alpha_t, alpha_s = self.alpha_t[prev_timestep], self.alpha_t[timestep]\n        sigma_t, _ = self.sigma_t[prev_timestep], self.sigma_t[timestep]\n        h = lambda_t - lambda_s\n        if self.config.algorithm_type == \"deis\":\n            x_t = (alpha_t / alpha_s) * sample - (sigma_t * (torch.exp(h) - 1.0)) * model_output\n        else:\n            raise NotImplementedError(\"only support log-rho multistep deis now\")\n        return x_t",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 250, "column": 4 },
          "end": { "row": 250, "column": 4 }
        }
      }
    }
  ],
  [
    "1362",
    {
      "pageContent": "def multistep_deis_second_order_update(\n        self,\n        model_output_list: List[torch.FloatTensor],\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the second-order multistep DEIS.\n\n        Args:\n            model_output_list (`List[torch.FloatTensor]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1 = prev_timestep, timestep_list[-1], timestep_list[-2]\n        m0, m1 = model_output_list[-1], model_output_list[-2]\n        alpha_t, alpha_s0, alpha_s1 = self.alpha_t[t], self.alpha_t[s0], self.alpha_t[s1]\n        sigma_t, sigma_s0, sigma_s1 = self.sigma_t[t], self.sigma_t[s0], self.sigma_t[s1]\n\n        rho_t, rho_s0, rho_s1 = sigma_t / alpha_t, sigma_s0 / alpha_s0, sigma_s1 / alpha_s1\n\n        if self.config.algorithm_type == \"deis\":\n\n            def ind_fn(t, b, c):\n                # Integrate[(log(t) - log(c)) / (log(b) - log(c)), {t}]\n                return t * (-np.log(c) + np.log(t) - 1) / (np.log(b) - np.log(c))\n\n            coef1 = ind_fn(rho_t, rho_s0, rho_s1) - ind_fn(rho_s0, rho_s0, rho_s1)\n  ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 280, "column": 4 },
          "end": { "row": 280, "column": 4 }
        }
      }
    }
  ],
  [
    "1363",
    {
      "pageContent": "def multistep_deis_third_order_update(\n        self,\n        model_output_list: List[torch.FloatTensor],\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the third-order multistep DEIS.\n\n        Args:\n            model_output_list (`List[torch.FloatTensor]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1, s2 = prev_timestep, timestep_list[-1], timestep_list[-2], timestep_list[-3]\n        m0, m1, m2 = model_output_list[-1], model_output_list[-2], model_output_list[-3]\n        alpha_t, alpha_s0, alpha_s1, alpha_s2 = self.alpha_t[t], self.alpha_t[s0], self.alpha_t[s1], self.alpha_t[s2]\n        sigma_t, sigma_s0, sigma_s1, simga_s2 = self.sigma_t[t], self.sigma_t[s0], self.sigma_t[s1], self.sigma_t[s2]\n        rho_t, rho_s0, rho_s1, rho_s2 = (\n            sigma_t / alpha_t,\n            sigma_s0 / alpha_s0,\n            sigma_s1 / alpha_s1,\n            simga_s2 / alpha_s2,\n        )\n\n        if self.config.algorithm_type == \"deis\":\n\n            def ind_fn(t, b, c, d):\n                # Integrate[(log(t) - log(c)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 322, "column": 4 },
          "end": { "row": 322, "column": 4 }
        }
      }
    }
  ],
  [
    "1364",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the multistep DEIS.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~scheduling_utils.SchedulerOutput`] or `tuple`: [`~scheduling_utils.SchedulerOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.timesteps.device)\n        step_index = (self.timesteps == timestep).nonzero()\n        if len(step_index) == 0:\n            step_index = len(self.timesteps) - 1\n        else:\n            step_index = step_index.item()\n        prev_timestep = 0 if step_index == len(self.timesteps) - 1 else self.timesteps[step_index + 1]\n        lower_order_final = (\n            (step_ind",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 379, "column": 4 },
          "end": { "row": 379, "column": 4 }
        }
      }
    }
  ],
  [
    "1365",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, *args, **kwargs) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 447, "column": 4 },
          "end": { "row": 447, "column": 4 }
        }
      }
    }
  ],
  [
    "1366",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_deis_multistep.py",
        "range": {
          "start": { "row": 460, "column": 4 },
          "end": { "row": 460, "column": 4 }
        }
      }
    }
  ],
  [
    "1367",
    {
      "pageContent": "class LMSDiscreteSchedulerState:\n    common: CommonSchedulerState\n\n    # setable values\n    init_noise_sigma: jnp.ndarray\n    timesteps: jnp.ndarray\n    sigmas: jnp.ndarray\n    num_inference_steps: Optional[int] = None\n\n    # running values\n    derivatives: Optional[jnp.ndarray] = None\n\n    @classmethod\n    def create(\n        cls, common: CommonSchedulerState, init_noise_sigma: jnp.ndarray, timesteps: jnp.ndarray, sigmas: jnp.ndarray\n    ):\n        return cls(common=common, init_noise_sigma=init_noise_sigma, timesteps=timesteps, sigmas=sigmas)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "1368",
    {
      "pageContent": "class FlaxLMSDiscreteScheduler(FlaxSchedulerMixin, ConfigMixin):\n    \"\"\"\n    Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by\n    Katherine Crowson:\n    https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear` or `scaled_linear`.\n        trained_betas (`jnp.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        prediction_type (`str`, default `epsilon`, optional):\n            prediction type of the scheduler function, one of `epsilon` (predicting the noise of the diffusion\n            process), `sample` (directly predicting the noisy sample`) or `v_prediction` (see section 2.4\n            https://imagen.",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 56, "column": 0 },
          "end": { "row": 56, "column": 0 }
        }
      }
    }
  ],
  [
    "1369",
    {
      "pageContent": "def create_state(self, common: Optional[CommonSchedulerState] = None) -> LMSDiscreteSchedulerState:\n        if common is None:\n            common = CommonSchedulerState.create(self)\n\n        timesteps = jnp.arange(0, self.config.num_train_timesteps).round()[::-1]\n        sigmas = ((1 - common.alphas_cumprod) / common.alphas_cumprod) ** 0.5\n\n        # standard deviation of the initial noise distribution\n        init_noise_sigma = sigmas.max()\n\n        return LMSDiscreteSchedulerState.create(\n            common=common,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n            sigmas=sigmas,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 105, "column": 4 },
          "end": { "row": 105, "column": 4 }
        }
      }
    }
  ],
  [
    "1370",
    {
      "pageContent": "def scale_model_input(self, state: LMSDiscreteSchedulerState, sample: jnp.ndarray, timestep: int) -> jnp.ndarray:\n        \"\"\"\n        Scales the denoising model input by `(sigma**2 + 1) ** 0.5` to match the K-LMS algorithm.\n\n        Args:\n            state (`LMSDiscreteSchedulerState`):\n                the `FlaxLMSDiscreteScheduler` state data class instance.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            timestep (`int`):\n                current discrete timestep in the diffusion chain.\n\n        Returns:\n            `jnp.ndarray`: scaled input sample\n        \"\"\"\n        (step_index,) = jnp.where(state.timesteps == timestep, size=1)\n        step_index = step_index[0]\n\n        sigma = state.sigmas[step_index]\n        sample = sample / ((sigma**2 + 1) ** 0.5)\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "1371",
    {
      "pageContent": "def get_lms_coefficient(self, state: LMSDiscreteSchedulerState, order, t, current_order):\n        \"\"\"\n        Compute a linear multistep coefficient.\n\n        Args:\n            order (TODO):\n            t (TODO):\n            current_order (TODO):\n        \"\"\"\n\n        def lms_derivative(tau):\n            prod = 1.0\n            for k in range(order):\n                if current_order == k:\n                    continue\n                prod *= (tau - state.sigmas[t - k]) / (state.sigmas[t - current_order] - state.sigmas[t - k])\n            return prod\n\n        integrated_coeff = integrate.quad(lms_derivative, state.sigmas[t], state.sigmas[t + 1], epsrel=1e-4)[0]\n\n        return integrated_coeff",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 144, "column": 4 },
          "end": { "row": 144, "column": 4 }
        }
      }
    }
  ],
  [
    "1372",
    {
      "pageContent": "def set_timesteps(\n        self, state: LMSDiscreteSchedulerState, num_inference_steps: int, shape: Tuple = ()\n    ) -> LMSDiscreteSchedulerState:\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`LMSDiscreteSchedulerState`):\n                the `FlaxLMSDiscreteScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n\n        timesteps = jnp.linspace(self.config.num_train_timesteps - 1, 0, num_inference_steps, dtype=self.dtype)\n\n        low_idx = jnp.floor(timesteps).astype(jnp.int32)\n        high_idx = jnp.ceil(timesteps).astype(jnp.int32)\n\n        frac = jnp.mod(timesteps, 1.0)\n\n        sigmas = ((1 - state.common.alphas_cumprod) / state.common.alphas_cumprod) ** 0.5\n        sigmas = (1 - frac) * sigmas[low_idx] + frac * sigmas[high_idx]\n        sigmas = jnp.concatenate([sigmas, jnp.array([0.0], dtype=self.dtype)])\n\n        timesteps = timesteps.astype(jnp.int32)\n\n        # initial running values\n        derivatives = jnp.zeros((0,) + shape, dtype=self.dtype)\n\n        return state.replace(\n            timesteps=timesteps,\n            sigmas=sigmas,\n            num_inference_steps=num_inference_steps,\n            derivatives=derivatives,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 166, "column": 4 },
          "end": { "row": 166, "column": 4 }
        }
      }
    }
  ],
  [
    "1373",
    {
      "pageContent": "def step(\n        self,\n        state: LMSDiscreteSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n        order: int = 4,\n        return_dict: bool = True,\n    ) -> Union[FlaxLMSSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            state (`LMSDiscreteSchedulerState`): the `FlaxLMSDiscreteScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            order: coefficient for multi-step inference.\n            return_dict (`bool`): option for returning tuple rather than FlaxLMSSchedulerOutput class\n\n        Returns:\n            [`FlaxLMSSchedulerOutput`] or `tuple`: [`FlaxLMSSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if state.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        sigma = state.sigmas[timestep]\n\n        # 1. compute predicted original sample (x_0) from sigma-scaled predicted noise\n      ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 202, "column": 4 },
          "end": { "row": 202, "column": 4 }
        }
      }
    }
  ],
  [
    "1374",
    {
      "pageContent": "def add_noise(\n        self,\n        state: LMSDiscreteSchedulerState,\n        original_samples: jnp.ndarray,\n        noise: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ) -> jnp.ndarray:\n        sigma = state.sigmas[timesteps].flatten()\n        sigma = broadcast_to_shape_from_left(sigma, noise.shape)\n\n        noisy_samples = original_samples + noise * sigma\n\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete_flax.py",
        "range": {
          "start": { "row": 267, "column": 4 },
          "end": { "row": 267, "column": 4 }
        }
      }
    }
  ],
  [
    "1375",
    {
      "pageContent": "class KarrasDiffusionSchedulers(Enum):\n    DDIMScheduler = 1\n    DDPMScheduler = 2\n    PNDMScheduler = 3\n    LMSDiscreteScheduler = 4\n    EulerDiscreteScheduler = 5\n    HeunDiscreteScheduler = 6\n    EulerAncestralDiscreteScheduler = 7\n    DPMSolverMultistepScheduler = 8\n    DPMSolverSinglestepScheduler = 9\n    KDPM2DiscreteScheduler = 10\n    KDPM2AncestralDiscreteScheduler = 11\n    DEISMultistepScheduler = 12\n    UniPCMultistepScheduler = 13",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1376",
    {
      "pageContent": "class SchedulerOutput(BaseOutput):\n    \"\"\"\n    Base class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "1377",
    {
      "pageContent": "class SchedulerMixin:\n    \"\"\"\n    Mixin containing common functions for the schedulers.\n\n    Class attributes:\n        - **_compatibles** (`List[str]`) -- A list of classes that are compatible with the parent class, so that\n          `from_config` can be used from a class different than the one used to save the config (should be overridden\n          by parent class).\n    \"\"\"\n\n    config_name = SCHEDULER_CONFIG_NAME\n    _compatibles = []\n    has_compatibles = True\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        pretrained_model_name_or_path: Dict[str, Any] = None,\n        subfolder: Optional[str] = None,\n        return_unused_kwargs=False,\n        **kwargs,\n    ):\n        r\"\"\"\n        Instantiate a Scheduler class from a pre-defined JSON configuration file inside a directory or Hub repo.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *model id* of a model repo on huggingface.co. Valid model ids should have an\n                      organization name, like `google/ddpm-celebahq-256`.\n                    - A path to a *directory* containing the schedluer configurations saved using\n                      [`~SchedulerMixin.save_pretrained`], e.g., `./my_model_directory/`.\n            subfolder (`str`, *optional*):\n                In case the relevant files are located inside a subfolder of the model repo (either remote in\n                huggingface.co or downloaded locally), you can specify the folder name here.\n            return_unused_kwargs",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils.py",
        "range": {
          "start": { "row": 61, "column": 0 },
          "end": { "row": 61, "column": 0 }
        }
      }
    }
  ],
  [
    "1378",
    {
      "pageContent": "def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n        \"\"\"\n        Save a scheduler configuration object to the directory `save_directory`, so that it can be re-loaded using the\n        [`~SchedulerMixin.from_pretrained`] class method.\n\n        Args:\n            save_directory (`str` or `os.PathLike`):\n                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n        \"\"\"\n        self.save_config(save_directory=save_directory, push_to_hub=push_to_hub, **kwargs)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_utils.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "1379",
    {
      "pageContent": "class ScoreSdeVeSchedulerState:\n    # setable values\n    timesteps: Optional[jnp.ndarray] = None\n    discrete_sigmas: Optional[jnp.ndarray] = None\n    sigmas: Optional[jnp.ndarray] = None\n\n    @classmethod\n    def create(cls):\n        return cls()",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1380",
    {
      "pageContent": "class FlaxSdeVeOutput(FlaxSchedulerOutput):\n    \"\"\"\n    Output class for the ScoreSdeVeScheduler's step function output.\n\n    Args:\n        state (`ScoreSdeVeSchedulerState`):\n        prev_sample (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        prev_sample_mean (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)` for images):\n            Mean averaged `prev_sample`. Same as `prev_sample`, only mean-averaged over previous timesteps.\n    \"\"\"\n\n    state: ScoreSdeVeSchedulerState\n    prev_sample: jnp.ndarray\n    prev_sample_mean: Optional[jnp.ndarray] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "1381",
    {
      "pageContent": "class FlaxScoreSdeVeScheduler(FlaxSchedulerMixin, ConfigMixin):\n    \"\"\"\n    The variance exploding stochastic differential equation (SDE) scheduler.\n\n    For more information, see the original paper: https://arxiv.org/abs/2011.13456\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        snr (`float`):\n            coefficient weighting the step from the model_output sample (from the network) to the random noise.\n        sigma_min (`float`):\n                initial noise scale for sigma sequence in sampling procedure. The minimum sigma should mirror the\n                distribution of the data.\n        sigma_max (`float`): maximum value used for the range of continuous timesteps passed into the model.\n        sampling_eps (`float`): the end value of sampling, where timesteps decrease progressively from 1 to\n        epsilon.\n        correct_steps (`int`): number of correction steps performed on a produced sample.\n    \"\"\"\n\n    @property\n    def has_state(self):\n        return True\n\n    @register_to_config\n    def __init__(\n        self,\n        num_train_timesteps: int = 2000,\n        snr: float = 0.15,\n        sigma_min: float = 0.01,\n     ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 58, "column": 0 },
          "end": { "row": 58, "column": 0 }
        }
      }
    }
  ],
  [
    "1382",
    {
      "pageContent": "def create_state(self):\n        state = ScoreSdeVeSchedulerState.create()\n        return self.set_sigmas(\n            state,\n            self.config.num_train_timesteps,\n            self.config.sigma_min,\n            self.config.sigma_max,\n            self.config.sampling_eps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 98, "column": 4 },
          "end": { "row": 98, "column": 4 }
        }
      }
    }
  ],
  [
    "1383",
    {
      "pageContent": "def set_timesteps(\n        self, state: ScoreSdeVeSchedulerState, num_inference_steps: int, shape: Tuple = (), sampling_eps: float = None\n    ) -> ScoreSdeVeSchedulerState:\n        \"\"\"\n        Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`ScoreSdeVeSchedulerState`): the `FlaxScoreSdeVeScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            sampling_eps (`float`, optional):\n                final timestep value (overrides value given at Scheduler instantiation).\n\n        \"\"\"\n        sampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps\n\n        timesteps = jnp.linspace(1, sampling_eps, num_inference_steps)\n        return state.replace(timesteps=timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "1384",
    {
      "pageContent": "def set_sigmas(\n        self,\n        state: ScoreSdeVeSchedulerState,\n        num_inference_steps: int,\n        sigma_min: float = None,\n        sigma_max: float = None,\n        sampling_eps: float = None,\n    ) -> ScoreSdeVeSchedulerState:\n        \"\"\"\n        Sets the noise scales used for the diffusion chain. Supporting function to be run before inference.\n\n        The sigmas control the weight of the `drift` and `diffusion` components of sample update.\n\n        Args:\n            state (`ScoreSdeVeSchedulerState`): the `FlaxScoreSdeVeScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            sigma_min (`float`, optional):\n                initial noise scale value (overrides value given at Scheduler instantiation).\n            sigma_max (`float`, optional):\n                final noise scale value (overrides value given at Scheduler instantiation).\n            sampling_eps (`float`, optional):\n                final timestep value (overrides value given at Scheduler instantiation).\n        \"\"\"\n        sigma_min = sigma_min if sigma_min is not None else self.config.sigma_min\n        sigma_max = sigma_max if sigma_max is not None else self.config.sigma_max\n        sampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps\n        if state.timesteps is None:\n            state = self.set_timesteps(state, num_inference_steps, sampling_eps)\n\n        discrete_sigmas = jnp.exp(jnp.linspace(jnp.log(sigma_min), jnp.log(sigma_m",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 127, "column": 4 },
          "end": { "row": 127, "column": 4 }
        }
      }
    }
  ],
  [
    "1385",
    {
      "pageContent": "def step_pred(\n        self,\n        state: ScoreSdeVeSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n        key: random.KeyArray,\n        return_dict: bool = True,\n    ) -> Union[FlaxSdeVeOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            state (`ScoreSdeVeSchedulerState`): the `FlaxScoreSdeVeScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            generator: random number generator.\n            return_dict (`bool`): option for returning tuple rather than FlaxSdeVeOutput class\n\n        Returns:\n            [`FlaxSdeVeOutput`] or `tuple`: [`FlaxSdeVeOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if state.timesteps is None:\n            raise ValueError(\n                \"`state.timesteps` is not set, you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        timestep = timestep * jnp.ones(\n            sample.shape[0],\n        )\n        timesteps = (timestep * (len(state.timesteps) - 1)).long()\n\n        sigma = state.discrete_s",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 165, "column": 4 },
          "end": { "row": 165, "column": 4 }
        }
      }
    }
  ],
  [
    "1386",
    {
      "pageContent": "def step_correct(\n        self,\n        state: ScoreSdeVeSchedulerState,\n        model_output: jnp.ndarray,\n        sample: jnp.ndarray,\n        key: random.KeyArray,\n        return_dict: bool = True,\n    ) -> Union[FlaxSdeVeOutput, Tuple]:\n        \"\"\"\n        Correct the predicted sample based on the output model_output of the network. This is often run repeatedly\n        after making the prediction for the previous timestep.\n\n        Args:\n            state (`ScoreSdeVeSchedulerState`): the `FlaxScoreSdeVeScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            generator: random number generator.\n            return_dict (`bool`): option for returning tuple rather than FlaxSdeVeOutput class\n\n        Returns:\n            [`FlaxSdeVeOutput`] or `tuple`: [`FlaxSdeVeOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if state.timesteps is None:\n            raise ValueError(\n                \"`state.timesteps` is not set, you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        # For small batch sizes, the paper \"suggest replacing norm(z) with sqrt(d), where d is the dim. of z\"\n        # sample noise for correction\n        key = random.split(key, num=1)\n        noise = random.normal(key=key, shape=sample.shape)\n\n        # compute step size from the model_output,",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_sde_ve_flax.py",
        "range": {
          "start": { "row": 225, "column": 4 },
          "end": { "row": 225, "column": 4 }
        }
      }
    }
  ],
  [
    "1387",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999) -> torch.Tensor:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_heun_discrete.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1388",
    {
      "pageContent": "class HeunDiscreteScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Implements Algorithm 2 (Heun steps) from Karras et al. (2022). for discrete beta schedules. Based on the original\n    k-diffusion implementation by Katherine Crowson:\n    https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L90\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model. beta_start (`float`): the\n        starting `beta` value of inference. beta_end (`float`): the final `beta` value. beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear` or `scaled_linear`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small`,\n            `fixed_small_log`, `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\n        prediction_type (`str`, default `epsilon`, optional):\n            prediction ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_heun_discrete.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "1389",
    {
      "pageContent": "def index_for_timestep(self, timestep):\n        indices = (self.timesteps == timestep).nonzero()\n        if self.state_in_first_order:\n            pos = -1\n        else:\n            pos = 0\n        return indices[pos].item()",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_heun_discrete.py",
        "range": {
          "start": { "row": 114, "column": 4 },
          "end": { "row": 114, "column": 4 }
        }
      }
    }
  ],
  [
    "1390",
    {
      "pageContent": "def scale_model_input(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[float, torch.FloatTensor],\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Args:\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n            sample (`torch.FloatTensor`): input sample timestep (`int`, optional): current timestep\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        step_index = self.index_for_timestep(timestep)\n\n        sigma = self.sigmas[step_index]\n        sample = sample / ((sigma**2 + 1) ** 0.5)\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_heun_discrete.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "1391",
    {
      "pageContent": "def set_timesteps(\n        self,\n        num_inference_steps: int,\n        device: Union[str, torch.device] = None,\n        num_train_timesteps: Optional[int] = None,\n    ):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n\n        num_train_timesteps = num_train_timesteps or self.config.num_train_timesteps\n\n        timesteps = np.linspace(0, num_train_timesteps - 1, num_inference_steps, dtype=float)[::-1].copy()\n\n        sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)\n        sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)\n        sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)\n        sigmas = torch.from_numpy(sigmas).to(device=device)\n        self.sigmas = torch.cat([sigmas[:1], sigmas[1:-1].repeat_interleave(2), sigmas[-1:]])\n\n        # standard deviation of the initial noise distribution\n        self.init_noise_sigma = self.sigmas.max()\n\n        timesteps = torch.from_numpy(timesteps)\n        timesteps = torch.cat([timesteps[:1], timesteps[1:].repeat_interleave(2)])\n\n        if str(device).startswith(\"mps\"):\n            # mps does not support float64\n            s",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_heun_discrete.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "1392",
    {
      "pageContent": "def step(\n        self,\n        model_output: Union[torch.FloatTensor, np.ndarray],\n        timestep: Union[float, torch.FloatTensor],\n        sample: Union[torch.FloatTensor, np.ndarray],\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Args:\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n            model_output (`torch.FloatTensor` or `np.ndarray`): direct output from learned diffusion model. timestep\n            (`int`): current discrete timestep in the diffusion chain. sample (`torch.FloatTensor` or `np.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n        Returns:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        step_index = self.index_for_timestep(timestep)\n\n        if self.state_in_first_order:\n            sigma = self.sigmas[step_index]\n            sigma_next = self.sigmas[step_index + 1]\n        else:\n            # 2nd order / Heun's method\n            sigma = self.sigmas[step_index - 1]\n            sigma_next = self.sigmas[step_index]\n\n        # currently only gamma=0 is supported. This usually works best anyways",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_heun_discrete.py",
        "range": {
          "start": { "row": 188, "column": 4 },
          "end": { "row": 188, "column": 4 }
        }
      }
    }
  ],
  [
    "1393",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\n        self.sigmas = self.sigmas.to(device=original_samples.device, dtype=original_samples.dtype)\n        if original_samples.device.type == \"mps\" and torch.is_floating_point(timesteps):\n            # mps does not support float64\n            self.timesteps = self.timesteps.to(original_samples.device, dtype=torch.float32)\n            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)\n        else:\n            self.timesteps = self.timesteps.to(original_samples.device)\n            timesteps = timesteps.to(original_samples.device)\n\n        step_indices = [self.index_for_timestep(t) for t in timesteps]\n\n        sigma = self.sigmas[step_indices].flatten()\n        while len(sigma.shape) < len(original_samples.shape):\n            sigma = sigma.unsqueeze(-1)\n\n        noisy_samples = original_samples + noise * sigma\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_heun_discrete.py",
        "range": {
          "start": { "row": 272, "column": 4 },
          "end": { "row": 272, "column": 4 }
        }
      }
    }
  ],
  [
    "1394",
    {
      "pageContent": "class LMSDiscreteSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "1395",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 47, "column": 0 },
          "end": { "row": 47, "column": 0 }
        }
      }
    }
  ],
  [
    "1396",
    {
      "pageContent": "class LMSDiscreteScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by\n    Katherine Crowson:\n    https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear` or `scaled_linear`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        prediction_type (`str`, default `epsilon`, optional):\n            prediction type of the scheduler function, one of `epsilon` (predicting the noise of the diffusion\n            process), `sample` (directly predicting the noisy sample`) or `v_prediction` (see section 2.4\n            https://imagen.research.",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 76, "column": 0 },
          "end": { "row": 76, "column": 0 }
        }
      }
    }
  ],
  [
    "1397",
    {
      "pageContent": "def scale_model_input(\n        self, sample: torch.FloatTensor, timestep: Union[float, torch.FloatTensor]\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Scales the denoising model input by `(sigma**2 + 1) ** 0.5` to match the K-LMS algorithm.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`float` or `torch.FloatTensor`): the current timestep in the diffusion chain\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.timesteps.device)\n        step_index = (self.timesteps == timestep).nonzero().item()\n        sigma = self.sigmas[step_index]\n        sample = sample / ((sigma**2 + 1) ** 0.5)\n        self.is_scale_input_called = True\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "1398",
    {
      "pageContent": "def get_lms_coefficient(self, order, t, current_order):\n        \"\"\"\n        Compute a linear multistep coefficient.\n\n        Args:\n            order (TODO):\n            t (TODO):\n            current_order (TODO):\n        \"\"\"\n\n        def lms_derivative(tau):\n            prod = 1.0\n            for k in range(order):\n                if current_order == k:\n                    continue\n                prod *= (tau - self.sigmas[t - k]) / (self.sigmas[t - current_order] - self.sigmas[t - k])\n            return prod\n\n        integrated_coeff = integrate.quad(lms_derivative, self.sigmas[t], self.sigmas[t + 1], epsrel=1e-4)[0]\n\n        return integrated_coeff",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 168, "column": 4 },
          "end": { "row": 168, "column": 4 }
        }
      }
    }
  ],
  [
    "1399",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n\n        timesteps = np.linspace(0, self.config.num_train_timesteps - 1, num_inference_steps, dtype=float)[::-1].copy()\n        sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)\n        sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)\n        sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)\n\n        self.sigmas = torch.from_numpy(sigmas).to(device=device)\n        if str(device).startswith(\"mps\"):\n            # mps does not support float64\n            self.timesteps = torch.from_numpy(timesteps).to(device, dtype=torch.float32)\n        else:\n            self.timesteps = torch.from_numpy(timesteps).to(device=device)\n\n        self.derivatives = []",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 190, "column": 4 },
          "end": { "row": 190, "column": 4 }
        }
      }
    }
  ],
  [
    "1400",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: Union[float, torch.FloatTensor],\n        sample: torch.FloatTensor,\n        order: int = 4,\n        return_dict: bool = True,\n    ) -> Union[LMSDiscreteSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`float`): current timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            order: coefficient for multi-step inference.\n            return_dict (`bool`): option for returning tuple rather than LMSDiscreteSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`.\n            When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if not self.is_scale_input_called:\n            warnings.warn(\n                \"The `scale_model_input` function should be called before `step` to ensure correct denoising. \"\n                \"See `StableDiffusionPipeline` for a usage example.\"\n            )\n\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.times",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 216, "column": 4 },
          "end": { "row": 216, "column": 4 }
        }
      }
    }
  ],
  [
    "1401",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\n        sigmas = self.sigmas.to(device=original_samples.device, dtype=original_samples.dtype)\n        if original_samples.device.type == \"mps\" and torch.is_floating_point(timesteps):\n            # mps does not support float64\n            schedule_timesteps = self.timesteps.to(original_samples.device, dtype=torch.float32)\n            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)\n        else:\n            schedule_timesteps = self.timesteps.to(original_samples.device)\n            timesteps = timesteps.to(original_samples.device)\n\n        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n\n        sigma = sigmas[step_indices].flatten()\n        while len(sigma.shape) < len(original_samples.shape):\n            sigma = sigma.unsqueeze(-1)\n\n        noisy_samples = original_samples + noise * sigma\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_lms_discrete.py",
        "range": {
          "start": { "row": 286, "column": 4 },
          "end": { "row": 286, "column": 4 }
        }
      }
    }
  ],
  [
    "1402",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1403",
    {
      "pageContent": "class UniPCMultistepScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    UniPC is a training-free framework designed for the fast sampling of diffusion models, which consists of a\n    corrector (UniC) and a predictor (UniP) that share a unified analytical form and support arbitrary orders. UniPC is\n    by desinged model-agnostic, supporting pixel-space/latent-space DPMs on unconditional/conditional sampling. It can\n    also be applied to both noise prediction model and data prediction model. The corrector UniC can be also applied\n    after any off-the-shelf solvers to increase the order of accuracy.\n\n    For more details, see the original paper: https://arxiv.org/abs/2302.04867\n\n    Currently, we support the multistep UniPC for both noise prediction models and data prediction models. We recommend\n    to use `solver_order=2` for guided sampling, and `solver_order=3` for unconditional sampling.\n\n    We also support the \"dynamic thresholding\" method in Imagen (https://arxiv.org/abs/2205.11487). For pixel-space\n    diffusion models, you can set both `predict_x0=True` and `thresholding=True` to use the dynamic thresholding. Note\n    that the thresholding method is unsuitable for latent-space diffusion models (such as stable-diffusion).\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~S",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 56, "column": 0 },
          "end": { "row": 56, "column": 0 }
        }
      }
    }
  ],
  [
    "1404",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        timesteps = (\n            np.linspace(0, self.num_train_timesteps - 1, num_inference_steps + 1)\n            .round()[::-1][:-1]\n            .copy()\n            .astype(np.int64)\n        )\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n        self.model_outputs = [\n            None,\n        ] * self.config.solver_order\n        self.lower_order_nums = 0\n        self.last_sample = None\n        if self.solver_p:\n            self.solver_p.set_timesteps(num_inference_steps, device=device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 186, "column": 4 },
          "end": { "row": 186, "column": 4 }
        }
      }
    }
  ],
  [
    "1405",
    {
      "pageContent": "def _threshold_sample(self, sample: torch.FloatTensor) -> torch.FloatTensor:\n        # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n        dynamic_max_val = (\n            sample.flatten(1)\n            .abs()\n            .quantile(self.config.dynamic_thresholding_ratio, dim=1)\n            .clamp_min(self.config.sample_max_value)\n            .view(-1, *([1] * (sample.ndim - 1)))\n        )\n        return sample.clamp(-dynamic_max_val, dynamic_max_val) / dynamic_max_val",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 213, "column": 4 },
          "end": { "row": 213, "column": 4 }
        }
      }
    }
  ],
  [
    "1406",
    {
      "pageContent": "def convert_model_output(\n        self, model_output: torch.FloatTensor, timestep: int, sample: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        r\"\"\"\n        Convert the model output to the corresponding type that the algorithm PC needs.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the converted model output.\n        \"\"\"\n        if self.predict_x0:\n            if self.config.prediction_type == \"epsilon\":\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                x0_pred = (sample - sigma_t * model_output) / alpha_t\n            elif self.config.prediction_type == \"sample\":\n                x0_pred = model_output\n            elif self.config.prediction_type == \"v_prediction\":\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                x0_pred = alpha_t * sample - sigma_t * model_output\n            else:\n                raise ValueError(\n                    f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n                    \" `v_prediction` for the UniPCMultistepScheduler.\"\n                )\n\n            if self.config.thresholding:\n                # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n                orig_dtype = x0_pre",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 224, "column": 4 },
          "end": { "row": 224, "column": 4 }
        }
      }
    }
  ],
  [
    "1407",
    {
      "pageContent": "def multistep_uni_p_bh_update(\n        self,\n        model_output: torch.FloatTensor,\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n        order: int,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the UniP (B(h) version). Alternatively, `self.solver_p` is used if is specified.\n\n        Args:\n            model_output (`torch.FloatTensor`):\n                direct outputs from learned diffusion model at the current timestep.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            order (`int`): the order of UniP at this step, also the p in UniPC-p.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        timestep_list = self.timestep_list\n        model_output_list = self.model_outputs\n\n        s0, t = self.timestep_list[-1], prev_timestep\n        m0 = model_output_list[-1]\n        x = sample\n\n        if self.solver_p:\n            x_t = self.solver_p.step(model_output, s0, x).prev_sample\n            return x_t\n\n        lambda_t, lambda_s0 = self.lambda_t[t], self.lambda_t[s0]\n        alpha_t, alpha_s0 = self.alpha_t[t], self.alpha_t[s0]\n        sigma_t, sigma_s0 = self.sigma_t[t], self.sigma_t[s0]\n\n        h = lambda_t - lambda_s0\n        device = sample.device\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            si = timestep_list[-(i + 1)]\n            mi = model_output_list[-(i + 1)]\n            lam",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 278, "column": 4 },
          "end": { "row": 278, "column": 4 }
        }
      }
    }
  ],
  [
    "1408",
    {
      "pageContent": "def multistep_uni_c_bh_update(\n        self,\n        this_model_output: torch.FloatTensor,\n        this_timestep: int,\n        last_sample: torch.FloatTensor,\n        this_sample: torch.FloatTensor,\n        order: int,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the UniC (B(h) version).\n\n        Args:\n            this_model_output (`torch.FloatTensor`): the model outputs at `x_t`\n            this_timestep (`int`): the current timestep `t`\n            last_sample (`torch.FloatTensor`): the generated sample before the last predictor: `x_{t-1}`\n            this_sample (`torch.FloatTensor`): the generated sample after the last predictor: `x_{t}`\n            order (`int`): the `p` of UniC-p at this step. Note that the effective order of accuracy\n                should be order + 1\n\n        Returns:\n            `torch.FloatTensor`: the corrected sample tensor at the current timestep.\n        \"\"\"\n        timestep_list = self.timestep_list\n        model_output_list = self.model_outputs\n\n        s0, t = timestep_list[-1], this_timestep\n        m0 = model_output_list[-1]\n        x = last_sample\n        x_t = this_sample\n        model_t = this_model_output\n\n        lambda_t, lambda_s0 = self.lambda_t[t], self.lambda_t[s0]\n        alpha_t, alpha_s0 = self.alpha_t[t], self.alpha_t[s0]\n        sigma_t, sigma_s0 = self.sigma_t[t], self.sigma_t[s0]\n\n        h = lambda_t - lambda_s0\n        device = this_sample.device\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            si = timestep_list[-(i + 1)]\n            mi = model_output_list[-(i + 1)]\n   ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 383, "column": 4 },
          "end": { "row": 383, "column": 4 }
        }
      }
    }
  ],
  [
    "1409",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the multistep UniPC.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~scheduling_utils.SchedulerOutput`] or `tuple`: [`~scheduling_utils.SchedulerOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.timesteps.device)\n        step_index = (self.timesteps == timestep).nonzero()\n        if len(step_index) == 0:\n            step_index = len(self.timesteps) - 1\n        else:\n            step_index = step_index.item()\n\n        use_corrector = (\n            step_index > 0 and step_index - 1 not in self.disable_corrector and self.last_sample is not None\n        )\n\n     ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 489, "column": 4 },
          "end": { "row": 489, "column": 4 }
        }
      }
    }
  ],
  [
    "1410",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, *args, **kwargs) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 573, "column": 4 },
          "end": { "row": 573, "column": 4 }
        }
      }
    }
  ],
  [
    "1411",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_unipc_multistep.py",
        "range": {
          "start": { "row": 586, "column": 4 },
          "end": { "row": 586, "column": 4 }
        }
      }
    }
  ],
  [
    "1412",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999) -> torch.Tensor:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "1413",
    {
      "pageContent": "class KDPM2AncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Scheduler created by @crowsonkb in [k_diffusion](https://github.com/crowsonkb/k-diffusion), see:\n    https://github.com/crowsonkb/k-diffusion/blob/5b3af030dd83e0297272d861c19477735d0317ec/k_diffusion/sampling.py#L188\n\n    Scheduler inspired by DPM-Solver-2 and Algorthim 2 from Karras et al. (2022).\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model. beta_start (`float`): the\n        starting `beta` value of inference. beta_end (`float`): the final `beta` value. beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear` or `scaled_linear`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small`,\n            `fixed_small_log`, `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\n        prediction_type (`str`, default `epsilon`, optional):",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 55, "column": 0 },
          "end": { "row": 55, "column": 0 }
        }
      }
    }
  ],
  [
    "1414",
    {
      "pageContent": "def index_for_timestep(self, timestep):\n        indices = (self.timesteps == timestep).nonzero()\n        if self.state_in_first_order:\n            pos = -1\n        else:\n            pos = 0\n        return indices[pos].item()",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 116, "column": 4 },
          "end": { "row": 116, "column": 4 }
        }
      }
    }
  ],
  [
    "1415",
    {
      "pageContent": "def scale_model_input(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[float, torch.FloatTensor],\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Args:\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n            sample (`torch.FloatTensor`): input sample timestep (`int`, optional): current timestep\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        step_index = self.index_for_timestep(timestep)\n\n        if self.state_in_first_order:\n            sigma = self.sigmas[step_index]\n        else:\n            sigma = self.sigmas_interpol[step_index - 1]\n\n        sample = sample / ((sigma**2 + 1) ** 0.5)\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "1416",
    {
      "pageContent": "def set_timesteps(\n        self,\n        num_inference_steps: int,\n        device: Union[str, torch.device] = None,\n        num_train_timesteps: Optional[int] = None,\n    ):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n\n        num_train_timesteps = num_train_timesteps or self.config.num_train_timesteps\n\n        timesteps = np.linspace(0, num_train_timesteps - 1, num_inference_steps, dtype=float)[::-1].copy()\n\n        sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)\n        self.log_sigmas = torch.from_numpy(np.log(sigmas)).to(device)\n\n        sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)\n        sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)\n        sigmas = torch.from_numpy(sigmas).to(device=device)\n\n        # compute up and down sigmas\n        sigmas_next = sigmas.roll(-1)\n        sigmas_next[-1] = 0.0\n        sigmas_up = (sigmas_next**2 * (sigmas**2 - sigmas_next**2) / sigmas**2) ** 0.5\n        sigmas_down = (sigmas_next**2 - sigmas_up**2) ** 0.5\n        sigmas_down[-1] = 0.0\n\n        # compute interpolated sigmas\n        sigmas_interpol = sigmas.log().lerp(s",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "1417",
    {
      "pageContent": "def sigma_to_t(self, sigma):\n        # get log sigma\n        log_sigma = sigma.log()\n\n        # get distribution\n        dists = log_sigma - self.log_sigmas[:, None]\n\n        # get sigmas range\n        low_idx = dists.ge(0).cumsum(dim=0).argmax(dim=0).clamp(max=self.log_sigmas.shape[0] - 2)\n        high_idx = low_idx + 1\n\n        low = self.log_sigmas[low_idx]\n        high = self.log_sigmas[high_idx]\n\n        # interpolate sigmas\n        w = (low - log_sigma) / (low - high)\n        w = w.clamp(0, 1)\n\n        # transform interpolation to time range\n        t = (1 - w) * low_idx + w * high_idx\n        t = t.view(sigma.shape)\n        return t",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 210, "column": 4 },
          "end": { "row": 210, "column": 4 }
        }
      }
    }
  ],
  [
    "1418",
    {
      "pageContent": "def step(\n        self,\n        model_output: Union[torch.FloatTensor, np.ndarray],\n        timestep: Union[float, torch.FloatTensor],\n        sample: Union[torch.FloatTensor, np.ndarray],\n        generator: Optional[torch.Generator] = None,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Args:\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n            model_output (`torch.FloatTensor` or `np.ndarray`): direct output from learned diffusion model. timestep\n            (`int`): current discrete timestep in the diffusion chain. sample (`torch.FloatTensor` or `np.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n        Returns:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.SchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        step_index = self.index_for_timestep(timestep)\n\n        if self.state_in_first_order:\n            sigma = self.sigmas[step_index]\n            sigma_interpol = self.sigmas_interpol[step_index]\n            sigma_up = self.sigmas_up[step_index]\n            sigma_down = self.sigmas_down[step_index - 1]\n        else:\n            # 2nd order / KPDM2's method\n     ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 237, "column": 4 },
          "end": { "row": 237, "column": 4 }
        }
      }
    }
  ],
  [
    "1419",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\n        self.sigmas = self.sigmas.to(device=original_samples.device, dtype=original_samples.dtype)\n        if original_samples.device.type == \"mps\" and torch.is_floating_point(timesteps):\n            # mps does not support float64\n            self.timesteps = self.timesteps.to(original_samples.device, dtype=torch.float32)\n            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)\n        else:\n            self.timesteps = self.timesteps.to(original_samples.device)\n            timesteps = timesteps.to(original_samples.device)\n\n        step_indices = [self.index_for_timestep(t) for t in timesteps]\n\n        sigma = self.sigmas[step_indices].flatten()\n        while len(sigma.shape) < len(original_samples.shape):\n            sigma = sigma.unsqueeze(-1)\n\n        noisy_samples = original_samples + noise * sigma\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py",
        "range": {
          "start": { "row": 325, "column": 4 },
          "end": { "row": 325, "column": 4 }
        }
      }
    }
  ],
  [
    "1420",
    {
      "pageContent": "class VQDiffusionSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.LongTensor` of shape `(batch size, num latent pixels)`):\n            Computed sample x_{t-1} of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n    \"\"\"\n\n    prev_sample: torch.LongTensor",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1421",
    {
      "pageContent": "def index_to_log_onehot(x: torch.LongTensor, num_classes: int) -> torch.FloatTensor:\n    \"\"\"\n    Convert batch of vector of class indices into batch of log onehot vectors\n\n    Args:\n        x (`torch.LongTensor` of shape `(batch size, vector length)`):\n            Batch of class indices\n\n        num_classes (`int`):\n            number of classes to be used for the onehot vectors\n\n    Returns:\n        `torch.FloatTensor` of shape `(batch size, num classes, vector length)`:\n            Log onehot vectors\n    \"\"\"\n    x_onehot = F.one_hot(x, num_classes)\n    x_onehot = x_onehot.permute(0, 2, 1)\n    log_x = torch.log(x_onehot.float().clamp(min=1e-30))\n    return log_x",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "1422",
    {
      "pageContent": "def gumbel_noised(logits: torch.FloatTensor, generator: Optional[torch.Generator]) -> torch.FloatTensor:\n    \"\"\"\n    Apply gumbel noise to `logits`\n    \"\"\"\n    uniform = torch.rand(logits.shape, device=logits.device, generator=generator)\n    gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n    noised = gumbel_noise + logits\n    return noised",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 61, "column": 0 },
          "end": { "row": 61, "column": 0 }
        }
      }
    }
  ],
  [
    "1423",
    {
      "pageContent": "def alpha_schedules(num_diffusion_timesteps: int, alpha_cum_start=0.99999, alpha_cum_end=0.000009):\n    \"\"\"\n    Cumulative and non-cumulative alpha schedules.\n\n    See section 4.1.\n    \"\"\"\n    att = (\n        np.arange(0, num_diffusion_timesteps) / (num_diffusion_timesteps - 1) * (alpha_cum_end - alpha_cum_start)\n        + alpha_cum_start\n    )\n    att = np.concatenate(([1], att))\n    at = att[1:] / att[:-1]\n    att = np.concatenate((att[1:], [1]))\n    return at, att",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 71, "column": 0 },
          "end": { "row": 71, "column": 0 }
        }
      }
    }
  ],
  [
    "1424",
    {
      "pageContent": "def gamma_schedules(num_diffusion_timesteps: int, gamma_cum_start=0.000009, gamma_cum_end=0.99999):\n    \"\"\"\n    Cumulative and non-cumulative gamma schedules.\n\n    See section 4.1.\n    \"\"\"\n    ctt = (\n        np.arange(0, num_diffusion_timesteps) / (num_diffusion_timesteps - 1) * (gamma_cum_end - gamma_cum_start)\n        + gamma_cum_start\n    )\n    ctt = np.concatenate(([0], ctt))\n    one_minus_ctt = 1 - ctt\n    one_minus_ct = one_minus_ctt[1:] / one_minus_ctt[:-1]\n    ct = 1 - one_minus_ct\n    ctt = np.concatenate((ctt[1:], [0]))\n    return ct, ctt",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 87, "column": 0 },
          "end": { "row": 87, "column": 0 }
        }
      }
    }
  ],
  [
    "1425",
    {
      "pageContent": "class VQDiffusionScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image.\n\n    The VQ-diffusion scheduler converts the transformer's output into a sample for the unnoised image at the previous\n    diffusion timestep.\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2111.14822\n\n    Args:\n        num_vec_classes (`int`):\n            The number of classes of the vector embeddings of the latent pixels. Includes the class for the masked\n            latent pixel.\n\n        num_train_timesteps (`int`):\n            Number of diffusion steps used to train the model.\n\n        alpha_cum_start (`float`):\n            The starting cumulative alpha value.\n\n        alpha_cum_end (`float`):\n            The ending cumulative alpha value.\n\n        gamma_cum_start (`float`):\n            The starting cumulative gamma value.\n\n        gamma_cum_end (`float`):\n            The ending cumulative gamma value.\n    \"\"\"\n\n    order = 1\n\n    @register_to_config\n    def __init__(\n        self,\n        num_vec_classes: int,\n        num_train_timesteps: int = 100,\n        alpha_cum_start: float = 0.99999,\n        alpha",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 105, "column": 0 },
          "end": { "row": 105, "column": 0 }
        }
      }
    }
  ],
  [
    "1426",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n\n            device (`str` or `torch.device`):\n                device to place the timesteps and the diffusion process parameters (alpha, beta, gamma) on.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        timesteps = np.arange(0, self.num_inference_steps)[::-1].copy()\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n\n        self.log_at = self.log_at.to(device)\n        self.log_bt = self.log_bt.to(device)\n        self.log_ct = self.log_ct.to(device)\n        self.log_cumprod_at = self.log_cumprod_at.to(device)\n        self.log_cumprod_bt = self.log_cumprod_bt.to(device)\n        self.log_cumprod_ct = self.log_cumprod_ct.to(device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 189, "column": 4 },
          "end": { "row": 189, "column": 4 }
        }
      }
    }
  ],
  [
    "1427",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: torch.long,\n        sample: torch.LongTensor,\n        generator: Optional[torch.Generator] = None,\n        return_dict: bool = True,\n    ) -> Union[VQDiffusionSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the\n        docstring for `self.q_posterior` for more in depth docs on how Equation (11) is computed.\n\n        Args:\n            log_p_x_0: (`torch.FloatTensor` of shape `(batch size, num classes - 1, num latent pixels)`):\n                The log probabilities for the predicted classes of the initial latent pixels. Does not include a\n                prediction for the masked class as the initial unnoised image cannot be masked.\n\n            t (`torch.long`):\n                The timestep that determines which transition matrices are used.\n\n            x_t: (`torch.LongTensor` of shape `(batch size, num latent pixels)`):\n                The classes of each latent pixel at time `t`\n\n            generator: (`torch.Generator` or None):\n                RNG for the noise applied to p(x_{t-1} | x_t) before it is sampled from.\n\n            return_dict (`bool`):\n                option for returning tuple rather than VQDiffusionSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.VQDiffusionSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.VQDiffusionSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`.\n            When returning a tu",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 211, "column": 4 },
          "end": { "row": 211, "column": 4 }
        }
      }
    }
  ],
  [
    "1428",
    {
      "pageContent": "def q_posterior(self, log_p_x_0, x_t, t):\n        \"\"\"\n        Calculates the log probabilities for the predicted classes of the image at timestep `t-1`. I.e. Equation (11).\n\n        Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only\n        forward probabilities.\n\n        Equation (11) stated in terms of forward probabilities via Equation (5):\n\n        Where:\n        - the sum is over x_0 = {C_0 ... C_{k-1}} (classes for x_0)\n\n        p(x_{t-1} | x_t) = sum( q(x_t | x_{t-1}) * q(x_{t-1} | x_0) * p(x_0) / q(x_t | x_0) )\n\n        Args:\n            log_p_x_0: (`torch.FloatTensor` of shape `(batch size, num classes - 1, num latent pixels)`):\n                The log probabilities for the predicted classes of the initial latent pixels. Does not include a\n                prediction for the masked class as the initial unnoised image cannot be masked.\n\n            x_t: (`torch.LongTensor` of shape `(batch size, num latent pixels)`):\n                The classes of each latent pixel at time `t`\n\n            t (torch.Long):\n                The timestep that determines which transition matrix is used.\n\n        Returns:\n            `torch.FloatTensor` of shape `(batch size, num classes, num latent pixels)`:\n                The log probabilities for the predicted classes of the image at timestep `t-1`. I.e. Equation (11).\n        \"\"\"\n        log_onehot_x_t = index_to_log_onehot(x_t, self.num_embed)\n\n        log_q_x_t_given_x_0 = self.log_Q_t_transitioning_to_known_class(\n            t=t, x_t=x_t, log_onehot_x_t=log_onehot_x_t, cumula",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 259, "column": 4 },
          "end": { "row": 259, "column": 4 }
        }
      }
    }
  ],
  [
    "1429",
    {
      "pageContent": "def log_Q_t_transitioning_to_known_class(\n        self, *, t: torch.int, x_t: torch.LongTensor, log_onehot_x_t: torch.FloatTensor, cumulative: bool\n    ):\n        \"\"\"\n        Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each\n        latent pixel in `x_t`.\n\n        See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix\n        is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.\n\n        Args:\n            t (torch.Long):\n                The timestep that determines which transition matrix is used.\n\n            x_t (`torch.LongTensor` of shape `(batch size, num latent pixels)`):\n                The classes of each latent pixel at time `t`.\n\n            log_onehot_x_t (`torch.FloatTensor` of shape `(batch size, num classes, num latent pixels)`):\n                The log one-hot vectors of `x_t`\n\n            cumulative (`bool`):\n                If cumulative is `False`, we use the single step transition matrix `t-1`->`t`. If cumulative is `True`,\n                we use the cumulative transition matrix `0`->`t`.\n\n        Returns:\n            `torch.FloatTensor` of shape `(batch size, num classes - 1, num latent pixels)`:\n                Each _column_ of the returned matrix is a _row_ of log probabilities of the complete probability\n                transition matrix.\n\n                When non cumulative, returns `self.num_classes - 1` rows because the initial latent pixel cannot be\n                masked.\n\n                Where:\n",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 378, "column": 4 },
          "end": { "row": 378, "column": 4 }
        }
      }
    }
  ],
  [
    "1430",
    {
      "pageContent": "def apply_cumulative_transitions(self, q, t):\n        bsz = q.shape[0]\n        a = self.log_cumprod_at[t]\n        b = self.log_cumprod_bt[t]\n        c = self.log_cumprod_ct[t]\n\n        num_latent_pixels = q.shape[2]\n        c = c.expand(bsz, 1, num_latent_pixels)\n\n        q = (q + a).logaddexp(b)\n        q = torch.cat((q, c), dim=1)\n\n        return q",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_vq_diffusion.py",
        "range": {
          "start": { "row": 483, "column": 4 },
          "end": { "row": 483, "column": 4 }
        }
      }
    }
  ],
  [
    "1431",
    {
      "pageContent": "class KarrasVeOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        derivative (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Derivative of predicted original image sample (x_0).\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    derivative: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1432",
    {
      "pageContent": "class KarrasVeScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and\n    the VE column of Table 1 from [1] for reference.\n\n    [1] Karras, Tero, et al. \"Elucidating the Design Space of Diffusion-Based Generative Models.\"\n    https://arxiv.org/abs/2206.00364 [2] Song, Yang, et al. \"Score-based generative modeling through stochastic\n    differential equations.\" https://arxiv.org/abs/2011.13456\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details on the parameters, see the original paper's Appendix E.: \"Elucidating the Design Space of\n    Diffusion-Based Generative Models.\" https://arxiv.org/abs/2206.00364. The grid search values used to find the\n    optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.\n\n    Args:\n        sigma_min (`float`): minimum noise magnitude\n        sigma_max (`float`): maximum noise magnitude\n        s_noise (`float`): the amount of additional noise to counteract loss of detail during sampling.\n            A reasonable range is [1.000, 1.011].\n        s_churn (`float`): the parameter controlling the overall amount of stochasticity.\n        ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve.py",
        "range": {
          "start": { "row": 47, "column": 0 },
          "end": { "row": 47, "column": 0 }
        }
      }
    }
  ],
  [
    "1433",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve.py",
        "range": {
          "start": { "row": 99, "column": 4 },
          "end": { "row": 99, "column": 4 }
        }
      }
    }
  ],
  [
    "1434",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        timesteps = np.arange(0, self.num_inference_steps)[::-1].copy()\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n        schedule = [\n            (\n                self.config.sigma_max**2\n                * (self.config.sigma_min**2 / self.config.sigma_max**2) ** (i / (num_inference_steps - 1))\n            )\n            for i in self.timesteps\n        ]\n        self.schedule = torch.tensor(schedule, dtype=torch.float32, device=device)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve.py",
        "range": {
          "start": { "row": 113, "column": 4 },
          "end": { "row": 113, "column": 4 }
        }
      }
    }
  ],
  [
    "1435",
    {
      "pageContent": "def add_noise_to_input(\n        self, sample: torch.FloatTensor, sigma: float, generator: Optional[torch.Generator] = None\n    ) -> Tuple[torch.FloatTensor, float]:\n        \"\"\"\n        Explicit Langevin-like \"churn\" step of adding noise to the sample according to a factor gamma_i  0 to reach a\n        higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.\n\n        TODO Args:\n        \"\"\"\n        if self.config.s_min <= sigma <= self.config.s_max:\n            gamma = min(self.config.s_churn / self.num_inference_steps, 2**0.5 - 1)\n        else:\n            gamma = 0\n\n        # sample eps ~ N(0, S_noise^2 * I)\n        eps = self.config.s_noise * randn_tensor(sample.shape, generator=generator).to(sample.device)\n        sigma_hat = sigma + gamma * sigma\n        sample_hat = sample + ((sigma_hat**2 - sigma**2) ** 0.5 * eps)\n\n        return sample_hat, sigma_hat",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve.py",
        "range": {
          "start": { "row": 134, "column": 4 },
          "end": { "row": 134, "column": 4 }
        }
      }
    }
  ],
  [
    "1436",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        sigma_hat: float,\n        sigma_prev: float,\n        sample_hat: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[KarrasVeOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            sigma_hat (`float`): TODO\n            sigma_prev (`float`): TODO\n            sample_hat (`torch.FloatTensor`): TODO\n            return_dict (`bool`): option for returning tuple rather than KarrasVeOutput class\n\n            KarrasVeOutput: updated sample in the diffusion chain and derivative (TODO double check).\n        Returns:\n            [`~schedulers.scheduling_karras_ve.KarrasVeOutput`] or `tuple`:\n            [`~schedulers.scheduling_karras_ve.KarrasVeOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n\n        pred_original_sample = sample_hat + sigma_hat * model_output\n        derivative = (sample_hat - pred_original_sample) / sigma_hat\n        sample_prev = sample_hat + (sigma_prev - sigma_hat) * derivative\n\n        if not return_dict:\n            return (sample_prev, derivative)\n\n        return KarrasVeOutput(\n            prev_sample=sample_prev, derivative=derivative, pred_original_sample=pred_original_sample\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve.py",
        "range": {
          "start": { "row": 155, "column": 4 },
          "end": { "row": 155, "column": 4 }
        }
      }
    }
  ],
  [
    "1437",
    {
      "pageContent": "def step_correct(\n        self,\n        model_output: torch.FloatTensor,\n        sigma_hat: float,\n        sigma_prev: float,\n        sample_hat: torch.FloatTensor,\n        sample_prev: torch.FloatTensor,\n        derivative: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[KarrasVeOutput, Tuple]:\n        \"\"\"\n        Correct the predicted sample based on the output model_output of the network. TODO complete description\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            sigma_hat (`float`): TODO\n            sigma_prev (`float`): TODO\n            sample_hat (`torch.FloatTensor`): TODO\n            sample_prev (`torch.FloatTensor`): TODO\n            derivative (`torch.FloatTensor`): TODO\n            return_dict (`bool`): option for returning tuple rather than KarrasVeOutput class\n\n        Returns:\n            prev_sample (TODO): updated sample in the diffusion chain. derivative (TODO): TODO\n\n        \"\"\"\n        pred_original_sample = sample_prev + sigma_prev * model_output\n        derivative_corr = (sample_prev - pred_original_sample) / sigma_prev\n        sample_prev = sample_hat + (sigma_prev - sigma_hat) * (0.5 * derivative + 0.5 * derivative_corr)\n\n        if not return_dict:\n            return (sample_prev, derivative)\n\n        return KarrasVeOutput(\n            prev_sample=sample_prev, derivative=derivative, pred_original_sample=pred_original_sample\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_karras_ve.py",
        "range": {
          "start": { "row": 193, "column": 4 },
          "end": { "row": 193, "column": 4 }
        }
      }
    }
  ],
  [
    "1438",
    {
      "pageContent": "class DPMSolverMultistepSchedulerState:\n    common: CommonSchedulerState\n    alpha_t: jnp.ndarray\n    sigma_t: jnp.ndarray\n    lambda_t: jnp.ndarray\n\n    # setable values\n    init_noise_sigma: jnp.ndarray\n    timesteps: jnp.ndarray\n    num_inference_steps: Optional[int] = None\n\n    # running values\n    model_outputs: Optional[jnp.ndarray] = None\n    lower_order_nums: Optional[jnp.int32] = None\n    prev_timestep: Optional[jnp.int32] = None\n    cur_sample: Optional[jnp.ndarray] = None\n\n    @classmethod\n    def create(\n        cls,\n        common: CommonSchedulerState,\n        alpha_t: jnp.ndarray,\n        sigma_t: jnp.ndarray,\n        lambda_t: jnp.ndarray,\n        init_noise_sigma: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ):\n        return cls(\n            common=common,\n            alpha_t=alpha_t,\n            sigma_t=sigma_t,\n            lambda_t=lambda_t,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1439",
    {
      "pageContent": "class FlaxDPMSolverMultistepScheduler(FlaxSchedulerMixin, ConfigMixin):\n    \"\"\"\n    DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with\n    the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality\n    samples, and it can generate quite good samples even in only 10 steps.\n\n    For more details, see the original paper: https://arxiv.org/abs/2206.00927 and https://arxiv.org/abs/2211.01095\n\n    Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We\n    recommend to use `solver_order=2` for guided sampling, and `solver_order=3` for unconditional sampling.\n\n    We also support the \"dynamic thresholding\" method in Imagen (https://arxiv.org/abs/2205.11487). For pixel-space\n    diffusion models, you can set both `algorithm_type=\"dpmsolver++\"` and `thresholding=True` to use the dynamic\n    thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as\n    stable-diffusion).\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2206.00927 and https://arxiv.org/abs/2211.010",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 76, "column": 0 },
          "end": { "row": 76, "column": 0 }
        }
      }
    }
  ],
  [
    "1440",
    {
      "pageContent": "def create_state(self, common: Optional[CommonSchedulerState] = None) -> DPMSolverMultistepSchedulerState:\n        if common is None:\n            common = CommonSchedulerState.create(self)\n\n        # Currently we only support VP-type noise schedule\n        alpha_t = jnp.sqrt(common.alphas_cumprod)\n        sigma_t = jnp.sqrt(1 - common.alphas_cumprod)\n        lambda_t = jnp.log(alpha_t) - jnp.log(sigma_t)\n\n        # settings for DPM-Solver\n        if self.config.algorithm_type not in [\"dpmsolver\", \"dpmsolver++\"]:\n            raise NotImplementedError(f\"{self.config.algorithm_type} does is not implemented for {self.__class__}\")\n        if self.config.solver_type not in [\"midpoint\", \"heun\"]:\n            raise NotImplementedError(f\"{self.config.solver_type} does is not implemented for {self.__class__}\")\n\n        # standard deviation of the initial noise distribution\n        init_noise_sigma = jnp.array(1.0, dtype=self.dtype)\n\n        timesteps = jnp.arange(0, self.config.num_train_timesteps).round()[::-1]\n\n        return DPMSolverMultistepSchedulerState.create(\n            common=common,\n            alpha_t=alpha_t,\n            sigma_t=sigma_t,\n            lambda_t=lambda_t,\n            init_noise_sigma=init_noise_sigma,\n            timesteps=timesteps,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 169, "column": 4 },
          "end": { "row": 169, "column": 4 }
        }
      }
    }
  ],
  [
    "1441",
    {
      "pageContent": "def set_timesteps(\n        self, state: DPMSolverMultistepSchedulerState, num_inference_steps: int, shape: Tuple\n    ) -> DPMSolverMultistepSchedulerState:\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`DPMSolverMultistepSchedulerState`):\n                the `FlaxDPMSolverMultistepScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            shape (`Tuple`):\n                the shape of the samples to be generated.\n        \"\"\"\n\n        timesteps = (\n            jnp.linspace(0, self.config.num_train_timesteps - 1, num_inference_steps + 1)\n            .round()[::-1][:-1]\n            .astype(jnp.int32)\n        )\n\n        # initial running values\n\n        model_outputs = jnp.zeros((self.config.solver_order,) + shape, dtype=self.dtype)\n        lower_order_nums = jnp.int32(0)\n        prev_timestep = jnp.int32(-1)\n        cur_sample = jnp.zeros(shape, dtype=self.dtype)\n\n        return state.replace(\n            num_inference_steps=num_inference_steps,\n            timesteps=timesteps,\n            model_outputs=model_outputs,\n            lower_order_nums=lower_order_nums,\n            prev_timestep=prev_timestep,\n            cur_sample=cur_sample,\n        )",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 198, "column": 4 },
          "end": { "row": 198, "column": 4 }
        }
      }
    }
  ],
  [
    "1442",
    {
      "pageContent": "def convert_model_output(\n        self,\n        state: DPMSolverMultistepSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n    ) -> jnp.ndarray:\n        \"\"\"\n        Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs.\n\n        DPM-Solver is designed to discretize an integral of the noise prediction model, and DPM-Solver++ is designed to\n        discretize an integral of the data prediction model. So we need to first convert the model output to the\n        corresponding type to match the algorithm.\n\n        Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or\n        DPM-Solver++ for both noise prediction model and data prediction model.\n\n        Args:\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `jnp.ndarray`: the converted model output.\n        \"\"\"\n        # DPM-Solver++ needs to solve an integral of the data prediction model.\n        if self.config.algorithm_type == \"dpmsolver++\":\n            if self.config.prediction_type == \"epsilon\":\n                alpha_t, sigma_t = state.alpha_t[timestep], state.sigma_t[timestep]\n                x0_pred = (sample - sigma_t * model_output) / alpha_t\n            elif self.config.prediction_type == \"sample\":\n       ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 235, "column": 4 },
          "end": { "row": 235, "column": 4 }
        }
      }
    }
  ],
  [
    "1443",
    {
      "pageContent": "def dpm_solver_first_order_update(\n        self,\n        state: DPMSolverMultistepSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        prev_timestep: int,\n        sample: jnp.ndarray,\n    ) -> jnp.ndarray:\n        \"\"\"\n        One step for the first-order DPM-Solver (equivalent to DDIM).\n\n        See https://arxiv.org/abs/2206.00927 for the detailed derivation.\n\n        Args:\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `jnp.ndarray`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0 = prev_timestep, timestep\n        m0 = model_output\n        lambda_t, lambda_s = state.lambda_t[t], state.lambda_t[s0]\n        alpha_t, alpha_s = state.alpha_t[t], state.alpha_t[s0]\n        sigma_t, sigma_s = state.sigma_t[t], state.sigma_t[s0]\n        h = lambda_t - lambda_s\n        if self.config.algorithm_type == \"dpmsolver++\":\n            x_t = (sigma_t / sigma_s) * sample - (alpha_t * (jnp.exp(-h) - 1.0)) * m0\n        elif self.config.algorithm_type == \"dpmsolver\":\n            x_t = (alpha_t / alpha_s) * sample - (sigma_t * (jnp.exp(h) - 1.0)) * m0\n        return x_t",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 305, "column": 4 },
          "end": { "row": 305, "column": 4 }
        }
      }
    }
  ],
  [
    "1444",
    {
      "pageContent": "def multistep_dpm_solver_second_order_update(\n        self,\n        state: DPMSolverMultistepSchedulerState,\n        model_output_list: jnp.ndarray,\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: jnp.ndarray,\n    ) -> jnp.ndarray:\n        \"\"\"\n        One step for the second-order multistep DPM-Solver.\n\n        Args:\n            model_output_list (`List[jnp.ndarray]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `jnp.ndarray`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1 = prev_timestep, timestep_list[-1], timestep_list[-2]\n        m0, m1 = model_output_list[-1], model_output_list[-2]\n        lambda_t, lambda_s0, lambda_s1 = state.lambda_t[t], state.lambda_t[s0], state.lambda_t[s1]\n        alpha_t, alpha_s0 = state.alpha_t[t], state.alpha_t[s0]\n        sigma_t, sigma_s0 = state.sigma_t[t], state.sigma_t[s0]\n        h, h_0 = lambda_t - lambda_s0, lambda_s0 - lambda_s1\n        r0 = h_0 / h\n        D0, D1 = m0, (1.0 / r0) * (m0 - m1)\n        if self.config.algorithm_type == \"dpmsolver++\":\n            # See https://arxiv.org/abs/2211.01095 for detailed derivations\n            if self.config.solver_type == \"midpoint\":\n                x_t = (\n               ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 340, "column": 4 },
          "end": { "row": 340, "column": 4 }
        }
      }
    }
  ],
  [
    "1445",
    {
      "pageContent": "def multistep_dpm_solver_third_order_update(\n        self,\n        state: DPMSolverMultistepSchedulerState,\n        model_output_list: jnp.ndarray,\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: jnp.ndarray,\n    ) -> jnp.ndarray:\n        \"\"\"\n        One step for the third-order multistep DPM-Solver.\n\n        Args:\n            model_output_list (`List[jnp.ndarray]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `jnp.ndarray`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1, s2 = prev_timestep, timestep_list[-1], timestep_list[-2], timestep_list[-3]\n        m0, m1, m2 = model_output_list[-1], model_output_list[-2], model_output_list[-3]\n        lambda_t, lambda_s0, lambda_s1, lambda_s2 = (\n            state.lambda_t[t],\n            state.lambda_t[s0],\n            state.lambda_t[s1],\n            state.lambda_t[s2],\n        )\n        alpha_t, alpha_s0 = state.alpha_t[t], state.alpha_t[s0]\n        sigma_t, sigma_s0 = state.sigma_t[t], state.sigma_t[s0]\n        h, h_0, h_1 = lambda_t - lambda_s0, lambda_s0 - lambda_s1, lambda_s1 - lambda_s2\n        r0, r1 = h_0 / h, h_1 / h\n        D0 = m0\n        D1_0, D1_1 = (1.0 / r0) * (m0 - m1), (1.0 / r1) * (m1 - m2)\n    ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 400, "column": 4 },
          "end": { "row": 400, "column": 4 }
        }
      }
    }
  ],
  [
    "1446",
    {
      "pageContent": "def step(\n        self,\n        state: DPMSolverMultistepSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n        return_dict: bool = True,\n    ) -> Union[FlaxDPMSolverMultistepSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by DPM-Solver. Core function to propagate the diffusion process\n        from the learned model outputs (most often the predicted noise).\n\n        Args:\n            state (`DPMSolverMultistepSchedulerState`):\n                the `FlaxDPMSolverMultistepScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than FlaxDPMSolverMultistepSchedulerOutput class\n\n        Returns:\n            [`FlaxDPMSolverMultistepSchedulerOutput`] or `tuple`: [`FlaxDPMSolverMultistepSchedulerOutput`] if\n            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if state.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        (step_index,) = jnp.where(state.timesteps == timestep, size=1)\n        step_index = step_index[0]\n\n        prev_time",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 456, "column": 4 },
          "end": { "row": 456, "column": 4 }
        }
      }
    }
  ],
  [
    "1447",
    {
      "pageContent": "def scale_model_input(\n        self, state: DPMSolverMultistepSchedulerState, sample: jnp.ndarray, timestep: Optional[int] = None\n    ) -> jnp.ndarray:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            state (`DPMSolverMultistepSchedulerState`):\n                the `FlaxDPMSolverMultistepScheduler` state data class instance.\n            sample (`jnp.ndarray`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `jnp.ndarray`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 593, "column": 4 },
          "end": { "row": 593, "column": 4 }
        }
      }
    }
  ],
  [
    "1448",
    {
      "pageContent": "def add_noise(\n        self,\n        state: DPMSolverMultistepSchedulerState,\n        original_samples: jnp.ndarray,\n        noise: jnp.ndarray,\n        timesteps: jnp.ndarray,\n    ) -> jnp.ndarray:\n        return add_noise_common(state.common, original_samples, noise, timesteps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py",
        "range": {
          "start": { "row": 611, "column": 4 },
          "end": { "row": 611, "column": 4 }
        }
      }
    }
  ],
  [
    "1449",
    {
      "pageContent": "class IPNDMScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb's amazing k-diffusion\n    [library](https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296)\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2202.09778\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n    \"\"\"\n\n    order = 1\n\n    @register_to_config\n    def __init__(\n        self, num_train_timesteps: int = 1000, trained_betas: Optional[Union[np.ndarray, List[float]]] = None\n    ):\n        # set `betas`, `alphas`, `timesteps`\n        self.set_timesteps(num_train_timesteps)\n\n        # standard deviation of the initial noise distribution\n        self.init_noise_sigma = 1.0\n\n        # For now we only support F-PNDM, i.e. the runge-kutta method\n        # For more information on the algorithm please take a look at the paper: https://arxiv.org/pdf/2202.09778.pdf\n        # mainly at formula (9), (12), (13) and the Algorithm 2.\n        self.pndm_order = 4\n\n        # running values\n        self.ets = []\n\n    def set_t",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ipndm.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1450",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        steps = torch.linspace(1, 0, num_inference_steps + 1)[:-1]\n        steps = torch.cat([steps, torch.tensor([0.0])])\n\n        if self.config.trained_betas is not None:\n            self.betas = torch.tensor(self.config.trained_betas, dtype=torch.float32)\n        else:\n            self.betas = torch.sin(steps * math.pi / 2) ** 2\n\n        self.alphas = (1.0 - self.betas**2) ** 0.5\n\n        timesteps = (torch.atan2(self.betas, self.alphas) / math.pi * 2)[:-1]\n        self.timesteps = timesteps.to(device)\n\n        self.ets = []",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ipndm.py",
        "range": {
          "start": { "row": 60, "column": 4 },
          "end": { "row": 60, "column": 4 }
        }
      }
    }
  ],
  [
    "1451",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple\n        times to approximate the solution.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~scheduling_utils.SchedulerOutput`] or `tuple`: [`~scheduling_utils.SchedulerOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        timestep_index = (self.timesteps == timestep).nonzero().item()\n        prev_timestep_index = timestep_index + 1\n\n        ets = sample * self.betas[timestep_index] + model_output * self.alphas[timestep_index]\n        self.ets.append(ets)\n\n        if len(self.ets) == 1:\n            ets = self.ets[-1]\n        elif len(self.ets) == 2:\n            ets = (3 * s",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ipndm.py",
        "range": {
          "start": { "row": 84, "column": 4 },
          "end": { "row": 84, "column": 4 }
        }
      }
    }
  ],
  [
    "1452",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, *args, **kwargs) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ipndm.py",
        "range": {
          "start": { "row": 134, "column": 4 },
          "end": { "row": 134, "column": 4 }
        }
      }
    }
  ],
  [
    "1453",
    {
      "pageContent": "def _get_prev_sample(self, sample, timestep_index, prev_timestep_index, ets):\n        alpha = self.alphas[timestep_index]\n        sigma = self.betas[timestep_index]\n\n        next_alpha = self.alphas[prev_timestep_index]\n        next_sigma = self.betas[prev_timestep_index]\n\n        pred = (sample - sigma * ets) / max(alpha, 1e-8)\n        prev_sample = next_alpha * pred + ets * next_sigma\n\n        return prev_sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ipndm.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "1454",
    {
      "pageContent": "class DDIMSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n            `pred_original_sample` can be used to preview progress or for guidance.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n    pred_original_sample: Optional[torch.FloatTensor] = None",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_inverse.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "1455",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999) -> torch.Tensor:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_inverse.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "1456",
    {
      "pageContent": "class DDIMInverseScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    DDIMInverseScheduler is the reverse scheduler of [`DDIMScheduler`].\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    For more details, see the original paper: https://arxiv.org/abs/2010.02502\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`float`): the starting `beta` value of inference.\n        beta_end (`float`): the final `beta` value.\n        beta_schedule (`str`):\n            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n        trained_betas (`np.ndarray`, optional):\n            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n        clip_sample (`bool`, default `True`):\n            option to clip predicted sample between -1 and 1 for numerical stability.\n        set_alpha_to_one (`bool`, default `True`):\n            each diffusion step uses the value of alphas product at that step and at the previous one. For the final\n            step there is no previous alpha. When this option is `True` the previous alpha produ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_inverse.py",
        "range": {
          "start": { "row": 77, "column": 0 },
          "end": { "row": 77, "column": 0 }
        }
      }
    }
  ],
  [
    "1457",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n            timestep (`int`, optional): current timestep\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_inverse.py",
        "range": {
          "start": { "row": 159, "column": 4 },
          "end": { "row": 159, "column": 4 }
        }
      }
    }
  ],
  [
    "1458",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n\n        if num_inference_steps > self.config.num_train_timesteps:\n            raise ValueError(\n                f\"`num_inference_steps`: {num_inference_steps} cannot be larger than `self.config.train_timesteps`:\"\n                f\" {self.config.num_train_timesteps} as the unet model trained with this scheduler can only handle\"\n                f\" maximal {self.config.num_train_timesteps} timesteps.\"\n            )\n\n        self.num_inference_steps = num_inference_steps\n        step_ratio = self.config.num_train_timesteps // self.num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # casting to int to avoid issues when num_inference_step is power of 3\n        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round().copy().astype(np.int64)\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n        self.timesteps += self.config.steps_offset",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_inverse.py",
        "range": {
          "start": { "row": 173, "column": 4 },
          "end": { "row": 173, "column": 4 }
        }
      }
    }
  ],
  [
    "1459",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        eta: float = 0.0,\n        use_clipped_model_output: bool = False,\n        variance_noise: Optional[torch.FloatTensor] = None,\n        return_dict: bool = True,\n    ) -> Union[DDIMSchedulerOutput, Tuple]:\n        e_t = model_output\n\n        x = sample\n        prev_timestep = timestep + self.config.num_train_timesteps // self.num_inference_steps\n\n        a_t = self.alphas_cumprod[timestep - 1]\n        a_prev = self.alphas_cumprod[prev_timestep - 1] if prev_timestep >= 0 else self.final_alpha_cumprod\n\n        pred_x0 = (x - (1 - a_t) ** 0.5 * e_t) / a_t.sqrt()\n\n        dir_xt = (1.0 - a_prev).sqrt() * e_t\n\n        prev_sample = a_prev.sqrt() * pred_x0 + dir_xt\n\n        if not return_dict:\n            return (prev_sample, pred_x0)\n        return DDIMSchedulerOutput(prev_sample=prev_sample, pred_original_sample=pred_x0)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_ddim_inverse.py",
        "range": {
          "start": { "row": 197, "column": 4 },
          "end": { "row": 197, "column": 4 }
        }
      }
    }
  ],
  [
    "1460",
    {
      "pageContent": "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n    (1-beta) over time from t = [0,1].\n\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n    to that part of the diffusion process.\n\n\n    Args:\n        num_diffusion_timesteps (`int`): the number of betas to produce.\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n\n    Returns:\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n    \"\"\"\n\n    def alpha_bar(time_step):\n        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return torch.tensor(betas, dtype=torch.float32)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "1461",
    {
      "pageContent": "class DPMSolverSinglestepScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with\n    the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality\n    samples, and it can generate quite good samples even in only 10 steps.\n\n    For more details, see the original paper: https://arxiv.org/abs/2206.00927 and https://arxiv.org/abs/2211.01095\n\n    Currently, we support the singlestep DPM-Solver for both noise prediction models and data prediction models. We\n    recommend to use `solver_order=2` for guided sampling, and `solver_order=3` for unconditional sampling.\n\n    We also support the \"dynamic thresholding\" method in Imagen (https://arxiv.org/abs/2205.11487). For pixel-space\n    diffusion models, you can set both `algorithm_type=\"dpmsolver++\"` and `thresholding=True` to use the dynamic\n    thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as\n    stable-diffusion).\n\n    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n    [`SchedulerMixin`] provides general loading and saving functionality via the [`SchedulerMixin.save_pretrained`] and\n    [`~SchedulerMixin.from_pretrained`] functions.\n\n    Args:\n        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n        beta_start (`",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 56, "column": 0 },
          "end": { "row": 56, "column": 0 }
        }
      }
    }
  ],
  [
    "1462",
    {
      "pageContent": "def get_order_list(self, num_inference_steps: int) -> List[int]:\n        \"\"\"\n        Computes the solver order at each time step.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n        steps = num_inference_steps\n        order = self.solver_order\n        if self.lower_order_final:\n            if order == 3:\n                if steps % 3 == 0:\n                    orders = [1, 2, 3] * (steps // 3 - 1) + [1, 2] + [1]\n                elif steps % 3 == 1:\n                    orders = [1, 2, 3] * (steps // 3) + [1]\n                else:\n                    orders = [1, 2, 3] * (steps // 3) + [1, 2]\n            elif order == 2:\n                if steps % 2 == 0:\n                    orders = [1, 2] * (steps // 2)\n                else:\n                    orders = [1, 2] * (steps // 2) + [1]\n            elif order == 1:\n                orders = [1] * steps\n        else:\n            if order == 3:\n                orders = [1, 2, 3] * (steps // 3)\n            elif order == 2:\n                orders = [1, 2] * (steps // 2)\n            elif order == 1:\n                orders = [1] * steps\n        return orders",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "1463",
    {
      "pageContent": "def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n        \"\"\"\n        Sets the timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, optional):\n                the device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        timesteps = (\n            np.linspace(0, self.num_train_timesteps - 1, num_inference_steps + 1)\n            .round()[::-1][:-1]\n            .copy()\n            .astype(np.int64)\n        )\n        self.timesteps = torch.from_numpy(timesteps).to(device)\n        self.model_outputs = [None] * self.config.solver_order\n        self.sample = None\n        self.orders = self.get_order_list(num_inference_steps)",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 217, "column": 4 },
          "end": { "row": 217, "column": 4 }
        }
      }
    }
  ],
  [
    "1464",
    {
      "pageContent": "def _threshold_sample(self, sample: torch.FloatTensor) -> torch.FloatTensor:\n        # Dynamic thresholding in https://arxiv.org/abs/2205.11487\n        dynamic_max_val = (\n            sample.flatten(1)\n            .abs()\n            .quantile(self.config.dynamic_thresholding_ratio, dim=1)\n            .clamp_min(self.config.sample_max_value)\n            .view(-1, *([1] * (sample.ndim - 1)))\n        )\n        return sample.clamp(-dynamic_max_val, dynamic_max_val) / dynamic_max_val",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 240, "column": 4 },
          "end": { "row": 240, "column": 4 }
        }
      }
    }
  ],
  [
    "1465",
    {
      "pageContent": "def convert_model_output(\n        self, model_output: torch.FloatTensor, timestep: int, sample: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        \"\"\"\n        Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs.\n\n        DPM-Solver is designed to discretize an integral of the noise prediction model, and DPM-Solver++ is designed to\n        discretize an integral of the data prediction model. So we need to first convert the model output to the\n        corresponding type to match the algorithm.\n\n        Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or\n        DPM-Solver++ for both noise prediction model and data prediction model.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the converted model output.\n        \"\"\"\n        # DPM-Solver++ needs to solve an integral of the data prediction model.\n        if self.config.algorithm_type == \"dpmsolver++\":\n            if self.config.prediction_type == \"epsilon\":\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                x0_pred = (sample - sigma_t * model_output) / alpha_t\n            elif self.config.prediction_type == \"sample\":\n                x0_pred = model_output\n        ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 251, "column": 4 },
          "end": { "row": 251, "column": 4 }
        }
      }
    }
  ],
  [
    "1466",
    {
      "pageContent": "def dpm_solver_first_order_update(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the first-order DPM-Solver (equivalent to DDIM).\n\n        See https://arxiv.org/abs/2206.00927 for the detailed derivation.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        lambda_t, lambda_s = self.lambda_t[prev_timestep], self.lambda_t[timestep]\n        alpha_t, alpha_s = self.alpha_t[prev_timestep], self.alpha_t[timestep]\n        sigma_t, sigma_s = self.sigma_t[prev_timestep], self.sigma_t[timestep]\n        h = lambda_t - lambda_s\n        if self.config.algorithm_type == \"dpmsolver++\":\n            x_t = (sigma_t / sigma_s) * sample - (alpha_t * (torch.exp(-h) - 1.0)) * model_output\n        elif self.config.algorithm_type == \"dpmsolver\":\n            x_t = (alpha_t / alpha_s) * sample - (sigma_t * (torch.exp(h) - 1.0)) * model_output\n        return x_t",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 314, "column": 4 },
          "end": { "row": 314, "column": 4 }
        }
      }
    }
  ],
  [
    "1467",
    {
      "pageContent": "def singlestep_dpm_solver_second_order_update(\n        self,\n        model_output_list: List[torch.FloatTensor],\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the second-order singlestep DPM-Solver.\n\n        It computes the solution at time `prev_timestep` from the time `timestep_list[-2]`.\n\n        Args:\n            model_output_list (`List[torch.FloatTensor]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1 = prev_timestep, timestep_list[-1], timestep_list[-2]\n        m0, m1 = model_output_list[-1], model_output_list[-2]\n        lambda_t, lambda_s0, lambda_s1 = self.lambda_t[t], self.lambda_t[s0], self.lambda_t[s1]\n        alpha_t, alpha_s1 = self.alpha_t[t], self.alpha_t[s1]\n        sigma_t, sigma_s1 = self.sigma_t[t], self.sigma_t[s1]\n        h, h_0 = lambda_t - lambda_s1, lambda_s0 - lambda_s1\n        r0 = h_0 / h\n        D0, D1 = m1, (1.0 / r0) * (m0 - m1)\n        if self.config.algorithm_type == \"dpmsolver++\":\n            # See https://arxiv.org/abs/2211.01095 for detailed derivations\n            ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 346, "column": 4 },
          "end": { "row": 346, "column": 4 }
        }
      }
    }
  ],
  [
    "1468",
    {
      "pageContent": "def singlestep_dpm_solver_third_order_update(\n        self,\n        model_output_list: List[torch.FloatTensor],\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the third-order singlestep DPM-Solver.\n\n        It computes the solution at time `prev_timestep` from the time `timestep_list[-3]`.\n\n        Args:\n            model_output_list (`List[torch.FloatTensor]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        t, s0, s1, s2 = prev_timestep, timestep_list[-1], timestep_list[-2], timestep_list[-3]\n        m0, m1, m2 = model_output_list[-1], model_output_list[-2], model_output_list[-3]\n        lambda_t, lambda_s0, lambda_s1, lambda_s2 = (\n            self.lambda_t[t],\n            self.lambda_t[s0],\n            self.lambda_t[s1],\n            self.lambda_t[s2],\n        )\n        alpha_t, alpha_s2 = self.alpha_t[t], self.alpha_t[s2]\n        sigma_t, sigma_s2 = self.sigma_t[t], self.sigma_t[s2]\n        h, h_0, h_1 = lambda_t - lambda_s2, lambda_s0 - lambda_s2, lambda_s1 - lambda_s2\n        r0, r1 = h_0 / h, h_1 / h\n        ",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 407, "column": 4 },
          "end": { "row": 407, "column": 4 }
        }
      }
    }
  ],
  [
    "1469",
    {
      "pageContent": "def singlestep_dpm_solver_update(\n        self,\n        model_output_list: List[torch.FloatTensor],\n        timestep_list: List[int],\n        prev_timestep: int,\n        sample: torch.FloatTensor,\n        order: int,\n    ) -> torch.FloatTensor:\n        \"\"\"\n        One step for the singlestep DPM-Solver.\n\n        Args:\n            model_output_list (`List[torch.FloatTensor]`):\n                direct outputs from learned diffusion model at current and latter timesteps.\n            timestep (`int`): current and latter discrete timestep in the diffusion chain.\n            prev_timestep (`int`): previous discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            order (`int`):\n                the solver order at this step.\n\n        Returns:\n            `torch.FloatTensor`: the sample tensor at the previous timestep.\n        \"\"\"\n        if order == 1:\n            return self.dpm_solver_first_order_update(model_output_list[-1], timestep_list[-1], prev_timestep, sample)\n        elif order == 2:\n            return self.singlestep_dpm_solver_second_order_update(\n                model_output_list, timestep_list, prev_timestep, sample\n            )\n        elif order == 3:\n            return self.singlestep_dpm_solver_third_order_update(\n                model_output_list, timestep_list, prev_timestep, sample\n            )\n        else:\n            raise ValueError(f\"Order must be 1, 2, 3, got {order}\")",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 478, "column": 4 },
          "end": { "row": 478, "column": 4 }
        }
      }
    }
  ],
  [
    "1470",
    {
      "pageContent": "def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: int,\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[SchedulerOutput, Tuple]:\n        \"\"\"\n        Step function propagating the sample with the singlestep DPM-Solver.\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                current instance of sample being created by diffusion process.\n            return_dict (`bool`): option for returning tuple rather than SchedulerOutput class\n\n        Returns:\n            [`~scheduling_utils.SchedulerOutput`] or `tuple`: [`~scheduling_utils.SchedulerOutput`] if `return_dict` is\n            True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n            )\n\n        if isinstance(timestep, torch.Tensor):\n            timestep = timestep.to(self.timesteps.device)\n        step_index = (self.timesteps == timestep).nonzero()\n        if len(step_index) == 0:\n            step_index = len(self.timesteps) - 1\n        else:\n            step_index = step_index.item()\n        prev_timestep = 0 if step_index == len(self.timesteps) - 1 else self.timesteps[step_index + 1]\n\n        model_output = self.convert_model_o",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 515, "column": 4 },
          "end": { "row": 515, "column": 4 }
        }
      }
    }
  ],
  [
    "1471",
    {
      "pageContent": "def scale_model_input(self, sample: torch.FloatTensor, *args, **kwargs) -> torch.FloatTensor:\n        \"\"\"\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n        current timestep.\n\n        Args:\n            sample (`torch.FloatTensor`): input sample\n\n        Returns:\n            `torch.FloatTensor`: scaled input sample\n        \"\"\"\n        return sample",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 571, "column": 4 },
          "end": { "row": 571, "column": 4 }
        }
      }
    }
  ],
  [
    "1472",
    {
      "pageContent": "def add_noise(\n        self,\n        original_samples: torch.FloatTensor,\n        noise: torch.FloatTensor,\n        timesteps: torch.IntTensor,\n    ) -> torch.FloatTensor:\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        return noisy_samples",
      "metadata": {
        "source": "src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py",
        "range": {
          "start": { "row": 584, "column": 4 },
          "end": { "row": 584, "column": 4 }
        }
      }
    }
  ],
  [
    "1473",
    {
      "pageContent": "def import_flax_or_no_model(module, class_name):\n    try:\n        # 1. First make sure that if a Flax object is present, import this one\n        class_obj = getattr(module, \"Flax\" + class_name)\n    except AttributeError:\n        # 2. If this doesn't work, it's not a model and we don't append \"Flax\"\n        class_obj = getattr(module, class_name)\n    except AttributeError:\n        raise ValueError(f\"Neither Flax{class_name} nor {class_name} exist in {module}\")\n\n    return class_obj",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_flax_utils.py",
        "range": {
          "start": { "row": 65, "column": 0 },
          "end": { "row": 65, "column": 0 }
        }
      }
    }
  ],
  [
    "1474",
    {
      "pageContent": "class FlaxImagePipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for image pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_flax_utils.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "1475",
    {
      "pageContent": "class FlaxDiffusionPipeline(ConfigMixin):\n    r\"\"\"\n    Base class for all models.\n\n    [`FlaxDiffusionPipeline`] takes care of storing all components (models, schedulers, processors) for diffusion\n    pipelines and handles methods for loading, downloading and saving models as well as a few methods common to all\n    pipelines to:\n\n        - enabling/disabling the progress bar for the denoising iteration\n\n    Class attributes:\n\n        - **config_name** ([`str`]) -- name of the config file that will store the class and module names of all\n          components of the diffusion pipeline.\n    \"\"\"\n    config_name = \"model_index.json\"\n\n    def register_modules(self, **kwargs):\n        # import it here to avoid circular import\n        from diffusers import pipelines\n\n        for name, module in kwargs.items():\n            if module is None:\n                register_dict = {name: (None, None)}\n            else:\n                # retrieve library\n                library = module.__module__.split(\".\")[0]\n\n                # check if the module is a pipeline module\n                pipeline_dir = module.__module__.split(\".\")[-2]\n                path = module.__module__.split(\".\")\n                is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipeline_dir)\n\n                # if library is not in LOADABLE_CLASSES, then it is a custom module.\n                # Or if it's a pipeline module, then the module is inside the pipeline\n                # folder so we set the library to module name.\n                if library not in LOADABLE_CLASSES or is_pipeline_module:\n          ",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_flax_utils.py",
        "range": {
          "start": { "row": 92, "column": 0 },
          "end": { "row": 92, "column": 0 }
        }
      }
    }
  ],
  [
    "1476",
    {
      "pageContent": "def register_modules(self, **kwargs):\n        # import it here to avoid circular import\n        from diffusers import pipelines\n\n        for name, module in kwargs.items():\n            if module is None:\n                register_dict = {name: (None, None)}\n            else:\n                # retrieve library\n                library = module.__module__.split(\".\")[0]\n\n                # check if the module is a pipeline module\n                pipeline_dir = module.__module__.split(\".\")[-2]\n                path = module.__module__.split(\".\")\n                is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipeline_dir)\n\n                # if library is not in LOADABLE_CLASSES, then it is a custom module.\n                # Or if it's a pipeline module, then the module is inside the pipeline\n                # folder so we set the library to module name.\n                if library not in LOADABLE_CLASSES or is_pipeline_module:\n                    library = pipeline_dir\n\n                # retrieve class_name\n                class_name = module.__class__.__name__\n\n                register_dict = {name: (library, class_name)}\n\n            # save model index config\n            self.register_to_config(**register_dict)\n\n            # set models\n            setattr(self, name, module)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_flax_utils.py",
        "range": {
          "start": { "row": 109, "column": 4 },
          "end": { "row": 109, "column": 4 }
        }
      }
    }
  ],
  [
    "1477",
    {
      "pageContent": "def save_pretrained(self, save_directory: Union[str, os.PathLike], params: Union[Dict, FrozenDict]):\n        # TODO: handle inference_state\n        \"\"\"\n        Save all variables of the pipeline that can be saved and loaded as well as the pipelines configuration file to\n        a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading\n        method. The pipeline can easily be re-loaded using the `[`~FlaxDiffusionPipeline.from_pretrained`]` class\n        method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n        \"\"\"\n        self.save_config(save_directory)\n\n        model_index_dict = dict(self.config)\n        model_index_dict.pop(\"_class_name\")\n        model_index_dict.pop(\"_diffusers_version\")\n        model_index_dict.pop(\"_module\", None)\n\n        for pipeline_component_name in model_index_dict.keys():\n            sub_model = getattr(self, pipeline_component_name)\n            if sub_model is None:\n                # edge case for saving a pipeline with safety_checker=None\n                continue\n\n            model_cls = sub_model.__class__\n\n            save_method_name = None\n            # search for the model's base class in LOADABLE_CLASSES\n            for library_name, library_classes in LOADABLE_CLASSES.items():\n                library = importlib.import_module(library_name)\n                for base_class, save_load_methods in library_classes.items():\n                    class_candidate = getattr(library, base_cla",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_flax_utils.py",
        "range": {
          "start": { "row": 142, "column": 4 },
          "end": { "row": 142, "column": 4 }
        }
      }
    }
  ],
  [
    "1478",
    {
      "pageContent": "def progress_bar(self, iterable):\n        if not hasattr(self, \"_progress_bar_config\"):\n            self._progress_bar_config = {}\n        elif not isinstance(self._progress_bar_config, dict):\n            raise ValueError(\n                f\"`self._progress_bar_config` should be of type `dict`, but is {type(self._progress_bar_config)}.\"\n            )\n\n        return tqdm(iterable, **self._progress_bar_config)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_flax_utils.py",
        "range": {
          "start": { "row": 537, "column": 4 },
          "end": { "row": 537, "column": 4 }
        }
      }
    }
  ],
  [
    "1479",
    {
      "pageContent": "class DDIMPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n            [`DDPMScheduler`], or [`DDIMScheduler`].\n    \"\"\"\n\n    def __init__(self, unet, scheduler):\n        super().__init__()\n\n        # make sure scheduler can always be converted to DDIM\n        scheduler = DDIMScheduler.from_config(scheduler.config)\n\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        eta: float = 0.0,\n        num_inference_steps: int = 50,\n        use_clipped_model_output: Optional[bool] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation dete",
      "metadata": {
        "source": "src/diffusers/pipelines/ddim/pipeline_ddim.py",
        "range": {
          "start": { "row": 23, "column": 0 },
          "end": { "row": 23, "column": 0 }
        }
      }
    }
  ],
  [
    "1480",
    {
      "pageContent": "def __init__(self, unet, scheduler):\n        super().__init__()\n\n        # make sure scheduler can always be converted to DDIM\n        scheduler = DDIMScheduler.from_config(scheduler.config)\n\n        self.register_modules(unet=unet, scheduler=scheduler)",
      "metadata": {
        "source": "src/diffusers/pipelines/ddim/pipeline_ddim.py",
        "range": {
          "start": { "row": 35, "column": 4 },
          "end": { "row": 35, "column": 4 }
        }
      }
    }
  ],
  [
    "1481",
    {
      "pageContent": "def _preprocess_image(image: Union[List, PIL.Image.Image, torch.Tensor]):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/repaint/pipeline_repaint.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1482",
    {
      "pageContent": "def _preprocess_mask(mask: Union[List, PIL.Image.Image, torch.Tensor]):\n    if isinstance(mask, torch.Tensor):\n        return mask\n    elif isinstance(mask, PIL.Image.Image):\n        mask = [mask]\n\n    if isinstance(mask[0], PIL.Image.Image):\n        w, h = mask[0].size\n        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n        mask = [np.array(m.convert(\"L\").resize((w, h), resample=PIL_INTERPOLATION[\"nearest\"]))[None, :] for m in mask]\n        mask = np.concatenate(mask, axis=0)\n        mask = mask.astype(np.float32) / 255.0\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n    elif isinstance(mask[0], torch.Tensor):\n        mask = torch.cat(mask, dim=0)\n    return mask",
      "metadata": {
        "source": "src/diffusers/pipelines/repaint/pipeline_repaint.py",
        "range": {
          "start": { "row": 52, "column": 0 },
          "end": { "row": 52, "column": 0 }
        }
      }
    }
  ],
  [
    "1483",
    {
      "pageContent": "class RePaintPipeline(DiffusionPipeline):\n    unet: UNet2DModel\n    scheduler: RePaintScheduler\n\n    def __init__(self, unet, scheduler):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        image: Union[torch.Tensor, PIL.Image.Image],\n        mask_image: Union[torch.Tensor, PIL.Image.Image],\n        num_inference_steps: int = 250,\n        eta: float = 0.0,\n        jump_length: int = 10,\n        jump_n_sample: int = 10,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            image (`torch.FloatTensor` or `PIL.Image.Image`):\n                The original image to inpaint on.\n            mask_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                The mask_image where 0.0 values define which part of the original image to inpaint (change).\n            num_inference_steps (`int`, *optional*, defaults to 1000):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            eta (`float`):\n                The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM\n                and 1.0 is DDPM scheduler respectively.\n            jump_length (`int`, *optional*, defaults to 10):\n                The number of steps taken forward in time before",
      "metadata": {
        "source": "src/diffusers/pipelines/repaint/pipeline_repaint.py",
        "range": {
          "start": { "row": 72, "column": 0 },
          "end": { "row": 72, "column": 0 }
        }
      }
    }
  ],
  [
    "1484",
    {
      "pageContent": "class LDMPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqvae ([`VQModel`]):\n            Vector-quantized (VQ) Model to encode and decode images to and from latent representations.\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            [`DDIMScheduler`] is to be used in combination with `unet` to denoise the encoded image latents.\n    \"\"\"\n\n    def __init__(self, vqvae: VQModel, unet: UNet2DModel, scheduler: DDIMScheduler):\n        super().__init__()\n        self.register_modules(vqvae=vqvae, unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        eta: float = 0.0,\n        num_inference_steps: int = 50,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[Tuple, ImagePipelineOutput]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                Number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            n",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion_uncond/pipeline_latent_diffusion_uncond.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1485",
    {
      "pageContent": "class TransformationModelOutput(ModelOutput):\n    \"\"\"\n    Base class for text model's outputs that also contains a pooling of the last hidden states.\n\n    Args:\n        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n            The text embeddings obtained by applying the projection layer to the pooler_output.\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    projection_state: Optional[torch.FloatTensor] = None\n    last_h",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py",
        "range": {
          "start": { "row": 10, "column": 0 },
          "end": { "row": 10, "column": 0 }
        }
      }
    }
  ],
  [
    "1486",
    {
      "pageContent": "class RobertaSeriesConfig(XLMRobertaConfig):\n    def __init__(\n        self,\n        pad_token_id=1,\n        bos_token_id=0,\n        eos_token_id=2,\n        project_dim=512,\n        pooler_fn=\"cls\",\n        learn_encoder=False,\n        use_attention_mask=True,\n        **kwargs,\n    ):\n        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n        self.project_dim = project_dim\n        self.pooler_fn = pooler_fn\n        self.learn_encoder = learn_encoder\n        self.use_attention_mask = use_attention_mask",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "1487",
    {
      "pageContent": "def __init__(\n        self,\n        pad_token_id=1,\n        bos_token_id=0,\n        eos_token_id=2,\n        project_dim=512,\n        pooler_fn=\"cls\",\n        learn_encoder=False,\n        use_attention_mask=True,\n        **kwargs,\n    ):\n        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n        self.project_dim = project_dim\n        self.pooler_fn = pooler_fn\n        self.learn_encoder = learn_encoder\n        self.use_attention_mask = use_attention_mask",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "1488",
    {
      "pageContent": "class RobertaSeriesModelWithTransformation(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n    base_model_prefix = \"roberta\"\n    config_class = RobertaSeriesConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.roberta = XLMRobertaModel(config)\n        self.transformation = nn.Linear(config.hidden_size, config.project_dim)\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n    ):\n        r\"\"\" \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=enc",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "1489",
    {
      "pageContent": "def __init__(self, config):\n        super().__init__(config)\n        self.roberta = XLMRobertaModel(config)\n        self.transformation = nn.Linear(config.hidden_size, config.project_dim)\n        self.post_init()",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py",
        "range": {
          "start": { "row": 63, "column": 4 },
          "end": { "row": 63, "column": 4 }
        }
      }
    }
  ],
  [
    "1490",
    {
      "pageContent": "def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n    ):\n        r\"\"\" \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        projection_state = self.transformation(outputs.last_hidden_state)\n\n        return TransformationModelOutput(\n            projection_state=projection_state,\n            last_hidden_state=outputs.last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py",
        "range": {
          "start": { "row": 69, "column": 4 },
          "end": { "row": 69, "column": 4 }
        }
      }
    }
  ],
  [
    "1491",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 67, "column": 0 },
          "end": { "row": 67, "column": 0 }
        }
      }
    }
  ],
  [
    "1492",
    {
      "pageContent": "class AltDiffusionImg2ImgPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image to image generation using Alt Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`RobertaSeriesModelWithTransformation`]):\n            Frozen text-encoder. Alt Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.RobertaSeriesModelWithTransformation),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`XLMRobertaTokenizer`):\n            Tokenizer of class\n            [XLMRobertaTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.XLMRobertaTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could ",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 89, "column": 0 },
          "end": { "row": 89, "column": 0 }
        }
      }
    }
  ],
  [
    "1493",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: RobertaSeriesModelWithTransformation,\n        tokenizer: XLMRobertaTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `c",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 118, "column": 4 },
          "end": { "row": 118, "column": 4 }
        }
      }
    }
  ],
  [
    "1494",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 207, "column": 4 },
          "end": { "row": 207, "column": 4 }
        }
      }
    }
  ],
  [
    "1495",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 232, "column": 4 },
          "end": { "row": 232, "column": 4 }
        }
      }
    }
  ],
  [
    "1496",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 278, "column": 4 },
          "end": { "row": 278, "column": 4 }
        }
      }
    }
  ],
  [
    "1497",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 416, "column": 4 },
          "end": { "row": 416, "column": 4 }
        }
      }
    }
  ],
  [
    "1498",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 426, "column": 4 },
          "end": { "row": 426, "column": 4 }
        }
      }
    }
  ],
  [
    "1499",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 434, "column": 4 },
          "end": { "row": 434, "column": 4 }
        }
      }
    }
  ],
  [
    "1500",
    {
      "pageContent": "def check_inputs(\n        self, prompt, strength, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None\n    ):\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 451, "column": 4 },
          "end": { "row": 451, "column": 4 }
        }
      }
    }
  ],
  [
    "1501",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:]\n\n        return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 491, "column": 4 },
          "end": { "row": 491, "column": 4 }
        }
      }
    }
  ],
  [
    "1502",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n            )\n\n        image = image.to(device=device, dtype=dtype)\n\n        batch_size = batch_size * num_images_per_prompt\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            init_latents = [\n                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n            ]\n            init_latents = torch.cat(init_latents, dim=0)\n        else:\n            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n            # expand init_latents for batch_size\n            deprecation_message = (\n                f\"You have passed {batch_size} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n                \" images (`image`). Initial images are now duplicating to match the number of text prompts. ",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py",
        "range": {
          "start": { "row": 500, "column": 4 },
          "end": { "row": 500, "column": 4 }
        }
      }
    }
  ],
  [
    "1503",
    {
      "pageContent": "class AltDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Alt Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`RobertaSeriesModelWithTransformation`]):\n            Frozen text-encoder. Alt Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.RobertaSeriesModelWithTransformation),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`XLMRobertaTokenizer`):\n            Tokenizer of class\n            [XLMRobertaTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.XLMRobertaTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offens",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 51, "column": 0 },
          "end": { "row": 51, "column": 0 }
        }
      }
    }
  ],
  [
    "1504",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: RobertaSeriesModelWithTransformation,\n        tokenizer: XLMRobertaTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `c",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 80, "column": 4 },
          "end": { "row": 80, "column": 4 }
        }
      }
    }
  ],
  [
    "1505",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 169, "column": 4 },
          "end": { "row": 169, "column": 4 }
        }
      }
    }
  ],
  [
    "1506",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 178, "column": 4 },
          "end": { "row": 178, "column": 4 }
        }
      }
    }
  ],
  [
    "1507",
    {
      "pageContent": "def enable_vae_tiling(self):\n        r\"\"\"\n        Enable tiled VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor into tiles to compute decoding and encoding in\n        several steps. This is useful to save a large amount of memory and to allow the processing of larger images.\n        \"\"\"\n        self.vae.enable_tiling()",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 185, "column": 4 },
          "end": { "row": 185, "column": 4 }
        }
      }
    }
  ],
  [
    "1508",
    {
      "pageContent": "def disable_vae_tiling(self):\n        r\"\"\"\n        Disable tiled VAE decoding. If `enable_vae_tiling` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_tiling()",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 194, "column": 4 },
          "end": { "row": 194, "column": 4 }
        }
      }
    }
  ],
  [
    "1509",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 201, "column": 4 },
          "end": { "row": 201, "column": 4 }
        }
      }
    }
  ],
  [
    "1510",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "1511",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 272, "column": 4 },
          "end": { "row": 272, "column": 4 }
        }
      }
    }
  ],
  [
    "1512",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 410, "column": 4 },
          "end": { "row": 410, "column": 4 }
        }
      }
    }
  ],
  [
    "1513",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 420, "column": 4 },
          "end": { "row": 420, "column": 4 }
        }
      }
    }
  ],
  [
    "1514",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 428, "column": 4 },
          "end": { "row": 428, "column": 4 }
        }
      }
    }
  ],
  [
    "1515",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 445, "column": 4 },
          "end": { "row": 445, "column": 4 }
        }
      }
    }
  ],
  [
    "1516",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py",
        "range": {
          "start": { "row": 492, "column": 4 },
          "end": { "row": 492, "column": 4 }
        }
      }
    }
  ],
  [
    "1517",
    {
      "pageContent": "class AltDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Alt Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n        nsfw_content_detected (`List[bool]`)\n            List of flags denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, or `None` if safety checking could not be performed.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]",
      "metadata": {
        "source": "src/diffusers/pipelines/alt_diffusion/__init__.py",
        "range": {
          "start": { "row": 12, "column": 0 },
          "end": { "row": 12, "column": 0 }
        }
      }
    }
  ],
  [
    "1518",
    {
      "pageContent": "class ImagePipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for image pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 108, "column": 0 },
          "end": { "row": 108, "column": 0 }
        }
      }
    }
  ],
  [
    "1519",
    {
      "pageContent": "class AudioPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for audio pipelines.\n\n    Args:\n        audios (`np.ndarray`)\n            List of denoised samples of shape `(batch_size, num_channels, sample_rate)`. Numpy array present the\n            denoised audio samples of the diffusion pipeline.\n    \"\"\"\n\n    audios: np.ndarray",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 122, "column": 0 },
          "end": { "row": 122, "column": 0 }
        }
      }
    }
  ],
  [
    "1520",
    {
      "pageContent": "def is_safetensors_compatible(filenames, variant=None) -> bool:\n    \"\"\"\n    Checking for safetensors compatibility:\n    - By default, all models are saved with the default pytorch serialization, so we use the list of default pytorch\n      files to know which safetensors files are needed.\n    - The model is safetensors compatible only if there is a matching safetensors file for every default pytorch file.\n\n    Converting default pytorch serialized filenames to safetensors serialized filenames:\n    - For models from the diffusers library, just replace the \".bin\" extension with \".safetensors\"\n    - For models from the transformers library, the filename changes from \"pytorch_model\" to \"model\", and the \".bin\"\n      extension is replaced with \".safetensors\"\n    \"\"\"\n    pt_filenames = []\n\n    sf_filenames = set()\n\n    for filename in filenames:\n        _, extension = os.path.splitext(filename)\n\n        if extension == \".bin\":\n            pt_filenames.append(filename)\n        elif extension == \".safetensors\":\n            sf_filenames.add(filename)\n\n    for filename in pt_filenames:\n        #  filename = 'foo/bar/baz.bam' -> path = 'foo/bar', filename = 'baz', extention = '.bam'\n        path, filename = os.path.split(filename)\n        filename, extension = os.path.splitext(filename)\n\n        if filename == \"pytorch_model\":\n            filename = \"model\"\n        elif filename == f\"pytorch_model.{variant}\":\n            filename = f\"model.{variant}\"\n        else:\n            filename = filename\n\n        expected_sf_filename = os.path.join(path, filename)\n        expected_sf_filename = ",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 135, "column": 0 },
          "end": { "row": 135, "column": 0 }
        }
      }
    }
  ],
  [
    "1521",
    {
      "pageContent": "def variant_compatible_siblings(filenames, variant=None) -> Union[List[os.PathLike], str]:\n    weight_names = [\n        WEIGHTS_NAME,\n        SAFETENSORS_WEIGHTS_NAME,\n        FLAX_WEIGHTS_NAME,\n        ONNX_WEIGHTS_NAME,\n        ONNX_EXTERNAL_WEIGHTS_NAME,\n    ]\n\n    if is_transformers_available():\n        weight_names += [TRANSFORMERS_WEIGHTS_NAME, TRANSFORMERS_SAFE_WEIGHTS_NAME, TRANSFORMERS_FLAX_WEIGHTS_NAME]\n\n    # model_pytorch, diffusion_model_pytorch, ...\n    weight_prefixes = [w.split(\".\")[0] for w in weight_names]\n    # .bin, .safetensors, ...\n    weight_suffixs = [w.split(\".\")[-1] for w in weight_names]\n\n    variant_file_regex = (\n        re.compile(f\"({'|'.join(weight_prefixes)})(.{variant}.)({'|'.join(weight_suffixs)})\")\n        if variant is not None\n        else None\n    )\n    non_variant_file_regex = re.compile(f\"{'|'.join(weight_names)}\")\n\n    if variant is not None:\n        variant_filenames = set(f for f in filenames if variant_file_regex.match(f.split(\"/\")[-1]) is not None)\n    else:\n        variant_filenames = set()\n\n    non_variant_filenames = set(f for f in filenames if non_variant_file_regex.match(f.split(\"/\")[-1]) is not None)\n\n    usable_filenames = set(variant_filenames)\n    for f in non_variant_filenames:\n        variant_filename = f\"{f.split('.')[0]}.{variant}.{f.split('.')[1]}\"\n        if variant_filename not in usable_filenames:\n            usable_filenames.add(f)\n\n    return usable_filenames, variant_filenames",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 181, "column": 0 },
          "end": { "row": 181, "column": 0 }
        }
      }
    }
  ],
  [
    "1522",
    {
      "pageContent": "def warn_deprecated_model_variant(pretrained_model_name_or_path, use_auth_token, variant, revision, model_filenames):\n    info = model_info(\n        pretrained_model_name_or_path,\n        use_auth_token=use_auth_token,\n        revision=None,\n    )\n    filenames = set(sibling.rfilename for sibling in info.siblings)\n    comp_model_filenames, _ = variant_compatible_siblings(filenames, variant=revision)\n    comp_model_filenames = [\".\".join(f.split(\".\")[:1] + f.split(\".\")[2:]) for f in comp_model_filenames]\n\n    if set(comp_model_filenames) == set(model_filenames):\n        warnings.warn(\n            f\"You are loading the variant {revision} from {pretrained_model_name_or_path} via `revision='{revision}'` even though you can load it via `variant=`{revision}`. Loading model variants via `revision='{revision}'` is deprecated and will be removed in diffusers v1. Please use `variant='{revision}'` instead.\",\n            FutureWarning,\n        )\n    else:\n        warnings.warn(\n            f\"You are loading the variant {revision} from {pretrained_model_name_or_path} via `revision='{revision}'`. This behavior is deprecated and will be removed in diffusers v1. One should use `variant='{revision}'` instead. However, it appears that {pretrained_model_name_or_path} currently does not have the required variant filenames in the 'main' branch. \\n The Diffusers team and community would be very grateful if you could open an issue: https://github.com/huggingface/diffusers/issues/new with the title '{pretrained_model_name_or_path} is missing {revision} files' so that the correct variant file can be",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 221, "column": 0 },
          "end": { "row": 221, "column": 0 }
        }
      }
    }
  ],
  [
    "1523",
    {
      "pageContent": "def maybe_raise_or_warn(\n    library_name, library, class_name, importable_classes, passed_class_obj, name, is_pipeline_module\n):\n    \"\"\"Simple helper method to raise or warn in case incorrect module has been passed\"\"\"\n    if not is_pipeline_module:\n        library = importlib.import_module(library_name)\n        class_obj = getattr(library, class_name)\n        class_candidates = {c: getattr(library, c, None) for c in importable_classes.keys()}\n\n        expected_class_obj = None\n        for class_name, class_candidate in class_candidates.items():\n            if class_candidate is not None and issubclass(class_obj, class_candidate):\n                expected_class_obj = class_candidate\n\n        if not issubclass(passed_class_obj[name].__class__, expected_class_obj):\n            raise ValueError(\n                f\"{passed_class_obj[name]} is of type: {type(passed_class_obj[name])}, but should be\"\n                f\" {expected_class_obj}\"\n            )\n    else:\n        logger.warning(\n            f\"You have passed a non-standard module {passed_class_obj[name]}. We cannot verify whether it\"\n            \" has the correct type\"\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 243, "column": 0 },
          "end": { "row": 243, "column": 0 }
        }
      }
    }
  ],
  [
    "1524",
    {
      "pageContent": "def get_class_obj_and_candidates(library_name, class_name, importable_classes, pipelines, is_pipeline_module):\n    \"\"\"Simple helper method to retrieve class object of module as well as potential parent class objects\"\"\"\n    if is_pipeline_module:\n        pipeline_module = getattr(pipelines, library_name)\n\n        class_obj = getattr(pipeline_module, class_name)\n        class_candidates = {c: class_obj for c in importable_classes.keys()}\n    else:\n        # else we just import it from the library.\n        library = importlib.import_module(library_name)\n\n        class_obj = getattr(library, class_name)\n        class_candidates = {c: getattr(library, c, None) for c in importable_classes.keys()}\n\n    return class_obj, class_candidates",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 269, "column": 0 },
          "end": { "row": 269, "column": 0 }
        }
      }
    }
  ],
  [
    "1525",
    {
      "pageContent": "def load_sub_model(\n    library_name: str,\n    class_name: str,\n    importable_classes: List[Any],\n    pipelines: Any,\n    is_pipeline_module: bool,\n    pipeline_class: Any,\n    torch_dtype: torch.dtype,\n    provider: Any,\n    sess_options: Any,\n    device_map: Optional[Union[Dict[str, torch.device], str]],\n    model_variants: Dict[str, str],\n    name: str,\n    from_flax: bool,\n    variant: str,\n    low_cpu_mem_usage: bool,\n    cached_folder: Union[str, os.PathLike],\n):\n    \"\"\"Helper method to load the module `name` from `library_name` and `class_name`\"\"\"\n    # retrieve class candidates\n    class_obj, class_candidates = get_class_obj_and_candidates(\n        library_name, class_name, importable_classes, pipelines, is_pipeline_module\n    )\n\n    load_method_name = None\n    # retrive load method name\n    for class_name, class_candidate in class_candidates.items():\n        if class_candidate is not None and issubclass(class_obj, class_candidate):\n            load_method_name = importable_classes[class_name][1]\n\n    # if load method name is None, then we have a dummy module -> raise Error\n    if load_method_name is None:\n        none_module = class_obj.__module__\n        is_dummy_path = none_module.startswith(DUMMY_MODULES_FOLDER) or none_module.startswith(\n            TRANSFORMERS_DUMMY_MODULES_FOLDER\n        )\n        if is_dummy_path and \"dummy\" in none_module:\n            # call class_obj for nice error message of missing requirements\n            class_obj()\n\n        raise ValueError(\n            f\"The component {class_obj} of {pipeline_class} cannot be loaded as it does not ",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 286, "column": 0 },
          "end": { "row": 286, "column": 0 }
        }
      }
    }
  ],
  [
    "1526",
    {
      "pageContent": "class DiffusionPipeline(ConfigMixin):\n    r\"\"\"\n    Base class for all models.\n\n    [`DiffusionPipeline`] takes care of storing all components (models, schedulers, processors) for diffusion pipelines\n    and handles methods for loading, downloading and saving models as well as a few methods common to all pipelines to:\n\n        - move all PyTorch modules to the device of your choice\n        - enabling/disabling the progress bar for the denoising iteration\n\n    Class attributes:\n\n        - **config_name** (`str`) -- name of the config file that will store the class and module names of all\n          components of the diffusion pipeline.\n        - **_optional_components** (List[`str`]) -- list of all components that are optional so they don't have to be\n          passed for the pipeline to function (should be overridden by subclasses).\n    \"\"\"\n    config_name = \"model_index.json\"\n    _optional_components = []\n\n    def register_modules(self, **kwargs):\n        # import it here to avoid circular import\n        from diffusers import pipelines\n\n        for name, module in kwargs.items():\n            # retrieve library\n            if module is None:\n                register_dict = {name: (None, None)}\n            else:\n                library = module.__module__.split(\".\")[0]\n\n                # check if the module is a pipeline module\n                pipeline_dir = module.__module__.split(\".\")[-2] if len(module.__module__.split(\".\")) > 2 else None\n                path = module.__module__.split(\".\")\n                is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipel",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 392, "column": 0 },
          "end": { "row": 392, "column": 0 }
        }
      }
    }
  ],
  [
    "1527",
    {
      "pageContent": "def register_modules(self, **kwargs):\n        # import it here to avoid circular import\n        from diffusers import pipelines\n\n        for name, module in kwargs.items():\n            # retrieve library\n            if module is None:\n                register_dict = {name: (None, None)}\n            else:\n                library = module.__module__.split(\".\")[0]\n\n                # check if the module is a pipeline module\n                pipeline_dir = module.__module__.split(\".\")[-2] if len(module.__module__.split(\".\")) > 2 else None\n                path = module.__module__.split(\".\")\n                is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipeline_dir)\n\n                # if library is not in LOADABLE_CLASSES, then it is a custom module.\n                # Or if it's a pipeline module, then the module is inside the pipeline\n                # folder so we set the library to module name.\n                if library not in LOADABLE_CLASSES or is_pipeline_module:\n                    library = pipeline_dir\n\n                # retrieve class_name\n                class_name = module.__class__.__name__\n\n                register_dict = {name: (library, class_name)}\n\n            # save model index config\n            self.register_to_config(**register_dict)\n\n            # set models\n            setattr(self, name, module)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 412, "column": 4 },
          "end": { "row": 412, "column": 4 }
        }
      }
    }
  ],
  [
    "1528",
    {
      "pageContent": "def save_pretrained(\n        self,\n        save_directory: Union[str, os.PathLike],\n        safe_serialization: bool = False,\n        variant: Optional[str] = None,\n    ):\n        \"\"\"\n        Save all variables of the pipeline that can be saved and loaded as well as the pipelines configuration file to\n        a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading\n        method. The pipeline can easily be re-loaded using the `[`~DiffusionPipeline.from_pretrained`]` class method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n            safe_serialization (`bool`, *optional*, defaults to `False`):\n                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n            variant (`str`, *optional*):\n                If specified, weights are saved in the format pytorch_model.<variant>.bin.\n        \"\"\"\n        self.save_config(save_directory)\n\n        model_index_dict = dict(self.config)\n        model_index_dict.pop(\"_class_name\")\n        model_index_dict.pop(\"_diffusers_version\")\n        model_index_dict.pop(\"_module\", None)\n\n        expected_modules, optional_kwargs = self._get_signature_keys(self)\n\n        def is_saveable_module(name, value):\n            if name not in expected_modules:\n                return False\n            if name in self._optional_components and value[0] is None:\n                return False\n            return True\n\n        model_index_dict = {k:",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 445, "column": 4 },
          "end": { "row": 445, "column": 4 }
        }
      }
    }
  ],
  [
    "1529",
    {
      "pageContent": "def to(self, torch_device: Optional[Union[str, torch.device]] = None, silence_dtype_warnings: bool = False):\n        if torch_device is None:\n            return self\n\n        # throw warning if pipeline is in \"offloaded\"-mode but user tries to manually set to GPU.\n        def module_is_sequentially_offloaded(module):\n            if not is_accelerate_available() or is_accelerate_version(\"<\", \"0.14.0\"):\n                return False\n\n            return hasattr(module, \"_hf_hook\") and not isinstance(module._hf_hook, accelerate.hooks.CpuOffload)\n\n        def module_is_offloaded(module):\n            if not is_accelerate_available() or is_accelerate_version(\"<\", \"0.17.0.dev0\"):\n                return False\n\n            return hasattr(module, \"_hf_hook\") and isinstance(module._hf_hook, accelerate.hooks.CpuOffload)\n\n        # .to(\"cuda\") would raise an error if the pipeline is sequentially offloaded, so we raise our own to make it clearer\n        pipeline_is_sequentially_offloaded = any(\n            module_is_sequentially_offloaded(module) for _, module in self.components.items()\n        )\n        if pipeline_is_sequentially_offloaded and torch.device(torch_device).type == \"cuda\":\n            raise ValueError(\n                \"It seems like you have activated sequential model offloading by calling `enable_sequential_cpu_offload`, but are now attempting to move the pipeline to GPU. This is not compatible with offloading. Please, move your pipeline `.to('cpu')` or consider removing the move altogether if you use sequential offloading.\"\n            )\n\n        # Display a warning in thi",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 514, "column": 4 },
          "end": { "row": 514, "column": 4 }
        }
      }
    }
  ],
  [
    "1530",
    {
      "pageContent": "def progress_bar(self, iterable=None, total=None):\n        if not hasattr(self, \"_progress_bar_config\"):\n            self._progress_bar_config = {}\n        elif not isinstance(self._progress_bar_config, dict):\n            raise ValueError(\n                f\"`self._progress_bar_config` should be of type `dict`, but is {type(self._progress_bar_config)}.\"\n            )\n\n        if iterable is not None:\n            return tqdm(iterable, **self._progress_bar_config)\n        elif total is not None:\n            return tqdm(total=total, **self._progress_bar_config)\n        else:\n            raise ValueError(\"Either `total` or `iterable` has to be defined.\")",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 1244, "column": 4 },
          "end": { "row": 1244, "column": 4 }
        }
      }
    }
  ],
  [
    "1531",
    {
      "pageContent": "def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):\n        r\"\"\"\n        Enable memory efficient attention as implemented in xformers.\n\n        When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n        time. Speed up at training time is not guaranteed.\n\n        Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n        is used.\n\n        Parameters:\n            attention_op (`Callable`, *optional*):\n                Override the default `None` operator for use as `op` argument to the\n                [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n                function of xFormers.\n\n        Examples:\n\n        ```py\n        >>> import torch\n        >>> from diffusers import DiffusionPipeline\n        >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n        >>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n        >>> pipe = pipe.to(\"cuda\")\n        >>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n        >>> # Workaround for not accepting attention shape using VAE for Flash Attention\n        >>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n        ```\n        \"\"\"\n        self.set_use_memory_efficient_attention_xformers(True, attention_op)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 1262, "column": 4 },
          "end": { "row": 1262, "column": 4 }
        }
      }
    }
  ],
  [
    "1532",
    {
      "pageContent": "def disable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Disable memory efficient attention as implemented in xformers.\n        \"\"\"\n        self.set_use_memory_efficient_attention_xformers(False)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 1294, "column": 4 },
          "end": { "row": 1294, "column": 4 }
        }
      }
    }
  ],
  [
    "1533",
    {
      "pageContent": "def set_use_memory_efficient_attention_xformers(\n        self, valid: bool, attention_op: Optional[Callable] = None\n    ) -> None:\n        # Recursively walk through all the children.\n        # Any children which exposes the set_use_memory_efficient_attention_xformers method\n        # gets the message\n        def fn_recursive_set_mem_eff(module: torch.nn.Module):\n            if hasattr(module, \"set_use_memory_efficient_attention_xformers\"):\n                module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n\n            for child in module.children():\n                fn_recursive_set_mem_eff(child)\n\n        module_names, _, _ = self.extract_init_dict(dict(self.config))\n        for module_name in module_names:\n            module = getattr(self, module_name)\n            if isinstance(module, torch.nn.Module):\n                fn_recursive_set_mem_eff(module)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 1300, "column": 4 },
          "end": { "row": 1300, "column": 4 }
        }
      }
    }
  ],
  [
    "1534",
    {
      "pageContent": "def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                `\"max\"`, maxium amount of memory will be saved by running only one slice at a time. If a number is\n                provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n                must be a multiple of `slice_size`.\n        \"\"\"\n        self.set_attention_slice(slice_size)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 1319, "column": 4 },
          "end": { "row": 1319, "column": 4 }
        }
      }
    }
  ],
  [
    "1535",
    {
      "pageContent": "def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 1335, "column": 4 },
          "end": { "row": 1335, "column": 4 }
        }
      }
    }
  ],
  [
    "1536",
    {
      "pageContent": "def set_attention_slice(self, slice_size: Optional[int]):\n        module_names, _, _ = self.extract_init_dict(dict(self.config))\n        for module_name in module_names:\n            module = getattr(self, module_name)\n            if isinstance(module, torch.nn.Module) and hasattr(module, \"set_attention_slice\"):\n                module.set_attention_slice(slice_size)",
      "metadata": {
        "source": "src/diffusers/pipelines/pipeline_utils.py",
        "range": {
          "start": { "row": 1343, "column": 4 },
          "end": { "row": 1343, "column": 4 }
        }
      }
    }
  ],
  [
    "1537",
    {
      "pageContent": "class LearnedClassifierFreeSamplingEmbeddings(ModelMixin, ConfigMixin):\n    \"\"\"\n    Utility class for storing learned text embeddings for classifier free sampling\n    \"\"\"\n\n    @register_to_config\n    def __init__(self, learnable: bool, hidden_size: Optional[int] = None, length: Optional[int] = None):\n        super().__init__()\n\n        self.learnable = learnable\n\n        if self.learnable:\n            assert hidden_size is not None, \"learnable=True requires `hidden_size` to be set\"\n            assert length is not None, \"learnable=True requires `length` to be set\"\n\n            embeddings = torch.zeros(length, hidden_size)\n        else:\n            embeddings = None\n\n        self.embeddings = torch.nn.Parameter(embeddings)",
      "metadata": {
        "source": "src/diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "1538",
    {
      "pageContent": "class VQDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using VQ Diffusion\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vqvae ([`VQModel`]):\n            Vector Quantized Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent\n            representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. VQ Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        transformer ([`Transformer2DModel`]):\n            Conditional transformer to denoise the encoded image latents.\n        scheduler ([`VQDiffusionScheduler`]):\n            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n    \"\"\"\n\n    vqvae: VQModel\n    text_encoder: CLIPTextModel\n    tokenizer: CLIPTokenizer\n    transformer: Transformer2DModel\n    learned_classifier_free_sampling_embeddings: LearnedClassifierFreeSamplingEmbeddings\n    scheduler: VQDiffusionScheduler\n\n    def __init__(\n     ",
      "metadata": {
        "source": "src/diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py",
        "range": {
          "start": { "row": 51, "column": 0 },
          "end": { "row": 51, "column": 0 }
        }
      }
    }
  ],
  [
    "1539",
    {
      "pageContent": "def __init__(\n        self,\n        vqvae: VQModel,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        transformer: Transformer2DModel,\n        scheduler: VQDiffusionScheduler,\n        learned_classifier_free_sampling_embeddings: LearnedClassifierFreeSamplingEmbeddings,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            vqvae=vqvae,\n            transformer=transformer,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            scheduler=scheduler,\n            learned_classifier_free_sampling_embeddings=learned_classifier_free_sampling_embeddings,\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py",
        "range": {
          "start": { "row": 82, "column": 4 },
          "end": { "row": 82, "column": 4 }
        }
      }
    }
  ],
  [
    "1540",
    {
      "pageContent": "def _encode_prompt(self, prompt, num_images_per_prompt, do_classifier_free_guidance):\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n\n        if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n            removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n            )\n            text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n        prompt_embeds = self.text_encoder(text_input_ids.to(self.device))[0]\n\n        # NOTE: This additional step of normalizing the text embeddings is from VQ-Diffusion.\n        # While CLIP does normalize the pooled output of the text transformer when combining\n        # the image and text embeddings, CLIP does not directly normalize the last hidden state.\n        #\n        # CLIP normalizing the pooled output.\n        # https://github.com/huggingface/transformers/blob/d92e22d1f28324f513f3080e5c47c071a3916721/src/transformers/models/clip/modeling_clip.py#L1052-L1053\n        prompt_embeds = prompt_embeds / prompt_embeds.norm(dim=-1, keepdim=True)\n\n        # duplicate ",
      "metadata": {
        "source": "src/diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py",
        "range": {
          "start": { "row": 102, "column": 4 },
          "end": { "row": 102, "column": 4 }
        }
      }
    }
  ],
  [
    "1541",
    {
      "pageContent": "def truncate(self, log_p_x_0: torch.FloatTensor, truncation_rate: float) -> torch.FloatTensor:\n        \"\"\"\n        Truncates log_p_x_0 such that for each column vector, the total cumulative probability is `truncation_rate` The\n        lowest probabilities that would increase the cumulative probability above `truncation_rate` are set to zero.\n        \"\"\"\n        sorted_log_p_x_0, indices = torch.sort(log_p_x_0, 1, descending=True)\n        sorted_p_x_0 = torch.exp(sorted_log_p_x_0)\n        keep_mask = sorted_p_x_0.cumsum(dim=1) < truncation_rate\n\n        # Ensure that at least the largest probability is not zeroed out\n        all_true = torch.full_like(keep_mask[:, 0:1, :], True)\n        keep_mask = torch.cat((all_true, keep_mask), dim=1)\n        keep_mask = keep_mask[:, :-1, :]\n\n        keep_mask = keep_mask.gather(1, indices.argsort(1))\n\n        rv = log_p_x_0.clone()\n\n        rv[~keep_mask] = -torch.inf  # -inf = log(0)\n\n        return rv",
      "metadata": {
        "source": "src/diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py",
        "range": {
          "start": { "row": 309, "column": 4 },
          "end": { "row": 309, "column": 4 }
        }
      }
    }
  ],
  [
    "1542",
    {
      "pageContent": "class ScoreSdeVePipeline(DiffusionPipeline):\n    r\"\"\"\n    Parameters:\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image. scheduler ([`SchedulerMixin`]):\n            The [`ScoreSdeVeScheduler`] scheduler to be used in combination with `unet` to denoise the encoded image.\n    \"\"\"\n    unet: UNet2DModel\n    scheduler: ScoreSdeVeScheduler\n\n    def __init__(self, unet: UNet2DModel, scheduler: DiffusionPipeline):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        num_inference_steps: int = 2000,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose betwe",
      "metadata": {
        "source": "src/diffusers/pipelines/score_sde_ve/pipeline_score_sde_ve.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1543",
    {
      "pageContent": "class OnnxRuntimeModel:\n    def __init__(self, model=None, **kwargs):\n        logger.info(\"`diffusers.OnnxRuntimeModel` is experimental and might change in the future.\")\n        self.model = model\n        self.model_save_dir = kwargs.get(\"model_save_dir\", None)\n        self.latest_model_name = kwargs.get(\"latest_model_name\", ONNX_WEIGHTS_NAME)\n\n    def __call__(self, **kwargs):\n        inputs = {k: np.array(v) for k, v in kwargs.items()}\n        return self.model.run(None, inputs)\n\n    @staticmethod\n    def load_model(path: Union[str, Path], provider=None, sess_options=None):\n        \"\"\"\n        Loads an ONNX Inference session with an ExecutionProvider. Default provider is `CPUExecutionProvider`\n\n        Arguments:\n            path (`str` or `Path`):\n                Directory from which to load\n            provider(`str`, *optional*):\n                Onnxruntime execution provider to use for loading the model, defaults to `CPUExecutionProvider`\n        \"\"\"\n        if provider is None:\n            logger.info(\"No onnxruntime provider specified, using CPUExecutionProvider\")\n            provider = \"CPUExecutionProvider\"\n\n        return ort.InferenceSession(path, providers=[provider], sess_options=sess_options)\n\n    def _save_pretrained(self, save_directory: Union[str, Path], file_name: Optional[str] = None, **kwargs):\n        \"\"\"\n        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n        [`~optimum.onnxruntime.modeling_ort.ORTModel.from_pretrained`] class method. It will always save the\n        latest_model_name.\n\n        Argu",
      "metadata": {
        "source": "src/diffusers/pipelines/onnx_utils.py",
        "range": {
          "start": { "row": 50, "column": 0 },
          "end": { "row": 50, "column": 0 }
        }
      }
    }
  ],
  [
    "1544",
    {
      "pageContent": "def __init__(self, model=None, **kwargs):\n        logger.info(\"`diffusers.OnnxRuntimeModel` is experimental and might change in the future.\")\n        self.model = model\n        self.model_save_dir = kwargs.get(\"model_save_dir\", None)\n        self.latest_model_name = kwargs.get(\"latest_model_name\", ONNX_WEIGHTS_NAME)",
      "metadata": {
        "source": "src/diffusers/pipelines/onnx_utils.py",
        "range": {
          "start": { "row": 51, "column": 4 },
          "end": { "row": 51, "column": 4 }
        }
      }
    }
  ],
  [
    "1545",
    {
      "pageContent": "def _save_pretrained(self, save_directory: Union[str, Path], file_name: Optional[str] = None, **kwargs):\n        \"\"\"\n        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n        [`~optimum.onnxruntime.modeling_ort.ORTModel.from_pretrained`] class method. It will always save the\n        latest_model_name.\n\n        Arguments:\n            save_directory (`str` or `Path`):\n                Directory where to save the model file.\n            file_name(`str`, *optional*):\n                Overwrites the default model file name from `\"model.onnx\"` to `file_name`. This allows you to save the\n                model with a different name.\n        \"\"\"\n        model_file_name = file_name if file_name is not None else ONNX_WEIGHTS_NAME\n\n        src_path = self.model_save_dir.joinpath(self.latest_model_name)\n        dst_path = Path(save_directory).joinpath(model_file_name)\n        try:\n            shutil.copyfile(src_path, dst_path)\n        except shutil.SameFileError:\n            pass\n\n        # copy external weights (for models >2GB)\n        src_path = self.model_save_dir.joinpath(ONNX_EXTERNAL_WEIGHTS_NAME)\n        if src_path.exists():\n            dst_path = Path(save_directory).joinpath(ONNX_EXTERNAL_WEIGHTS_NAME)\n            try:\n                shutil.copyfile(src_path, dst_path)\n            except shutil.SameFileError:\n                pass",
      "metadata": {
        "source": "src/diffusers/pipelines/onnx_utils.py",
        "range": {
          "start": { "row": 78, "column": 4 },
          "end": { "row": 78, "column": 4 }
        }
      }
    }
  ],
  [
    "1546",
    {
      "pageContent": "def save_pretrained(\n        self,\n        save_directory: Union[str, os.PathLike],\n        **kwargs,\n    ):\n        \"\"\"\n        Save a model to a directory, so that it can be re-loaded using the [`~OnnxModel.from_pretrained`] class\n        method.:\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n            return\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        # saving model weights/files\n        self._save_pretrained(save_directory, **kwargs)",
      "metadata": {
        "source": "src/diffusers/pipelines/onnx_utils.py",
        "range": {
          "start": { "row": 109, "column": 4 },
          "end": { "row": 109, "column": 4 }
        }
      }
    }
  ],
  [
    "1547",
    {
      "pageContent": "class Mel(ConfigMixin, SchedulerMixin):\n    \"\"\"\n    Parameters:\n        x_res (`int`): x resolution of spectrogram (time)\n        y_res (`int`): y resolution of spectrogram (frequency bins)\n        sample_rate (`int`): sample rate of audio\n        n_fft (`int`): number of Fast Fourier Transforms\n        hop_length (`int`): hop length (a higher number is recommended for lower than 256 y_res)\n        top_db (`int`): loudest in decibels\n        n_iter (`int`): number of iterations for Griffin Linn mel inversion\n    \"\"\"\n\n    config_name = \"mel_config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        x_res: int = 256,\n        y_res: int = 256,\n        sample_rate: int = 22050,\n        n_fft: int = 2048,\n        hop_length: int = 512,\n        top_db: int = 80,\n        n_iter: int = 32,\n    ):\n        self.hop_length = hop_length\n        self.sr = sample_rate\n        self.n_fft = n_fft\n        self.top_db = top_db\n        self.n_iter = n_iter\n        self.set_resolution(x_res, y_res)\n        self.audio = None\n\n        if not _librosa_can_be_imported:\n            raise ValueError(_import_error)\n\n    def set_resolution(self, x_res: int, y_res: int):\n        \"\"\"Set resolution.\n\n        Args:\n            x_res (`int`): x resolution of spectrogram (time)\n            y_res (`int`): y resolution of spectrogram (frequency bins)\n        \"\"\"\n        self.x_res = x_res\n        self.y_res = y_res\n        self.n_mels = self.y_res\n        self.slice_size = self.x_res * self.hop_length - 1\n\n    def load_audio(self, audio_file: str = None, raw_audio: np.ndarray = None):\n     ",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "1548",
    {
      "pageContent": "def set_resolution(self, x_res: int, y_res: int):\n        \"\"\"Set resolution.\n\n        Args:\n            x_res (`int`): x resolution of spectrogram (time)\n            y_res (`int`): y resolution of spectrogram (frequency bins)\n        \"\"\"\n        self.x_res = x_res\n        self.y_res = y_res\n        self.n_mels = self.y_res\n        self.slice_size = self.x_res * self.hop_length - 1",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 72, "column": 4 },
          "end": { "row": 72, "column": 4 }
        }
      }
    }
  ],
  [
    "1549",
    {
      "pageContent": "def load_audio(self, audio_file: str = None, raw_audio: np.ndarray = None):\n        \"\"\"Load audio.\n\n        Args:\n            audio_file (`str`): must be a file on disk due to Librosa limitation or\n            raw_audio (`np.ndarray`): audio as numpy array\n        \"\"\"\n        if audio_file is not None:\n            self.audio, _ = librosa.load(audio_file, mono=True, sr=self.sr)\n        else:\n            self.audio = raw_audio\n\n        # Pad with silence if necessary.\n        if len(self.audio) < self.x_res * self.hop_length:\n            self.audio = np.concatenate([self.audio, np.zeros((self.x_res * self.hop_length - len(self.audio),))])",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 84, "column": 4 },
          "end": { "row": 84, "column": 4 }
        }
      }
    }
  ],
  [
    "1550",
    {
      "pageContent": "def get_number_of_slices(self) -> int:\n        \"\"\"Get number of slices in audio.\n\n        Returns:\n            `int`: number of spectograms audio can be sliced into\n        \"\"\"\n        return len(self.audio) // self.slice_size",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 100, "column": 4 },
          "end": { "row": 100, "column": 4 }
        }
      }
    }
  ],
  [
    "1551",
    {
      "pageContent": "def get_audio_slice(self, slice: int = 0) -> np.ndarray:\n        \"\"\"Get slice of audio.\n\n        Args:\n            slice (`int`): slice number of audio (out of get_number_of_slices())\n\n        Returns:\n            `np.ndarray`: audio as numpy array\n        \"\"\"\n        return self.audio[self.slice_size * slice : self.slice_size * (slice + 1)]",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "1552",
    {
      "pageContent": "def get_sample_rate(self) -> int:\n        \"\"\"Get sample rate:\n\n        Returns:\n            `int`: sample rate of audio\n        \"\"\"\n        return self.sr",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 119, "column": 4 },
          "end": { "row": 119, "column": 4 }
        }
      }
    }
  ],
  [
    "1553",
    {
      "pageContent": "def audio_slice_to_image(self, slice: int) -> Image.Image:\n        \"\"\"Convert slice of audio to spectrogram.\n\n        Args:\n            slice (`int`): slice number of audio to convert (out of get_number_of_slices())\n\n        Returns:\n            `PIL Image`: grayscale image of x_res x y_res\n        \"\"\"\n        S = librosa.feature.melspectrogram(\n            y=self.get_audio_slice(slice), sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels\n        )\n        log_S = librosa.power_to_db(S, ref=np.max, top_db=self.top_db)\n        bytedata = (((log_S + self.top_db) * 255 / self.top_db).clip(0, 255) + 0.5).astype(np.uint8)\n        image = Image.fromarray(bytedata)\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 127, "column": 4 },
          "end": { "row": 127, "column": 4 }
        }
      }
    }
  ],
  [
    "1554",
    {
      "pageContent": "def image_to_audio(self, image: Image.Image) -> np.ndarray:\n        \"\"\"Converts spectrogram to audio.\n\n        Args:\n            image (`PIL Image`): x_res x y_res grayscale image\n\n        Returns:\n            audio (`np.ndarray`): raw audio\n        \"\"\"\n        bytedata = np.frombuffer(image.tobytes(), dtype=\"uint8\").reshape((image.height, image.width))\n        log_S = bytedata.astype(\"float\") * self.top_db / 255 - self.top_db\n        S = librosa.db_to_power(log_S)\n        audio = librosa.feature.inverse.mel_to_audio(\n            S, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length, n_iter=self.n_iter\n        )\n        return audio",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/mel.py",
        "range": {
          "start": { "row": 144, "column": 4 },
          "end": { "row": 144, "column": 4 }
        }
      }
    }
  ],
  [
    "1555",
    {
      "pageContent": "class AudioDiffusionPipeline(DiffusionPipeline):\n    \"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqae ([`AutoencoderKL`]): Variational AutoEncoder for Latent Audio Diffusion or None\n        unet ([`UNet2DConditionModel`]): UNET model\n        mel ([`Mel`]): transform audio <-> spectrogram\n        scheduler ([`DDIMScheduler` or `DDPMScheduler`]): de-noising scheduler\n    \"\"\"\n\n    _optional_components = [\"vqvae\"]\n\n    def __init__(\n        self,\n        vqvae: AutoencoderKL,\n        unet: UNet2DConditionModel,\n        mel: Mel,\n        scheduler: Union[DDIMScheduler, DDPMScheduler],\n    ):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler, mel=mel, vqvae=vqvae)\n\n    def get_input_dims(self) -> Tuple:\n        \"\"\"Returns dimension of input image\n\n        Returns:\n            `Tuple`: (height, width)\n        \"\"\"\n        input_module = self.vqvae if self.vqvae is not None else self.unet\n        # For backwards compatibility\n        sample_size = (\n            (input_module.sample_size, input_module.sample_size)\n            if type(input_module.sample_size) == int\n            else input_module.sample_size\n        )\n        return sample_size\n\n    def get_default_steps(self) -> int:\n        \"\"\"Returns default number of steps recommended for inference\n\n        Returns:\n            `int`: number of steps\n        \"\"\"\n        return 50 if",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/pipeline_audio_diffusion.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "1556",
    {
      "pageContent": "def __init__(\n        self,\n        vqvae: AutoencoderKL,\n        unet: UNet2DConditionModel,\n        mel: Mel,\n        scheduler: Union[DDIMScheduler, DDPMScheduler],\n    ):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler, mel=mel, vqvae=vqvae)",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/pipeline_audio_diffusion.py",
        "range": {
          "start": { "row": 43, "column": 4 },
          "end": { "row": 43, "column": 4 }
        }
      }
    }
  ],
  [
    "1557",
    {
      "pageContent": "def get_input_dims(self) -> Tuple:\n        \"\"\"Returns dimension of input image\n\n        Returns:\n            `Tuple`: (height, width)\n        \"\"\"\n        input_module = self.vqvae if self.vqvae is not None else self.unet\n        # For backwards compatibility\n        sample_size = (\n            (input_module.sample_size, input_module.sample_size)\n            if type(input_module.sample_size) == int\n            else input_module.sample_size\n        )\n        return sample_size",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/pipeline_audio_diffusion.py",
        "range": {
          "start": { "row": 53, "column": 4 },
          "end": { "row": 53, "column": 4 }
        }
      }
    }
  ],
  [
    "1558",
    {
      "pageContent": "def get_default_steps(self) -> int:\n        \"\"\"Returns default number of steps recommended for inference\n\n        Returns:\n            `int`: number of steps\n        \"\"\"\n        return 50 if isinstance(self.scheduler, DDIMScheduler) else 1000",
      "metadata": {
        "source": "src/diffusers/pipelines/audio_diffusion/pipeline_audio_diffusion.py",
        "range": {
          "start": { "row": 68, "column": 4 },
          "end": { "row": 68, "column": 4 }
        }
      }
    }
  ],
  [
    "1559",
    {
      "pageContent": "class StableDiffusionPipelineSafe(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Safe Latent Diffusion.\n\n    The implementation is based on the [`StableDiffusionPipeline`]\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images coul",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "1560",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: SafeStableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n        safety_concept: Optional[str] = (\n            \"an image showing hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity,\"\n            \" bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child\"\n            \" abuse, brutality, cruelty\"\n        )\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 53, "column": 4 },
          "end": { "row": 53, "column": 4 }
        }
      }
    }
  ],
  [
    "1561",
    {
      "pageContent": "def enable_sequential_cpu_offload(self):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(\"cuda\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 169, "column": 4 },
          "end": { "row": 169, "column": 4 }
        }
      }
    }
  ],
  [
    "1562",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt,\n        enable_safety_guidance,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n        if not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was trun",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 205, "column": 4 },
          "end": { "row": 205, "column": 4 }
        }
      }
    }
  ],
  [
    "1563",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype, enable_safety_guidance):\n        if self.safety_checker is not None:\n            images = image.copy()\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n            flagged_images = np.zeros((2, *image.shape[1:]))\n            if any(has_nsfw_concept):\n                logger.warning(\n                    \"Potential NSFW content was detected in one or more images. A black image will be returned\"\n                    \" instead.\"\n                    f\"{'You may look at this images in the `unsafe_images` variable of the output at your own discretion.' if enable_safety_guidance else 'Try again with a different prompt and/or seed.'}\"\n                )\n                for idx, has_nsfw_concept in enumerate(has_nsfw_concept):\n                    if has_nsfw_concept:\n                        flagged_images[idx] = images[idx]\n                        image[idx] = np.zeros(image[idx].shape)  # black image\n        else:\n            has_nsfw_concept = None\n            flagged_images = None\n        return image, has_nsfw_concept, flagged_images",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 340, "column": 4 },
          "end": { "row": 340, "column": 4 }
        }
      }
    }
  ],
  [
    "1564",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 364, "column": 4 },
          "end": { "row": 364, "column": 4 }
        }
      }
    }
  ],
  [
    "1565",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 373, "column": 4 },
          "end": { "row": 373, "column": 4 }
        }
      }
    }
  ],
  [
    "1566",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 391, "column": 4 },
          "end": { "row": 391, "column": 4 }
        }
      }
    }
  ],
  [
    "1567",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 439, "column": 4 },
          "end": { "row": 439, "column": 4 }
        }
      }
    }
  ],
  [
    "1568",
    {
      "pageContent": "def perform_safety_guidance(\n        self,\n        enable_safety_guidance,\n        safety_momentum,\n        noise_guidance,\n        noise_pred_out,\n        i,\n        sld_guidance_scale,\n        sld_warmup_steps,\n        sld_threshold,\n        sld_momentum_scale,\n        sld_mom_beta,\n    ):\n        # Perform SLD guidance\n        if enable_safety_guidance:\n            if safety_momentum is None:\n                safety_momentum = torch.zeros_like(noise_guidance)\n            noise_pred_text, noise_pred_uncond = noise_pred_out[0], noise_pred_out[1]\n            noise_pred_safety_concept = noise_pred_out[2]\n\n            # Equation 6\n            scale = torch.clamp(torch.abs((noise_pred_text - noise_pred_safety_concept)) * sld_guidance_scale, max=1.0)\n\n            # Equation 6\n            safety_concept_scale = torch.where(\n                (noise_pred_text - noise_pred_safety_concept) >= sld_threshold, torch.zeros_like(scale), scale\n            )\n\n            # Equation 4\n            noise_guidance_safety = torch.mul((noise_pred_safety_concept - noise_pred_uncond), safety_concept_scale)\n\n            # Equation 7\n            noise_guidance_safety = noise_guidance_safety + sld_momentum_scale * safety_momentum\n\n            # Equation 8\n            safety_momentum = sld_mom_beta * safety_momentum + (1 - sld_mom_beta) * noise_guidance_safety\n\n            if i >= sld_warmup_steps:  # Warmup\n                # Equation 3\n                noise_guidance = noise_guidance - noise_guidance_safety\n        return noise_guidance, safety_momentum",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py",
        "range": {
          "start": { "row": 456, "column": 4 },
          "end": { "row": 456, "column": 4 }
        }
      }
    }
  ],
  [
    "1569",
    {
      "pageContent": "class SafetyConfig(object):\n    WEAK = {\n        \"sld_warmup_steps\": 15,\n        \"sld_guidance_scale\": 20,\n        \"sld_threshold\": 0.0,\n        \"sld_momentum_scale\": 0.0,\n        \"sld_mom_beta\": 0.0,\n    }\n    MEDIUM = {\n        \"sld_warmup_steps\": 10,\n        \"sld_guidance_scale\": 1000,\n        \"sld_threshold\": 0.01,\n        \"sld_momentum_scale\": 0.3,\n        \"sld_mom_beta\": 0.4,\n    }\n    STRONG = {\n        \"sld_warmup_steps\": 7,\n        \"sld_guidance_scale\": 2000,\n        \"sld_threshold\": 0.025,\n        \"sld_momentum_scale\": 0.5,\n        \"sld_mom_beta\": 0.7,\n    }\n    MAX = {\n        \"sld_warmup_steps\": 0,\n        \"sld_guidance_scale\": 5000,\n        \"sld_threshold\": 1.0,\n        \"sld_momentum_scale\": 0.5,\n        \"sld_mom_beta\": 0.7,\n    }",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/__init__.py",
        "range": {
          "start": { "row": 12, "column": 0 },
          "end": { "row": 12, "column": 0 }
        }
      }
    }
  ],
  [
    "1570",
    {
      "pageContent": "class StableDiffusionSafePipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Safe Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n        nsfw_content_detected (`List[bool]`)\n            List of flags denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, or `None` if safety checking could not be performed.\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images that were flagged by the safety checker any may contain \"not-safe-for-work\"\n            (nsfw) content, or `None` if no safety check was performed or no images were flagged.\n        applied_safety_concept (`str`)\n            The safety concept that was applied for safety guidance, or `None` if safety guidance was disabled\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]\n    unsafe_images: Optional[Union[List[PIL.Image.Image], np.ndarray]]\n    applied_safety_concept: Optional[str]",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/__init__.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "1571",
    {
      "pageContent": "def cosine_distance(image_embeds, text_embeds):\n    normalized_image_embeds = nn.functional.normalize(image_embeds)\n    normalized_text_embeds = nn.functional.normalize(text_embeds)\n    return torch.mm(normalized_image_embeds, normalized_text_embeds.t())",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/safety_checker.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1572",
    {
      "pageContent": "class SafeStableDiffusionSafetyChecker(PreTrainedModel):\n    config_class = CLIPConfig\n\n    _no_split_modules = [\"CLIPEncoderLayer\"]\n\n    def __init__(self, config: CLIPConfig):\n        super().__init__(config)\n\n        self.vision_model = CLIPVisionModel(config.vision_config)\n        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n\n        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n\n        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)\n        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)\n\n    @torch.no_grad()\n    def forward(self, clip_input, images):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().float().numpy()\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().float().numpy()\n\n        result = []\n        batch_size = image_embeds.shape[0]\n        for i in range(batch_size):\n            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n\n            # increase this value to create a stronger `nfsw` filter\n            # at the cost of",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/safety_checker.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "1573",
    {
      "pageContent": "def __init__(self, config: CLIPConfig):\n        super().__init__(config)\n\n        self.vision_model = CLIPVisionModel(config.vision_config)\n        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n\n        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n\n        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)\n        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion_safe/safety_checker.py",
        "range": {
          "start": { "row": 35, "column": 4 },
          "end": { "row": 35, "column": 4 }
        }
      }
    }
  ],
  [
    "1574",
    {
      "pageContent": "def preprocess(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "1575",
    {
      "pageContent": "class LDMSuperResolutionPipeline(DiffusionPipeline):\n    r\"\"\"\n    A pipeline for image super-resolution using Latent\n\n    This class inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqvae ([`VQModel`]):\n            Vector-quantized (VQ) VAE Model to encode and decode images to and from latent representations.\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latens. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], [`EulerDiscreteScheduler`],\n            [`EulerAncestralDiscreteScheduler`], [`DPMSolverMultistepScheduler`], or [`PNDMScheduler`].\n    \"\"\"\n\n    def __init__(\n        self,\n        vqvae: VQModel,\n        unet: UNet2DModel,\n        scheduler: Union[\n            DDIMScheduler,\n            PNDMScheduler,\n            LMSDiscreteScheduler,\n            EulerDiscreteScheduler,\n            EulerAncestralDiscreteScheduler,\n            DPMSolverMultistepScheduler,\n        ],\n    ):\n        super().__init__()\n        self.register_modules(vqvae=vqvae, unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        image: Union[torch.Tensor, PIL.Image.Image] = None,\n        batch_size: Optional[int] = 1,\n        num_inference_steps: Optional[int] = 100,\n        eta: Optional[fl",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1576",
    {
      "pageContent": "def __init__(\n        self,\n        vqvae: VQModel,\n        unet: UNet2DModel,\n        scheduler: Union[\n            DDIMScheduler,\n            PNDMScheduler,\n            LMSDiscreteScheduler,\n            EulerDiscreteScheduler,\n            EulerAncestralDiscreteScheduler,\n            DPMSolverMultistepScheduler,\n        ],\n    ):\n        super().__init__()\n        self.register_modules(vqvae=vqvae, unet=unet, scheduler=scheduler)",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "1577",
    {
      "pageContent": "class LDMTextToImagePipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqvae ([`VQModel`]):\n            Vector-quantized (VQ) Model to encode and decode images to and from latent representations.\n        bert ([`LDMBertModel`]):\n            Text-encoder model based on [BERT](https://huggingface.co/docs/transformers/model_doc/bert) architecture.\n        tokenizer (`transformers.BertTokenizer`):\n            Tokenizer of class\n            [BertTokenizer](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n    \"\"\"\n\n    def __init__(\n        self,\n        vqvae: Union[VQModel, AutoencoderKL],\n        bert: PreTrainedModel,\n        tokenizer: PreTrainedTokenizer,\n        unet: Union[UNet2DModel, UNet2DConditionModel],\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n    ):\n        super().__init__()\n        self.register_modules(vqvae=vqvae, bert=bert, tokenizer=tokenizer, unet=unet, scheduler=scheduler)\n        self.vae_scale_factor = 2 ** (len",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1578",
    {
      "pageContent": "def __init__(\n        self,\n        vqvae: Union[VQModel, AutoencoderKL],\n        bert: PreTrainedModel,\n        tokenizer: PreTrainedTokenizer,\n        unet: Union[UNet2DModel, UNet2DConditionModel],\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n    ):\n        super().__init__()\n        self.register_modules(vqvae=vqvae, bert=bert, tokenizer=tokenizer, unet=unet, scheduler=scheduler)\n        self.vae_scale_factor = 2 ** (len(self.vqvae.config.block_out_channels) - 1)",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 50, "column": 4 },
          "end": { "row": 50, "column": 4 }
        }
      }
    }
  ],
  [
    "1579",
    {
      "pageContent": "class LDMBertConfig(PretrainedConfig):\n    model_type = \"ldmbert\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    attribute_map = {\"num_attention_heads\": \"encoder_attention_heads\", \"hidden_size\": \"d_model\"}\n\n    def __init__(\n        self,\n        vocab_size=30522,\n        max_position_embeddings=77,\n        encoder_layers=32,\n        encoder_ffn_dim=5120,\n        encoder_attention_heads=8,\n        head_dim=64,\n        encoder_layerdrop=0.0,\n        activation_function=\"gelu\",\n        d_model=1280,\n        dropout=0.1,\n        attention_dropout=0.0,\n        activation_dropout=0.0,\n        init_std=0.02,\n        classifier_dropout=0.0,\n        scale_embedding=False,\n        use_cache=True,\n        pad_token_id=0,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.d_model = d_model\n        self.encoder_ffn_dim = encoder_ffn_dim\n        self.encoder_layers = encoder_layers\n        self.encoder_attention_heads = encoder_attention_heads\n        self.head_dim = head_dim\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.activation_dropout = activation_dropout\n        self.activation_function = activation_function\n        self.init_std = init_std\n        self.encoder_layerdrop = encoder_layerdrop\n        self.classifier_dropout = classifier_dropout\n        self.use_cache = use_cache\n        self.num_hidden_layers = encoder_layers\n        self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n\n        super().__in",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 219, "column": 0 },
          "end": { "row": 219, "column": 0 }
        }
      }
    }
  ],
  [
    "1580",
    {
      "pageContent": "def __init__(\n        self,\n        vocab_size=30522,\n        max_position_embeddings=77,\n        encoder_layers=32,\n        encoder_ffn_dim=5120,\n        encoder_attention_heads=8,\n        head_dim=64,\n        encoder_layerdrop=0.0,\n        activation_function=\"gelu\",\n        d_model=1280,\n        dropout=0.1,\n        attention_dropout=0.0,\n        activation_dropout=0.0,\n        init_std=0.02,\n        classifier_dropout=0.0,\n        scale_embedding=False,\n        use_cache=True,\n        pad_token_id=0,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.d_model = d_model\n        self.encoder_ffn_dim = encoder_ffn_dim\n        self.encoder_layers = encoder_layers\n        self.encoder_attention_heads = encoder_attention_heads\n        self.head_dim = head_dim\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.activation_dropout = activation_dropout\n        self.activation_function = activation_function\n        self.init_std = init_std\n        self.encoder_layerdrop = encoder_layerdrop\n        self.classifier_dropout = classifier_dropout\n        self.use_cache = use_cache\n        self.num_hidden_layers = encoder_layers\n        self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n\n        super().__init__(pad_token_id=pad_token_id, **kwargs)",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 224, "column": 4 },
          "end": { "row": 224, "column": 4 }
        }
      }
    }
  ],
  [
    "1581",
    {
      "pageContent": "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 266, "column": 0 },
          "end": { "row": 266, "column": 0 }
        }
      }
    }
  ],
  [
    "1582",
    {
      "pageContent": "class LDMBertAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        head_dim: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = head_dim\n        self.inner_dim = head_dim * num_heads\n\n        self.scaling = self.head_dim**-0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)\n        self.out_proj = nn.Linear(self.inner_dim, embed_dim)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention l",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 281, "column": 0 },
          "end": { "row": 281, "column": 0 }
        }
      }
    }
  ],
  [
    "1583",
    {
      "pageContent": "def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        head_dim: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = head_dim\n        self.inner_dim = head_dim * num_heads\n\n        self.scaling = self.head_dim**-0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)\n        self.out_proj = nn.Linear(self.inner_dim, embed_dim)",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 284, "column": 4 },
          "end": { "row": 284, "column": 4 }
        }
      }
    }
  ],
  [
    "1584",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n\n        bsz, tgt_len, _ = hidden_states.size()\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_state",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 311, "column": 4 },
          "end": { "row": 311, "column": 4 }
        }
      }
    }
  ],
  [
    "1585",
    {
      "pageContent": "class LDMBertEncoderLayer(nn.Module):\n    def __init__(self, config: LDMBertConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n        self.self_attn = LDMBertAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.encoder_attention_heads,\n            head_dim=config.head_dim,\n            dropout=config.attention_dropout,\n        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.FloatTensor,\n        attention_mask: torch.FloatTensor,\n        layer_head_mask: torch.FloatTensor,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            output_attentions (`bool`, *optional*",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 425, "column": 0 },
          "end": { "row": 425, "column": 0 }
        }
      }
    }
  ],
  [
    "1586",
    {
      "pageContent": "def __init__(self, config: LDMBertConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n        self.self_attn = LDMBertAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.encoder_attention_heads,\n            head_dim=config.head_dim,\n            dropout=config.attention_dropout,\n        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 426, "column": 4 },
          "end": { "row": 426, "column": 4 }
        }
      }
    }
  ],
  [
    "1587",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states: torch.FloatTensor,\n        attention_mask: torch.FloatTensor,\n        layer_head_mask: torch.FloatTensor,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states, attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.activation_fn(self",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 443, "column": 4 },
          "end": { "row": 443, "column": 4 }
        }
      }
    }
  ],
  [
    "1588",
    {
      "pageContent": "class LDMBertPreTrainedModel(PreTrainedModel):\n    config_class = LDMBertConfig\n    base_model_prefix = \"model\"\n    _supports_gradient_checkpointing = True\n    _keys_to_ignore_on_load_unexpected = [r\"encoder\\.version\", r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.init_std\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (LDMBertEncoder,)):\n            module.gradient_checkpointing = value\n\n    @property\n    def dummy_inputs(self):\n        pad_token = self.config.pad_token_id\n        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n        dummy_inputs = {\n            \"attention_mask\": input_ids.ne(pad_token),\n            \"input_ids\": input_ids,\n        }\n        return dummy_inputs",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 495, "column": 0 },
          "end": { "row": 495, "column": 0 }
        }
      }
    }
  ],
  [
    "1589",
    {
      "pageContent": "def _init_weights(self, module):\n        std = self.config.init_std\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 501, "column": 4 },
          "end": { "row": 501, "column": 4 }
        }
      }
    }
  ],
  [
    "1590",
    {
      "pageContent": "class LDMBertEncoder(LDMBertPreTrainedModel):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n    [`LDMBertEncoderLayer`].\n\n    Args:\n        config: LDMBertConfig\n        embed_tokens (nn.Embedding): output embedding\n    \"\"\"\n\n    def __init__(self, config: LDMBertConfig):\n        super().__init__(config)\n\n        self.dropout = config.dropout\n\n        embed_dim = config.d_model\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim)\n        self.embed_positions = nn.Embedding(config.max_position_embeddings, embed_dim)\n        self.layers = nn.ModuleList([LDMBertEncoderLayer(config) for _ in range(config.encoder_layers)])\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Un",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 527, "column": 0 },
          "end": { "row": 527, "column": 0 }
        }
      }
    }
  ],
  [
    "1591",
    {
      "pageContent": "def __init__(self, config: LDMBertConfig):\n        super().__init__(config)\n\n        self.dropout = config.dropout\n\n        embed_dim = config.d_model\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim)\n        self.embed_positions = nn.Embedding(config.max_position_embeddings, embed_dim)\n        self.layers = nn.ModuleList([LDMBertEncoderLayer(config) for _ in range(config.encoder_layers)])\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 537, "column": 4 },
          "end": { "row": 537, "column": 4 }
        }
      }
    }
  ],
  [
    "1592",
    {
      "pageContent": "def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the ",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 561, "column": 4 },
          "end": { "row": 561, "column": 4 }
        }
      }
    }
  ],
  [
    "1593",
    {
      "pageContent": "class LDMBertModel(LDMBertPreTrainedModel):\n    _no_split_modules = []\n\n    def __init__(self, config: LDMBertConfig):\n        super().__init__(config)\n        self.model = LDMBertEncoder(config)\n        self.to_logits = nn.Linear(config.hidden_size, config.vocab_size)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        return outputs",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 694, "column": 0 },
          "end": { "row": 694, "column": 0 }
        }
      }
    }
  ],
  [
    "1594",
    {
      "pageContent": "def __init__(self, config: LDMBertConfig):\n        super().__init__(config)\n        self.model = LDMBertEncoder(config)\n        self.to_logits = nn.Linear(config.hidden_size, config.vocab_size)",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 697, "column": 4 },
          "end": { "row": 697, "column": 4 }
        }
      }
    }
  ],
  [
    "1595",
    {
      "pageContent": "def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        return outputs",
      "metadata": {
        "source": "src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py",
        "range": {
          "start": { "row": 702, "column": 4 },
          "end": { "row": 702, "column": 4 }
        }
      }
    }
  ],
  [
    "1596",
    {
      "pageContent": "class PNDMPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        unet (`UNet2DModel`): U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            The `PNDMScheduler` to be used in combination with `unet` to denoise the encoded image.\n    \"\"\"\n\n    unet: UNet2DModel\n    scheduler: PNDMScheduler\n\n    def __init__(self, unet: UNet2DModel, scheduler: PNDMScheduler):\n        super().__init__()\n\n        scheduler = PNDMScheduler.from_config(scheduler.config)\n\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        num_inference_steps: int = 50,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, `optional`, defaults to 1): The number of images to generate.\n            num_inference_steps (`int`, `optional`, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            generator (`torch.Generator`, `optional`): A [torch\n                generator](https://p",
      "metadata": {
        "source": "src/diffusers/pipelines/pndm/pipeline_pndm.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1597",
    {
      "pageContent": "def __init__(self, unet: UNet2DModel, scheduler: PNDMScheduler):\n        super().__init__()\n\n        scheduler = PNDMScheduler.from_config(scheduler.config)\n\n        self.register_modules(unet=unet, scheduler=scheduler)",
      "metadata": {
        "source": "src/diffusers/pipelines/pndm/pipeline_pndm.py",
        "range": {
          "start": { "row": 39, "column": 4 },
          "end": { "row": 39, "column": 4 }
        }
      }
    }
  ],
  [
    "1598",
    {
      "pageContent": "class VersatileDiffusionDualGuidedPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqvae ([`VQModel`]):\n            Vector-quantized (VQ) Model to encode and decode images to and from latent representations.\n        bert ([`LDMBertModel`]):\n            Text-encoder model based on [BERT](https://huggingface.co/docs/transformers/model_doc/bert) architecture.\n        tokenizer (`transformers.BertTokenizer`):\n            Tokenizer of class\n            [BertTokenizer](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n    \"\"\"\n    tokenizer: CLIPTokenizer\n    image_feature_extractor: CLIPFeatureExtractor\n    text_encoder: CLIPTextModelWithProjection\n    image_encoder: CLIPVisionModelWithProjection\n    image_unet: UNet2DConditionModel\n    text_unet: UNetFlatConditionModel\n    vae: AutoencoderKL\n    scheduler: KarrasDiffusionSchedulers\n\n    _optional_components = [\"text_unet\"]\n\n    def __init__(\n        self,\n        tokenizer: CLIPTokenizer,\n        image_feature_extracto",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "1599",
    {
      "pageContent": "def __init__(\n        self,\n        tokenizer: CLIPTokenizer,\n        image_feature_extractor: CLIPFeatureExtractor,\n        text_encoder: CLIPTextModelWithProjection,\n        image_encoder: CLIPVisionModelWithProjection,\n        image_unet: UNet2DConditionModel,\n        text_unet: UNetFlatConditionModel,\n        vae: AutoencoderKL,\n        scheduler: KarrasDiffusionSchedulers,\n    ):\n        super().__init__()\n        self.register_modules(\n            tokenizer=tokenizer,\n            image_feature_extractor=image_feature_extractor,\n            text_encoder=text_encoder,\n            image_encoder=image_encoder,\n            image_unet=image_unet,\n            text_unet=text_unet,\n            vae=vae,\n            scheduler=scheduler,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n\n        if self.text_unet is not None and (\n            \"dual_cross_attention\" not in self.image_unet.config or not self.image_unet.config.dual_cross_attention\n        ):\n            # if loading from a universal checkpoint rather than a saved dual-guided pipeline\n            self._convert_to_dual_attention()",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 67, "column": 4 },
          "end": { "row": 67, "column": 4 }
        }
      }
    }
  ],
  [
    "1600",
    {
      "pageContent": "def _convert_to_dual_attention(self):\n        \"\"\"\n        Replace image_unet's `Transformer2DModel` blocks with `DualTransformer2DModel` that contains transformer blocks\n        from both `image_unet` and `text_unet`\n        \"\"\"\n        for name, module in self.image_unet.named_modules():\n            if isinstance(module, Transformer2DModel):\n                parent_name, index = name.rsplit(\".\", 1)\n                index = int(index)\n\n                image_transformer = self.image_unet.get_submodule(parent_name)[index]\n                text_transformer = self.text_unet.get_submodule(parent_name)[index]\n\n                config = image_transformer.config\n                dual_transformer = DualTransformer2DModel(\n                    num_attention_heads=config.num_attention_heads,\n                    attention_head_dim=config.attention_head_dim,\n                    in_channels=config.in_channels,\n                    num_layers=config.num_layers,\n                    dropout=config.dropout,\n                    norm_num_groups=config.norm_num_groups,\n                    cross_attention_dim=config.cross_attention_dim,\n                    attention_bias=config.attention_bias,\n                    sample_size=config.sample_size,\n                    num_vector_embeds=config.num_vector_embeds,\n                    activation_fn=config.activation_fn,\n                    num_embeds_ada_norm=config.num_embeds_ada_norm,\n                )\n                dual_transformer.transformers[0] = image_transformer\n                dual_transformer.transformers[1] = text_transformer\n\n                self",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 100, "column": 4 },
          "end": { "row": 100, "column": 4 }
        }
      }
    }
  ],
  [
    "1601",
    {
      "pageContent": "def _revert_dual_attention(self):\n        \"\"\"\n        Revert the image_unet `DualTransformer2DModel` blocks back to `Transformer2DModel` with image_unet weights Call\n        this function if you reuse `image_unet` in another pipeline, e.g. `VersatileDiffusionPipeline`\n        \"\"\"\n        for name, module in self.image_unet.named_modules():\n            if isinstance(module, DualTransformer2DModel):\n                parent_name, index = name.rsplit(\".\", 1)\n                index = int(index)\n                self.image_unet.get_submodule(parent_name)[index] = module.transformers[0]\n\n        self.image_unet.register_to_config(dual_cross_attention=False)",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 134, "column": 4 },
          "end": { "row": 134, "column": 4 }
        }
      }
    }
  ],
  [
    "1602",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.image_unet, self.text_unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 147, "column": 4 },
          "end": { "row": 147, "column": 4 }
        }
      }
    }
  ],
  [
    "1603",
    {
      "pageContent": "def _encode_text_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n        \"\"\"\n\n        def normalize_embeddings(encoder_output):\n            embeds = self.text_encoder.text_projection(encoder_output.last_hidden_state)\n            embeds_pooled = encoder_output.text_embeds\n            embeds = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n            return embeds\n\n        batch_size = len(prompt)\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n        if not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up ",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "1604",
    {
      "pageContent": "def _encode_image_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n        \"\"\"\n\n        def normalize_embeddings(encoder_output):\n            embeds = self.image_encoder.vision_model.post_layernorm(encoder_output.last_hidden_state)\n            embeds = self.image_encoder.visual_projection(embeds)\n            embeds_pooled = embeds[:, 0:1]\n            embeds = embeds / torch.norm(embeds_pooled, dim=-1, keepdim=True)\n            return embeds\n\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        image_input = self.image_feature_extractor(images=prompt, return_tensors=\"pt\")\n        pixel_values = image_input.pixel_values.to(device).to(self.image_encoder.dtype)\n        image_embeddings = self.image_encoder(pixel_values)\n        image_embeddings = normalize_embeddings(image_embeddings)\n\n        # duplicate image embeddings for each generation per prompt, using mps friendly method\n        bs_embed, seq_len, _ = image_embeddings.shape\n        image_embeddings = image_embeddings.repeat(1, num_images_per_prompt, 1)\n        image_embeddings",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 274, "column": 4 },
          "end": { "row": 274, "column": 4 }
        }
      }
    }
  ],
  [
    "1605",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 330, "column": 4 },
          "end": { "row": 330, "column": 4 }
        }
      }
    }
  ],
  [
    "1606",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 339, "column": 4 },
          "end": { "row": 339, "column": 4 }
        }
      }
    }
  ],
  [
    "1607",
    {
      "pageContent": "def check_inputs(self, prompt, image, height, width, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, PIL.Image.Image) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` `PIL.Image` or `list` but is {type(prompt)}\")\n        if not isinstance(image, str) and not isinstance(image, PIL.Image.Image) and not isinstance(image, list):\n            raise ValueError(f\"`image` has to be of type `str` `PIL.Image` or `list` but is {type(image)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 356, "column": 4 },
          "end": { "row": 356, "column": 4 }
        }
      }
    }
  ],
  [
    "1608",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 374, "column": 4 },
          "end": { "row": 374, "column": 4 }
        }
      }
    }
  ],
  [
    "1609",
    {
      "pageContent": "def set_transformer_params(self, mix_ratio: float = 0.5, condition_types: Tuple = (\"text\", \"image\")):\n        for name, module in self.image_unet.named_modules():\n            if isinstance(module, DualTransformer2DModel):\n                module.mix_ratio = mix_ratio\n\n                for i, type in enumerate(condition_types):\n                    if type == \"text\":\n                        module.condition_lengths[i] = self.text_encoder.config.max_position_embeddings\n                        module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n                    else:\n                        module.condition_lengths[i] = 257\n                        module.transformer_index_for_condition[i] = 0",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py",
        "range": {
          "start": { "row": 391, "column": 4 },
          "end": { "row": 391, "column": 4 }
        }
      }
    }
  ],
  [
    "1610",
    {
      "pageContent": "class VersatileDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionMegaSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "1611",
    {
      "pageContent": "def __init__(\n        self,\n        tokenizer: CLIPTokenizer,\n        image_feature_extractor: CLIPFeatureExtractor,\n        text_encoder: CLIPTextModel,\n        image_encoder: CLIPVisionModel,\n        image_unet: UNet2DConditionModel,\n        text_unet: UNet2DConditionModel,\n        vae: AutoencoderKL,\n        scheduler: KarrasDiffusionSchedulers,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            tokenizer=tokenizer,\n            image_feature_extractor=image_feature_extractor,\n            text_encoder=text_encoder,\n            image_encoder=image_encoder,\n            image_unet=image_unet,\n            text_unet=text_unet,\n            vae=vae,\n            scheduler=scheduler,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion.py",
        "range": {
          "start": { "row": 56, "column": 4 },
          "end": { "row": 56, "column": 4 }
        }
      }
    }
  ],
  [
    "1612",
    {
      "pageContent": "def get_down_block(\n    down_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    temb_channels,\n    add_downsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    resnet_groups=None,\n    cross_attention_dim=None,\n    downsample_padding=None,\n    dual_cross_attention=False,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    upcast_attention=False,\n    resnet_time_scale_shift=\"default\",\n):\n    down_block_type = down_block_type[7:] if down_block_type.startswith(\"UNetRes\") else down_block_type\n    if down_block_type == \"DownBlockFlat\":\n        return DownBlockFlat(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif down_block_type == \"CrossAttnDownBlockFlat\":\n        if cross_attention_dim is None:\n            raise ValueError(\"cross_attention_dim must be specified for CrossAttnDownBlockFlat\")\n        return CrossAttnDownBlockFlat(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n   ",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 20, "column": 0 },
          "end": { "row": 20, "column": 0 }
        }
      }
    }
  ],
  [
    "1613",
    {
      "pageContent": "def get_up_block(\n    up_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    prev_output_channel,\n    temb_channels,\n    add_upsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    resnet_groups=None,\n    cross_attention_dim=None,\n    dual_cross_attention=False,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    upcast_attention=False,\n    resnet_time_scale_shift=\"default\",\n):\n    up_block_type = up_block_type[7:] if up_block_type.startswith(\"UNetRes\") else up_block_type\n    if up_block_type == \"UpBlockFlat\":\n        return UpBlockFlat(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif up_block_type == \"CrossAttnUpBlockFlat\":\n        if cross_attention_dim is None:\n            raise ValueError(\"cross_attention_dim must be specified for CrossAttnUpBlockFlat\")\n        return CrossAttnUpBlockFlat(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=res",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 76, "column": 0 },
          "end": { "row": 76, "column": 0 }
        }
      }
    }
  ],
  [
    "1614",
    {
      "pageContent": "class UNetFlatConditionModel(ModelMixin, ConfigMixin):\n    r\"\"\"\n    UNetFlatConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a\n    timestep and returns sample shaped output.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the models (such as downloading or saving, etc.)\n\n    Parameters:\n        sample_size (`int` or `Tuple[int, int]`, *optional*, defaults to `None`):\n            Height and width of input/output sample.\n        in_channels (`int`, *optional*, defaults to 4): The number of channels in the input sample.\n        out_channels (`int`, *optional*, defaults to 4): The number of channels in the output.\n        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.\n        flip_sin_to_cos (`bool`, *optional*, defaults to `False`):\n            Whether to flip the sin to cos in the time embedding.\n        freq_shift (`int`, *optional*, defaults to 0): The frequency shift to apply to the time embedding.\n        down_block_types (`Tuple[str]`, *optional*, defaults to `(\"CrossAttnDownBlockFlat\", \"CrossAttnDownBlockFlat\", \"CrossAttnDownBlockFlat\", \"DownBlockFlat\")`):\n            The tuple of downsample blocks to use.\n        mid_block_type (`str`, *optional*, defaults to `\"UNetMidBlockFlatCrossAttn\"`):\n            The mid block type. Choose from `UNetMidBlockFlatCrossAttn` or `UNetMidBlockFlatSimpleCrossAttn`, will skip\n            the mid block layer if `None`.\n        up_block_types (`Tup",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 133, "column": 0 },
          "end": { "row": 133, "column": 0 }
        }
      }
    }
  ],
  [
    "1615",
    {
      "pageContent": "def set_attn_processor(self, processor: Union[AttnProcessor, Dict[str, AttnProcessor]]):\n        r\"\"\"\n        Parameters:\n            `processor (`dict` of `AttnProcessor` or `AttnProcessor`):\n                The instantiated processor class or a dictionary of processor classes that will be set as the processor\n                of **all** `CrossAttention` layers.\n            In case `processor` is a dict, the key needs to define the path to the corresponding cross attention processor. This is strongly recommended when setting trainablae attention processors.:\n\n        \"\"\"\n        count = len(self.attn_processors.keys())\n\n        if isinstance(processor, dict) and len(processor) != count:\n            raise ValueError(\n                f\"A dict of processors was passed, but the number of processors {len(processor)} does not match the\"\n                f\" number of attention layers: {count}. Please make sure to pass {count} processor classes.\"\n            )\n\n        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):\n            if hasattr(module, \"set_processor\"):\n                if not isinstance(processor, dict):\n                    module.set_processor(processor)\n                else:\n                    module.set_processor(processor.pop(f\"{name}.processor\"))\n\n            for sub_name, child in module.named_children():\n                fn_recursive_attn_processor(f\"{name}.{sub_name}\", child, processor)\n\n        for name, module in self.named_children():\n            fn_recursive_attn_processor(name, module, processor)",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 477, "column": 4 },
          "end": { "row": 477, "column": 4 }
        }
      }
    }
  ],
  [
    "1616",
    {
      "pageContent": "def set_attention_slice(self, slice_size):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int` or `list(int)`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                `\"max\"`, maxium amount of memory will be saved by running only one slice at a time. If a number is\n                provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n                must be a multiple of `slice_size`.\n        \"\"\"\n        sliceable_head_dims = []\n\n        def fn_recursive_retrieve_slicable_dims(module: torch.nn.Module):\n            if hasattr(module, \"set_attention_slice\"):\n                sliceable_head_dims.append(module.sliceable_head_dim)\n\n            for child in module.children():\n                fn_recursive_retrieve_slicable_dims(child)\n\n        # retrieve number of attention layers\n        for module in self.children():\n            fn_recursive_retrieve_slicable_dims(module)\n\n        num_slicable_layers = len(sliceable_head_dims)\n\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = [dim // 2 for dim in sliceable_head_dims]\n        el",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 507, "column": 4 },
          "end": { "row": 507, "column": 4 }
        }
      }
    }
  ],
  [
    "1617",
    {
      "pageContent": "def forward(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[torch.Tensor, float, int],\n        encoder_hidden_states: torch.Tensor,\n        class_labels: Optional[torch.Tensor] = None,\n        timestep_cond: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        down_block_additional_residuals: Optional[Tuple[torch.Tensor]] = None,\n        mid_block_additional_residual: Optional[torch.Tensor] = None,\n        return_dict: bool = True,\n    ) -> Union[UNet2DConditionOutput, Tuple]:\n        r\"\"\"\n        Args:\n            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n            timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps\n            encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n                `self.processor` in\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n\n        Returns:\n            [`~models.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:\n            [`~models.unet_2d_condition.UNet2D",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 576, "column": 4 },
          "end": { "row": 576, "column": 4 }
        }
      }
    }
  ],
  [
    "1618",
    {
      "pageContent": "class LinearMultiDim(nn.Linear):\n    def __init__(self, in_features, out_features=None, second_dim=4, *args, **kwargs):\n        in_features = [in_features, second_dim, 1] if isinstance(in_features, int) else list(in_features)\n        if out_features is None:\n            out_features = in_features\n        out_features = [out_features, second_dim, 1] if isinstance(out_features, int) else list(out_features)\n        self.in_features_multidim = in_features\n        self.out_features_multidim = out_features\n        super().__init__(np.array(in_features).prod(), np.array(out_features).prod())\n\n    def forward(self, input_tensor, *args, **kwargs):\n        shape = input_tensor.shape\n        n_dim = len(self.in_features_multidim)\n        input_tensor = input_tensor.reshape(*shape[0:-n_dim], self.in_features)\n        output_tensor = super().forward(input_tensor)\n        output_tensor = output_tensor.view(*shape[0:-n_dim], *self.out_features_multidim)\n        return output_tensor",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 747, "column": 0 },
          "end": { "row": 747, "column": 0 }
        }
      }
    }
  ],
  [
    "1619",
    {
      "pageContent": "def __init__(self, in_features, out_features=None, second_dim=4, *args, **kwargs):\n        in_features = [in_features, second_dim, 1] if isinstance(in_features, int) else list(in_features)\n        if out_features is None:\n            out_features = in_features\n        out_features = [out_features, second_dim, 1] if isinstance(out_features, int) else list(out_features)\n        self.in_features_multidim = in_features\n        self.out_features_multidim = out_features\n        super().__init__(np.array(in_features).prod(), np.array(out_features).prod())",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 748, "column": 4 },
          "end": { "row": 748, "column": 4 }
        }
      }
    }
  ],
  [
    "1620",
    {
      "pageContent": "def forward(self, input_tensor, *args, **kwargs):\n        shape = input_tensor.shape\n        n_dim = len(self.in_features_multidim)\n        input_tensor = input_tensor.reshape(*shape[0:-n_dim], self.in_features)\n        output_tensor = super().forward(input_tensor)\n        output_tensor = output_tensor.view(*shape[0:-n_dim], *self.out_features_multidim)\n        return output_tensor",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 757, "column": 4 },
          "end": { "row": 757, "column": 4 }
        }
      }
    }
  ],
  [
    "1621",
    {
      "pageContent": "class ResnetBlockFlat(nn.Module):\n    def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        dropout=0.0,\n        temb_channels=512,\n        groups=32,\n        groups_out=None,\n        pre_norm=True,\n        eps=1e-6,\n        time_embedding_norm=\"default\",\n        use_in_shortcut=None,\n        second_dim=4,\n        **kwargs,\n    ):\n        super().__init__()\n        self.pre_norm = pre_norm\n        self.pre_norm = True\n\n        in_channels = [in_channels, second_dim, 1] if isinstance(in_channels, int) else list(in_channels)\n        self.in_channels_prod = np.array(in_channels).prod()\n        self.channels_multidim = in_channels\n\n        if out_channels is not None:\n            out_channels = [out_channels, second_dim, 1] if isinstance(out_channels, int) else list(out_channels)\n            out_channels_prod = np.array(out_channels).prod()\n            self.out_channels_multidim = out_channels\n        else:\n            out_channels_prod = self.in_channels_prod\n            self.out_channels_multidim = self.channels_multidim\n        self.time_embedding_norm = time_embedding_norm\n\n        if groups_out is None:\n            groups_out = groups\n\n        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=self.in_channels_prod, eps=eps, affine=True)\n        self.conv1 = torch.nn.Conv2d(self.in_channels_prod, out_channels_prod, kernel_size=1, padding=0)\n\n        if temb_channels is not None:\n            self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels_prod)\n        else:\n            self.time_emb_proj = None\n\n       ",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 766, "column": 0 },
          "end": { "row": 766, "column": 0 }
        }
      }
    }
  ],
  [
    "1622",
    {
      "pageContent": "def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        dropout=0.0,\n        temb_channels=512,\n        groups=32,\n        groups_out=None,\n        pre_norm=True,\n        eps=1e-6,\n        time_embedding_norm=\"default\",\n        use_in_shortcut=None,\n        second_dim=4,\n        **kwargs,\n    ):\n        super().__init__()\n        self.pre_norm = pre_norm\n        self.pre_norm = True\n\n        in_channels = [in_channels, second_dim, 1] if isinstance(in_channels, int) else list(in_channels)\n        self.in_channels_prod = np.array(in_channels).prod()\n        self.channels_multidim = in_channels\n\n        if out_channels is not None:\n            out_channels = [out_channels, second_dim, 1] if isinstance(out_channels, int) else list(out_channels)\n            out_channels_prod = np.array(out_channels).prod()\n            self.out_channels_multidim = out_channels\n        else:\n            out_channels_prod = self.in_channels_prod\n            self.out_channels_multidim = self.channels_multidim\n        self.time_embedding_norm = time_embedding_norm\n\n        if groups_out is None:\n            groups_out = groups\n\n        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=self.in_channels_prod, eps=eps, affine=True)\n        self.conv1 = torch.nn.Conv2d(self.in_channels_prod, out_channels_prod, kernel_size=1, padding=0)\n\n        if temb_channels is not None:\n            self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels_prod)\n        else:\n            self.time_emb_proj = None\n\n        self.norm2 = torch.nn.GroupNorm(num_g",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 767, "column": 4 },
          "end": { "row": 767, "column": 4 }
        }
      }
    }
  ],
  [
    "1623",
    {
      "pageContent": "def forward(self, input_tensor, temb):\n        shape = input_tensor.shape\n        n_dim = len(self.channels_multidim)\n        input_tensor = input_tensor.reshape(*shape[0:-n_dim], self.in_channels_prod, 1, 1)\n        input_tensor = input_tensor.view(-1, self.in_channels_prod, 1, 1)\n\n        hidden_states = input_tensor\n\n        hidden_states = self.norm1(hidden_states)\n        hidden_states = self.nonlinearity(hidden_states)\n        hidden_states = self.conv1(hidden_states)\n\n        if temb is not None:\n            temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n            hidden_states = hidden_states + temb\n\n        hidden_states = self.norm2(hidden_states)\n        hidden_states = self.nonlinearity(hidden_states)\n\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.conv2(hidden_states)\n\n        if self.conv_shortcut is not None:\n            input_tensor = self.conv_shortcut(input_tensor)\n\n        output_tensor = input_tensor + hidden_states\n\n        output_tensor = output_tensor.view(*shape[0:-n_dim], -1)\n        output_tensor = output_tensor.view(*shape[0:-n_dim], *self.out_channels_multidim)\n\n        return output_tensor",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 827, "column": 4 },
          "end": { "row": 827, "column": 4 }
        }
      }
    }
  ],
  [
    "1624",
    {
      "pageContent": "class DownBlockFlat(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    LinearMultiDim(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.down",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 861, "column": 0 },
          "end": { "row": 861, "column": 0 }
        }
      }
    }
  ],
  [
    "1625",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    LinearMultiDim(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n        self.gradie",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 862, "column": 4 },
          "end": { "row": 862, "column": 4 }
        }
      }
    }
  ],
  [
    "1626",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 913, "column": 4 },
          "end": { "row": 913, "column": 4 }
        }
      }
    }
  ],
  [
    "1627",
    {
      "pageContent": "class CrossAttnDownBlockFlat(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_downsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            if not dual_cross_attention",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 941, "column": 0 },
          "end": { "row": 941, "column": 0 }
        }
      }
    }
  ],
  [
    "1628",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_downsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            if not dual_cross_attention:\n                attentions.append(\n        ",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 942, "column": 4 },
          "end": { "row": 942, "column": 4 }
        }
      }
    }
  ],
  [
    "1629",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        # TODO(Patrick, William) - attention mask is not used\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(attn, return_dict=False),\n                    hidden_states,\n                    encoder_hidden_states,\n                    cross_attention_kwargs,\n                )[0]\n            else:\n                hidden_states = resnet(hidden_states, temb)\n                hidden_states = attn(\n                    hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                ).sample\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states =",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1028, "column": 4 },
          "end": { "row": 1028, "column": 4 }
        }
      }
    }
  ],
  [
    "1630",
    {
      "pageContent": "class UpBlockFlat(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels)])\n        else:\n            self.upsamplers = None\n\n        self",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1073, "column": 0 },
          "end": { "row": 1073, "column": 0 }
        }
      }
    }
  ],
  [
    "1631",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels)])\n        else:\n            self.upsamplers = None\n\n        self.gradient_checkpointing = False",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1074, "column": 4 },
          "end": { "row": 1074, "column": 4 }
        }
      }
    }
  ],
  [
    "1632",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states, upsample_size)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1121, "column": 4 },
          "end": { "row": 1121, "column": 4 }
        }
      }
    }
  ],
  [
    "1633",
    {
      "pageContent": "class CrossAttnUpBlockFlat(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        prev_output_channel: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_upsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_s",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1148, "column": 0 },
          "end": { "row": 1148, "column": 0 }
        }
      }
    }
  ],
  [
    "1634",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        prev_output_channel: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_upsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlockFlat(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=r",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1149, "column": 4 },
          "end": { "row": 1149, "column": 4 }
        }
      }
    }
  ],
  [
    "1635",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        res_hidden_states_tuple,\n        temb=None,\n        encoder_hidden_states=None,\n        cross_attention_kwargs=None,\n        upsample_size=None,\n        attention_mask=None,\n    ):\n        # TODO(Patrick, William) - attention mask is not used\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(attn, return_dict=False),\n                    hidden_states,\n                    encoder_hidden_states,\n                    cross_attention_kwargs,\n                )[0]\n            else:\n                hidden_states = resnet(hidden_states, temb)\n                hidden_states = attn(\n                    hidden_states,\n         ",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1231, "column": 4 },
          "end": { "row": 1231, "column": 4 }
        }
      }
    }
  ],
  [
    "1636",
    {
      "pageContent": "class UNetMidBlockFlatCrossAttn(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlockFlat(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            if not dual_cross_attention:\n                attentions.append(\n                    Transformer2DModel(\n                        attn_num_head_channel",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1282, "column": 0 },
          "end": { "row": 1282, "column": 0 }
        }
      }
    }
  ],
  [
    "1637",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlockFlat(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            if not dual_cross_attention:\n                attentions.append(\n                    Transformer2DModel(\n                        attn_num_head_channels,\n                        in_channels // attn_n",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1283, "column": 4 },
          "end": { "row": 1283, "column": 4 }
        }
      }
    }
  ],
  [
    "1638",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            hidden_states = attn(\n                hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                cross_attention_kwargs=cross_attention_kwargs,\n            ).sample\n            hidden_states = resnet(hidden_states, temb)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1367, "column": 4 },
          "end": { "row": 1367, "column": 4 }
        }
      }
    }
  ],
  [
    "1639",
    {
      "pageContent": "class UNetMidBlockFlatSimpleCrossAttn(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        self.num_heads = in_channels // self.attn_num_head_channels\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlockFlat(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            attentions.append(\n                CrossAttention(\n                    query_dim=in_channels,\n                    cross_attention_dim=in_channels,\n                    heads=self.",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1383, "column": 0 },
          "end": { "row": 1383, "column": 0 }
        }
      }
    }
  ],
  [
    "1640",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        self.num_heads = in_channels // self.attn_num_head_channels\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlockFlat(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            attentions.append(\n                CrossAttention(\n                    query_dim=in_channels,\n                    cross_attention_dim=in_channels,\n                    heads=self.num_heads,\n                    dim_head=attn_num_head_",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1384, "column": 4 },
          "end": { "row": 1384, "column": 4 }
        }
      }
    }
  ],
  [
    "1641",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            # attn\n            hidden_states = attn(\n                hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                attention_mask=attention_mask,\n                **cross_attention_kwargs,\n            )\n\n            # resnet\n            hidden_states = resnet(hidden_states, temb)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py",
        "range": {
          "start": { "row": 1457, "column": 4 },
          "end": { "row": 1457, "column": 4 }
        }
      }
    }
  ],
  [
    "1642",
    {
      "pageContent": "class VersatileDiffusionImageVariationPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqvae ([`VQModel`]):\n            Vector-quantized (VQ) Model to encode and decode images to and from latent representations.\n        bert ([`LDMBertModel`]):\n            Text-encoder model based on [BERT](https://huggingface.co/docs/transformers/model_doc/bert) architecture.\n        tokenizer (`transformers.BertTokenizer`):\n            Tokenizer of class\n            [BertTokenizer](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n    \"\"\"\n    image_feature_extractor: CLIPFeatureExtractor\n    image_encoder: CLIPVisionModelWithProjection\n    image_unet: UNet2DConditionModel\n    vae: AutoencoderKL\n    scheduler: KarrasDiffusionSchedulers\n\n    def __init__(\n        self,\n        image_feature_extractor: CLIPFeatureExtractor,\n        image_encoder: CLIPVisionModelWithProjection,\n        image_unet: UNet2DConditionModel,\n        vae: AutoencoderKL,\n        scheduler: KarrasDiffusionSc",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "1643",
    {
      "pageContent": "def __init__(\n        self,\n        image_feature_extractor: CLIPFeatureExtractor,\n        image_encoder: CLIPVisionModelWithProjection,\n        image_unet: UNet2DConditionModel,\n        vae: AutoencoderKL,\n        scheduler: KarrasDiffusionSchedulers,\n    ):\n        super().__init__()\n        self.register_modules(\n            image_feature_extractor=image_feature_extractor,\n            image_encoder=image_encoder,\n            image_unet=image_unet,\n            vae=vae,\n            scheduler=scheduler,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 56, "column": 4 },
          "end": { "row": 56, "column": 4 }
        }
      }
    }
  ],
  [
    "1644",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.image_unet, self.text_unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 74, "column": 4 },
          "end": { "row": 74, "column": 4 }
        }
      }
    }
  ],
  [
    "1645",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n\n        def normalize_embeddings(encoder_output):\n            embeds = self.image_encoder.vision_model.post_layernorm(encoder_output.last_hidden_state)\n            embeds = self.image_encoder.visual_projection(embeds)\n            embeds_pooled = embeds[:, 0:1]\n            embeds = embeds / torch.norm(embeds_pooled, dim=-1, keepdim=True)\n            return embeds\n\n        if isinstance(prompt, torch.Tensor) and len(prompt.shape) == 4:\n            prompt = [p for p in prompt]\n\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        image_input = self.image_feature_extractor(images=prompt, return_tensors=\"pt\")\n        pixel_values = image_input.pixel_values.to(device).to(self.image_encoder.dtype)\n        image_embeddings = s",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 110, "column": 4 },
          "end": { "row": 110, "column": 4 }
        }
      }
    }
  ],
  [
    "1646",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 190, "column": 4 },
          "end": { "row": 190, "column": 4 }
        }
      }
    }
  ],
  [
    "1647",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 199, "column": 4 },
          "end": { "row": 199, "column": 4 }
        }
      }
    }
  ],
  [
    "1648",
    {
      "pageContent": "def check_inputs(self, image, height, width, callback_steps):\n        if (\n            not isinstance(image, torch.Tensor)\n            and not isinstance(image, PIL.Image.Image)\n            and not isinstance(image, list)\n        ):\n            raise ValueError(\n                \"`image` has to be of type `torch.FloatTensor` or `PIL.Image.Image` or `List[PIL.Image.Image]` but is\"\n                f\" {type(image)}\"\n            )\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 217, "column": 4 },
          "end": { "row": 217, "column": 4 }
        }
      }
    }
  ],
  [
    "1649",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py",
        "range": {
          "start": { "row": 240, "column": 4 },
          "end": { "row": 240, "column": 4 }
        }
      }
    }
  ],
  [
    "1650",
    {
      "pageContent": "class VersatileDiffusionTextToImagePipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqvae ([`VQModel`]):\n            Vector-quantized (VQ) Model to encode and decode images to and from latent representations.\n        bert ([`LDMBertModel`]):\n            Text-encoder model based on [BERT](https://huggingface.co/docs/transformers/model_doc/bert) architecture.\n        tokenizer (`transformers.BertTokenizer`):\n            Tokenizer of class\n            [BertTokenizer](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n    \"\"\"\n    tokenizer: CLIPTokenizer\n    image_feature_extractor: CLIPFeatureExtractor\n    text_encoder: CLIPTextModelWithProjection\n    image_unet: UNet2DConditionModel\n    text_unet: UNetFlatConditionModel\n    vae: AutoencoderKL\n    scheduler: KarrasDiffusionSchedulers\n\n    _optional_components = [\"text_unet\"]\n\n    def __init__(\n        self,\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModelWithProjection,\n        image_unet: UNet2DC",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1651",
    {
      "pageContent": "def __init__(\n        self,\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModelWithProjection,\n        image_unet: UNet2DConditionModel,\n        text_unet: UNetFlatConditionModel,\n        vae: AutoencoderKL,\n        scheduler: KarrasDiffusionSchedulers,\n    ):\n        super().__init__()\n        self.register_modules(\n            tokenizer=tokenizer,\n            text_encoder=text_encoder,\n            image_unet=image_unet,\n            text_unet=text_unet,\n            vae=vae,\n            scheduler=scheduler,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n\n        if self.text_unet is not None:\n            self._swap_unet_attention_blocks()",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 59, "column": 4 },
          "end": { "row": 59, "column": 4 }
        }
      }
    }
  ],
  [
    "1652",
    {
      "pageContent": "def _swap_unet_attention_blocks(self):\n        \"\"\"\n        Swap the `Transformer2DModel` blocks between the image and text UNets\n        \"\"\"\n        for name, module in self.image_unet.named_modules():\n            if isinstance(module, Transformer2DModel):\n                parent_name, index = name.rsplit(\".\", 1)\n                index = int(index)\n                self.image_unet.get_submodule(parent_name)[index], self.text_unet.get_submodule(parent_name)[index] = (\n                    self.text_unet.get_submodule(parent_name)[index],\n                    self.image_unet.get_submodule(parent_name)[index],\n                )",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 82, "column": 4 },
          "end": { "row": 82, "column": 4 }
        }
      }
    }
  ],
  [
    "1653",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.image_unet, self.text_unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 98, "column": 4 },
          "end": { "row": 98, "column": 4 }
        }
      }
    }
  ],
  [
    "1654",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n\n        def normalize_embeddings(encoder_output):\n            embeds = self.text_encoder.text_projection(encoder_output.last_hidden_state)\n            embeds_pooled = encoder_output.text_embeds\n            embeds = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n            return embeds\n\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n        if not torch.equal(text_input_",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 134, "column": 4 },
          "end": { "row": 134, "column": 4 }
        }
      }
    }
  ],
  [
    "1655",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 247, "column": 4 },
          "end": { "row": 247, "column": 4 }
        }
      }
    }
  ],
  [
    "1656",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 256, "column": 4 },
          "end": { "row": 256, "column": 4 }
        }
      }
    }
  ],
  [
    "1657",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 274, "column": 4 },
          "end": { "row": 274, "column": 4 }
        }
      }
    }
  ],
  [
    "1658",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py",
        "range": {
          "start": { "row": 322, "column": 4 },
          "end": { "row": 322, "column": 4 }
        }
      }
    }
  ],
  [
    "1659",
    {
      "pageContent": "class DiTPipeline(DiffusionPipeline):\n    r\"\"\"\n    This pipeline inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        transformer ([`Transformer2DModel`]):\n            Class conditioned Transformer in Diffusion model to denoise the encoded image latents.\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        scheduler ([`DDIMScheduler`]):\n            A scheduler to be used in combination with `dit` to denoise the encoded image latents.\n    \"\"\"\n\n    def __init__(\n        self,\n        transformer: Transformer2DModel,\n        vae: AutoencoderKL,\n        scheduler: KarrasDiffusionSchedulers,\n        id2label: Optional[Dict[int, str]] = None,\n    ):\n        super().__init__()\n        self.register_modules(transformer=transformer, vae=vae, scheduler=scheduler)\n\n        # create a imagenet -> id dictionary for easier use\n        self.labels = {}\n        if id2label is not None:\n            for key, value in id2label.items():\n                for label in value.split(\",\"):\n                    self.labels[label.lstrip().rstrip()] = int(key)\n            self.labels = dict(sorted(self.labels.items()))\n\n    def get_label_ids(self, label: Union[str, List[str]]) -> List[int]:\n        r\"\"\"\n\n        Map label strings, *e.g.* from ImageNet, to corresponding class ids.\n\n        Parameters:\n            label (`s",
      "metadata": {
        "source": "src/diffusers/pipelines/dit/pipeline_dit.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "1660",
    {
      "pageContent": "def __init__(\n        self,\n        transformer: Transformer2DModel,\n        vae: AutoencoderKL,\n        scheduler: KarrasDiffusionSchedulers,\n        id2label: Optional[Dict[int, str]] = None,\n    ):\n        super().__init__()\n        self.register_modules(transformer=transformer, vae=vae, scheduler=scheduler)\n\n        # create a imagenet -> id dictionary for easier use\n        self.labels = {}\n        if id2label is not None:\n            for key, value in id2label.items():\n                for label in value.split(\",\"):\n                    self.labels[label.lstrip().rstrip()] = int(key)\n            self.labels = dict(sorted(self.labels.items()))",
      "metadata": {
        "source": "src/diffusers/pipelines/dit/pipeline_dit.py",
        "range": {
          "start": { "row": 44, "column": 4 },
          "end": { "row": 44, "column": 4 }
        }
      }
    }
  ],
  [
    "1661",
    {
      "pageContent": "def get_label_ids(self, label: Union[str, List[str]]) -> List[int]:\n        r\"\"\"\n\n        Map label strings, *e.g.* from ImageNet, to corresponding class ids.\n\n        Parameters:\n            label (`str` or `dict` of `str`): label strings to be mapped to class ids.\n\n        Returns:\n            `list` of `int`: Class ids to be processed by pipeline.\n        \"\"\"\n\n        if not isinstance(label, list):\n            label = list(label)\n\n        for l in label:\n            if l not in self.labels:\n                raise ValueError(\n                    f\"{l} does not exist. Please make sure to select one of the following labels: \\n {self.labels}.\"\n                )\n\n        return [self.labels[l] for l in label]",
      "metadata": {
        "source": "src/diffusers/pipelines/dit/pipeline_dit.py",
        "range": {
          "start": { "row": 62, "column": 4 },
          "end": { "row": 62, "column": 4 }
        }
      }
    }
  ],
  [
    "1662",
    {
      "pageContent": "class UnCLIPTextProjModel(ModelMixin, ConfigMixin):\n    \"\"\"\n    Utility class for CLIP embeddings. Used to combine the image and text embeddings into a format usable by the\n    decoder.\n\n    For more details, see the original paper: https://arxiv.org/abs/2204.06125 section 2.1\n    \"\"\"\n\n    @register_to_config\n    def __init__(\n        self,\n        *,\n        clip_extra_context_tokens: int = 4,\n        clip_embeddings_dim: int = 768,\n        time_embed_dim: int,\n        cross_attention_dim,\n    ):\n        super().__init__()\n\n        self.learned_classifier_free_guidance_embeddings = nn.Parameter(torch.zeros(clip_embeddings_dim))\n\n        # parameters for additional clip time embeddings\n        self.embedding_proj = nn.Linear(clip_embeddings_dim, time_embed_dim)\n        self.clip_image_embeddings_project_to_time_embeddings = nn.Linear(clip_embeddings_dim, time_embed_dim)\n\n        # parameters for encoder hidden states\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.clip_extra_context_tokens_proj = nn.Linear(\n            clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim\n        )\n        self.encoder_hidden_states_proj = nn.Linear(clip_embeddings_dim, cross_attention_dim)\n        self.text_encoder_hidden_states_norm = nn.LayerNorm(cross_attention_dim)\n\n    def forward(self, *, image_embeddings, prompt_embeds, text_encoder_hidden_states, do_classifier_free_guidance):\n        if do_classifier_free_guidance:\n            # Add the classifier free guidance embeddings to the image embeddings\n            image_embeddings_batch_",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/text_proj.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "1663",
    {
      "pageContent": "def forward(self, *, image_embeddings, prompt_embeds, text_encoder_hidden_states, do_classifier_free_guidance):\n        if do_classifier_free_guidance:\n            # Add the classifier free guidance embeddings to the image embeddings\n            image_embeddings_batch_size = image_embeddings.shape[0]\n            classifier_free_guidance_embeddings = self.learned_classifier_free_guidance_embeddings.unsqueeze(0)\n            classifier_free_guidance_embeddings = classifier_free_guidance_embeddings.expand(\n                image_embeddings_batch_size, -1\n            )\n            image_embeddings = torch.cat([classifier_free_guidance_embeddings, image_embeddings], dim=0)\n\n        # The image embeddings batch size and the text embeddings batch size are equal\n        assert image_embeddings.shape[0] == prompt_embeds.shape[0]\n\n        batch_size = prompt_embeds.shape[0]\n\n        # \"Specifically, we modify the architecture described in Nichol et al. (2021) by projecting and\n        # adding CLIP embeddings to the existing timestep embedding, ...\n        time_projected_prompt_embeds = self.embedding_proj(prompt_embeds)\n        time_projected_image_embeddings = self.clip_image_embeddings_project_to_time_embeddings(image_embeddings)\n        additive_clip_time_embeddings = time_projected_image_embeddings + time_projected_prompt_embeds\n\n        # ... and by projecting CLIP embeddings into four\n        # extra tokens of context that are concatenated to the sequence of outputs from the GLIDE text encoder\"\n        clip_extra_context_tokens = self.clip_extra_context_tokens_proj(image_embeddi",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/text_proj.py",
        "range": {
          "start": { "row": 54, "column": 4 },
          "end": { "row": 54, "column": 4 }
        }
      }
    }
  ],
  [
    "1664",
    {
      "pageContent": "class UnCLIPImageVariationPipeline(DiffusionPipeline):\n    \"\"\"\n    Pipeline to generate variations from an input image using unCLIP\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        text_encoder ([`CLIPTextModelWithProjection`]):\n            Frozen text-encoder.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `image_encoder`.\n        image_encoder ([`CLIPVisionModelWithProjection`]):\n            Frozen CLIP image-encoder. unCLIP Image Variation uses the vision portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModelWithProjection),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        text_proj ([`UnCLIPTextProjModel`]):\n            Utility class to prepare and combine the embeddings before they are passed to the decoder.\n        decoder ([`UNet2DConditionModel`]):\n            The decoder to invert the image embedding into an image.\n        super_res_first ([`UNet2DModel`]):\n            Super resolution unet. Used in all but the last step of the super resolution diff",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "1665",
    {
      "pageContent": "def __init__(\n        self,\n        decoder: UNet2DConditionModel,\n        text_encoder: CLIPTextModelWithProjection,\n        tokenizer: CLIPTokenizer,\n        text_proj: UnCLIPTextProjModel,\n        feature_extractor: CLIPFeatureExtractor,\n        image_encoder: CLIPVisionModelWithProjection,\n        super_res_first: UNet2DModel,\n        super_res_last: UNet2DModel,\n        decoder_scheduler: UnCLIPScheduler,\n        super_res_scheduler: UnCLIPScheduler,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            decoder=decoder,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            text_proj=text_proj,\n            feature_extractor=feature_extractor,\n            image_encoder=image_encoder,\n            super_res_first=super_res_first,\n            super_res_last=super_res_last,\n            decoder_scheduler=decoder_scheduler,\n            super_res_scheduler=super_res_scheduler,\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py",
        "range": {
          "start": { "row": 83, "column": 4 },
          "end": { "row": 83, "column": 4 }
        }
      }
    }
  ],
  [
    "1666",
    {
      "pageContent": "def prepare_latents(self, shape, dtype, device, generator, latents, scheduler):\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        latents = latents * scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py",
        "range": {
          "start": { "row": 112, "column": 4 },
          "end": { "row": 112, "column": 4 }
        }
      }
    }
  ],
  [
    "1667",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance):\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        text_mask = text_inputs.attention_mask.bool().to(device)\n        text_encoder_output = self.text_encoder(text_input_ids.to(device))\n\n        prompt_embeds = text_encoder_output.text_embeds\n        text_encoder_hidden_states = text_encoder_output.last_hidden_state\n\n        prompt_embeds = prompt_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n        text_encoder_hidden_states = text_encoder_hidden_states.repeat_interleave(num_images_per_prompt, dim=0)\n        text_mask = text_mask.repeat_interleave(num_images_per_prompt, dim=0)\n\n        if do_classifier_free_guidance:\n            uncond_tokens = [\"\"] * batch_size\n\n            max_length = text_input_ids.shape[-1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            uncond_text_mask = uncond_input.attention_mask.bool().to(device)\n            negative_prompt_embeds_text_encoder_output = self.text_encoder(uncond_input.input_ids.to(device))\n\n            negative_prompt_embeds = negative_prompt",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py",
        "range": {
          "start": { "row": 123, "column": 4 },
          "end": { "row": 123, "column": 4 }
        }
      }
    }
  ],
  [
    "1668",
    {
      "pageContent": "def _encode_image(self, image, device, num_images_per_prompt, image_embeddings: Optional[torch.Tensor] = None):\n        dtype = next(self.image_encoder.parameters()).dtype\n\n        if image_embeddings is None:\n            if not isinstance(image, torch.Tensor):\n                image = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n            image = image.to(device=device, dtype=dtype)\n            image_embeddings = self.image_encoder(image).image_embeds\n\n        image_embeddings = image_embeddings.repeat_interleave(num_images_per_prompt, dim=0)\n\n        return image_embeddings",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py",
        "range": {
          "start": { "row": 186, "column": 4 },
          "end": { "row": 186, "column": 4 }
        }
      }
    }
  ],
  [
    "1669",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, the pipeline's\n        models have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only\n        when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        models = [\n            self.decoder,\n            self.text_proj,\n            self.text_encoder,\n            self.super_res_first,\n            self.super_res_last,\n        ]\n        for cpu_offloaded_model in models:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py",
        "range": {
          "start": { "row": 200, "column": 4 },
          "end": { "row": 200, "column": 4 }
        }
      }
    }
  ],
  [
    "1670",
    {
      "pageContent": "class UnCLIPPipeline(DiffusionPipeline):\n    \"\"\"\n    Pipeline for text-to-image generation using unCLIP\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        text_encoder ([`CLIPTextModelWithProjection`]):\n            Frozen text-encoder.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        prior ([`PriorTransformer`]):\n            The canonincal unCLIP prior to approximate the image embedding from the text embedding.\n        text_proj ([`UnCLIPTextProjModel`]):\n            Utility class to prepare and combine the embeddings before they are passed to the decoder.\n        decoder ([`UNet2DConditionModel`]):\n            The decoder to invert the image embedding into an image.\n        super_res_first ([`UNet2DModel`]):\n            Super resolution unet. Used in all but the last step of the super resolution diffusion process.\n        super_res_last ([`UNet2DModel`]):\n            Super resolution unet. Used in the last step of the super resolution diffusion process.\n        prior_scheduler ([`UnCLIPScheduler`]):\n            Scheduler used in the prior denoising process. Just a modified DDPMScheduler.\n        decoder_scheduler ([`UnCLIPScheduler`]):\n            Scheduler used in the decoder denoising process. Just a modified DDPMScheduler.\n   ",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "1671",
    {
      "pageContent": "def __init__(\n        self,\n        prior: PriorTransformer,\n        decoder: UNet2DConditionModel,\n        text_encoder: CLIPTextModelWithProjection,\n        tokenizer: CLIPTokenizer,\n        text_proj: UnCLIPTextProjModel,\n        super_res_first: UNet2DModel,\n        super_res_last: UNet2DModel,\n        prior_scheduler: UnCLIPScheduler,\n        decoder_scheduler: UnCLIPScheduler,\n        super_res_scheduler: UnCLIPScheduler,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            prior=prior,\n            decoder=decoder,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            text_proj=text_proj,\n            super_res_first=super_res_first,\n            super_res_last=super_res_last,\n            prior_scheduler=prior_scheduler,\n            decoder_scheduler=decoder_scheduler,\n            super_res_scheduler=super_res_scheduler,\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip.py",
        "range": {
          "start": { "row": 77, "column": 4 },
          "end": { "row": 77, "column": 4 }
        }
      }
    }
  ],
  [
    "1672",
    {
      "pageContent": "def prepare_latents(self, shape, dtype, device, generator, latents, scheduler):\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        latents = latents * scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip.py",
        "range": {
          "start": { "row": 105, "column": 4 },
          "end": { "row": 105, "column": 4 }
        }
      }
    }
  ],
  [
    "1673",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        text_model_output: Optional[Union[CLIPTextModelOutput, Tuple]] = None,\n        text_attention_mask: Optional[torch.Tensor] = None,\n    ):\n        if text_model_output is None:\n            batch_size = len(prompt) if isinstance(prompt, list) else 1\n            # get prompt text embeddings\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n            text_mask = text_inputs.attention_mask.bool().to(device)\n\n            untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n                text_input_ids, untruncated_ids\n            ):\n                removed_text = self.tokenizer.batch_decode(\n                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n                )\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n\n            text_encoder_output = self.text_encoder(",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip.py",
        "range": {
          "start": { "row": 116, "column": 4 },
          "end": { "row": 116, "column": 4 }
        }
      }
    }
  ],
  [
    "1674",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, the pipeline's\n        models have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only\n        when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        # TODO: self.prior.post_process_latents is not covered by the offload hooks, so it fails if added to the list\n        models = [\n            self.decoder,\n            self.text_proj,\n            self.text_encoder,\n            self.super_res_first,\n            self.super_res_last,\n        ]\n        for cpu_offloaded_model in models:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/unclip/pipeline_unclip.py",
        "range": {
          "start": { "row": 207, "column": 4 },
          "end": { "row": 207, "column": 4 }
        }
      }
    }
  ],
  [
    "1675",
    {
      "pageContent": "class PaintByExampleImageEncoder(CLIPPreTrainedModel):\n    def __init__(self, config, proj_size=768):\n        super().__init__(config)\n        self.proj_size = proj_size\n\n        self.model = CLIPVisionModel(config)\n        self.mapper = PaintByExampleMapper(config)\n        self.final_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.proj_out = nn.Linear(config.hidden_size, self.proj_size)\n\n        # uncondition for scaling\n        self.uncond_vector = nn.Parameter(torch.randn((1, 1, self.proj_size)))\n\n    def forward(self, pixel_values, return_uncond_vector=False):\n        clip_output = self.model(pixel_values=pixel_values)\n        latent_states = clip_output.pooler_output\n        latent_states = self.mapper(latent_states[:, None])\n        latent_states = self.final_layer_norm(latent_states)\n        latent_states = self.proj_out(latent_states)\n        if return_uncond_vector:\n            return latent_states, self.uncond_vector\n\n        return latent_states",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/image_encoder.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1676",
    {
      "pageContent": "def __init__(self, config, proj_size=768):\n        super().__init__(config)\n        self.proj_size = proj_size\n\n        self.model = CLIPVisionModel(config)\n        self.mapper = PaintByExampleMapper(config)\n        self.final_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.proj_out = nn.Linear(config.hidden_size, self.proj_size)\n\n        # uncondition for scaling\n        self.uncond_vector = nn.Parameter(torch.randn((1, 1, self.proj_size)))",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/image_encoder.py",
        "range": {
          "start": { "row": 25, "column": 4 },
          "end": { "row": 25, "column": 4 }
        }
      }
    }
  ],
  [
    "1677",
    {
      "pageContent": "def forward(self, pixel_values, return_uncond_vector=False):\n        clip_output = self.model(pixel_values=pixel_values)\n        latent_states = clip_output.pooler_output\n        latent_states = self.mapper(latent_states[:, None])\n        latent_states = self.final_layer_norm(latent_states)\n        latent_states = self.proj_out(latent_states)\n        if return_uncond_vector:\n            return latent_states, self.uncond_vector\n\n        return latent_states",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/image_encoder.py",
        "range": {
          "start": { "row": 37, "column": 4 },
          "end": { "row": 37, "column": 4 }
        }
      }
    }
  ],
  [
    "1678",
    {
      "pageContent": "class PaintByExampleMapper(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        num_layers = (config.num_hidden_layers + 1) // 5\n        hid_size = config.hidden_size\n        num_heads = 1\n        self.blocks = nn.ModuleList(\n            [\n                BasicTransformerBlock(hid_size, num_heads, hid_size, activation_fn=\"gelu\", attention_bias=True)\n                for _ in range(num_layers)\n            ]\n        )\n\n    def forward(self, hidden_states):\n        for block in self.blocks:\n            hidden_states = block(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/image_encoder.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "1679",
    {
      "pageContent": "def __init__(self, config):\n        super().__init__()\n        num_layers = (config.num_hidden_layers + 1) // 5\n        hid_size = config.hidden_size\n        num_heads = 1\n        self.blocks = nn.ModuleList(\n            [\n                BasicTransformerBlock(hid_size, num_heads, hid_size, activation_fn=\"gelu\", attention_bias=True)\n                for _ in range(num_layers)\n            ]\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/image_encoder.py",
        "range": {
          "start": { "row": 50, "column": 4 },
          "end": { "row": 50, "column": 4 }
        }
      }
    }
  ],
  [
    "1680",
    {
      "pageContent": "def forward(self, hidden_states):\n        for block in self.blocks:\n            hidden_states = block(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/image_encoder.py",
        "range": {
          "start": { "row": 62, "column": 4 },
          "end": { "row": 62, "column": 4 }
        }
      }
    }
  ],
  [
    "1681",
    {
      "pageContent": "def prepare_mask_and_masked_image(image, mask):\n    \"\"\"\n    Prepares a pair (image, mask) to be consumed by the Paint by Example pipeline. This means that those inputs will be\n    converted to ``torch.Tensor`` with shapes ``batch x channels x height x width`` where ``channels`` is ``3`` for the\n    ``image`` and ``1`` for the ``mask``.\n\n    The ``image`` will be converted to ``torch.float32`` and normalized to be in ``[-1, 1]``. The ``mask`` will be\n    binarized (``mask > 0.5``) and cast to ``torch.float32`` too.\n\n    Args:\n        image (Union[np.array, PIL.Image, torch.Tensor]): The image to inpaint.\n            It can be a ``PIL.Image``, or a ``height x width x 3`` ``np.array`` or a ``channels x height x width``\n            ``torch.Tensor`` or a ``batch x channels x height x width`` ``torch.Tensor``.\n        mask (_type_): The mask to apply to the image, i.e. regions to inpaint.\n            It can be a ``PIL.Image``, or a ``height x width`` ``np.array`` or a ``1 x height x width``\n            ``torch.Tensor`` or a ``batch x 1 x height x width`` ``torch.Tensor``.\n\n\n    Raises:\n        ValueError: ``torch.Tensor`` images should be in the ``[-1, 1]`` range. ValueError: ``torch.Tensor`` mask\n        should be in the ``[0, 1]`` range. ValueError: ``mask`` and ``image`` should have the same spatial dimensions.\n        TypeError: ``mask`` is a ``torch.Tensor`` but ``image`` is not\n            (ot the other way around).\n\n    Returns:\n        tuple[torch.Tensor]: The pair (mask, masked_image) as ``torch.Tensor`` with 4\n            dimensions: ``batch x channels x height x width`",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "1682",
    {
      "pageContent": "class PaintByExamplePipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for image-guided image inpainting using Stable Diffusion. *This is an experimental feature*.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        image_encoder ([`PaintByExampleImageEncoder`]):\n            Encodes the example input image. The unet is conditioned on the example image instead of a text prompt.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLIPFeatureExtractor`])",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 136, "column": 0 },
          "end": { "row": 136, "column": 0 }
        }
      }
    }
  ],
  [
    "1683",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        image_encoder: PaintByExampleImageEncoder,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = False,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            vae=vae,\n            image_encoder=image_encoder,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 165, "column": 4 },
          "end": { "row": 165, "column": 4 }
        }
      }
    }
  ],
  [
    "1684",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.vae, self.image_encoder]:\n            cpu_offload(cpu_offloaded_model, execution_device=device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 188, "column": 4 },
          "end": { "row": 188, "column": 4 }
        }
      }
    }
  ],
  [
    "1685",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 227, "column": 4 },
          "end": { "row": 227, "column": 4 }
        }
      }
    }
  ],
  [
    "1686",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 238, "column": 4 },
          "end": { "row": 238, "column": 4 }
        }
      }
    }
  ],
  [
    "1687",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 256, "column": 4 },
          "end": { "row": 256, "column": 4 }
        }
      }
    }
  ],
  [
    "1688",
    {
      "pageContent": "def check_inputs(self, image, height, width, callback_steps):\n        if (\n            not isinstance(image, torch.Tensor)\n            and not isinstance(image, PIL.Image.Image)\n            and not isinstance(image, list)\n        ):\n            raise ValueError(\n                \"`image` has to be of type `torch.FloatTensor` or `PIL.Image.Image` or `List[PIL.Image.Image]` but is\"\n                f\" {type(image)}\"\n            )\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 265, "column": 4 },
          "end": { "row": 265, "column": 4 }
        }
      }
    }
  ],
  [
    "1689",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 288, "column": 4 },
          "end": { "row": 288, "column": 4 }
        }
      }
    }
  ],
  [
    "1690",
    {
      "pageContent": "def prepare_mask_latents(\n        self, mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance\n    ):\n        # resize the mask to latents shape as we concatenate the mask to the latents\n        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n        # and half precision\n        mask = torch.nn.functional.interpolate(\n            mask, size=(height // self.vae_scale_factor, width // self.vae_scale_factor)\n        )\n        mask = mask.to(device=device, dtype=dtype)\n\n        masked_image = masked_image.to(device=device, dtype=dtype)\n\n        # encode the mask image into latents space so we can concatenate it to the latents\n        if isinstance(generator, list):\n            masked_image_latents = [\n                self.vae.encode(masked_image[i : i + 1]).latent_dist.sample(generator=generator[i])\n                for i in range(batch_size)\n            ]\n            masked_image_latents = torch.cat(masked_image_latents, dim=0)\n        else:\n            masked_image_latents = self.vae.encode(masked_image).latent_dist.sample(generator=generator)\n        masked_image_latents = self.vae.config.scaling_factor * masked_image_latents\n\n        # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n        if mask.shape[0] < batch_size:\n            if not batch_size % mask.shape[0] == 0:\n                raise ValueError(\n                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n                  ",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 306, "column": 4 },
          "end": { "row": 306, "column": 4 }
        }
      }
    }
  ],
  [
    "1691",
    {
      "pageContent": "def _encode_image(self, image, device, num_images_per_prompt, do_classifier_free_guidance):\n        dtype = next(self.image_encoder.parameters()).dtype\n\n        if not isinstance(image, torch.Tensor):\n            image = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n        image = image.to(device=device, dtype=dtype)\n        image_embeddings, negative_prompt_embeds = self.image_encoder(image, return_uncond_vector=True)\n\n        # duplicate image embeddings for each generation per prompt, using mps friendly method\n        bs_embed, seq_len, _ = image_embeddings.shape\n        image_embeddings = image_embeddings.repeat(1, num_images_per_prompt, 1)\n        image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        if do_classifier_free_guidance:\n            negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1)\n            negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])\n\n        return image_embeddings",
      "metadata": {
        "source": "src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py",
        "range": {
          "start": { "row": 357, "column": 4 },
          "end": { "row": 357, "column": 4 }
        }
      }
    }
  ],
  [
    "1692",
    {
      "pageContent": "class DanceDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        unet ([`UNet1DModel`]): U-Net architecture to denoise the encoded image.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n            [`IPNDMScheduler`].\n    \"\"\"\n\n    def __init__(self, unet, scheduler):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        num_inference_steps: int = 100,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        audio_length_in_s: Optional[float] = None,\n        return_dict: bool = True,\n    ) -> Union[AudioPipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of audio samples to generate.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality audio sample at\n                the expense of slower inference.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n           ",
      "metadata": {
        "source": "src/diffusers/pipelines/dance_diffusion/pipeline_dance_diffusion.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "1693",
    {
      "pageContent": "class StableDiffusionPanoramaPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using \"MultiDiffusion: Fusing Diffusion Paths for Controlled Image\n    Generation\".\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.).\n\n    To generate panorama-like images, be sure to pass the `width` parameter accordingly when using the pipeline. Our\n    recommendation for the `width` value is 2048. This is the default value of the `width` parameter for this pipeline.\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "1694",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: DDIMScheduler,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if isinstance(scheduler, PNDMScheduler):\n            logger.error(\"PNDMScheduler for this pipeline is currently not supported.\")\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker,",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 82, "column": 4 },
          "end": { "row": 82, "column": 4 }
        }
      }
    }
  ],
  [
    "1695",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 127, "column": 4 },
          "end": { "row": 127, "column": 4 }
        }
      }
    }
  ],
  [
    "1696",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 137, "column": 4 },
          "end": { "row": 137, "column": 4 }
        }
      }
    }
  ],
  [
    "1697",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 145, "column": 4 },
          "end": { "row": 145, "column": 4 }
        }
      }
    }
  ],
  [
    "1698",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 190, "column": 4 },
          "end": { "row": 190, "column": 4 }
        }
      }
    }
  ],
  [
    "1699",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 329, "column": 4 },
          "end": { "row": 329, "column": 4 }
        }
      }
    }
  ],
  [
    "1700",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 340, "column": 4 },
          "end": { "row": 340, "column": 4 }
        }
      }
    }
  ],
  [
    "1701",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 349, "column": 4 },
          "end": { "row": 349, "column": 4 }
        }
      }
    }
  ],
  [
    "1702",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 367, "column": 4 },
          "end": { "row": 367, "column": 4 }
        }
      }
    }
  ],
  [
    "1703",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 415, "column": 4 },
          "end": { "row": 415, "column": 4 }
        }
      }
    }
  ],
  [
    "1704",
    {
      "pageContent": "def get_views(self, panorama_height, panorama_width, window_size=64, stride=8):\n        # Here, we define the mappings F_i (see Eq. 7 in the MultiDiffusion paper https://arxiv.org/abs/2302.08113)\n        panorama_height /= 8\n        panorama_width /= 8\n        num_blocks_height = (panorama_height - window_size) // stride + 1\n        num_blocks_width = (panorama_width - window_size) // stride + 1\n        total_num_blocks = int(num_blocks_height * num_blocks_width)\n        views = []\n        for i in range(total_num_blocks):\n            h_start = int((i // num_blocks_width) * stride)\n            h_end = h_start + window_size\n            w_start = int((i % num_blocks_width) * stride)\n            w_end = w_start + window_size\n            views.append((h_start, h_end, w_start, w_end))\n        return views",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py",
        "range": {
          "start": { "row": 432, "column": 4 },
          "end": { "row": 432, "column": 4 }
        }
      }
    }
  ],
  [
    "1705",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n\n        image = [np.array(i.resize((w, h)))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1706",
    {
      "pageContent": "class OnnxStableDiffusionUpscalePipeline(StableDiffusionUpscalePipeline):\n    def __init__(\n        self,\n        vae: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: Any,\n        unet: OnnxRuntimeModel,\n        low_res_scheduler: DDPMScheduler,\n        scheduler: Any,\n        max_noise_level: int = 350,\n    ):\n        super().__init__(vae, text_encoder, tokenizer, unet, low_res_scheduler, scheduler, max_noise_level)\n\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]],\n        num_inference_steps: int = 75,\n        guidance_scale: float = 9.0,\n        noise_level: int = 20,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n    ):\n        # 1. Check inputs\n        self.check_inputs(prompt, image, noise_level, callback_steps)\n\n        # 2. Define call parameters\n        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 47, "column": 0 },
          "end": { "row": 47, "column": 0 }
        }
      }
    }
  ],
  [
    "1707",
    {
      "pageContent": "def __init__(\n        self,\n        vae: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: Any,\n        unet: OnnxRuntimeModel,\n        low_res_scheduler: DDPMScheduler,\n        scheduler: Any,\n        max_noise_level: int = 350,\n    ):\n        super().__init__(vae, text_encoder, tokenizer, unet, low_res_scheduler, scheduler, max_noise_level)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "1708",
    {
      "pageContent": "def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        image: Union[torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]],\n        num_inference_steps: int = 75,\n        guidance_scale: float = 9.0,\n        noise_level: int = 20,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n    ):\n        # 1. Check inputs\n        self.check_inputs(prompt, image, noise_level, callback_steps)\n\n        # 2. Define call parameters\n        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Encode input prompt\n        text_embeddings = self._encode_prompt(\n            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n        )\n\n        latents_dtype = ORT_TO_PT_TYPE[str(text_embeddings.dtype)]\n\n        # 4. Preprocess image\n        image = preprocess(im",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 60, "column": 4 },
          "end": { "row": 60, "column": 4 }
        }
      }
    }
  ],
  [
    "1709",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / 0.08333 * latents\n        image = self.vae(latent_sample=latents)[0]\n        image = np.clip(image / 2 + 0.5, 0, 1)\n        image = image.transpose((0, 2, 3, 1))\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 194, "column": 4 },
          "end": { "row": 194, "column": 4 }
        }
      }
    }
  ],
  [
    "1710",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n            )\n\n        # if hasattr(text_inputs, \"attention_mask\"):\n        #     attention_mask = text_inputs.attention_mask.to(device)\n        # else:\n        #     attention_mask = None\n\n        # no positional arguments to text_encoder\n        text_embeddings = self.text_encoder(\n            input_ids=text_input_ids.int().to(device),\n            # attention_mask=attention_mask,\n        )\n        text_embeddings = text_embeddings[0]\n\n        bs_embed, seq_len, _ = text_embeddings.shape\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        text_e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 201, "column": 4 },
          "end": { "row": 201, "column": 4 }
        }
      }
    }
  ],
  [
    "1711",
    {
      "pageContent": "class StableDiffusionImageVariationPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline to generate variations from an input image using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        image_encoder ([`CLIPVisionModelWithProjection`]):\n            Frozen CLIP image-encoder. Stable Diffusion Image Variation uses the vision portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModelWithProjection),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLI",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1712",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        image_encoder: CLIPVisionModelWithProjection,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warn(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 62, "column": 4 },
          "end": { "row": 62, "column": 4 }
        }
      }
    }
  ],
  [
    "1713",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.image_encoder, self.vae, self.safety_checker]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "1714",
    {
      "pageContent": "def _encode_image(self, image, device, num_images_per_prompt, do_classifier_free_guidance):\n        dtype = next(self.image_encoder.parameters()).dtype\n\n        if not isinstance(image, torch.Tensor):\n            image = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n        image = image.to(device=device, dtype=dtype)\n        image_embeddings = self.image_encoder(image).image_embeds\n        image_embeddings = image_embeddings.unsqueeze(1)\n\n        # duplicate image embeddings for each generation per prompt, using mps friendly method\n        bs_embed, seq_len, _ = image_embeddings.shape\n        image_embeddings = image_embeddings.repeat(1, num_images_per_prompt, 1)\n        image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        if do_classifier_free_guidance:\n            negative_prompt_embeds = torch.zeros_like(image_embeddings)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])\n\n        return image_embeddings",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 158, "column": 4 },
          "end": { "row": 158, "column": 4 }
        }
      }
    }
  ],
  [
    "1715",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 184, "column": 4 },
          "end": { "row": 184, "column": 4 }
        }
      }
    }
  ],
  [
    "1716",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 195, "column": 4 },
          "end": { "row": 195, "column": 4 }
        }
      }
    }
  ],
  [
    "1717",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 204, "column": 4 },
          "end": { "row": 204, "column": 4 }
        }
      }
    }
  ],
  [
    "1718",
    {
      "pageContent": "def check_inputs(self, image, height, width, callback_steps):\n        if (\n            not isinstance(image, torch.Tensor)\n            and not isinstance(image, PIL.Image.Image)\n            and not isinstance(image, list)\n        ):\n            raise ValueError(\n                \"`image` has to be of type `torch.FloatTensor` or `PIL.Image.Image` or `List[PIL.Image.Image]` but is\"\n                f\" {type(image)}\"\n            )\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 221, "column": 4 },
          "end": { "row": 221, "column": 4 }
        }
      }
    }
  ],
  [
    "1719",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py",
        "range": {
          "start": { "row": 244, "column": 4 },
          "end": { "row": 244, "column": 4 }
        }
      }
    }
  ],
  [
    "1720",
    {
      "pageContent": "class OnnxStableDiffusionPipeline(DiffusionPipeline):\n    vae_encoder: OnnxRuntimeModel\n    vae_decoder: OnnxRuntimeModel\n    text_encoder: OnnxRuntimeModel\n    tokenizer: CLIPTokenizer\n    unet: OnnxRuntimeModel\n    scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler]\n    safety_checker: OnnxRuntimeModel\n    feature_extractor: CLIPFeatureExtractor\n\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae_encoder: OnnxRuntimeModel,\n        vae_decoder: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: CLIPTokenizer,\n        unet: OnnxRuntimeModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: OnnxRuntimeModel,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "1721",
    {
      "pageContent": "def __init__(\n        self,\n        vae_encoder: OnnxRuntimeModel,\n        vae_decoder: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: CLIPTokenizer,\n        unet: OnnxRuntimeModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: OnnxRuntimeModel,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {sch",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 44, "column": 4 },
          "end": { "row": 44, "column": 4 }
        }
      }
    }
  ],
  [
    "1722",
    {
      "pageContent": "def _encode_prompt(self, prompt, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"np\").input_ids\n\n        if not np.array_equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 113, "column": 4 },
          "end": { "row": 113, "column": 4 }
        }
      }
    }
  ],
  [
    "1723",
    {
      "pageContent": "def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        height: Optional[int] = 512,\n        width: Optional[int] = 512,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[np.random.RandomState] = None,\n        latents: Optional[np.ndarray] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, np.ndarray], None]] = None,\n        callback_steps: int = 1,\n    ):\n        if isinstance(prompt, str):\n            batch_size = 1\n        elif isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if generator is None:\n            generator = np.random\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n      ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 190, "column": 4 },
          "end": { "row": 190, "column": 4 }
        }
      }
    }
  ],
  [
    "1724",
    {
      "pageContent": "class StableDiffusionOnnxPipeline(OnnxStableDiffusionPipeline):\n    def __init__(\n        self,\n        vae_encoder: OnnxRuntimeModel,\n        vae_decoder: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: CLIPTokenizer,\n        unet: OnnxRuntimeModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: OnnxRuntimeModel,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        deprecation_message = \"Please use `OnnxStableDiffusionPipeline` instead of `StableDiffusionOnnxPipeline`.\"\n        deprecate(\"StableDiffusionOnnxPipeline\", \"1.0.0\", deprecation_message)\n        super().__init__(\n            vae_encoder=vae_encoder,\n            vae_decoder=vae_decoder,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 325, "column": 0 },
          "end": { "row": 325, "column": 0 }
        }
      }
    }
  ],
  [
    "1725",
    {
      "pageContent": "def __init__(\n        self,\n        vae_encoder: OnnxRuntimeModel,\n        vae_decoder: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: CLIPTokenizer,\n        unet: OnnxRuntimeModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: OnnxRuntimeModel,\n        feature_extractor: CLIPFeatureExtractor,\n    ):\n        deprecation_message = \"Please use `OnnxStableDiffusionPipeline` instead of `StableDiffusionOnnxPipeline`.\"\n        deprecate(\"StableDiffusionOnnxPipeline\", \"1.0.0\", deprecation_message)\n        super().__init__(\n            vae_encoder=vae_encoder,\n            vae_decoder=vae_decoder,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py",
        "range": {
          "start": { "row": 326, "column": 4 },
          "end": { "row": 326, "column": 4 }
        }
      }
    }
  ],
  [
    "1726",
    {
      "pageContent": "class ModelWrapper:\n    def __init__(self, model, alphas_cumprod):\n        self.model = model\n        self.alphas_cumprod = alphas_cumprod\n\n    def apply_model(self, *args, **kwargs):\n        if len(args) == 3:\n            encoder_hidden_states = args[-1]\n            args = args[:2]\n        if kwargs.get(\"cond\", None) is not None:\n            encoder_hidden_states = kwargs.pop(\"cond\")\n        return self.model(*args, encoder_hidden_states=encoder_hidden_states, **kwargs).sample",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "1727",
    {
      "pageContent": "def apply_model(self, *args, **kwargs):\n        if len(args) == 3:\n            encoder_hidden_states = args[-1]\n            args = args[:2]\n        if kwargs.get(\"cond\", None) is not None:\n            encoder_hidden_states = kwargs.pop(\"cond\")\n        return self.model(*args, encoder_hidden_states=encoder_hidden_states, **kwargs).sample",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 34, "column": 4 },
          "end": { "row": 34, "column": 4 }
        }
      }
    }
  ],
  [
    "1728",
    {
      "pageContent": "class StableDiffusionKDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    <Tip warning={true}>\n\n        This is an experimental pipeline and is likely to change in the future.\n\n    </Tip>\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 43, "column": 0 },
          "end": { "row": 43, "column": 0 }
        }
      }
    }
  ],
  [
    "1729",
    {
      "pageContent": "def __init__(\n        self,\n        vae,\n        text_encoder,\n        tokenizer,\n        unet,\n        scheduler,\n        safety_checker,\n        feature_extractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        logger.info(\n            f\"{self.__class__} is an experimntal pipeline and is likely to change in the future. We recommend to use\"\n            \" this pipeline for fast experimentation / iteration if needed, but advice to rely on existing pipelines\"\n            \" as defined in https://huggingface.co/docs/diffusers/api/schedulers#implemented-schedulers for\"\n            \" production settings.\"\n        )\n\n        # get correct sigmas from LMS\n        scheduler = LMSDiscreteScheduler.from_config(scheduler.config)\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.register_to_config(requires_safety_checker=requires_safety_checker)\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n\n        model = ModelWrapper(unet, scheduler.alphas_cumprod)\n        if scheduler.prediction_type == \"v_prediction\":\n            self.k_diffusion_model = CompVisVDenoiser(model)\n        else:\n            self.k_diffusion_model = CompVisDenoiser(model)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 78, "column": 4 },
          "end": { "row": 78, "column": 4 }
        }
      }
    }
  ],
  [
    "1730",
    {
      "pageContent": "def set_scheduler(self, scheduler_type: str):\n        library = importlib.import_module(\"k_diffusion\")\n        sampling = getattr(library, \"sampling\")\n        self.sampler = getattr(sampling, scheduler_type)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 118, "column": 4 },
          "end": { "row": 118, "column": 4 }
        }
      }
    }
  ],
  [
    "1731",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "1732",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 150, "column": 4 },
          "end": { "row": 150, "column": 4 }
        }
      }
    }
  ],
  [
    "1733",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 198, "column": 4 },
          "end": { "row": 198, "column": 4 }
        }
      }
    }
  ],
  [
    "1734",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 337, "column": 4 },
          "end": { "row": 337, "column": 4 }
        }
      }
    }
  ],
  [
    "1735",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 348, "column": 4 },
          "end": { "row": 348, "column": 4 }
        }
      }
    }
  ],
  [
    "1736",
    {
      "pageContent": "def check_inputs(self, prompt, height, width, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 356, "column": 4 },
          "end": { "row": 356, "column": 4 }
        }
      }
    }
  ],
  [
    "1737",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py",
        "range": {
          "start": { "row": 371, "column": 4 },
          "end": { "row": 371, "column": 4 }
        }
      }
    }
  ],
  [
    "1738",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "1739",
    {
      "pageContent": "class OnnxStableDiffusionImg2ImgPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image to image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Ple",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 55, "column": 0 },
          "end": { "row": 55, "column": 0 }
        }
      }
    }
  ],
  [
    "1740",
    {
      "pageContent": "def __init__(\n        self,\n        vae_encoder: OnnxRuntimeModel,\n        vae_decoder: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: CLIPTokenizer,\n        unet: OnnxRuntimeModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: OnnxRuntimeModel,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {sch",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "1741",
    {
      "pageContent": "def _encode_prompt(self, prompt, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"np\").input_ids\n\n        if not np.array_equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 163, "column": 4 },
          "end": { "row": 163, "column": 4 }
        }
      }
    }
  ],
  [
    "1742",
    {
      "pageContent": "def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        image: Union[np.ndarray, PIL.Image.Image] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[np.random.RandomState] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, np.ndarray], None]] = None,\n        callback_steps: int = 1,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process.\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1. `image`\n                will be used as a starting point, adding more noise to it the larger the `strength`. The number of\n                denoising steps depends on the amount of noise initially added. When `strength` is 1, added noise will\n                be maximum and the denoising process will run for the full number of iterations specified in\n                `num_inf",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 240, "column": 4 },
          "end": { "row": 240, "column": 4 }
        }
      }
    }
  ],
  [
    "1743",
    {
      "pageContent": "class StableUnCLIPImageNormalizer(ModelMixin, ConfigMixin):\n    \"\"\"\n    This class is used to hold the mean and standard deviation of the CLIP embedder used in stable unCLIP.\n\n    It is used to normalize the image embeddings before the noise is applied and un-normalize the noised image\n    embeddings.\n    \"\"\"\n\n    @register_to_config\n    def __init__(\n        self,\n        embedding_dim: int = 768,\n    ):\n        super().__init__()\n\n        self.mean = nn.Parameter(torch.zeros(1, embedding_dim))\n        self.std = nn.Parameter(torch.ones(1, embedding_dim))\n\n    def scale(self, embeds):\n        embeds = (embeds - self.mean) * 1.0 / self.std\n        return embeds\n\n    def unscale(self, embeds):\n        embeds = (embeds * self.std) + self.mean\n        return embeds",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/stable_unclip_image_normalizer.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "1744",
    {
      "pageContent": "def preprocess(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "1745",
    {
      "pageContent": "def preprocess_mask(mask, scale_factor=8):\n    mask = mask.convert(\"L\")\n    w, h = mask.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    mask = mask.resize((w // scale_factor, h // scale_factor), resample=PIL.Image.NEAREST)\n    mask = np.array(mask).astype(np.float32) / 255.0\n    mask = np.tile(mask, (4, 1, 1))\n    mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?\n    mask = 1 - mask  # repaint white, keep black\n    return mask",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "1746",
    {
      "pageContent": "class OnnxStableDiffusionInpaintPipelineLegacy(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image inpainting using Stable Diffusion. This is a *legacy feature* for Onnx pipelines to\n    provide compatibility with StableDiffusionInpaintPipelineLegacy and may be removed in the future.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionS",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "1747",
    {
      "pageContent": "def __init__(\n        self,\n        vae_encoder: OnnxRuntimeModel,\n        vae_decoder: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: CLIPTokenizer,\n        unet: OnnxRuntimeModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: OnnxRuntimeModel,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {sch",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 79, "column": 4 },
          "end": { "row": 79, "column": 4 }
        }
      }
    }
  ],
  [
    "1748",
    {
      "pageContent": "def _encode_prompt(self, prompt, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"np\").input_ids\n\n        if not np.array_equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 149, "column": 4 },
          "end": { "row": 149, "column": 4 }
        }
      }
    }
  ],
  [
    "1749",
    {
      "pageContent": "def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        image: Union[np.ndarray, PIL.Image.Image] = None,\n        mask_image: Union[np.ndarray, PIL.Image.Image] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[np.random.RandomState] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, np.ndarray], None]] = None,\n        callback_steps: int = 1,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            image (`nd.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`nd.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n              ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "1750",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "1751",
    {
      "pageContent": "class StableDiffusionDepth2ImgPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image to image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n    \"\"\"\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        schedule",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 56, "column": 0 },
          "end": { "row": 56, "column": 0 }
        }
      }
    }
  ],
  [
    "1752",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        depth_estimator: DPTForDepthEstimation,\n        feature_extractor: DPTFeatureExtractor,\n    ):\n        super().__init__()\n\n        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n            version.parse(unet.config._diffusers_version).base_version\n        ) < version.parse(\"0.9.0.dev0\")\n        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:\n            deprecation_message = (\n                \"The configuration file of the unet has set the default `sample_size` to smaller than\"\n                \" 64 which seems highly unlikely .If you're checkpoint is a fine-tuned version of any of the\"\n                \" following: \\n- CompVis/stable-diffusion-v1-4 \\n- CompVis/stable-diffusion-v1-3 \\n-\"\n                \" CompVis/stable-diffusion-v1-2 \\n- CompVis/stable-diffusion-v1-1 \\n- runwayml/stable-diffusion-v1-5\"\n                \" \\n- runwayml/stable-diffusion-inpainting \\n you should change 'sample_size' to 64 in the\"\n                \" configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`\"\n                \" in the config might lead to incorrect results in future versions. If you have downloaded this\"\n                \" checkpoint from the Hugging Face Hub, it would ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 79, "column": 4 },
          "end": { "row": 79, "column": 4 }
        }
      }
    }
  ],
  [
    "1753",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.depth_estimator]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 123, "column": 4 },
          "end": { "row": 123, "column": 4 }
        }
      }
    }
  ],
  [
    "1754",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 160, "column": 4 },
          "end": { "row": 160, "column": 4 }
        }
      }
    }
  ],
  [
    "1755",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 299, "column": 4 },
          "end": { "row": 299, "column": 4 }
        }
      }
    }
  ],
  [
    "1756",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 310, "column": 4 },
          "end": { "row": 310, "column": 4 }
        }
      }
    }
  ],
  [
    "1757",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 319, "column": 4 },
          "end": { "row": 319, "column": 4 }
        }
      }
    }
  ],
  [
    "1758",
    {
      "pageContent": "def check_inputs(\n        self, prompt, strength, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None\n    ):\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 337, "column": 4 },
          "end": { "row": 337, "column": 4 }
        }
      }
    }
  ],
  [
    "1759",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:]\n\n        return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 378, "column": 4 },
          "end": { "row": 378, "column": 4 }
        }
      }
    }
  ],
  [
    "1760",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n            )\n\n        image = image.to(device=device, dtype=dtype)\n\n        batch_size = batch_size * num_images_per_prompt\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            init_latents = [\n                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n            ]\n            init_latents = torch.cat(init_latents, dim=0)\n        else:\n            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n            # expand init_latents for batch_size\n            deprecation_message = (\n                f\"You have passed {batch_size} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n                \" images (`image`). Initial images are now duplicating to match the number of text prompts. ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 388, "column": 4 },
          "end": { "row": 388, "column": 4 }
        }
      }
    }
  ],
  [
    "1761",
    {
      "pageContent": "def prepare_depth_map(self, image, depth_map, batch_size, do_classifier_free_guidance, dtype, device):\n        if isinstance(image, PIL.Image.Image):\n            image = [image]\n        else:\n            image = [img for img in image]\n\n        if isinstance(image[0], PIL.Image.Image):\n            width, height = image[0].size\n        else:\n            height, width = image[0].shape[-2:]\n\n        if depth_map is None:\n            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n            pixel_values = pixel_values.to(device=device)\n            # The DPT-Hybrid model uses batch-norm layers which are not compatible with fp16.\n            # So we use `torch.autocast` here for half precision inference.\n            context_manger = torch.autocast(\"cuda\", dtype=dtype) if device.type == \"cuda\" else contextlib.nullcontext()\n            with context_manger:\n                depth_map = self.depth_estimator(pixel_values).predicted_depth\n        else:\n            depth_map = depth_map.to(device=device, dtype=dtype)\n\n        depth_map = torch.nn.functional.interpolate(\n            depth_map.unsqueeze(1),\n            size=(height // self.vae_scale_factor, width // self.vae_scale_factor),\n            mode=\"bicubic\",\n            align_corners=False,\n        )\n\n        depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n        depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n        depth_map = 2.0 * (depth_map - depth_min) / (depth_max - depth_min) - 1.0\n        depth_map = depth_map.to(dtype)\n\n        # duplicate mask and ma",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py",
        "range": {
          "start": { "row": 440, "column": 4 },
          "end": { "row": 440, "column": 4 }
        }
      }
    }
  ],
  [
    "1762",
    {
      "pageContent": "class StableDiffusionControlNetPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion with ControlNet guidance.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        controlnet ([`ControlNetModel`]):\n            Provides additional conditioning to the unet during the denoising process\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChec",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 87, "column": 0 },
          "end": { "row": 87, "column": 0 }
        }
      }
    }
  ],
  [
    "1763",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        controlnet: ControlNetModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modu",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 118, "column": 4 },
          "end": { "row": 118, "column": 4 }
        }
      }
    }
  ],
  [
    "1764",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 162, "column": 4 },
          "end": { "row": 162, "column": 4 }
        }
      }
    }
  ],
  [
    "1765",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 172, "column": 4 },
          "end": { "row": 172, "column": 4 }
        }
      }
    }
  ],
  [
    "1766",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae, controlnet, and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.controlnet]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 179, "column": 4 },
          "end": { "row": 179, "column": 4 }
        }
      }
    }
  ],
  [
    "1767",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            # the safety checker can offload the vae again\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # control net hook has be manually offloaded as it alternates with unet\n        cpu_offload_with_hook(self.controlnet, device)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 200, "column": 4 },
          "end": { "row": 200, "column": 4 }
        }
      }
    }
  ],
  [
    "1768",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 248, "column": 4 },
          "end": { "row": 248, "column": 4 }
        }
      }
    }
  ],
  [
    "1769",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 387, "column": 4 },
          "end": { "row": 387, "column": 4 }
        }
      }
    }
  ],
  [
    "1770",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 398, "column": 4 },
          "end": { "row": 398, "column": 4 }
        }
      }
    }
  ],
  [
    "1771",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 407, "column": 4 },
          "end": { "row": 407, "column": 4 }
        }
      }
    }
  ],
  [
    "1772",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        image,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negat",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 424, "column": 4 },
          "end": { "row": 424, "column": 4 }
        }
      }
    }
  ],
  [
    "1773",
    {
      "pageContent": "def prepare_image(self, image, width, height, batch_size, num_images_per_prompt, device, dtype):\n        if not isinstance(image, torch.Tensor):\n            if isinstance(image, PIL.Image.Image):\n                image = [image]\n\n            if isinstance(image[0], PIL.Image.Image):\n                image = [\n                    np.array(i.resize((width, height), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image\n                ]\n                image = np.concatenate(image, axis=0)\n                image = np.array(image).astype(np.float32) / 255.0\n                image = image.transpose(0, 3, 1, 2)\n                image = torch.from_numpy(image)\n            elif isinstance(image[0], torch.Tensor):\n                image = torch.cat(image, dim=0)\n\n        image_batch_size = image.shape[0]\n\n        if image_batch_size == 1:\n            repeat_by = batch_size\n        else:\n            # image batch size is the same as prompt batch size\n            repeat_by = num_images_per_prompt\n\n        image = image.repeat_interleave(repeat_by, dim=0)\n\n        image = image.to(device=device, dtype=dtype)\n\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 503, "column": 4 },
          "end": { "row": 503, "column": 4 }
        }
      }
    }
  ],
  [
    "1774",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 534, "column": 4 },
          "end": { "row": 534, "column": 4 }
        }
      }
    }
  ],
  [
    "1775",
    {
      "pageContent": "def _default_height_width(self, height, width, image):\n        if isinstance(image, list):\n            image = image[0]\n\n        if height is None:\n            if isinstance(image, PIL.Image.Image):\n                height = image.height\n            elif isinstance(image, torch.Tensor):\n                height = image.shape[3]\n\n            height = (height // 8) * 8  # round down to nearest multiple of 8\n\n        if width is None:\n            if isinstance(image, PIL.Image.Image):\n                width = image.width\n            elif isinstance(image, torch.Tensor):\n                width = image.shape[2]\n\n            width = (width // 8) * 8  # round down to nearest multiple of 8\n\n        return height, width",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py",
        "range": {
          "start": { "row": 551, "column": 4 },
          "end": { "row": 551, "column": 4 }
        }
      }
    }
  ],
  [
    "1776",
    {
      "pageContent": "class StableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "1777",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n               ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 83, "column": 4 },
          "end": { "row": 83, "column": 4 }
        }
      }
    }
  ],
  [
    "1778",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 172, "column": 4 },
          "end": { "row": 172, "column": 4 }
        }
      }
    }
  ],
  [
    "1779",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 181, "column": 4 },
          "end": { "row": 181, "column": 4 }
        }
      }
    }
  ],
  [
    "1780",
    {
      "pageContent": "def enable_vae_tiling(self):\n        r\"\"\"\n        Enable tiled VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor into tiles to compute decoding and encoding in\n        several steps. This is useful to save a large amount of memory and to allow the processing of larger images.\n        \"\"\"\n        self.vae.enable_tiling()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 188, "column": 4 },
          "end": { "row": 188, "column": 4 }
        }
      }
    }
  ],
  [
    "1781",
    {
      "pageContent": "def disable_vae_tiling(self):\n        r\"\"\"\n        Disable tiled VAE decoding. If `enable_vae_tiling` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_tiling()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 197, "column": 4 },
          "end": { "row": 197, "column": 4 }
        }
      }
    }
  ],
  [
    "1782",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 204, "column": 4 },
          "end": { "row": 204, "column": 4 }
        }
      }
    }
  ],
  [
    "1783",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 229, "column": 4 },
          "end": { "row": 229, "column": 4 }
        }
      }
    }
  ],
  [
    "1784",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 275, "column": 4 },
          "end": { "row": 275, "column": 4 }
        }
      }
    }
  ],
  [
    "1785",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 413, "column": 4 },
          "end": { "row": 413, "column": 4 }
        }
      }
    }
  ],
  [
    "1786",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 423, "column": 4 },
          "end": { "row": 423, "column": 4 }
        }
      }
    }
  ],
  [
    "1787",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 431, "column": 4 },
          "end": { "row": 431, "column": 4 }
        }
      }
    }
  ],
  [
    "1788",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 448, "column": 4 },
          "end": { "row": 448, "column": 4 }
        }
      }
    }
  ],
  [
    "1789",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py",
        "range": {
          "start": { "row": 495, "column": 4 },
          "end": { "row": 495, "column": 4 }
        }
      }
    }
  ],
  [
    "1790",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 71, "column": 0 },
          "end": { "row": 71, "column": 0 }
        }
      }
    }
  ],
  [
    "1791",
    {
      "pageContent": "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image to image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please,",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 92, "column": 0 },
          "end": { "row": 92, "column": 0 }
        }
      }
    }
  ],
  [
    "1792",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n               ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 122, "column": 4 },
          "end": { "row": 122, "column": 4 }
        }
      }
    }
  ],
  [
    "1793",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 212, "column": 4 },
          "end": { "row": 212, "column": 4 }
        }
      }
    }
  ],
  [
    "1794",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 238, "column": 4 },
          "end": { "row": 238, "column": 4 }
        }
      }
    }
  ],
  [
    "1795",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 286, "column": 4 },
          "end": { "row": 286, "column": 4 }
        }
      }
    }
  ],
  [
    "1796",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 425, "column": 4 },
          "end": { "row": 425, "column": 4 }
        }
      }
    }
  ],
  [
    "1797",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 436, "column": 4 },
          "end": { "row": 436, "column": 4 }
        }
      }
    }
  ],
  [
    "1798",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 445, "column": 4 },
          "end": { "row": 445, "column": 4 }
        }
      }
    }
  ],
  [
    "1799",
    {
      "pageContent": "def check_inputs(\n        self, prompt, strength, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None\n    ):\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 462, "column": 4 },
          "end": { "row": 462, "column": 4 }
        }
      }
    }
  ],
  [
    "1800",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:]\n\n        return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 502, "column": 4 },
          "end": { "row": 502, "column": 4 }
        }
      }
    }
  ],
  [
    "1801",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n            )\n\n        image = image.to(device=device, dtype=dtype)\n\n        batch_size = batch_size * num_images_per_prompt\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            init_latents = [\n                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n            ]\n            init_latents = torch.cat(init_latents, dim=0)\n        else:\n            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n            # expand init_latents for batch_size\n            deprecation_message = (\n                f\"You have passed {batch_size} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n                \" images (`image`). Initial images are now duplicating to match the number of text prompts. ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 511, "column": 4 },
          "end": { "row": 511, "column": 4 }
        }
      }
    }
  ],
  [
    "1802",
    {
      "pageContent": "class Pix2PixInversionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        latents (`torch.FloatTensor`)\n            inverted latents tensor\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n    \"\"\"\n\n    latents: torch.FloatTensor\n    images: Union[List[PIL.Image.Image], np.ndarray]",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 52, "column": 0 },
          "end": { "row": 52, "column": 0 }
        }
      }
    }
  ],
  [
    "1803",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 172, "column": 0 },
          "end": { "row": 172, "column": 0 }
        }
      }
    }
  ],
  [
    "1804",
    {
      "pageContent": "def prepare_unet(unet: UNet2DConditionModel):\n    \"\"\"Modifies the UNet (`unet`) to perform Pix2Pix Zero optimizations.\"\"\"\n    pix2pix_zero_attn_procs = {}\n    for name in unet.attn_processors.keys():\n        module_name = name.replace(\".processor\", \"\")\n        module = unet.get_submodule(module_name)\n        if \"attn2\" in name:\n            pix2pix_zero_attn_procs[name] = Pix2PixZeroCrossAttnProcessor(is_pix2pix_zero=True)\n            module.requires_grad_(True)\n        else:\n            pix2pix_zero_attn_procs[name] = Pix2PixZeroCrossAttnProcessor(is_pix2pix_zero=False)\n            module.requires_grad_(False)\n\n    unet.set_attn_processor(pix2pix_zero_attn_procs)\n    return unet",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 193, "column": 0 },
          "end": { "row": 193, "column": 0 }
        }
      }
    }
  ],
  [
    "1805",
    {
      "pageContent": "class Pix2PixZeroL2Loss:\n    def __init__(self):\n        self.loss = 0.0\n\n    def compute_loss(self, predictions, targets):\n        self.loss += ((predictions - targets) ** 2).sum((1, 2)).mean(0)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 210, "column": 0 },
          "end": { "row": 210, "column": 0 }
        }
      }
    }
  ],
  [
    "1806",
    {
      "pageContent": "class Pix2PixZeroCrossAttnProcessor:\n    \"\"\"An attention processor class to store the attention weights.\n    In Pix2Pix Zero, it happens during computations in the cross-attention blocks.\"\"\"\n\n    def __init__(self, is_pix2pix_zero=False):\n        self.is_pix2pix_zero = is_pix2pix_zero\n        if self.is_pix2pix_zero:\n            self.reference_cross_attn_map = {}\n\n    def __call__(\n        self,\n        attn: CrossAttention,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        timestep=None,\n        loss=None,\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        if self.is_pix2pix_zero and timestep is not None:\n            # new bookkeeping to save the attention weights.\n            if loss is None:\n                self.reference_cross_attn_map[timestep.item()] = attention_probs.detach().cpu()\n            # compute loss\n            elif loss is not None:\n       ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 218, "column": 0 },
          "end": { "row": 218, "column": 0 }
        }
      }
    }
  ],
  [
    "1807",
    {
      "pageContent": "def __init__(self, is_pix2pix_zero=False):\n        self.is_pix2pix_zero = is_pix2pix_zero\n        if self.is_pix2pix_zero:\n            self.reference_cross_attn_map = {}",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 222, "column": 4 },
          "end": { "row": 222, "column": 4 }
        }
      }
    }
  ],
  [
    "1808",
    {
      "pageContent": "def __call__(\n        self,\n        attn: CrossAttention,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        timestep=None,\n        loss=None,\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        if self.is_pix2pix_zero and timestep is not None:\n            # new bookkeeping to save the attention weights.\n            if loss is None:\n                self.reference_cross_attn_map[timestep.item()] = attention_probs.detach().cpu()\n            # compute loss\n            elif loss is not None:\n                prev_attn_probs = self.reference_cross_attn_map.pop(timestep.item())\n                loss.compute_loss(attention_probs, prev_attn_probs.to(attention_probs.device))\n\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 227, "column": 4 },
          "end": { "row": 227, "column": 4 }
        }
      }
    }
  ],
  [
    "1809",
    {
      "pageContent": "class StableDiffusionPix2PixZeroPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for pixel-levl image editing using Pix2Pix Zero. Based on Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], [`EulerAncestralDiscreteScheduler`], or [`DDPMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be c",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 273, "column": 0 },
          "end": { "row": 273, "column": 0 }
        }
      }
    }
  ],
  [
    "1810",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDPMScheduler, DDIMScheduler, EulerAncestralDiscreteScheduler, LMSDiscreteScheduler],\n        feature_extractor: CLIPFeatureExtractor,\n        safety_checker: StableDiffusionSafetyChecker,\n        inverse_scheduler: DDIMInverseScheduler,\n        caption_generator: BlipForConditionalGeneration,\n        caption_processor: BlipProcessor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to u",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 311, "column": 4 },
          "end": { "row": 311, "column": 4 }
        }
      }
    }
  ],
  [
    "1811",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 359, "column": 4 },
          "end": { "row": 359, "column": 4 }
        }
      }
    }
  ],
  [
    "1812",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        hook = None\n        for cpu_offloaded_model in [self.vae, self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 384, "column": 4 },
          "end": { "row": 384, "column": 4 }
        }
      }
    }
  ],
  [
    "1813",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 428, "column": 4 },
          "end": { "row": 428, "column": 4 }
        }
      }
    }
  ],
  [
    "1814",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 567, "column": 4 },
          "end": { "row": 567, "column": 4 }
        }
      }
    }
  ],
  [
    "1815",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 578, "column": 4 },
          "end": { "row": 578, "column": 4 }
        }
      }
    }
  ],
  [
    "1816",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 587, "column": 4 },
          "end": { "row": 587, "column": 4 }
        }
      }
    }
  ],
  [
    "1817",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        image,\n        source_embeds,\n        target_embeds,\n        callback_steps,\n        prompt_embeds=None,\n    ):\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n        if source_embeds is None and target_embeds is None:\n            raise ValueError(\"`source_embeds` and `target_embeds` cannot be undefined.\")\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 604, "column": 4 },
          "end": { "row": 604, "column": 4 }
        }
      }
    }
  ],
  [
    "1818",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 636, "column": 4 },
          "end": { "row": 636, "column": 4 }
        }
      }
    }
  ],
  [
    "1819",
    {
      "pageContent": "def prepare_image_latents(self, image, batch_size, dtype, device, generator=None):\n        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n            )\n\n        image = image.to(device=device, dtype=dtype)\n\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            init_latents = [\n                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n            ]\n            init_latents = torch.cat(init_latents, dim=0)\n        else:\n            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] != 0:\n            raise ValueError(\n                f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n            )\n        else:\n            init_latents = torch.cat([init_latents], dim=0)\n\n        latents = init_latents\n\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 697, "column": 4 },
          "end": { "row": 697, "column": 4 }
        }
      }
    }
  ],
  [
    "1820",
    {
      "pageContent": "def auto_corr_loss(self, hidden_states, generator=None):\n        batch_size, channel, height, width = hidden_states.shape\n        if batch_size > 1:\n            raise ValueError(\"Only batch_size 1 is supported for now\")\n\n        hidden_states = hidden_states.squeeze(0)\n        # hidden_states must be shape [C,H,W] now\n        reg_loss = 0.0\n        for i in range(hidden_states.shape[0]):\n            noise = hidden_states[i][None, None, :, :]\n            while True:\n                roll_amount = torch.randint(noise.shape[2] // 2, (1,), generator=generator).item()\n                reg_loss += (noise * torch.roll(noise, shifts=roll_amount, dims=2)).mean() ** 2\n                reg_loss += (noise * torch.roll(noise, shifts=roll_amount, dims=3)).mean() ** 2\n\n                if noise.shape[2] <= 8:\n                    break\n                noise = F.avg_pool2d(noise, kernel_size=2)\n        return reg_loss",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 732, "column": 4 },
          "end": { "row": 732, "column": 4 }
        }
      }
    }
  ],
  [
    "1821",
    {
      "pageContent": "def kl_divergence(self, hidden_states):\n        mean = hidden_states.mean()\n        var = hidden_states.var()\n        return var + mean**2 - 1 - torch.log(var + 1e-7)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py",
        "range": {
          "start": { "row": 752, "column": 4 },
          "end": { "row": 752, "column": 4 }
        }
      }
    }
  ],
  [
    "1822",
    {
      "pageContent": "def jax_cosine_distance(emb_1, emb_2, eps=1e-12):\n    norm_emb_1 = jnp.divide(emb_1.T, jnp.clip(jnp.linalg.norm(emb_1, axis=1), a_min=eps)).T\n    norm_emb_2 = jnp.divide(emb_2.T, jnp.clip(jnp.linalg.norm(emb_2, axis=1), a_min=eps)).T\n    return jnp.matmul(norm_emb_1, norm_emb_2.T)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "1823",
    {
      "pageContent": "class FlaxStableDiffusionSafetyCheckerModule(nn.Module):\n    config: CLIPConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.vision_model = FlaxCLIPVisionModule(self.config.vision_config)\n        self.visual_projection = nn.Dense(self.config.projection_dim, use_bias=False, dtype=self.dtype)\n\n        self.concept_embeds = self.param(\"concept_embeds\", jax.nn.initializers.ones, (17, self.config.projection_dim))\n        self.special_care_embeds = self.param(\n            \"special_care_embeds\", jax.nn.initializers.ones, (3, self.config.projection_dim)\n        )\n\n        self.concept_embeds_weights = self.param(\"concept_embeds_weights\", jax.nn.initializers.ones, (17,))\n        self.special_care_embeds_weights = self.param(\"special_care_embeds_weights\", jax.nn.initializers.ones, (3,))\n\n    def __call__(self, clip_input):\n        pooled_output = self.vision_model(clip_input)[1]\n        image_embeds = self.visual_projection(pooled_output)\n\n        special_cos_dist = jax_cosine_distance(image_embeds, self.special_care_embeds)\n        cos_dist = jax_cosine_distance(image_embeds, self.concept_embeds)\n\n        # increase this value to create a stronger `nfsw` filter\n        # at the cost of increasing the possibility of filtering benign image inputs\n        adjustment = 0.0\n\n        special_scores = special_cos_dist - self.special_care_embeds_weights[None, :] + adjustment\n        special_scores = jnp.round(special_scores, 3)\n        is_special_care = jnp.any(special_scores > 0, axis=1, keepdims=True)\n        # Use a lower threshold if an image has any special car",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 30, "column": 0 },
          "end": { "row": 30, "column": 0 }
        }
      }
    }
  ],
  [
    "1824",
    {
      "pageContent": "def setup(self):\n        self.vision_model = FlaxCLIPVisionModule(self.config.vision_config)\n        self.visual_projection = nn.Dense(self.config.projection_dim, use_bias=False, dtype=self.dtype)\n\n        self.concept_embeds = self.param(\"concept_embeds\", jax.nn.initializers.ones, (17, self.config.projection_dim))\n        self.special_care_embeds = self.param(\n            \"special_care_embeds\", jax.nn.initializers.ones, (3, self.config.projection_dim)\n        )\n\n        self.concept_embeds_weights = self.param(\"concept_embeds_weights\", jax.nn.initializers.ones, (17,))\n        self.special_care_embeds_weights = self.param(\"special_care_embeds_weights\", jax.nn.initializers.ones, (3,))",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 34, "column": 4 },
          "end": { "row": 34, "column": 4 }
        }
      }
    }
  ],
  [
    "1825",
    {
      "pageContent": "def __call__(self, clip_input):\n        pooled_output = self.vision_model(clip_input)[1]\n        image_embeds = self.visual_projection(pooled_output)\n\n        special_cos_dist = jax_cosine_distance(image_embeds, self.special_care_embeds)\n        cos_dist = jax_cosine_distance(image_embeds, self.concept_embeds)\n\n        # increase this value to create a stronger `nfsw` filter\n        # at the cost of increasing the possibility of filtering benign image inputs\n        adjustment = 0.0\n\n        special_scores = special_cos_dist - self.special_care_embeds_weights[None, :] + adjustment\n        special_scores = jnp.round(special_scores, 3)\n        is_special_care = jnp.any(special_scores > 0, axis=1, keepdims=True)\n        # Use a lower threshold if an image has any special care concept\n        special_adjustment = is_special_care * 0.01\n\n        concept_scores = cos_dist - self.concept_embeds_weights[None, :] + special_adjustment\n        concept_scores = jnp.round(concept_scores, 3)\n        has_nsfw_concepts = jnp.any(concept_scores > 0, axis=1)\n\n        return has_nsfw_concepts",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 46, "column": 4 },
          "end": { "row": 46, "column": 4 }
        }
      }
    }
  ],
  [
    "1826",
    {
      "pageContent": "class FlaxStableDiffusionSafetyChecker(FlaxPreTrainedModel):\n    config_class = CLIPConfig\n    main_input_name = \"clip_input\"\n    module_class = FlaxStableDiffusionSafetyCheckerModule\n\n    def __init__(\n        self,\n        config: CLIPConfig,\n        input_shape: Optional[Tuple] = None,\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n    ):\n        if input_shape is None:\n            input_shape = (1, 224, 224, 3)\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.KeyArray, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # init input tensor\n        clip_input = jax.random.normal(rng, input_shape)\n\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        random_params = self.module.init(rngs, clip_input)[\"params\"]\n\n        return random_params\n\n    def __call__(\n        self,\n        clip_input,\n        params: dict = None,\n    ):\n        clip_input = jnp.transpose(clip_input, (0, 2, 3, 1))\n\n        return self.module.apply(\n            {\"params\": params or self.params},\n            jnp.array(clip_input, dtype=jnp.float32),\n            rngs={},\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 70, "column": 0 },
          "end": { "row": 70, "column": 0 }
        }
      }
    }
  ],
  [
    "1827",
    {
      "pageContent": "def __init__(\n        self,\n        config: CLIPConfig,\n        input_shape: Optional[Tuple] = None,\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n    ):\n        if input_shape is None:\n            input_shape = (1, 224, 224, 3)\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 75, "column": 4 },
          "end": { "row": 75, "column": 4 }
        }
      }
    }
  ],
  [
    "1828",
    {
      "pageContent": "def init_weights(self, rng: jax.random.KeyArray, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # init input tensor\n        clip_input = jax.random.normal(rng, input_shape)\n\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        random_params = self.module.init(rngs, clip_input)[\"params\"]\n\n        return random_params",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 89, "column": 4 },
          "end": { "row": 89, "column": 4 }
        }
      }
    }
  ],
  [
    "1829",
    {
      "pageContent": "def __call__(\n        self,\n        clip_input,\n        params: dict = None,\n    ):\n        clip_input = jnp.transpose(clip_input, (0, 2, 3, 1))\n\n        return self.module.apply(\n            {\"params\": params or self.params},\n            jnp.array(clip_input, dtype=jnp.float32),\n            rngs={},\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py",
        "range": {
          "start": { "row": 100, "column": 4 },
          "end": { "row": 100, "column": 4 }
        }
      }
    }
  ],
  [
    "1830",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "1831",
    {
      "pageContent": "def posterior_sample(scheduler, latents, timestep, clean_latents, generator, eta):\n    # 1. get previous step value (=t-1)\n    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n\n    if prev_timestep <= 0:\n        return clean_latents\n\n    # 2. compute alphas, betas\n    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n    alpha_prod_t_prev = (\n        scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n    )\n\n    variance = scheduler._get_variance(timestep, prev_timestep)\n    std_dev_t = eta * variance ** (0.5)\n\n    # direction pointing to x_t\n    e_t = (latents - alpha_prod_t ** (0.5) * clean_latents) / (1 - alpha_prod_t) ** (0.5)\n    dir_xt = (1.0 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * e_t\n    noise = std_dev_t * randn_tensor(\n        clean_latents.shape, dtype=clean_latents.dtype, device=clean_latents.device, generator=generator\n    )\n    prev_latents = alpha_prod_t_prev ** (0.5) * clean_latents + dir_xt + noise\n\n    return prev_latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 59, "column": 0 },
          "end": { "row": 59, "column": 0 }
        }
      }
    }
  ],
  [
    "1832",
    {
      "pageContent": "def compute_noise(scheduler, prev_latents, latents, timestep, noise_pred, eta):\n    # 1. get previous step value (=t-1)\n    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n\n    # 2. compute alphas, betas\n    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n    alpha_prod_t_prev = (\n        scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n    )\n\n    beta_prod_t = 1 - alpha_prod_t\n\n    # 3. compute predicted original sample from predicted noise also called\n    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n\n    # 4. Clip \"predicted x_0\"\n    if scheduler.config.clip_sample:\n        pred_original_sample = torch.clamp(pred_original_sample, -1, 1)\n\n    # 5. compute variance: \"sigma_t()\" -> see formula (16)\n    # _t = sqrt((1  _t1)/(1  _t)) * sqrt(1  _t/_t1)\n    variance = scheduler._get_variance(timestep, prev_timestep)\n    std_dev_t = eta * variance ** (0.5)\n\n    # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * noise_pred\n\n    noise = (prev_latents - (alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction)) / (\n        variance ** (0.5) * eta\n    )\n    return noise",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 86, "column": 0 },
          "end": { "row": 86, "column": 0 }
        }
      }
    }
  ],
  [
    "1833",
    {
      "pageContent": "class CycleDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image to image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer t",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 120, "column": 0 },
          "end": { "row": 120, "column": 0 }
        }
      }
    }
  ],
  [
    "1834",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: DDIMScheduler,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable D",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 149, "column": 4 },
          "end": { "row": 149, "column": 4 }
        }
      }
    }
  ],
  [
    "1835",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 224, "column": 4 },
          "end": { "row": 224, "column": 4 }
        }
      }
    }
  ],
  [
    "1836",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 250, "column": 4 },
          "end": { "row": 250, "column": 4 }
        }
      }
    }
  ],
  [
    "1837",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 298, "column": 4 },
          "end": { "row": 298, "column": 4 }
        }
      }
    }
  ],
  [
    "1838",
    {
      "pageContent": "def check_inputs(\n        self, prompt, strength, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None\n    ):\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 437, "column": 4 },
          "end": { "row": 437, "column": 4 }
        }
      }
    }
  ],
  [
    "1839",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 478, "column": 4 },
          "end": { "row": 478, "column": 4 }
        }
      }
    }
  ],
  [
    "1840",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 496, "column": 4 },
          "end": { "row": 496, "column": 4 }
        }
      }
    }
  ],
  [
    "1841",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 507, "column": 4 },
          "end": { "row": 507, "column": 4 }
        }
      }
    }
  ],
  [
    "1842",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:]\n\n        return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 516, "column": 4 },
          "end": { "row": 516, "column": 4 }
        }
      }
    }
  ],
  [
    "1843",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n        image = image.to(device=device, dtype=dtype)\n\n        batch_size = image.shape[0]\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            init_latents = [\n                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n            ]\n            init_latents = torch.cat(init_latents, dim=0)\n        else:\n            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n            # expand init_latents for batch_size\n            deprecation_message = (\n                f\"You have passed {batch_size} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n                \" images (`image`). Initial images are now duplicating to match the number of text prompts. Note\"\n                \" that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update\"\n                \" your script to pass as many initial images as text prompts to suppress this warning.\"\n            )\n   ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py",
        "range": {
          "start": { "row": 525, "column": 4 },
          "end": { "row": 525, "column": 4 }
        }
      }
    }
  ],
  [
    "1844",
    {
      "pageContent": "class StableUnCLIPImg2ImgPipeline(DiffusionPipeline):\n    \"\"\"\n    Pipeline for text-guided image to image generation using stable unCLIP.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Feature extractor for image pre-processing before being encoded.\n        image_encoder ([`CLIPVisionModelWithProjection`]):\n            CLIP vision model for encoding images.\n        image_normalizer ([`StableUnCLIPImageNormalizer`]):\n            Used to normalize the predicted image embeddings before the noise is applied and un-normalize the image\n            embeddings after the noise has been applied.\n        image_noising_scheduler ([`KarrasDiffusionSchedulers`]):\n            Noise schedule for adding noise to the predicted image embeddings. The amount of noise to add is determined\n            by `noise_level` in `StableUnCLIPPipeline.__call__`.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder.\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`KarrasDiffusionSchedulers`]):\n            A scheduler to be used in combination with `unet` to den",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 62, "column": 0 },
          "end": { "row": 62, "column": 0 }
        }
      }
    }
  ],
  [
    "1845",
    {
      "pageContent": "def __init__(\n        self,\n        # image encoding components\n        feature_extractor: CLIPFeatureExtractor,\n        image_encoder: CLIPVisionModelWithProjection,\n        # image noising components\n        image_normalizer: StableUnCLIPImageNormalizer,\n        image_noising_scheduler: KarrasDiffusionSchedulers,\n        # regular denoising components\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModel,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        # vae\n        vae: AutoencoderKL,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            feature_extractor=feature_extractor,\n            image_encoder=image_encoder,\n            image_normalizer=image_normalizer,\n            image_noising_scheduler=image_noising_scheduler,\n            tokenizer=tokenizer,\n            text_encoder=text_encoder,\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n        )\n\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "1846",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "1847",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 151, "column": 4 },
          "end": { "row": 151, "column": 4 }
        }
      }
    }
  ],
  [
    "1848",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, the pipeline's\n        models have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only\n        when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        # TODO: self.image_normalizer.{scale,unscale} are not covered by the offload hooks, so they fails if added to the list\n        models = [\n            self.image_encoder,\n            self.text_encoder,\n            self.unet,\n            self.vae,\n        ]\n        for cpu_offloaded_model in models:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 158, "column": 4 },
          "end": { "row": 158, "column": 4 }
        }
      }
    }
  ],
  [
    "1849",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 202, "column": 4 },
          "end": { "row": 202, "column": 4 }
        }
      }
    }
  ],
  [
    "1850",
    {
      "pageContent": "def _encode_image(\n        self,\n        image,\n        device,\n        batch_size,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        noise_level,\n        generator,\n        image_embeds,\n    ):\n        dtype = next(self.image_encoder.parameters()).dtype\n\n        if isinstance(image, PIL.Image.Image):\n            # the image embedding should repeated so it matches the total batch size of the prompt\n            repeat_by = batch_size\n        else:\n            # assume the image input is already properly batched and just needs to be repeated so\n            # it matches the num_images_per_prompt.\n            #\n            # NOTE(will) this is probably missing a few number of side cases. I.e. batched/non-batched\n            # `image_embeds`. If those happen to be common use cases, let's think harder about\n            # what the expected dimensions of inputs should be and how we handle the encoding.\n            repeat_by = num_images_per_prompt\n\n        if not image_embeds:\n            if not isinstance(image, torch.Tensor):\n                image = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n            image = image.to(device=device, dtype=dtype)\n            image_embeds = self.image_encoder(image).image_embeds\n\n        image_embeds = self.noise_image_embeddings(\n            image_embeds=image_embeds,\n            noise_level=noise_level,\n            generator=generator,\n        )\n\n        # duplicate image embeddings for each generation per prompt, using mps friendly method\n        image_embeds = image_embeds.unsqueeze(1)\n  ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 340, "column": 4 },
          "end": { "row": 340, "column": 4 }
        }
      }
    }
  ],
  [
    "1851",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 396, "column": 4 },
          "end": { "row": 396, "column": 4 }
        }
      }
    }
  ],
  [
    "1852",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 405, "column": 4 },
          "end": { "row": 405, "column": 4 }
        }
      }
    }
  ],
  [
    "1853",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        image,\n        height,\n        width,\n        callback_steps,\n        noise_level,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n        image_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Please make sure to define only one of the two.\"\n            )\n\n        if prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n\n        if prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                \"Provide either `negative_prompt` or `negative_prompt_embeds`. Cannot leave b",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 422, "column": 4 },
          "end": { "row": 422, "column": 4 }
        }
      }
    }
  ],
  [
    "1854",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 506, "column": 4 },
          "end": { "row": 506, "column": 4 }
        }
      }
    }
  ],
  [
    "1855",
    {
      "pageContent": "def noise_image_embeddings(\n        self,\n        image_embeds: torch.Tensor,\n        noise_level: int,\n        noise: Optional[torch.FloatTensor] = None,\n        generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"\n        Add noise to the image embeddings. The amount of noise is controlled by a `noise_level` input. A higher\n        `noise_level` increases the variance in the final un-noised images.\n\n        The noise is applied in two ways\n        1. A noise schedule is applied directly to the embeddings\n        2. A vector of sinusoidal time embeddings are appended to the output.\n\n        In both cases, the amount of noise is controlled by the same `noise_level`.\n\n        The embeddings are normalized before the noise is applied and un-normalized after the noise is applied.\n        \"\"\"\n        if noise is None:\n            noise = randn_tensor(\n                image_embeds.shape, generator=generator, device=image_embeds.device, dtype=image_embeds.dtype\n            )\n\n        noise_level = torch.tensor([noise_level] * image_embeds.shape[0], device=image_embeds.device)\n\n        image_embeds = self.image_normalizer.scale(image_embeds)\n\n        image_embeds = self.image_noising_scheduler.add_noise(image_embeds, timesteps=noise_level, noise=noise)\n\n        image_embeds = self.image_normalizer.unscale(image_embeds)\n\n        noise_level = get_timestep_embedding(\n            timesteps=noise_level, embedding_dim=image_embeds.shape[-1], flip_sin_to_cos=True, downscale_freq_shift=0\n        )\n\n        # `get_timestep_embeddings` does not contain any weights and will alway",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py",
        "range": {
          "start": { "row": 524, "column": 4 },
          "end": { "row": 524, "column": 4 }
        }
      }
    }
  ],
  [
    "1856",
    {
      "pageContent": "def prepare_mask_and_masked_image(image, mask, latents_shape):\n    image = np.array(image.convert(\"RGB\").resize((latents_shape[1] * 8, latents_shape[0] * 8)))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = image.astype(np.float32) / 127.5 - 1.0\n\n    image_mask = np.array(mask.convert(\"L\").resize((latents_shape[1] * 8, latents_shape[0] * 8)))\n    masked_image = image * (image_mask < 127.5)\n\n    mask = mask.resize((latents_shape[1], latents_shape[0]), PIL_INTERPOLATION[\"nearest\"])\n    mask = np.array(mask.convert(\"L\"))\n    mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n\n    return mask, masked_image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "1857",
    {
      "pageContent": "class OnnxStableDiffusionInpaintPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image inpainting using Stable Diffusion. *This is an experimental feature*.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive o",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 55, "column": 0 },
          "end": { "row": 55, "column": 0 }
        }
      }
    }
  ],
  [
    "1858",
    {
      "pageContent": "def __init__(\n        self,\n        vae_encoder: OnnxRuntimeModel,\n        vae_decoder: OnnxRuntimeModel,\n        text_encoder: OnnxRuntimeModel,\n        tokenizer: CLIPTokenizer,\n        unet: OnnxRuntimeModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: OnnxRuntimeModel,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n        logger.info(\"`OnnxStableDiffusionInpaintPipeline` is experimental and will very likely change in the future.\")\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.cl",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "1859",
    {
      "pageContent": "def _encode_prompt(self, prompt, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                prompt to be encoded\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"np\").input_ids\n\n        if not np.array_equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 164, "column": 4 },
          "end": { "row": 164, "column": 4 }
        }
      }
    }
  ],
  [
    "1860",
    {
      "pageContent": "def prepare_mask_and_masked_image(image, mask):\n    \"\"\"\n    Prepares a pair (image, mask) to be consumed by the Stable Diffusion pipeline. This means that those inputs will be\n    converted to ``torch.Tensor`` with shapes ``batch x channels x height x width`` where ``channels`` is ``3`` for the\n    ``image`` and ``1`` for the ``mask``.\n\n    The ``image`` will be converted to ``torch.float32`` and normalized to be in ``[-1, 1]``. The ``mask`` will be\n    binarized (``mask > 0.5``) and cast to ``torch.float32`` too.\n\n    Args:\n        image (Union[np.array, PIL.Image, torch.Tensor]): The image to inpaint.\n            It can be a ``PIL.Image``, or a ``height x width x 3`` ``np.array`` or a ``channels x height x width``\n            ``torch.Tensor`` or a ``batch x channels x height x width`` ``torch.Tensor``.\n        mask (_type_): The mask to apply to the image, i.e. regions to inpaint.\n            It can be a ``PIL.Image``, or a ``height x width`` ``np.array`` or a ``1 x height x width``\n            ``torch.Tensor`` or a ``batch x 1 x height x width`` ``torch.Tensor``.\n\n\n    Raises:\n        ValueError: ``torch.Tensor`` images should be in the ``[-1, 1]`` range. ValueError: ``torch.Tensor`` mask\n        should be in the ``[0, 1]`` range. ValueError: ``mask`` and ``image`` should have the same spatial dimensions.\n        TypeError: ``mask`` is a ``torch.Tensor`` but ``image`` is not\n            (ot the other way around).\n\n    Returns:\n        tuple[torch.Tensor]: The pair (mask, masked_image) as ``torch.Tensor`` with 4\n            dimensions: ``batch x channels x height x width`",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "1861",
    {
      "pageContent": "class StableDiffusionInpaintPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image inpainting using Stable Diffusion. *This is an experimental feature*.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or ha",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 139, "column": 0 },
          "end": { "row": 139, "column": 0 }
        }
      }
    }
  ],
  [
    "1862",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"skip_prk_steps\") and scheduler.config.skip_prk_steps is False:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration\"\n                \" `skip",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 168, "column": 4 },
          "end": { "row": 168, "column": 4 }
        }
      }
    }
  ],
  [
    "1863",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 259, "column": 4 },
          "end": { "row": 259, "column": 4 }
        }
      }
    }
  ],
  [
    "1864",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "1865",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 333, "column": 4 },
          "end": { "row": 333, "column": 4 }
        }
      }
    }
  ],
  [
    "1866",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 472, "column": 4 },
          "end": { "row": 472, "column": 4 }
        }
      }
    }
  ],
  [
    "1867",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 483, "column": 4 },
          "end": { "row": 483, "column": 4 }
        }
      }
    }
  ],
  [
    "1868",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 501, "column": 4 },
          "end": { "row": 501, "column": 4 }
        }
      }
    }
  ],
  [
    "1869",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 510, "column": 4 },
          "end": { "row": 510, "column": 4 }
        }
      }
    }
  ],
  [
    "1870",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 558, "column": 4 },
          "end": { "row": 558, "column": 4 }
        }
      }
    }
  ],
  [
    "1871",
    {
      "pageContent": "def prepare_mask_latents(\n        self, mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance\n    ):\n        # resize the mask to latents shape as we concatenate the mask to the latents\n        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n        # and half precision\n        mask = torch.nn.functional.interpolate(\n            mask, size=(height // self.vae_scale_factor, width // self.vae_scale_factor)\n        )\n        mask = mask.to(device=device, dtype=dtype)\n\n        masked_image = masked_image.to(device=device, dtype=dtype)\n\n        # encode the mask image into latents space so we can concatenate it to the latents\n        if isinstance(generator, list):\n            masked_image_latents = [\n                self.vae.encode(masked_image[i : i + 1]).latent_dist.sample(generator=generator[i])\n                for i in range(batch_size)\n            ]\n            masked_image_latents = torch.cat(masked_image_latents, dim=0)\n        else:\n            masked_image_latents = self.vae.encode(masked_image).latent_dist.sample(generator=generator)\n        masked_image_latents = self.vae.config.scaling_factor * masked_image_latents\n\n        # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n        if mask.shape[0] < batch_size:\n            if not batch_size % mask.shape[0] == 0:\n                raise ValueError(\n                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n                  ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 575, "column": 4 },
          "end": { "row": 575, "column": 4 }
        }
      }
    }
  ],
  [
    "1872",
    {
      "pageContent": "def shave_segments(path, n_shave_prefix_segments=1):\n    \"\"\"\n    Removes segments. Positive values shave the first segments, negative shave the last segments.\n    \"\"\"\n    if n_shave_prefix_segments >= 0:\n        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n    else:\n        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 65, "column": 0 },
          "end": { "row": 65, "column": 0 }
        }
      }
    }
  ],
  [
    "1873",
    {
      "pageContent": "def renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item.replace(\"in_layers.0\", \"norm1\")\n        new_item = new_item.replace(\"in_layers.2\", \"conv1\")\n\n        new_item = new_item.replace(\"out_layers.0\", \"norm2\")\n        new_item = new_item.replace(\"out_layers.3\", \"conv2\")\n\n        new_item = new_item.replace(\"emb_layers.1\", \"time_emb_proj\")\n        new_item = new_item.replace(\"skip_connection\", \"conv_shortcut\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 75, "column": 0 },
          "end": { "row": 75, "column": 0 }
        }
      }
    }
  ],
  [
    "1874",
    {
      "pageContent": "def renew_vae_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"nin_shortcut\", \"conv_shortcut\")\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 97, "column": 0 },
          "end": { "row": 97, "column": 0 }
        }
      }
    }
  ],
  [
    "1875",
    {
      "pageContent": "def renew_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        #         new_item = new_item.replace('norm.weight', 'group_norm.weight')\n        #         new_item = new_item.replace('norm.bias', 'group_norm.bias')\n\n        #         new_item = new_item.replace('proj_out.weight', 'proj_attn.weight')\n        #         new_item = new_item.replace('proj_out.bias', 'proj_attn.bias')\n\n        #         new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 113, "column": 0 },
          "end": { "row": 113, "column": 0 }
        }
      }
    }
  ],
  [
    "1876",
    {
      "pageContent": "def renew_vae_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"norm.weight\", \"group_norm.weight\")\n        new_item = new_item.replace(\"norm.bias\", \"group_norm.bias\")\n\n        new_item = new_item.replace(\"q.weight\", \"query.weight\")\n        new_item = new_item.replace(\"q.bias\", \"query.bias\")\n\n        new_item = new_item.replace(\"k.weight\", \"key.weight\")\n        new_item = new_item.replace(\"k.bias\", \"key.bias\")\n\n        new_item = new_item.replace(\"v.weight\", \"value.weight\")\n        new_item = new_item.replace(\"v.bias\", \"value.bias\")\n\n        new_item = new_item.replace(\"proj_out.weight\", \"proj_attn.weight\")\n        new_item = new_item.replace(\"proj_out.bias\", \"proj_attn.bias\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 134, "column": 0 },
          "end": { "row": 134, "column": 0 }
        }
      }
    }
  ],
  [
    "1877",
    {
      "pageContent": "def assign_to_checkpoint(\n    paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None\n):\n    \"\"\"\n    This does the final conversion step: take locally converted weights and apply a global renaming to them. It splits\n    attention layers, and takes into account additional replacements that may arise.\n\n    Assigns the weights to the new checkpoint.\n    \"\"\"\n    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n\n    # Splits the attention layers into three variables.\n    if attention_paths_to_split is not None:\n        for path, path_map in attention_paths_to_split.items():\n            old_tensor = old_checkpoint[path]\n            channels = old_tensor.shape[0] // 3\n\n            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else (-1)\n\n            num_heads = old_tensor.shape[0] // config[\"num_head_channels\"] // 3\n\n            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])\n            query, key, value = old_tensor.split(channels // num_heads, dim=1)\n\n            checkpoint[path_map[\"query\"]] = query.reshape(target_shape)\n            checkpoint[path_map[\"key\"]] = key.reshape(target_shape)\n            checkpoint[path_map[\"value\"]] = value.reshape(target_shape)\n\n    for path in paths:\n        new_path = path[\"new\"]\n\n        # These have already been assigned\n        if attention_paths_to_split is not None and new_path in attention_paths_to_split:\n            continue\n\n        # Global renaming happens here\n        new_pa",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 164, "column": 0 },
          "end": { "row": 164, "column": 0 }
        }
      }
    }
  ],
  [
    "1878",
    {
      "pageContent": "def conv_attn_to_linear(checkpoint):\n    keys = list(checkpoint.keys())\n    attn_keys = [\"query.weight\", \"key.weight\", \"value.weight\"]\n    for key in keys:\n        if \".\".join(key.split(\".\")[-2:]) in attn_keys:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0, 0]\n        elif \"proj_attn.weight\" in key:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0]",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 215, "column": 0 },
          "end": { "row": 215, "column": 0 }
        }
      }
    }
  ],
  [
    "1879",
    {
      "pageContent": "def create_unet_diffusers_config(original_config, image_size: int, controlnet=False):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the LDM model.\n    \"\"\"\n    if controlnet:\n        unet_params = original_config.model.params.control_stage_config.params\n    else:\n        unet_params = original_config.model.params.unet_config.params\n\n    vae_params = original_config.model.params.first_stage_config.params.ddconfig\n\n    block_out_channels = [unet_params.model_channels * mult for mult in unet_params.channel_mult]\n\n    down_block_types = []\n    resolution = 1\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnDownBlock2D\" if resolution in unet_params.attention_resolutions else \"DownBlock2D\"\n        down_block_types.append(block_type)\n        if i != len(block_out_channels) - 1:\n            resolution *= 2\n\n    up_block_types = []\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnUpBlock2D\" if resolution in unet_params.attention_resolutions else \"UpBlock2D\"\n        up_block_types.append(block_type)\n        resolution //= 2\n\n    vae_scale_factor = 2 ** (len(vae_params.ch_mult) - 1)\n\n    head_dim = unet_params.num_heads if \"num_heads\" in unet_params else None\n    use_linear_projection = (\n        unet_params.use_linear_in_transformer if \"use_linear_in_transformer\" in unet_params else False\n    )\n    if use_linear_projection:\n        # stable diffusion 2-base-512 and 2-768\n        if head_dim is None:\n            head_dim = [5, 10, 20, 20]\n\n    class_embed_type = None\n    projection_class_embeddings_input_dim ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 227, "column": 0 },
          "end": { "row": 227, "column": 0 }
        }
      }
    }
  ],
  [
    "1880",
    {
      "pageContent": "def create_vae_diffusers_config(original_config, image_size: int):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the LDM model.\n    \"\"\"\n    vae_params = original_config.model.params.first_stage_config.params.ddconfig\n    _ = original_config.model.params.first_stage_config.params.embed_dim\n\n    block_out_channels = [vae_params.ch * mult for mult in vae_params.ch_mult]\n    down_block_types = [\"DownEncoderBlock2D\"] * len(block_out_channels)\n    up_block_types = [\"UpDecoderBlock2D\"] * len(block_out_channels)\n\n    config = dict(\n        sample_size=image_size,\n        in_channels=vae_params.in_channels,\n        out_channels=vae_params.out_ch,\n        down_block_types=tuple(down_block_types),\n        up_block_types=tuple(up_block_types),\n        block_out_channels=tuple(block_out_channels),\n        latent_channels=vae_params.z_channels,\n        layers_per_block=vae_params.num_res_blocks,\n    )\n    return config",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 296, "column": 0 },
          "end": { "row": 296, "column": 0 }
        }
      }
    }
  ],
  [
    "1881",
    {
      "pageContent": "def create_diffusers_schedular(original_config):\n    schedular = DDIMScheduler(\n        num_train_timesteps=original_config.model.params.timesteps,\n        beta_start=original_config.model.params.linear_start,\n        beta_end=original_config.model.params.linear_end,\n        beta_schedule=\"scaled_linear\",\n    )\n    return schedular",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 320, "column": 0 },
          "end": { "row": 320, "column": 0 }
        }
      }
    }
  ],
  [
    "1882",
    {
      "pageContent": "def create_ldm_bert_config(original_config):\n    bert_params = original_config.model.parms.cond_stage_config.params\n    config = LDMBertConfig(\n        d_model=bert_params.n_embed,\n        encoder_layers=bert_params.n_layer,\n        encoder_ffn_dim=bert_params.n_embed * 4,\n    )\n    return config",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 330, "column": 0 },
          "end": { "row": 330, "column": 0 }
        }
      }
    }
  ],
  [
    "1883",
    {
      "pageContent": "def convert_ldm_unet_checkpoint(checkpoint, config, path=None, extract_ema=False, controlnet=False):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n\n    # extract state_dict for UNet\n    unet_state_dict = {}\n    keys = list(checkpoint.keys())\n\n    if controlnet:\n        unet_key = \"control_model.\"\n    else:\n        unet_key = \"model.diffusion_model.\"\n\n    # at least a 100 parameters have to start with `model_ema` in order for the checkpoint to be EMA\n    if sum(k.startswith(\"model_ema\") for k in keys) > 100 and extract_ema:\n        print(f\"Checkpoint {path} has both EMA and non-EMA weights.\")\n        print(\n            \"In this conversion only the EMA weights are extracted. If you want to instead extract the non-EMA\"\n            \" weights (useful to continue fine-tuning), please make sure to remove the `--extract_ema` flag.\"\n        )\n        for key in keys:\n            if key.startswith(\"model.diffusion_model\"):\n                flat_ema_key = \"model_ema.\" + \"\".join(key.split(\".\")[1:])\n                unet_state_dict[key.replace(unet_key, \"\")] = checkpoint.pop(flat_ema_key)\n    else:\n        if sum(k.startswith(\"model_ema\") for k in keys) > 100:\n            print(\n                \"In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA\"\n                \" weights (usually better for inference), please make sure to add the `--extract_ema` flag.\"\n            )\n\n        for key in keys:\n            if key.startswith(unet_key):\n                unet_state_dict[key.replace(unet_key, \"\")] = checkpo",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 340, "column": 0 },
          "end": { "row": 340, "column": 0 }
        }
      }
    }
  ],
  [
    "1884",
    {
      "pageContent": "def convert_ldm_vae_checkpoint(checkpoint, config):\n    # extract state dict for VAE\n    vae_state_dict = {}\n    vae_key = \"first_stage_model.\"\n    keys = list(checkpoint.keys())\n    for key in keys:\n        if key.startswith(vae_key):\n            vae_state_dict[key.replace(vae_key, \"\")] = checkpoint.get(key)\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"encoder.conv_in.weight\"] = vae_state_dict[\"encoder.conv_in.weight\"]\n    new_checkpoint[\"encoder.conv_in.bias\"] = vae_state_dict[\"encoder.conv_in.bias\"]\n    new_checkpoint[\"encoder.conv_out.weight\"] = vae_state_dict[\"encoder.conv_out.weight\"]\n    new_checkpoint[\"encoder.conv_out.bias\"] = vae_state_dict[\"encoder.conv_out.bias\"]\n    new_checkpoint[\"encoder.conv_norm_out.weight\"] = vae_state_dict[\"encoder.norm_out.weight\"]\n    new_checkpoint[\"encoder.conv_norm_out.bias\"] = vae_state_dict[\"encoder.norm_out.bias\"]\n\n    new_checkpoint[\"decoder.conv_in.weight\"] = vae_state_dict[\"decoder.conv_in.weight\"]\n    new_checkpoint[\"decoder.conv_in.bias\"] = vae_state_dict[\"decoder.conv_in.bias\"]\n    new_checkpoint[\"decoder.conv_out.weight\"] = vae_state_dict[\"decoder.conv_out.weight\"]\n    new_checkpoint[\"decoder.conv_out.bias\"] = vae_state_dict[\"decoder.conv_out.bias\"]\n    new_checkpoint[\"decoder.conv_norm_out.weight\"] = vae_state_dict[\"decoder.norm_out.weight\"]\n    new_checkpoint[\"decoder.conv_norm_out.bias\"] = vae_state_dict[\"decoder.norm_out.bias\"]\n\n    new_checkpoint[\"quant_conv.weight\"] = vae_state_dict[\"quant_conv.weight\"]\n    new_checkpoint[\"quant_conv.bias\"] = vae_state_dict[\"quant_conv.bias\"]\n    new_checkpoint[\"post_quant_conv.weigh",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 571, "column": 0 },
          "end": { "row": 571, "column": 0 }
        }
      }
    }
  ],
  [
    "1885",
    {
      "pageContent": "def convert_ldm_bert_checkpoint(checkpoint, config):\n    def _copy_attn_layer(hf_attn_layer, pt_attn_layer):\n        hf_attn_layer.q_proj.weight.data = pt_attn_layer.to_q.weight\n        hf_attn_layer.k_proj.weight.data = pt_attn_layer.to_k.weight\n        hf_attn_layer.v_proj.weight.data = pt_attn_layer.to_v.weight\n\n        hf_attn_layer.out_proj.weight = pt_attn_layer.to_out.weight\n        hf_attn_layer.out_proj.bias = pt_attn_layer.to_out.bias\n\n    def _copy_linear(hf_linear, pt_linear):\n        hf_linear.weight = pt_linear.weight\n        hf_linear.bias = pt_linear.bias\n\n    def _copy_layer(hf_layer, pt_layer):\n        # copy layer norms\n        _copy_linear(hf_layer.self_attn_layer_norm, pt_layer[0][0])\n        _copy_linear(hf_layer.final_layer_norm, pt_layer[1][0])\n\n        # copy attn\n        _copy_attn_layer(hf_layer.self_attn, pt_layer[0][1])\n\n        # copy MLP\n        pt_mlp = pt_layer[1][1]\n        _copy_linear(hf_layer.fc1, pt_mlp.net[0][0])\n        _copy_linear(hf_layer.fc2, pt_mlp.net[2])\n\n    def _copy_layers(hf_layers, pt_layers):\n        for i, hf_layer in enumerate(hf_layers):\n            if i != 0:\n                i += i\n            pt_layer = pt_layers[i : i + 2]\n            _copy_layer(hf_layer, pt_layer)\n\n    hf_model = LDMBertModel(config).eval()\n\n    # copy  embeds\n    hf_model.model.embed_tokens.weight = checkpoint.transformer.token_emb.weight\n    hf_model.model.embed_positions.weight.data = checkpoint.transformer.pos_emb.emb.weight\n\n    # copy layer norm\n    _copy_linear(hf_model.model.layer_norm, checkpoint.transformer.norm)\n\n    # copy hidden layer",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 678, "column": 0 },
          "end": { "row": 678, "column": 0 }
        }
      }
    }
  ],
  [
    "1886",
    {
      "pageContent": "def _copy_attn_layer(hf_attn_layer, pt_attn_layer):\n        hf_attn_layer.q_proj.weight.data = pt_attn_layer.to_q.weight\n        hf_attn_layer.k_proj.weight.data = pt_attn_layer.to_k.weight\n        hf_attn_layer.v_proj.weight.data = pt_attn_layer.to_v.weight\n\n        hf_attn_layer.out_proj.weight = pt_attn_layer.to_out.weight\n        hf_attn_layer.out_proj.bias = pt_attn_layer.to_out.bias",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 679, "column": 4 },
          "end": { "row": 679, "column": 4 }
        }
      }
    }
  ],
  [
    "1887",
    {
      "pageContent": "def _copy_layer(hf_layer, pt_layer):\n        # copy layer norms\n        _copy_linear(hf_layer.self_attn_layer_norm, pt_layer[0][0])\n        _copy_linear(hf_layer.final_layer_norm, pt_layer[1][0])\n\n        # copy attn\n        _copy_attn_layer(hf_layer.self_attn, pt_layer[0][1])\n\n        # copy MLP\n        pt_mlp = pt_layer[1][1]\n        _copy_linear(hf_layer.fc1, pt_mlp.net[0][0])\n        _copy_linear(hf_layer.fc2, pt_mlp.net[2])",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 691, "column": 4 },
          "end": { "row": 691, "column": 4 }
        }
      }
    }
  ],
  [
    "1888",
    {
      "pageContent": "def _copy_layers(hf_layers, pt_layers):\n        for i, hf_layer in enumerate(hf_layers):\n            if i != 0:\n                i += i\n            pt_layer = pt_layers[i : i + 2]\n            _copy_layer(hf_layer, pt_layer)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 704, "column": 4 },
          "end": { "row": 704, "column": 4 }
        }
      }
    }
  ],
  [
    "1889",
    {
      "pageContent": "def convert_ldm_clip_checkpoint(checkpoint):\n    text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n    keys = list(checkpoint.keys())\n\n    text_model_dict = {}\n\n    for key in keys:\n        if key.startswith(\"cond_stage_model.transformer\"):\n            text_model_dict[key[len(\"cond_stage_model.transformer.\") :]] = checkpoint[key]\n\n    text_model.load_state_dict(text_model_dict)\n\n    return text_model",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 728, "column": 0 },
          "end": { "row": 728, "column": 0 }
        }
      }
    }
  ],
  [
    "1890",
    {
      "pageContent": "def convert_paint_by_example_checkpoint(checkpoint):\n    config = CLIPVisionConfig.from_pretrained(\"openai/clip-vit-large-patch14\")\n    model = PaintByExampleImageEncoder(config)\n\n    keys = list(checkpoint.keys())\n\n    text_model_dict = {}\n\n    for key in keys:\n        if key.startswith(\"cond_stage_model.transformer\"):\n            text_model_dict[key[len(\"cond_stage_model.transformer.\") :]] = checkpoint[key]\n\n    # load clip vision\n    model.model.load_state_dict(text_model_dict)\n\n    # load mapper\n    keys_mapper = {\n        k[len(\"cond_stage_model.mapper.res\") :]: v\n        for k, v in checkpoint.items()\n        if k.startswith(\"cond_stage_model.mapper\")\n    }\n\n    MAPPING = {\n        \"attn.c_qkv\": [\"attn1.to_q\", \"attn1.to_k\", \"attn1.to_v\"],\n        \"attn.c_proj\": [\"attn1.to_out.0\"],\n        \"ln_1\": [\"norm1\"],\n        \"ln_2\": [\"norm3\"],\n        \"mlp.c_fc\": [\"ff.net.0.proj\"],\n        \"mlp.c_proj\": [\"ff.net.2\"],\n    }\n\n    mapped_weights = {}\n    for key, value in keys_mapper.items():\n        prefix = key[: len(\"blocks.i\")]\n        suffix = key.split(prefix)[-1].split(\".\")[-1]\n        name = key.split(prefix)[-1].split(suffix)[0][1:-1]\n        mapped_names = MAPPING[name]\n\n        num_splits = len(mapped_names)\n        for i, mapped_name in enumerate(mapped_names):\n            new_name = \".\".join([prefix, mapped_name, suffix])\n            shape = value.shape[0] // num_splits\n            mapped_weights[new_name] = value[i * shape : (i + 1) * shape]\n\n    model.mapper.load_state_dict(mapped_weights)\n\n    # load final layer norm\n    model.final_layer_norm.load_state_dict(\n    ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 768, "column": 0 },
          "end": { "row": 768, "column": 0 }
        }
      }
    }
  ],
  [
    "1891",
    {
      "pageContent": "def convert_open_clip_checkpoint(checkpoint):\n    text_model = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"text_encoder\")\n\n    keys = list(checkpoint.keys())\n\n    text_model_dict = {}\n\n    if \"cond_stage_model.model.text_projection\" in checkpoint:\n        d_model = int(checkpoint[\"cond_stage_model.model.text_projection\"].shape[0])\n    else:\n        d_model = 1024\n\n    text_model_dict[\"text_model.embeddings.position_ids\"] = text_model.text_model.embeddings.get_buffer(\"position_ids\")\n\n    for key in keys:\n        if \"resblocks.23\" in key:  # Diffusers drops the final layer and only uses the penultimate layer\n            continue\n        if key in textenc_conversion_map:\n            text_model_dict[textenc_conversion_map[key]] = checkpoint[key]\n        if key.startswith(\"cond_stage_model.model.transformer.\"):\n            new_key = key[len(\"cond_stage_model.model.transformer.\") :]\n            if new_key.endswith(\".in_proj_weight\"):\n                new_key = new_key[: -len(\".in_proj_weight\")]\n                new_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], new_key)\n                text_model_dict[new_key + \".q_proj.weight\"] = checkpoint[key][:d_model, :]\n                text_model_dict[new_key + \".k_proj.weight\"] = checkpoint[key][d_model : d_model * 2, :]\n                text_model_dict[new_key + \".v_proj.weight\"] = checkpoint[key][d_model * 2 :, :]\n            elif new_key.endswith(\".in_proj_bias\"):\n                new_key = new_key[: -len(\".in_proj_bias\")]\n                new_key = textenc_pattern.sub(lambda m: protected",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 835, "column": 0 },
          "end": { "row": 835, "column": 0 }
        }
      }
    }
  ],
  [
    "1892",
    {
      "pageContent": "def stable_unclip_image_encoder(original_config):\n    \"\"\"\n    Returns the image processor and clip image encoder for the img2img unclip pipeline.\n\n    We currently know of two types of stable unclip models which separately use the clip and the openclip image\n    encoders.\n    \"\"\"\n\n    image_embedder_config = original_config.model.params.embedder_config\n\n    sd_clip_image_embedder_class = image_embedder_config.target\n    sd_clip_image_embedder_class = sd_clip_image_embedder_class.split(\".\")[-1]\n\n    if sd_clip_image_embedder_class == \"ClipImageEmbedder\":\n        clip_model_name = image_embedder_config.params.model\n\n        if clip_model_name == \"ViT-L/14\":\n            feature_extractor = CLIPImageProcessor()\n            image_encoder = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n        else:\n            raise NotImplementedError(f\"Unknown CLIP checkpoint name in stable diffusion checkpoint {clip_model_name}\")\n\n    elif sd_clip_image_embedder_class == \"FrozenOpenCLIPImageEmbedder\":\n        feature_extractor = CLIPImageProcessor()\n        image_encoder = CLIPVisionModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n    else:\n        raise NotImplementedError(\n            f\"Unknown CLIP image embedder class in stable diffusion checkpoint {sd_clip_image_embedder_class}\"\n        )\n\n    return feature_extractor, image_encoder",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 878, "column": 0 },
          "end": { "row": 878, "column": 0 }
        }
      }
    }
  ],
  [
    "1893",
    {
      "pageContent": "def stable_unclip_image_noising_components(\n    original_config, clip_stats_path: Optional[str] = None, device: Optional[str] = None\n):\n    \"\"\"\n    Returns the noising components for the img2img and txt2img unclip pipelines.\n\n    Converts the stability noise augmentor into\n    1. a `StableUnCLIPImageNormalizer` for holding the CLIP stats\n    2. a `DDPMScheduler` for holding the noise schedule\n\n    If the noise augmentor config specifies a clip stats path, the `clip_stats_path` must be provided.\n    \"\"\"\n    noise_aug_config = original_config.model.params.noise_aug_config\n    noise_aug_class = noise_aug_config.target\n    noise_aug_class = noise_aug_class.split(\".\")[-1]\n\n    if noise_aug_class == \"CLIPEmbeddingNoiseAugmentation\":\n        noise_aug_config = noise_aug_config.params\n        embedding_dim = noise_aug_config.timestep_dim\n        max_noise_level = noise_aug_config.noise_schedule_config.timesteps\n        beta_schedule = noise_aug_config.noise_schedule_config.beta_schedule\n\n        image_normalizer = StableUnCLIPImageNormalizer(embedding_dim=embedding_dim)\n        image_noising_scheduler = DDPMScheduler(num_train_timesteps=max_noise_level, beta_schedule=beta_schedule)\n\n        if \"clip_stats_path\" in noise_aug_config:\n            if clip_stats_path is None:\n                raise ValueError(\"This stable unclip config requires a `clip_stats_path`\")\n\n            clip_mean, clip_std = torch.load(clip_stats_path, map_location=device)\n            clip_mean = clip_mean[None, :]\n            clip_std = clip_std[None, :]\n\n            clip_stats_state_dict = {\n                \"m",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 911, "column": 0 },
          "end": { "row": 911, "column": 0 }
        }
      }
    }
  ],
  [
    "1894",
    {
      "pageContent": "def convert_controlnet_checkpoint(\n    checkpoint, original_config, checkpoint_path, image_size, upcast_attention, extract_ema\n):\n    ctrlnet_config = create_unet_diffusers_config(original_config, image_size=image_size, controlnet=True)\n    ctrlnet_config[\"upcast_attention\"] = upcast_attention\n\n    ctrlnet_config.pop(\"sample_size\")\n\n    controlnet_model = ControlNetModel(**ctrlnet_config)\n\n    converted_ctrl_checkpoint = convert_ldm_unet_checkpoint(\n        checkpoint, ctrlnet_config, path=checkpoint_path, extract_ema=extract_ema, controlnet=True\n    )\n\n    controlnet_model.load_state_dict(converted_ctrl_checkpoint)\n\n    return controlnet_model",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 956, "column": 0 },
          "end": { "row": 956, "column": 0 }
        }
      }
    }
  ],
  [
    "1895",
    {
      "pageContent": "def download_from_original_stable_diffusion_ckpt(\n    checkpoint_path: str,\n    original_config_file: str = None,\n    image_size: int = 512,\n    prediction_type: str = None,\n    model_type: str = None,\n    extract_ema: bool = False,\n    scheduler_type: str = \"pndm\",\n    num_in_channels: Optional[int] = None,\n    upcast_attention: Optional[bool] = None,\n    device: str = None,\n    from_safetensors: bool = False,\n    stable_unclip: Optional[str] = None,\n    stable_unclip_prior: Optional[str] = None,\n    clip_stats_path: Optional[str] = None,\n    controlnet: Optional[bool] = None,\n) -> StableDiffusionPipeline:\n    \"\"\"\n    Load a Stable Diffusion pipeline object from a CompVis-style `.ckpt`/`.safetensors` file and (ideally) a `.yaml`\n    config file.\n\n    Although many of the arguments can be automatically inferred, some of these rely on brittle checks against the\n    global step count, which will likely fail for models that have undergone further fine-tuning. Therefore, it is\n    recommended that you override the default values and/or supply an `original_config_file` wherever possible.\n\n    Args:\n        checkpoint_path (`str`): Path to `.ckpt` file.\n        original_config_file (`str`):\n            Path to `.yaml` config file corresponding to the original architecture. If `None`, will be automatically\n            inferred by looking for a key that only exists in SD2.0 models.\n        image_size (`int`, *optional*, defaults to 512):\n            The image size that the model was trained on. Use 512 for Stable Diffusion v1.X and Stable Diffusion v2\n            Base. Use 768 for ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 975, "column": 0 },
          "end": { "row": 975, "column": 0 }
        }
      }
    }
  ],
  [
    "1896",
    {
      "pageContent": "def download_controlnet_from_original_ckpt(\n    checkpoint_path: str,\n    original_config_file: str,\n    image_size: int = 512,\n    extract_ema: bool = False,\n    num_in_channels: Optional[int] = None,\n    upcast_attention: Optional[bool] = None,\n    device: str = None,\n    from_safetensors: bool = False,\n) -> StableDiffusionPipeline:\n    if not is_omegaconf_available():\n        raise ValueError(BACKENDS_MAPPING[\"omegaconf\"][1])\n\n    from omegaconf import OmegaConf\n\n    if from_safetensors:\n        if not is_safetensors_available():\n            raise ValueError(BACKENDS_MAPPING[\"safetensors\"][1])\n\n        from safetensors import safe_open\n\n        checkpoint = {}\n        with safe_open(checkpoint_path, framework=\"pt\", device=\"cpu\") as f:\n            for key in f.keys():\n                checkpoint[key] = f.get_tensor(key)\n    else:\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n        else:\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    # NOTE: this while loop isn't great but this controlnet checkpoint has one additional\n    # \"state_dict\" key https://huggingface.co/thibaud/controlnet-canny-sd21\n    while \"state_dict\" in checkpoint:\n        checkpoint = checkpoint[\"state_dict\"]\n\n    original_config = OmegaConf.load(original_config_file)\n\n    if num_in_channels is not None:\n        original_config[\"model\"][\"params\"][\"unet_config\"][\"params\"][\"in_channels\"] = num_in_channels\n\n    if \"control_stage_config\" not in original_conf",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py",
        "range": {
          "start": { "row": 1305, "column": 0 },
          "end": { "row": 1305, "column": 0 }
        }
      }
    }
  ],
  [
    "1897",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 41, "column": 0 },
          "end": { "row": 41, "column": 0 }
        }
      }
    }
  ],
  [
    "1898",
    {
      "pageContent": "class StableDiffusionInstructPix2PixPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for pixel-level image editing by following text instructions. Based on Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 62, "column": 0 },
          "end": { "row": 62, "column": 0 }
        }
      }
    }
  ],
  [
    "1899",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modules(\n            vae=vae,\n           ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 91, "column": 4 },
          "end": { "row": 91, "column": 4 }
        }
      }
    }
  ],
  [
    "1900",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 398, "column": 4 },
          "end": { "row": 398, "column": 4 }
        }
      }
    }
  ],
  [
    "1901",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 424, "column": 4 },
          "end": { "row": 424, "column": 4 }
        }
      }
    }
  ],
  [
    "1902",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_ prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 471, "column": 4 },
          "end": { "row": 471, "column": 4 }
        }
      }
    }
  ],
  [
    "1903",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 611, "column": 4 },
          "end": { "row": 611, "column": 4 }
        }
      }
    }
  ],
  [
    "1904",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 622, "column": 4 },
          "end": { "row": 622, "column": 4 }
        }
      }
    }
  ],
  [
    "1905",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 640, "column": 4 },
          "end": { "row": 640, "column": 4 }
        }
      }
    }
  ],
  [
    "1906",
    {
      "pageContent": "def check_inputs(\n        self, prompt, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None\n    ):\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n            )\n\n        if prompt_embeds is not None and negative_prompt_embeds is not None:\n            if prompt_embeds.shape !",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 648, "column": 4 },
          "end": { "row": 648, "column": 4 }
        }
      }
    }
  ],
  [
    "1907",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 686, "column": 4 },
          "end": { "row": 686, "column": 4 }
        }
      }
    }
  ],
  [
    "1908",
    {
      "pageContent": "def prepare_image_latents(\n        self, image, batch_size, num_images_per_prompt, dtype, device, do_classifier_free_guidance, generator=None\n    ):\n        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n            )\n\n        image = image.to(device=device, dtype=dtype)\n\n        batch_size = batch_size * num_images_per_prompt\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if isinstance(generator, list):\n            image_latents = [self.vae.encode(image[i : i + 1]).latent_dist.mode() for i in range(batch_size)]\n            image_latents = torch.cat(image_latents, dim=0)\n        else:\n            image_latents = self.vae.encode(image).latent_dist.mode()\n\n        if batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] == 0:\n            # expand image_latents for batch_size\n            deprecation_message = (\n                f\"You have passed {batch_size} text prompts (`prompt`), but only {image_latents.shape[0]} initial\"\n                \" images (`image`). Initial images are now duplicating to match the number of text prompts. Note\"\n                \" that this behavior is deprecated and will be removed i",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py",
        "range": {
          "start": { "row": 703, "column": 4 },
          "end": { "row": 703, "column": 4 }
        }
      }
    }
  ],
  [
    "1909",
    {
      "pageContent": "class FlaxStableDiffusionImg2ImgPipeline(FlaxDiffusionPipeline):\n    r\"\"\"\n    Pipeline for image-to-image generation using Stable Diffusion.\n\n    This model inherits from [`FlaxDiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`FlaxAutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`FlaxCLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.FlaxCLIPTextModel),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`FlaxUNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`FlaxDDIMScheduler`], [`FlaxLMSDiscreteScheduler`], [`FlaxPNDMScheduler`], or\n            [`FlaxDPMSolverMultistepScheduler`].\n        safety_checker ([`FlaxStableDiffusionSafetyChecker`]):\n            Classification module that estimates wheth",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 104, "column": 0 },
          "end": { "row": 104, "column": 0 }
        }
      }
    }
  ],
  [
    "1910",
    {
      "pageContent": "def __init__(\n        self,\n        vae: FlaxAutoencoderKL,\n        text_encoder: FlaxCLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: FlaxUNet2DConditionModel,\n        scheduler: Union[\n            FlaxDDIMScheduler, FlaxPNDMScheduler, FlaxLMSDiscreteScheduler, FlaxDPMSolverMultistepScheduler\n        ],\n        safety_checker: FlaxStableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        dtype: jnp.dtype = jnp.float32,\n    ):\n        super().__init__()\n        self.dtype = dtype\n\n        if safety_checker is None:\n            logger.warn(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.vae_scale_facto",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 133, "column": 4 },
          "end": { "row": 133, "column": 4 }
        }
      }
    }
  ],
  [
    "1911",
    {
      "pageContent": "def prepare_inputs(self, prompt: Union[str, List[str]], image: Union[Image.Image, List[Image.Image]]):\n        if not isinstance(prompt, (str, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if not isinstance(image, (Image.Image, list)):\n            raise ValueError(f\"image has to be of type `PIL.Image.Image` or list but is {type(image)}\")\n\n        if isinstance(image, Image.Image):\n            image = [image]\n\n        processed_images = jnp.concatenate([preprocess(img, jnp.float32) for img in image])\n\n        text_input = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        return text_input.input_ids, processed_images",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 170, "column": 4 },
          "end": { "row": 170, "column": 4 }
        }
      }
    }
  ],
  [
    "1912",
    {
      "pageContent": "def _run_safety_checker(self, images, safety_model_params, jit=False):\n        # safety_model_params should already be replicated when jit is True\n        pil_images = [Image.fromarray(image) for image in images]\n        features = self.feature_extractor(pil_images, return_tensors=\"np\").pixel_values\n\n        if jit:\n            features = shard(features)\n            has_nsfw_concepts = _p_get_has_nsfw_concepts(self, features, safety_model_params)\n            has_nsfw_concepts = unshard(has_nsfw_concepts)\n            safety_model_params = unreplicate(safety_model_params)\n        else:\n            has_nsfw_concepts = self._get_has_nsfw_concepts(features, safety_model_params)\n\n        images_was_copied = False\n        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n            if has_nsfw_concept:\n                if not images_was_copied:\n                    images_was_copied = True\n                    images = images.copy()\n\n                images[idx] = np.zeros(images[idx].shape, dtype=np.uint8)  # black image\n\n            if any(has_nsfw_concepts):\n                warnings.warn(\n                    \"Potential NSFW content was detected in one or more images. A black image will be returned\"\n                    \" instead. Try again with a different prompt and/or seed.\"\n                )\n\n        return images, has_nsfw_concepts",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 195, "column": 4 },
          "end": { "row": 195, "column": 4 }
        }
      }
    }
  ],
  [
    "1913",
    {
      "pageContent": "def get_timestep_start(self, num_inference_steps, strength):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n\n        return t_start",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 225, "column": 4 },
          "end": { "row": 225, "column": 4 }
        }
      }
    }
  ],
  [
    "1914",
    {
      "pageContent": "def _generate(\n        self,\n        prompt_ids: jnp.array,\n        image: jnp.array,\n        params: Union[Dict, FrozenDict],\n        prng_seed: jax.random.KeyArray,\n        start_timestep: int,\n        num_inference_steps: int,\n        height: int,\n        width: int,\n        guidance_scale: float,\n        noise: Optional[jnp.array] = None,\n        neg_prompt_ids: Optional[jnp.array] = None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        # get prompt text embeddings\n        prompt_embeds = self.text_encoder(prompt_ids, params=params[\"text_encoder\"])[0]\n\n        # TODO: currently it is assumed `do_classifier_free_guidance = guidance_scale > 1.0`\n        # implement this conditional `do_classifier_free_guidance = guidance_scale > 1.0`\n        batch_size = prompt_ids.shape[0]\n\n        max_length = prompt_ids.shape[-1]\n\n        if neg_prompt_ids is None:\n            uncond_input = self.tokenizer(\n                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"np\"\n            ).input_ids\n        else:\n            uncond_input = neg_prompt_ids\n        negative_prompt_embeds = self.text_encoder(uncond_input, params=params[\"text_encoder\"])[0]\n        context = jnp.concatenate([negative_prompt_embeds, prompt_embeds])\n\n        latents_shape = (\n            batch_size,\n            self.unet.in_channels,\n            height // self.vae_scale_factor,\n            width // self.vae_scale_factor,\n        )\n        if noise is None:\n  ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 233, "column": 4 },
          "end": { "row": 233, "column": 4 }
        }
      }
    }
  ],
  [
    "1915",
    {
      "pageContent": "def _p_generate(\n    pipe,\n    prompt_ids,\n    image,\n    params,\n    prng_seed,\n    start_timestep,\n    num_inference_steps,\n    height,\n    width,\n    guidance_scale,\n    noise,\n    neg_prompt_ids,\n):\n    return pipe._generate(\n        prompt_ids,\n        image,\n        params,\n        prng_seed,\n        start_timestep,\n        num_inference_steps,\n        height,\n        width,\n        guidance_scale,\n        noise,\n        neg_prompt_ids,\n    )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 479, "column": 0 },
          "end": { "row": 479, "column": 0 }
        }
      }
    }
  ],
  [
    "1916",
    {
      "pageContent": "def unshard(x: jnp.ndarray):\n    # einops.rearrange(x, 'd b ... -> (d b) ...')\n    num_devices, batch_size = x.shape[:2]\n    rest = x.shape[2:]\n    return x.reshape(num_devices * batch_size, *rest)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 513, "column": 0 },
          "end": { "row": 513, "column": 0 }
        }
      }
    }
  ],
  [
    "1917",
    {
      "pageContent": "def preprocess(image, dtype):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = jnp.array(image).astype(dtype) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py",
        "range": {
          "start": { "row": 520, "column": 0 },
          "end": { "row": 520, "column": 0 }
        }
      }
    }
  ],
  [
    "1918",
    {
      "pageContent": "class StableUnCLIPPipeline(DiffusionPipeline):\n    \"\"\"\n    Pipeline for text-to-image generation using stable unCLIP.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        prior_tokenizer ([`CLIPTokenizer`]):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        prior_text_encoder ([`CLIPTextModelWithProjection`]):\n            Frozen text-encoder.\n        prior ([`PriorTransformer`]):\n            The canonincal unCLIP prior to approximate the image embedding from the text embedding.\n        prior_scheduler ([`KarrasDiffusionSchedulers`]):\n            Scheduler used in the prior denoising process.\n        image_normalizer ([`StableUnCLIPImageNormalizer`]):\n            Used to normalize the predicted image embeddings before the noise is applied and un-normalize the image\n            embeddings after the noise has been applied.\n        image_noising_scheduler ([`KarrasDiffusionSchedulers`]):\n            Noise schedule for adding noise to the predicted image embeddings. The amount of noise to add is determined\n            by `noise_level` in `StableUnCLIPPipeline.__call__`.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        text_enc",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "1919",
    {
      "pageContent": "def __init__(\n        self,\n        # prior components\n        prior_tokenizer: CLIPTokenizer,\n        prior_text_encoder: CLIPTextModelWithProjection,\n        prior: PriorTransformer,\n        prior_scheduler: KarrasDiffusionSchedulers,\n        # image noising components\n        image_normalizer: StableUnCLIPImageNormalizer,\n        image_noising_scheduler: KarrasDiffusionSchedulers,\n        # regular denoising components\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModelWithProjection,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        # vae\n        vae: AutoencoderKL,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            prior_tokenizer=prior_tokenizer,\n            prior_text_encoder=prior_text_encoder,\n            prior=prior,\n            prior_scheduler=prior_scheduler,\n            image_normalizer=image_normalizer,\n            image_noising_scheduler=image_noising_scheduler,\n            tokenizer=tokenizer,\n            text_encoder=text_encoder,\n            unet=unet,\n            scheduler=scheduler,\n            vae=vae,\n        )\n\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 102, "column": 4 },
          "end": { "row": 102, "column": 4 }
        }
      }
    }
  ],
  [
    "1920",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 139, "column": 4 },
          "end": { "row": 139, "column": 4 }
        }
      }
    }
  ],
  [
    "1921",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 149, "column": 4 },
          "end": { "row": 149, "column": 4 }
        }
      }
    }
  ],
  [
    "1922",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, the pipeline's\n        models have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only\n        when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        # TODO: self.prior.post_process_latents and self.image_noiser.{scale,unscale} are not covered by the offload hooks, so they fails if added to the list\n        models = [\n            self.prior_text_encoder,\n            self.text_encoder,\n            self.unet,\n            self.vae,\n        ]\n        for cpu_offloaded_model in models:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 156, "column": 4 },
          "end": { "row": 156, "column": 4 }
        }
      }
    }
  ],
  [
    "1923",
    {
      "pageContent": "def _encode_prior_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        text_model_output: Optional[Union[CLIPTextModelOutput, Tuple]] = None,\n        text_attention_mask: Optional[torch.Tensor] = None,\n    ):\n        if text_model_output is None:\n            batch_size = len(prompt) if isinstance(prompt, list) else 1\n            # get prompt text embeddings\n            text_inputs = self.prior_tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.prior_tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n            text_mask = text_inputs.attention_mask.bool().to(device)\n\n            untruncated_ids = self.prior_tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n                text_input_ids, untruncated_ids\n            ):\n                removed_text = self.prior_tokenizer.batch_decode(\n                    untruncated_ids[:, self.prior_tokenizer.model_max_length - 1 : -1]\n                )\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.prior_tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n                text_input_ids = text_input_ids[:, : self.prior_tokenizer.model_max_length]\n\n    ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 200, "column": 4 },
          "end": { "row": 200, "column": 4 }
        }
      }
    }
  ],
  [
    "1924",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 302, "column": 4 },
          "end": { "row": 302, "column": 4 }
        }
      }
    }
  ],
  [
    "1925",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 441, "column": 4 },
          "end": { "row": 441, "column": 4 }
        }
      }
    }
  ],
  [
    "1926",
    {
      "pageContent": "def prepare_prior_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the prior_scheduler step, since not all prior_schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other prior_schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.prior_scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the prior_scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.prior_scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 450, "column": 4 },
          "end": { "row": 450, "column": 4 }
        }
      }
    }
  ],
  [
    "1927",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 468, "column": 4 },
          "end": { "row": 468, "column": 4 }
        }
      }
    }
  ],
  [
    "1928",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        noise_level,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Please make sure to define only one of the two.\"\n            )\n\n        if prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n\n        if prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                \"Provide either `negative_prompt` or `negative_prompt_embeds`. Cannot leave both `negative_prompt` and `negative_prompt",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 485, "column": 4 },
          "end": { "row": 485, "column": 4 }
        }
      }
    }
  ],
  [
    "1929",
    {
      "pageContent": "def prepare_latents(self, shape, dtype, device, generator, latents, scheduler):\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        latents = latents * scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 546, "column": 4 },
          "end": { "row": 546, "column": 4 }
        }
      }
    }
  ],
  [
    "1930",
    {
      "pageContent": "def noise_image_embeddings(\n        self,\n        image_embeds: torch.Tensor,\n        noise_level: int,\n        noise: Optional[torch.FloatTensor] = None,\n        generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"\n        Add noise to the image embeddings. The amount of noise is controlled by a `noise_level` input. A higher\n        `noise_level` increases the variance in the final un-noised images.\n\n        The noise is applied in two ways\n        1. A noise schedule is applied directly to the embeddings\n        2. A vector of sinusoidal time embeddings are appended to the output.\n\n        In both cases, the amount of noise is controlled by the same `noise_level`.\n\n        The embeddings are normalized before the noise is applied and un-normalized after the noise is applied.\n        \"\"\"\n        if noise is None:\n            noise = randn_tensor(\n                image_embeds.shape, generator=generator, device=image_embeds.device, dtype=image_embeds.dtype\n            )\n\n        noise_level = torch.tensor([noise_level] * image_embeds.shape[0], device=image_embeds.device)\n\n        image_embeds = self.image_normalizer.scale(image_embeds)\n\n        image_embeds = self.image_noising_scheduler.add_noise(image_embeds, timesteps=noise_level, noise=noise)\n\n        image_embeds = self.image_normalizer.unscale(image_embeds)\n\n        noise_level = get_timestep_embedding(\n            timesteps=noise_level, embedding_dim=image_embeds.shape[-1], flip_sin_to_cos=True, downscale_freq_shift=0\n        )\n\n        # `get_timestep_embeddings` does not contain any weights and will alway",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py",
        "range": {
          "start": { "row": 557, "column": 4 },
          "end": { "row": 557, "column": 4 }
        }
      }
    }
  ],
  [
    "1931",
    {
      "pageContent": "def preprocess_image(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "1932",
    {
      "pageContent": "def preprocess_mask(mask, scale_factor=8):\n    if not isinstance(mask, torch.FloatTensor):\n        mask = mask.convert(\"L\")\n        w, h = mask.size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n        mask = mask.resize((w // scale_factor, h // scale_factor), resample=PIL_INTERPOLATION[\"nearest\"])\n        mask = np.array(mask).astype(np.float32) / 255.0\n        mask = np.tile(mask, (4, 1, 1))\n        mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?\n        mask = 1 - mask  # repaint white, keep black\n        mask = torch.from_numpy(mask)\n        return mask\n\n    else:\n        valid_mask_channel_sizes = [1, 3]\n        # if mask channel is fourth tensor dimension, permute dimensions to pytorch standard (B, C, H, W)\n        if mask.shape[3] in valid_mask_channel_sizes:\n            mask = mask.permute(0, 3, 1, 2)\n        elif mask.shape[1] not in valid_mask_channel_sizes:\n            raise ValueError(\n                f\"Mask channel dimension of size in {valid_mask_channel_sizes} should be second or fourth dimension,\"\n                f\" but received mask of shape {tuple(mask.shape)}\"\n            )\n        # (potentially) reduce mask channel dimension from 3 to 1 for broadcasting to latent shape\n        mask = mask.mean(dim=1, keepdim=True)\n        h, w = mask.shape[-2:]\n        h, w = map(lambda x: x - x % 8, (h, w))  # resize to integer multiple of 8\n        mask = torch.nn.functional.interpolate(mask, (h // scale_factor, w // scale_factor))\n        return mask",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 52, "column": 0 },
          "end": { "row": 52, "column": 0 }
        }
      }
    }
  ],
  [
    "1933",
    {
      "pageContent": "class StableDiffusionInpaintPipelineLegacy(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image inpainting using Stable Diffusion. *This is an experimental feature*.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 83, "column": 0 },
          "end": { "row": 83, "column": 0 }
        }
      }
    }
  ],
  [
    "1934",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n               ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 113, "column": 4 },
          "end": { "row": 113, "column": 4 }
        }
      }
    }
  ],
  [
    "1935",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 203, "column": 4 },
          "end": { "row": 203, "column": 4 }
        }
      }
    }
  ],
  [
    "1936",
    {
      "pageContent": "def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 229, "column": 4 },
          "end": { "row": 229, "column": 4 }
        }
      }
    }
  ],
  [
    "1937",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 277, "column": 4 },
          "end": { "row": 277, "column": 4 }
        }
      }
    }
  ],
  [
    "1938",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 416, "column": 4 },
          "end": { "row": 416, "column": 4 }
        }
      }
    }
  ],
  [
    "1939",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 427, "column": 4 },
          "end": { "row": 427, "column": 4 }
        }
      }
    }
  ],
  [
    "1940",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 436, "column": 4 },
          "end": { "row": 436, "column": 4 }
        }
      }
    }
  ],
  [
    "1941",
    {
      "pageContent": "def check_inputs(\n        self, prompt, strength, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None\n    ):\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 454, "column": 4 },
          "end": { "row": 454, "column": 4 }
        }
      }
    }
  ],
  [
    "1942",
    {
      "pageContent": "def get_timesteps(self, num_inference_steps, strength, device):\n        # get the original timestep using init_timestep\n        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n\n        t_start = max(num_inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:]\n\n        return timesteps, num_inference_steps - t_start",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 495, "column": 4 },
          "end": { "row": 495, "column": 4 }
        }
      }
    }
  ],
  [
    "1943",
    {
      "pageContent": "def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator):\n        image = image.to(device=self.device, dtype=dtype)\n        init_latent_dist = self.vae.encode(image).latent_dist\n        init_latents = init_latent_dist.sample(generator=generator)\n        init_latents = self.vae.config.scaling_factor * init_latents\n\n        # Expand init_latents for batch_size and num_images_per_prompt\n        init_latents = torch.cat([init_latents] * batch_size * num_images_per_prompt, dim=0)\n        init_latents_orig = init_latents\n\n        # add noise to latents using the timesteps\n        noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype)\n        init_latents = self.scheduler.add_noise(init_latents, noise, timestep)\n        latents = init_latents\n        return latents, init_latents_orig, noise",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py",
        "range": {
          "start": { "row": 504, "column": 4 },
          "end": { "row": 504, "column": 4 }
        }
      }
    }
  ],
  [
    "1944",
    {
      "pageContent": "class FlaxStableDiffusionPipeline(FlaxDiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`FlaxDiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`FlaxAutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`FlaxCLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.FlaxCLIPTextModel),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`FlaxUNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`FlaxDDIMScheduler`], [`FlaxLMSDiscreteScheduler`], [`FlaxPNDMScheduler`], or\n            [`FlaxDPMSolverMultistepScheduler`].\n        safety_checker ([`FlaxStableDiffusionSafetyChecker`]):\n            Classification module that estimates whether gener",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py",
        "range": {
          "start": { "row": 80, "column": 0 },
          "end": { "row": 80, "column": 0 }
        }
      }
    }
  ],
  [
    "1945",
    {
      "pageContent": "def __init__(\n        self,\n        vae: FlaxAutoencoderKL,\n        text_encoder: FlaxCLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: FlaxUNet2DConditionModel,\n        scheduler: Union[\n            FlaxDDIMScheduler, FlaxPNDMScheduler, FlaxLMSDiscreteScheduler, FlaxDPMSolverMultistepScheduler\n        ],\n        safety_checker: FlaxStableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        dtype: jnp.dtype = jnp.float32,\n    ):\n        super().__init__()\n        self.dtype = dtype\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n            version.parse(unet.config._diffusers_version).base_version\n        ) < version.parse(\"0.9.0.dev0\")\n        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py",
        "range": {
          "start": { "row": 109, "column": 4 },
          "end": { "row": 109, "column": 4 }
        }
      }
    }
  ],
  [
    "1946",
    {
      "pageContent": "def prepare_inputs(self, prompt: Union[str, List[str]]):\n        if not isinstance(prompt, (str, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        text_input = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        return text_input.input_ids",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py",
        "range": {
          "start": { "row": 167, "column": 4 },
          "end": { "row": 167, "column": 4 }
        }
      }
    }
  ],
  [
    "1947",
    {
      "pageContent": "def _run_safety_checker(self, images, safety_model_params, jit=False):\n        # safety_model_params should already be replicated when jit is True\n        pil_images = [Image.fromarray(image) for image in images]\n        features = self.feature_extractor(pil_images, return_tensors=\"np\").pixel_values\n\n        if jit:\n            features = shard(features)\n            has_nsfw_concepts = _p_get_has_nsfw_concepts(self, features, safety_model_params)\n            has_nsfw_concepts = unshard(has_nsfw_concepts)\n            safety_model_params = unreplicate(safety_model_params)\n        else:\n            has_nsfw_concepts = self._get_has_nsfw_concepts(features, safety_model_params)\n\n        images_was_copied = False\n        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n            if has_nsfw_concept:\n                if not images_was_copied:\n                    images_was_copied = True\n                    images = images.copy()\n\n                images[idx] = np.zeros(images[idx].shape, dtype=np.uint8)  # black image\n\n            if any(has_nsfw_concepts):\n                warnings.warn(\n                    \"Potential NSFW content was detected in one or more images. A black image will be returned\"\n                    \" instead. Try again with a different prompt and/or seed.\"\n                )\n\n        return images, has_nsfw_concepts",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py",
        "range": {
          "start": { "row": 184, "column": 4 },
          "end": { "row": 184, "column": 4 }
        }
      }
    }
  ],
  [
    "1948",
    {
      "pageContent": "def _generate(\n        self,\n        prompt_ids: jnp.array,\n        params: Union[Dict, FrozenDict],\n        prng_seed: jax.random.KeyArray,\n        num_inference_steps: int,\n        height: int,\n        width: int,\n        guidance_scale: float,\n        latents: Optional[jnp.array] = None,\n        neg_prompt_ids: Optional[jnp.array] = None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        # get prompt text embeddings\n        prompt_embeds = self.text_encoder(prompt_ids, params=params[\"text_encoder\"])[0]\n\n        # TODO: currently it is assumed `do_classifier_free_guidance = guidance_scale > 1.0`\n        # implement this conditional `do_classifier_free_guidance = guidance_scale > 1.0`\n        batch_size = prompt_ids.shape[0]\n\n        max_length = prompt_ids.shape[-1]\n\n        if neg_prompt_ids is None:\n            uncond_input = self.tokenizer(\n                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"np\"\n            ).input_ids\n        else:\n            uncond_input = neg_prompt_ids\n        negative_prompt_embeds = self.text_encoder(uncond_input, params=params[\"text_encoder\"])[0]\n        context = jnp.concatenate([negative_prompt_embeds, prompt_embeds])\n\n        latents_shape = (\n            batch_size,\n            self.unet.in_channels,\n            height // self.vae_scale_factor,\n            width // self.vae_scale_factor,\n        )\n        if latents is None:\n            latents = jax.random.normal(prng_seed, sh",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py",
        "range": {
          "start": { "row": 214, "column": 4 },
          "end": { "row": 214, "column": 4 }
        }
      }
    }
  ],
  [
    "1949",
    {
      "pageContent": "def _p_generate(\n    pipe,\n    prompt_ids,\n    params,\n    prng_seed,\n    num_inference_steps,\n    height,\n    width,\n    guidance_scale,\n    latents,\n    neg_prompt_ids,\n):\n    return pipe._generate(\n        prompt_ids,\n        params,\n        prng_seed,\n        num_inference_steps,\n        height,\n        width,\n        guidance_scale,\n        latents,\n        neg_prompt_ids,\n    )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py",
        "range": {
          "start": { "row": 432, "column": 0 },
          "end": { "row": 432, "column": 0 }
        }
      }
    }
  ],
  [
    "1950",
    {
      "pageContent": "def unshard(x: jnp.ndarray):\n    # einops.rearrange(x, 'd b ... -> (d b) ...')\n    num_devices, batch_size = x.shape[:2]\n    rest = x.shape[2:]\n    return x.reshape(num_devices * batch_size, *rest)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py",
        "range": {
          "start": { "row": 462, "column": 0 },
          "end": { "row": 462, "column": 0 }
        }
      }
    }
  ],
  [
    "1951",
    {
      "pageContent": "class AttentionStore:\n    @staticmethod\n    def get_empty_store():\n        return {\"down\": [], \"mid\": [], \"up\": []}\n\n    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n        if self.cur_att_layer >= 0 and is_cross:\n            if attn.shape[1] == self.attn_res**2:\n                self.step_store[place_in_unet].append(attn)\n\n        self.cur_att_layer += 1\n        if self.cur_att_layer == self.num_att_layers:\n            self.cur_att_layer = 0\n            self.between_steps()\n\n    def between_steps(self):\n        self.attention_store = self.step_store\n        self.step_store = self.get_empty_store()\n\n    def get_average_attention(self):\n        average_attention = self.attention_store\n        return average_attention\n\n    def aggregate_attention(self, from_where: List[str]) -> torch.Tensor:\n        \"\"\"Aggregates the attention across the different layers and heads at the specified resolution.\"\"\"\n        out = []\n        attention_maps = self.get_average_attention()\n        for location in from_where:\n            for item in attention_maps[location]:\n                cross_maps = item.reshape(-1, self.attn_res, self.attn_res, item.shape[-1])\n                out.append(cross_maps)\n        out = torch.cat(out, dim=0)\n        out = out.sum(0) / out.shape[0]\n        return out\n\n    def reset(self):\n        self.cur_att_layer = 0\n        self.step_store = self.get_empty_store()\n        self.attention_store = {}\n\n    def __init__(self, attn_res=16):\n        \"\"\"\n        Initialize an empty AttentionStore :param step_index: used to visualize only a specific step in the",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 70, "column": 0 },
          "end": { "row": 70, "column": 0 }
        }
      }
    }
  ],
  [
    "1952",
    {
      "pageContent": "def __call__(self, attn, is_cross: bool, place_in_unet: str):\n        if self.cur_att_layer >= 0 and is_cross:\n            if attn.shape[1] == self.attn_res**2:\n                self.step_store[place_in_unet].append(attn)\n\n        self.cur_att_layer += 1\n        if self.cur_att_layer == self.num_att_layers:\n            self.cur_att_layer = 0\n            self.between_steps()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 75, "column": 4 },
          "end": { "row": 75, "column": 4 }
        }
      }
    }
  ],
  [
    "1953",
    {
      "pageContent": "def aggregate_attention(self, from_where: List[str]) -> torch.Tensor:\n        \"\"\"Aggregates the attention across the different layers and heads at the specified resolution.\"\"\"\n        out = []\n        attention_maps = self.get_average_attention()\n        for location in from_where:\n            for item in attention_maps[location]:\n                cross_maps = item.reshape(-1, self.attn_res, self.attn_res, item.shape[-1])\n                out.append(cross_maps)\n        out = torch.cat(out, dim=0)\n        out = out.sum(0) / out.shape[0]\n        return out",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 93, "column": 4 },
          "end": { "row": 93, "column": 4 }
        }
      }
    }
  ],
  [
    "1954",
    {
      "pageContent": "def reset(self):\n        self.cur_att_layer = 0\n        self.step_store = self.get_empty_store()\n        self.attention_store = {}",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 105, "column": 4 },
          "end": { "row": 105, "column": 4 }
        }
      }
    }
  ],
  [
    "1955",
    {
      "pageContent": "def __init__(self, attn_res=16):\n        \"\"\"\n        Initialize an empty AttentionStore :param step_index: used to visualize only a specific step in the diffusion\n        process\n        \"\"\"\n        self.num_att_layers = -1\n        self.cur_att_layer = 0\n        self.step_store = self.get_empty_store()\n        self.attention_store = {}\n        self.curr_step_index = 0\n        self.attn_res = attn_res",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 110, "column": 4 },
          "end": { "row": 110, "column": 4 }
        }
      }
    }
  ],
  [
    "1956",
    {
      "pageContent": "class AttendExciteCrossAttnProcessor:\n    def __init__(self, attnstore, place_in_unet):\n        super().__init__()\n        self.attnstore = attnstore\n        self.place_in_unet = place_in_unet\n\n    def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states)\n\n        is_cross = encoder_hidden_states is not None\n        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n\n        # only need to store attention maps during the Attend and Excite process\n        if attention_probs.requires_grad:\n            self.attnstore(attention_probs, is_cross, self.place_in_unet)\n\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 123, "column": 0 },
          "end": { "row": 123, "column": 0 }
        }
      }
    }
  ],
  [
    "1957",
    {
      "pageContent": "def __init__(self, attnstore, place_in_unet):\n        super().__init__()\n        self.attnstore = attnstore\n        self.place_in_unet = place_in_unet",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "1958",
    {
      "pageContent": "def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states)\n\n        is_cross = encoder_hidden_states is not None\n        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n\n        # only need to store attention maps during the Attend and Excite process\n        if attention_probs.requires_grad:\n            self.attnstore(attention_probs, is_cross, self.place_in_unet)\n\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 129, "column": 4 },
          "end": { "row": 129, "column": 4 }
        }
      }
    }
  ],
  [
    "1959",
    {
      "pageContent": "class StableDiffusionAttendAndExcitePipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion and Attend and Excite.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n  ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 161, "column": 0 },
          "end": { "row": 161, "column": 0 }
        }
      }
    }
  ],
  [
    "1960",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modules(\n            vae=vae,\n           ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 190, "column": 4 },
          "end": { "row": 190, "column": 4 }
        }
      }
    }
  ],
  [
    "1961",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 232, "column": 4 },
          "end": { "row": 232, "column": 4 }
        }
      }
    }
  ],
  [
    "1962",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 242, "column": 4 },
          "end": { "row": 242, "column": 4 }
        }
      }
    }
  ],
  [
    "1963",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 250, "column": 4 },
          "end": { "row": 250, "column": 4 }
        }
      }
    }
  ],
  [
    "1964",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 295, "column": 4 },
          "end": { "row": 295, "column": 4 }
        }
      }
    }
  ],
  [
    "1965",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 434, "column": 4 },
          "end": { "row": 434, "column": 4 }
        }
      }
    }
  ],
  [
    "1966",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 445, "column": 4 },
          "end": { "row": 445, "column": 4 }
        }
      }
    }
  ],
  [
    "1967",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 454, "column": 4 },
          "end": { "row": 454, "column": 4 }
        }
      }
    }
  ],
  [
    "1968",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        indices,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `neg",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 471, "column": 4 },
          "end": { "row": 471, "column": 4 }
        }
      }
    }
  ],
  [
    "1969",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 545, "column": 4 },
          "end": { "row": 545, "column": 4 }
        }
      }
    }
  ],
  [
    "1970",
    {
      "pageContent": "def _aggregate_and_get_max_attention_per_token(\n        self,\n        indices: List[int],\n    ):\n        \"\"\"Aggregates the attention for each token and computes the max activation value for each token to alter.\"\"\"\n        attention_maps = self.attention_store.aggregate_attention(\n            from_where=(\"up\", \"down\", \"mid\"),\n        )\n        max_attention_per_index = self._compute_max_attention_per_index(\n            attention_maps=attention_maps,\n            indices=indices,\n        )\n        return max_attention_per_index",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 585, "column": 4 },
          "end": { "row": 585, "column": 4 }
        }
      }
    }
  ],
  [
    "1971",
    {
      "pageContent": "def _perform_iterative_refinement_step(\n        self,\n        latents: torch.Tensor,\n        indices: List[int],\n        loss: torch.Tensor,\n        threshold: float,\n        text_embeddings: torch.Tensor,\n        step_size: float,\n        t: int,\n        max_refinement_steps: int = 20,\n    ):\n        \"\"\"\n        Performs the iterative latent refinement introduced in the paper. Here, we continuously update the latent code\n        according to our loss objective until the given threshold is reached for all tokens.\n        \"\"\"\n        iteration = 0\n        target_loss = max(0, 1.0 - threshold)\n        while loss > target_loss:\n            iteration += 1\n\n            latents = latents.clone().detach().requires_grad_(True)\n            self.unet(latents, t, encoder_hidden_states=text_embeddings).sample\n            self.unet.zero_grad()\n\n            # Get max activation value for each subject token\n            max_attention_per_index = self._aggregate_and_get_max_attention_per_token(\n                indices=indices,\n            )\n\n            loss = self._compute_loss(max_attention_per_index)\n\n            if loss != 0:\n                latents = self._update_latent(latents, loss, step_size)\n\n            logger.info(f\"\\t Try {iteration}. loss: {loss}\")\n\n            if iteration >= max_refinement_steps:\n                logger.info(f\"\\t Exceeded max number of iterations ({max_refinement_steps})! \")\n                break\n\n        # Run one more time but don't compute gradients and update the latents.\n        # We just need to compute the new loss - the grad update will occur below\n   ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 613, "column": 4 },
          "end": { "row": 613, "column": 4 }
        }
      }
    }
  ],
  [
    "1972",
    {
      "pageContent": "def register_attention_control(self):\n        attn_procs = {}\n        cross_att_count = 0\n        for name in self.unet.attn_processors.keys():\n            if name.startswith(\"mid_block\"):\n                place_in_unet = \"mid\"\n            elif name.startswith(\"up_blocks\"):\n                place_in_unet = \"up\"\n            elif name.startswith(\"down_blocks\"):\n                place_in_unet = \"down\"\n            else:\n                continue\n\n            cross_att_count += 1\n            attn_procs[name] = AttendExciteCrossAttnProcessor(\n                attnstore=self.attention_store, place_in_unet=place_in_unet\n            )\n\n        self.unet.set_attn_processor(attn_procs)\n        self.attention_store.num_att_layers = cross_att_count",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 667, "column": 4 },
          "end": { "row": 667, "column": 4 }
        }
      }
    }
  ],
  [
    "1973",
    {
      "pageContent": "def get_indices(self, prompt: str) -> Dict[str, int]:\n        \"\"\"Utility function to list the indices of the tokens you wish to alte\"\"\"\n        ids = self.tokenizer(prompt).input_ids\n        indices = {i: tok for tok, i in zip(self.tokenizer.convert_ids_to_tokens(ids), range(len(ids)))}\n        return indices",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 688, "column": 4 },
          "end": { "row": 688, "column": 4 }
        }
      }
    }
  ],
  [
    "1974",
    {
      "pageContent": "class GaussianSmoothing(torch.nn.Module):\n    \"\"\"\n    Arguments:\n    Apply gaussian smoothing on a 1d, 2d or 3d tensor. Filtering is performed seperately for each channel in the input\n    using a depthwise convolution.\n        channels (int, sequence): Number of channels of the input tensors. Output will\n            have this number of channels as well.\n        kernel_size (int, sequence): Size of the gaussian kernel. sigma (float, sequence): Standard deviation of the\n        gaussian kernel. dim (int, optional): The number of dimensions of the data.\n            Default value is 2 (spatial).\n    \"\"\"\n\n    # channels=1, kernel_size=kernel_size, sigma=sigma, dim=2\n    def __init__(\n        self,\n        channels: int = 1,\n        kernel_size: int = 3,\n        sigma: float = 0.5,\n        dim: int = 2,\n    ):\n        super().__init__()\n\n        if isinstance(kernel_size, int):\n            kernel_size = [kernel_size] * dim\n        if isinstance(sigma, float):\n            sigma = [sigma] * dim\n\n        # The gaussian kernel is the product of the\n        # gaussian function of each dimension.\n        kernel = 1\n        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])\n        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n            mean = (size - 1) / 2\n            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * torch.exp(-(((mgrid - mean) / (2 * std)) ** 2))\n\n        # Make sure sum of values in gaussian kernel equals 1.\n        kernel = kernel / torch.sum(kernel)\n\n        # Reshape to depthwise convolutional weight\n       ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 980, "column": 0 },
          "end": { "row": 980, "column": 0 }
        }
      }
    }
  ],
  [
    "1975",
    {
      "pageContent": "def __init__(\n        self,\n        channels: int = 1,\n        kernel_size: int = 3,\n        sigma: float = 0.5,\n        dim: int = 2,\n    ):\n        super().__init__()\n\n        if isinstance(kernel_size, int):\n            kernel_size = [kernel_size] * dim\n        if isinstance(sigma, float):\n            sigma = [sigma] * dim\n\n        # The gaussian kernel is the product of the\n        # gaussian function of each dimension.\n        kernel = 1\n        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])\n        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n            mean = (size - 1) / 2\n            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * torch.exp(-(((mgrid - mean) / (2 * std)) ** 2))\n\n        # Make sure sum of values in gaussian kernel equals 1.\n        kernel = kernel / torch.sum(kernel)\n\n        # Reshape to depthwise convolutional weight\n        kernel = kernel.view(1, 1, *kernel.size())\n        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n\n        self.register_buffer(\"weight\", kernel)\n        self.groups = channels\n\n        if dim == 1:\n            self.conv = F.conv1d\n        elif dim == 2:\n            self.conv = F.conv2d\n        elif dim == 3:\n            self.conv = F.conv3d\n        else:\n            raise RuntimeError(\"Only 1, 2 and 3 dimensions are supported. Received {}.\".format(dim))",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 993, "column": 4 },
          "end": { "row": 993, "column": 4 }
        }
      }
    }
  ],
  [
    "1976",
    {
      "pageContent": "def forward(self, input):\n        \"\"\"\n        Arguments:\n        Apply gaussian filter to input.\n            input (torch.Tensor): Input to apply gaussian filter on.\n        Returns:\n            filtered (torch.Tensor): Filtered output.\n        \"\"\"\n        return self.conv(input, weight=self.weight.to(input.dtype), groups=self.groups)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py",
        "range": {
          "start": { "row": 1034, "column": 4 },
          "end": { "row": 1034, "column": 4 }
        }
      }
    }
  ],
  [
    "1977",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n\n        image = [np.array(i.resize((w, h)))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "1978",
    {
      "pageContent": "class StableDiffusionLatentUpscalePipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline to upscale the resolution of Stable Diffusion output images by a factor of 2.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`EulerDiscreteScheduler`].\n    \"\"\"\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: EulerDiscreteScheduler,",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 53, "column": 0 },
          "end": { "row": 53, "column": 0 }
        }
      }
    }
  ],
  [
    "1979",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: EulerDiscreteScheduler,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n        )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 76, "column": 4 },
          "end": { "row": 76, "column": 4 }
        }
      }
    }
  ],
  [
    "1980",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 94, "column": 4 },
          "end": { "row": 94, "column": 4 }
        }
      }
    }
  ],
  [
    "1981",
    {
      "pageContent": "def _encode_prompt(self, prompt, device, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `list(int)`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_length=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n\n        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n     ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 130, "column": 4 },
          "end": { "row": 130, "column": 4 }
        }
      }
    }
  ],
  [
    "1982",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 221, "column": 4 },
          "end": { "row": 221, "column": 4 }
        }
      }
    }
  ],
  [
    "1983",
    {
      "pageContent": "def check_inputs(self, prompt, image, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if (\n            not isinstance(image, torch.Tensor)\n            and not isinstance(image, PIL.Image.Image)\n            and not isinstance(image, list)\n        ):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or `list` but is {type(image)}\"\n            )\n\n        # verify batch size of prompt and image are same if image is a list or tensor\n        if isinstance(image, list) or isinstance(image, torch.Tensor):\n            if isinstance(prompt, str):\n                batch_size = 1\n            else:\n                batch_size = len(prompt)\n            if isinstance(image, list):\n                image_batch_size = len(image)\n            else:\n                image_batch_size = image.shape[0] if image.ndim == 4 else 1\n            if batch_size != image_batch_size:\n                raise ValueError(\n                    f\"`prompt` has batch size {batch_size} and `image` has batch size {image_batch_size}.\"\n                    \" Please make sure that passed `prompt` matches the batch size of `image`.\"\n                )\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of ",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 229, "column": 4 },
          "end": { "row": 229, "column": 4 }
        }
      }
    }
  ],
  [
    "1984",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height, width)\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py",
        "range": {
          "start": { "row": 267, "column": 4 },
          "end": { "row": 267, "column": 4 }
        }
      }
    }
  ],
  [
    "1985",
    {
      "pageContent": "class StableDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n        nsfw_content_detected (`List[bool]`)\n            List of flags denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, or `None` if safety checking could not be performed.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/__init__.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "1986",
    {
      "pageContent": "def cosine_distance(image_embeds, text_embeds):\n    normalized_image_embeds = nn.functional.normalize(image_embeds)\n    normalized_text_embeds = nn.functional.normalize(text_embeds)\n    return torch.mm(normalized_image_embeds, normalized_text_embeds.t())",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "1987",
    {
      "pageContent": "class StableDiffusionSafetyChecker(PreTrainedModel):\n    config_class = CLIPConfig\n\n    _no_split_modules = [\"CLIPEncoderLayer\"]\n\n    def __init__(self, config: CLIPConfig):\n        super().__init__(config)\n\n        self.vision_model = CLIPVisionModel(config.vision_config)\n        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n\n        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n\n        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)\n        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)\n\n    @torch.no_grad()\n    def forward(self, clip_input, images):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().float().numpy()\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().float().numpy()\n\n        result = []\n        batch_size = image_embeds.shape[0]\n        for i in range(batch_size):\n            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n\n            # increase this value to create a stronger `nfsw` filter\n            # at the cost of inc",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1988",
    {
      "pageContent": "def __init__(self, config: CLIPConfig):\n        super().__init__(config)\n\n        self.vision_model = CLIPVisionModel(config.vision_config)\n        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n\n        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n\n        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)\n        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/safety_checker.py",
        "range": {
          "start": { "row": 36, "column": 4 },
          "end": { "row": 36, "column": 4 }
        }
      }
    }
  ],
  [
    "1989",
    {
      "pageContent": "class FlaxStableDiffusionInpaintPipeline(FlaxDiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image inpainting using Stable Diffusion. *This is an experimental feature*.\n\n    This model inherits from [`FlaxDiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`FlaxAutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`FlaxCLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.FlaxCLIPTextModel),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`FlaxUNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`FlaxDDIMScheduler`], [`FlaxLMSDiscreteScheduler`], [`FlaxPNDMScheduler`], or\n            [`FlaxDPMSolverMultistepScheduler`].\n        safety_checker ([`FlaxStableDiffusionSafetyChecker`]):\n            Clas",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 101, "column": 0 },
          "end": { "row": 101, "column": 0 }
        }
      }
    }
  ],
  [
    "1990",
    {
      "pageContent": "def __init__(\n        self,\n        vae: FlaxAutoencoderKL,\n        text_encoder: FlaxCLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: FlaxUNet2DConditionModel,\n        scheduler: Union[\n            FlaxDDIMScheduler, FlaxPNDMScheduler, FlaxLMSDiscreteScheduler, FlaxDPMSolverMultistepScheduler\n        ],\n        safety_checker: FlaxStableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        dtype: jnp.dtype = jnp.float32,\n    ):\n        super().__init__()\n        self.dtype = dtype\n\n        if safety_checker is None:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n            version.parse(unet.config._diffusers_version).base_version\n        ) < version.parse(\"0.9.0.dev0\")\n        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 130, "column": 4 },
          "end": { "row": 130, "column": 4 }
        }
      }
    }
  ],
  [
    "1991",
    {
      "pageContent": "def prepare_inputs(\n        self,\n        prompt: Union[str, List[str]],\n        image: Union[Image.Image, List[Image.Image]],\n        mask: Union[Image.Image, List[Image.Image]],\n    ):\n        if not isinstance(prompt, (str, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if not isinstance(image, (Image.Image, list)):\n            raise ValueError(f\"image has to be of type `PIL.Image.Image` or list but is {type(image)}\")\n\n        if isinstance(image, Image.Image):\n            image = [image]\n\n        if not isinstance(mask, (Image.Image, list)):\n            raise ValueError(f\"image has to be of type `PIL.Image.Image` or list but is {type(image)}\")\n\n        if isinstance(mask, Image.Image):\n            mask = [mask]\n\n        processed_images = jnp.concatenate([preprocess_image(img, jnp.float32) for img in image])\n        processed_masks = jnp.concatenate([preprocess_mask(m, jnp.float32) for m in mask])\n        # processed_masks[processed_masks < 0.5] = 0\n        processed_masks = processed_masks.at[processed_masks < 0.5].set(0)\n        # processed_masks[processed_masks >= 0.5] = 1\n        processed_masks = processed_masks.at[processed_masks >= 0.5].set(1)\n\n        processed_masked_images = processed_images * (processed_masks < 0.5)\n\n        text_input = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        return text_input.input_ids, processed_masked_images,",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 188, "column": 4 },
          "end": { "row": 188, "column": 4 }
        }
      }
    }
  ],
  [
    "1992",
    {
      "pageContent": "def _run_safety_checker(self, images, safety_model_params, jit=False):\n        # safety_model_params should already be replicated when jit is True\n        pil_images = [Image.fromarray(image) for image in images]\n        features = self.feature_extractor(pil_images, return_tensors=\"np\").pixel_values\n\n        if jit:\n            features = shard(features)\n            has_nsfw_concepts = _p_get_has_nsfw_concepts(self, features, safety_model_params)\n            has_nsfw_concepts = unshard(has_nsfw_concepts)\n            safety_model_params = unreplicate(safety_model_params)\n        else:\n            has_nsfw_concepts = self._get_has_nsfw_concepts(features, safety_model_params)\n\n        images_was_copied = False\n        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n            if has_nsfw_concept:\n                if not images_was_copied:\n                    images_was_copied = True\n                    images = images.copy()\n\n                images[idx] = np.zeros(images[idx].shape, dtype=np.uint8)  # black image\n\n            if any(has_nsfw_concepts):\n                warnings.warn(\n                    \"Potential NSFW content was detected in one or more images. A black image will be returned\"\n                    \" instead. Try again with a different prompt and/or seed.\"\n                )\n\n        return images, has_nsfw_concepts",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 231, "column": 4 },
          "end": { "row": 231, "column": 4 }
        }
      }
    }
  ],
  [
    "1993",
    {
      "pageContent": "def _generate(\n        self,\n        prompt_ids: jnp.array,\n        mask: jnp.array,\n        masked_image: jnp.array,\n        params: Union[Dict, FrozenDict],\n        prng_seed: jax.random.KeyArray,\n        num_inference_steps: int,\n        height: int,\n        width: int,\n        guidance_scale: float,\n        latents: Optional[jnp.array] = None,\n        neg_prompt_ids: Optional[jnp.array] = None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        # get prompt text embeddings\n        prompt_embeds = self.text_encoder(prompt_ids, params=params[\"text_encoder\"])[0]\n\n        # TODO: currently it is assumed `do_classifier_free_guidance = guidance_scale > 1.0`\n        # implement this conditional `do_classifier_free_guidance = guidance_scale > 1.0`\n        batch_size = prompt_ids.shape[0]\n\n        max_length = prompt_ids.shape[-1]\n\n        if neg_prompt_ids is None:\n            uncond_input = self.tokenizer(\n                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"np\"\n            ).input_ids\n        else:\n            uncond_input = neg_prompt_ids\n        negative_prompt_embeds = self.text_encoder(uncond_input, params=params[\"text_encoder\"])[0]\n        context = jnp.concatenate([negative_prompt_embeds, prompt_embeds])\n\n        latents_shape = (\n            batch_size,\n            self.vae.config.latent_channels,\n            height // self.vae_scale_factor,\n            width // self.vae_scale_factor,\n        )\n        if la",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 261, "column": 4 },
          "end": { "row": 261, "column": 4 }
        }
      }
    }
  ],
  [
    "1994",
    {
      "pageContent": "def _p_generate(\n    pipe,\n    prompt_ids,\n    mask,\n    masked_image,\n    params,\n    prng_seed,\n    num_inference_steps,\n    height,\n    width,\n    guidance_scale,\n    latents,\n    neg_prompt_ids,\n):\n    return pipe._generate(\n        prompt_ids,\n        mask,\n        masked_image,\n        params,\n        prng_seed,\n        num_inference_steps,\n        height,\n        width,\n        guidance_scale,\n        latents,\n        neg_prompt_ids,\n    )",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 522, "column": 0 },
          "end": { "row": 522, "column": 0 }
        }
      }
    }
  ],
  [
    "1995",
    {
      "pageContent": "def unshard(x: jnp.ndarray):\n    # einops.rearrange(x, 'd b ... -> (d b) ...')\n    num_devices, batch_size = x.shape[:2]\n    rest = x.shape[2:]\n    return x.reshape(num_devices * batch_size, *rest)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 556, "column": 0 },
          "end": { "row": 556, "column": 0 }
        }
      }
    }
  ],
  [
    "1996",
    {
      "pageContent": "def preprocess_image(image, dtype):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = jnp.array(image).astype(dtype) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    return 2.0 * image - 1.0",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 563, "column": 0 },
          "end": { "row": 563, "column": 0 }
        }
      }
    }
  ],
  [
    "1997",
    {
      "pageContent": "def preprocess_mask(mask, dtype):\n    w, h = mask.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    mask = mask.resize((w, h))\n    mask = jnp.array(mask.convert(\"L\")).astype(dtype) / 255.0\n    mask = jnp.expand_dims(mask, axis=(0, 1))\n\n    return mask",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py",
        "range": {
          "start": { "row": 572, "column": 0 },
          "end": { "row": 572, "column": 0 }
        }
      }
    }
  ],
  [
    "1998",
    {
      "pageContent": "def preprocess(image):\n    if isinstance(image, torch.Tensor):\n        return image\n    elif isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n\n        image = [np.array(i.resize((w, h)))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "1999",
    {
      "pageContent": "class StableDiffusionUpscalePipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-guided image super-resolution using Stable Diffusion 2.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        low_res_scheduler ([`SchedulerMixin`]):\n            A scheduler used to add initial noise to the low res conditioning image. It must be an instance of\n            [`DDPMScheduler`].\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n    \"",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 52, "column": 0 },
          "end": { "row": 52, "column": 0 }
        }
      }
    }
  ],
  [
    "2000",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        low_res_scheduler: DDPMScheduler,\n        scheduler: KarrasDiffusionSchedulers,\n        max_noise_level: int = 350,\n    ):\n        super().__init__()\n\n        if hasattr(vae, \"config\"):\n            # check if vae has a config attribute `scaling_factor` and if it is set to 0.08333, else set it to 0.08333 and deprecate\n            is_vae_scaling_factor_set_to_0_08333 = (\n                hasattr(vae.config, \"scaling_factor\") and vae.config.scaling_factor == 0.08333\n            )\n            if not is_vae_scaling_factor_set_to_0_08333:\n                deprecation_message = (\n                    \"The configuration file of the vae does not contain `scaling_factor` or it is set to\"\n                    f\" {vae.config.scaling_factor}, which seems highly unlikely. If your checkpoint is a fine-tuned\"\n                    \" version of `stabilityai/stable-diffusion-x4-upscaler` you should change 'scaling_factor' to\"\n                    \" 0.08333 Please make sure to update the config accordingly, as not doing so might lead to\"\n                    \" incorrect results in future versions. If you have downloaded this checkpoint from the Hugging\"\n                    \" Face Hub, it would be very nice if you could open a Pull Request for the `vae/config.json` file\"\n                )\n                deprecate(\"wrong scaling_factor\", \"1.0.0\", deprecation_message, standard_warn=False)\n                vae.register_to_config(scaling",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 78, "column": 4 },
          "end": { "row": 78, "column": 4 }
        }
      }
    }
  ],
  [
    "2001",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 117, "column": 4 },
          "end": { "row": 117, "column": 4 }
        }
      }
    }
  ],
  [
    "2002",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 154, "column": 4 },
          "end": { "row": 154, "column": 4 }
        }
      }
    }
  ],
  [
    "2003",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 293, "column": 4 },
          "end": { "row": 293, "column": 4 }
        }
      }
    }
  ],
  [
    "2004",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 311, "column": 4 },
          "end": { "row": 311, "column": 4 }
        }
      }
    }
  ],
  [
    "2005",
    {
      "pageContent": "def check_inputs(self, prompt, image, noise_level, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if (\n            not isinstance(image, torch.Tensor)\n            and not isinstance(image, PIL.Image.Image)\n            and not isinstance(image, list)\n        ):\n            raise ValueError(\n                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or `list` but is {type(image)}\"\n            )\n\n        # verify batch size of prompt and image are same if image is a list or tensor\n        if isinstance(image, list) or isinstance(image, torch.Tensor):\n            if isinstance(prompt, str):\n                batch_size = 1\n            else:\n                batch_size = len(prompt)\n            if isinstance(image, list):\n                image_batch_size = len(image)\n            else:\n                image_batch_size = image.shape[0]\n            if batch_size != image_batch_size:\n                raise ValueError(\n                    f\"`prompt` has batch size {batch_size} and `image` has batch size {image_batch_size}.\"\n                    \" Please make sure that passed `prompt` matches the batch size of `image`.\"\n                )\n\n        # check noise level\n        if noise_level > self.config.max_noise_level:\n            raise ValueError(f\"`noise_level` has to be <= {self.config.max_noise_level} but is {noise_level}\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinsta",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 319, "column": 4 },
          "end": { "row": 319, "column": 4 }
        }
      }
    }
  ],
  [
    "2006",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height, width)\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py",
        "range": {
          "start": { "row": 360, "column": 4 },
          "end": { "row": 360, "column": 4 }
        }
      }
    }
  ],
  [
    "2007",
    {
      "pageContent": "class CrossAttnStoreProcessor:\n    def __init__(self):\n        self.attention_probs = None\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        self.attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(self.attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 50, "column": 0 },
          "end": { "row": 50, "column": 0 }
        }
      }
    }
  ],
  [
    "2008",
    {
      "pageContent": "def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        self.attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(self.attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 54, "column": 4 },
          "end": { "row": 54, "column": 4 }
        }
      }
    }
  ],
  [
    "2009",
    {
      "pageContent": "class StableDiffusionSAGPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [mo",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 90, "column": 0 },
          "end": { "row": 90, "column": 0 }
        }
      }
    }
  ],
  [
    "2010",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 119, "column": 4 },
          "end": { "row": 119, "column": 4 }
        }
      }
    }
  ],
  [
    "2011",
    {
      "pageContent": "def enable_vae_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding.\n\n        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n        steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.vae.enable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 145, "column": 4 },
          "end": { "row": 145, "column": 4 }
        }
      }
    }
  ],
  [
    "2012",
    {
      "pageContent": "def disable_vae_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.vae.disable_slicing()",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 155, "column": 4 },
          "end": { "row": 155, "column": 4 }
        }
      }
    }
  ],
  [
    "2013",
    {
      "pageContent": "def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 163, "column": 4 },
          "end": { "row": 163, "column": 4 }
        }
      }
    }
  ],
  [
    "2014",
    {
      "pageContent": "def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 208, "column": 4 },
          "end": { "row": 208, "column": 4 }
        }
      }
    }
  ],
  [
    "2015",
    {
      "pageContent": "def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 347, "column": 4 },
          "end": { "row": 347, "column": 4 }
        }
      }
    }
  ],
  [
    "2016",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 358, "column": 4 },
          "end": { "row": 358, "column": 4 }
        }
      }
    }
  ],
  [
    "2017",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 367, "column": 4 },
          "end": { "row": 367, "column": 4 }
        }
      }
    }
  ],
  [
    "2018",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 385, "column": 4 },
          "end": { "row": 385, "column": 4 }
        }
      }
    }
  ],
  [
    "2019",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 433, "column": 4 },
          "end": { "row": 433, "column": 4 }
        }
      }
    }
  ],
  [
    "2020",
    {
      "pageContent": "def sag_masking(self, original_latents, attn_map, t, eps):\n        # Same masking process as in SAG paper: https://arxiv.org/pdf/2210.00939.pdf\n        bh, hw1, hw2 = attn_map.shape\n        b, latent_channel, latent_h, latent_w = original_latents.shape\n        h = self.unet.attention_head_dim\n        if isinstance(h, list):\n            h = h[-1]\n        map_size = math.isqrt(hw1)\n\n        # Produce attention mask\n        attn_map = attn_map.reshape(b, h, hw1, hw2)\n        attn_mask = attn_map.mean(1, keepdim=False).sum(1, keepdim=False) > 1.0\n        attn_mask = (\n            attn_mask.reshape(b, map_size, map_size).unsqueeze(1).repeat(1, latent_channel, 1, 1).type(attn_map.dtype)\n        )\n        attn_mask = F.interpolate(attn_mask, (latent_h, latent_w))\n\n        # Blur according to the self-attention mask\n        degraded_latents = gaussian_blur_2d(original_latents, kernel_size=9, sigma=1.0)\n        degraded_latents = degraded_latents * attn_mask + original_latents * (1 - attn_mask)\n\n        # Noise it again to match the noise level\n        degraded_latents = self.scheduler.add_noise(degraded_latents, noise=eps, timesteps=t)\n\n        return degraded_latents",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 682, "column": 4 },
          "end": { "row": 682, "column": 4 }
        }
      }
    }
  ],
  [
    "2021",
    {
      "pageContent": "def pred_x0(self, sample, model_output, timestep):\n        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n\n        beta_prod_t = 1 - alpha_prod_t\n        if self.scheduler.config.prediction_type == \"epsilon\":\n            pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n        elif self.scheduler.config.prediction_type == \"sample\":\n            pred_original_sample = model_output\n        elif self.scheduler.config.prediction_type == \"v_prediction\":\n            pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output\n            # predict V\n            model_output = (alpha_prod_t**0.5) * model_output + (beta_prod_t**0.5) * sample\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.scheduler.config.prediction_type} must be one of `epsilon`, `sample`,\"\n                \" or `v_prediction`\"\n            )\n\n        return pred_original_sample",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 710, "column": 4 },
          "end": { "row": 710, "column": 4 }
        }
      }
    }
  ],
  [
    "2022",
    {
      "pageContent": "def pred_epsilon(self, sample, model_output, timestep):\n        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n\n        beta_prod_t = 1 - alpha_prod_t\n        if self.scheduler.config.prediction_type == \"epsilon\":\n            pred_eps = model_output\n        elif self.scheduler.config.prediction_type == \"sample\":\n            pred_eps = (sample - (alpha_prod_t**0.5) * model_output) / (beta_prod_t**0.5)\n        elif self.scheduler.config.prediction_type == \"v_prediction\":\n            pred_eps = (beta_prod_t**0.5) * sample + (alpha_prod_t**0.5) * model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.scheduler.config.prediction_type} must be one of `epsilon`, `sample`,\"\n                \" or `v_prediction`\"\n            )\n\n        return pred_eps",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 730, "column": 4 },
          "end": { "row": 730, "column": 4 }
        }
      }
    }
  ],
  [
    "2023",
    {
      "pageContent": "def gaussian_blur_2d(img, kernel_size, sigma):\n    ksize_half = (kernel_size - 1) * 0.5\n\n    x = torch.linspace(-ksize_half, ksize_half, steps=kernel_size)\n\n    pdf = torch.exp(-0.5 * (x / sigma).pow(2))\n\n    x_kernel = pdf / pdf.sum()\n    x_kernel = x_kernel.to(device=img.device, dtype=img.dtype)\n\n    kernel2d = torch.mm(x_kernel[:, None], x_kernel[None, :])\n    kernel2d = kernel2d.expand(img.shape[-3], 1, kernel2d.shape[0], kernel2d.shape[1])\n\n    padding = [kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2]\n\n    img = F.pad(img, padding, mode=\"reflect\")\n    img = F.conv2d(img, kernel2d, groups=img.shape[-3])\n\n    return img",
      "metadata": {
        "source": "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py",
        "range": {
          "start": { "row": 750, "column": 0 },
          "end": { "row": 750, "column": 0 }
        }
      }
    }
  ],
  [
    "2024",
    {
      "pageContent": "class SemanticStableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation with latent editing.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    This model builds on the implementation of ['StableDiffusionPipeline']\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latens. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`Q16SafetyChecker`]):\n            Classification module that estimates whether generated images could be cons",
      "metadata": {
        "source": "src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py",
        "range": {
          "start": { "row": 60, "column": 0 },
          "end": { "row": 60, "column": 0 }
        }
      }
    }
  ],
  [
    "2025",
    {
      "pageContent": "def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modules(\n            vae=vae,\n           ",
      "metadata": {
        "source": "src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py",
        "range": {
          "start": { "row": 92, "column": 4 },
          "end": { "row": 92, "column": 4 }
        }
      }
    }
  ],
  [
    "2026",
    {
      "pageContent": "def decode_latents(self, latents):\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image",
      "metadata": {
        "source": "src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py",
        "range": {
          "start": { "row": 134, "column": 4 },
          "end": { "row": 134, "column": 4 }
        }
      }
    }
  ],
  [
    "2027",
    {
      "pageContent": "def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs",
      "metadata": {
        "source": "src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py",
        "range": {
          "start": { "row": 143, "column": 4 },
          "end": { "row": 143, "column": 4 }
        }
      }
    }
  ],
  [
    "2028",
    {
      "pageContent": "def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embe",
      "metadata": {
        "source": "src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py",
        "range": {
          "start": { "row": 161, "column": 4 },
          "end": { "row": 161, "column": 4 }
        }
      }
    }
  ],
  [
    "2029",
    {
      "pageContent": "def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents",
      "metadata": {
        "source": "src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py",
        "range": {
          "start": { "row": 209, "column": 4 },
          "end": { "row": 209, "column": 4 }
        }
      }
    }
  ],
  [
    "2030",
    {
      "pageContent": "class SemanticStableDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n        nsfw_content_detected (`List[bool]`)\n            List of flags denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, or `None` if safety checking could not be performed.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]",
      "metadata": {
        "source": "src/diffusers/pipelines/semantic_stable_diffusion/__init__.py",
        "range": {
          "start": { "row": 12, "column": 0 },
          "end": { "row": 12, "column": 0 }
        }
      }
    }
  ],
  [
    "2031",
    {
      "pageContent": "class KarrasVePipeline(DiffusionPipeline):\n    r\"\"\"\n    Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and\n    the VE column of Table 1 from [1] for reference.\n\n    [1] Karras, Tero, et al. \"Elucidating the Design Space of Diffusion-Based Generative Models.\"\n    https://arxiv.org/abs/2206.00364 [2] Song, Yang, et al. \"Score-based generative modeling through stochastic\n    differential equations.\" https://arxiv.org/abs/2011.13456\n\n    Parameters:\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image.\n        scheduler ([`KarrasVeScheduler`]):\n            Scheduler for the diffusion process to be used in combination with `unet` to denoise the encoded image.\n    \"\"\"\n\n    # add type hints for linting\n    unet: UNet2DModel\n    scheduler: KarrasVeScheduler\n\n    def __init__(self, unet: UNet2DModel, scheduler: KarrasVeScheduler):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        num_inference_steps: int = 50,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[Tuple, ImagePipelineOutput]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(",
      "metadata": {
        "source": "src/diffusers/pipelines/stochastic_karras_ve/pipeline_stochastic_karras_ve.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "2032",
    {
      "pageContent": "class DDPMPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n            [`DDPMScheduler`], or [`DDIMScheduler`].\n    \"\"\"\n\n    def __init__(self, unet, scheduler):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        num_inference_steps: int = 1000,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            num_inference_steps (`int`, *optional*, defaults to 1000):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n             ",
      "metadata": {
        "source": "src/diffusers/pipelines/ddpm/pipeline_ddpm.py",
        "range": {
          "start": { "row": 23, "column": 0 },
          "end": { "row": 23, "column": 0 }
        }
      }
    }
  ],
  [
    "2033",
    {
      "pageContent": "def enable_full_determinism(seed: int):\n    \"\"\"\n    Helper function for reproducible behavior during distributed training. See\n    - https://pytorch.org/docs/stable/notes/randomness.html for pytorch\n    \"\"\"\n    # set seed first\n    set_seed(seed)\n\n    #  Enable PyTorch deterministic mode. This potentially requires either the environment\n    #  variable 'CUDA_LAUNCH_BLOCKING' or 'CUBLAS_WORKSPACE_CONFIG' to be set,\n    # depending on the CUDA version, so we set them both here\n    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n    torch.use_deterministic_algorithms(True)\n\n    # Enable CUDNN deterministic mode\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 11, "column": 0 },
          "end": { "row": 11, "column": 0 }
        }
      }
    }
  ],
  [
    "2034",
    {
      "pageContent": "def set_seed(seed: int):\n    \"\"\"\n    Args:\n    Helper function for reproducible behavior to set the seed in `random`, `numpy`, `torch`.\n        seed (`int`): The seed to set.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # ^^ safe to call this function even if cuda is not available",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 31, "column": 0 },
          "end": { "row": 31, "column": 0 }
        }
      }
    }
  ],
  [
    "2035",
    {
      "pageContent": "class EMAModel:\n    \"\"\"\n    Exponential Moving Average of models weights\n    \"\"\"\n\n    def __init__(\n        self,\n        parameters: Iterable[torch.nn.Parameter],\n        decay: float = 0.9999,\n        min_decay: float = 0.0,\n        update_after_step: int = 0,\n        use_ema_warmup: bool = False,\n        inv_gamma: Union[float, int] = 1.0,\n        power: Union[float, int] = 2 / 3,\n        model_cls: Optional[Any] = None,\n        model_config: Dict[str, Any] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            parameters (Iterable[torch.nn.Parameter]): The parameters to track.\n            decay (float): The decay factor for the exponential moving average.\n            min_decay (float): The minimum decay factor for the exponential moving average.\n            update_after_step (int): The number of steps to wait before starting to update the EMA weights.\n            use_ema_warmup (bool): Whether to use EMA warmup.\n            inv_gamma (float):\n                Inverse multiplicative factor of EMA warmup. Default: 1. Only used if `use_ema_warmup` is True.\n            power (float): Exponential factor of EMA warmup. Default: 2/3. Only used if `use_ema_warmup` is True.\n            device (Optional[Union[str, torch.device]]): The device to store the EMA weights on. If None, the EMA\n                        weights will be stored on CPU.\n\n        @crowsonkb's notes on EMA Warmup:\n            If gamma=1 and power=1, implements a simple average. gamma=1, power=2/3 are good values for models you plan\n            to train for a million or more steps (reaches decay f",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 45, "column": 0 },
          "end": { "row": 45, "column": 0 }
        }
      }
    }
  ],
  [
    "2036",
    {
      "pageContent": "def __init__(\n        self,\n        parameters: Iterable[torch.nn.Parameter],\n        decay: float = 0.9999,\n        min_decay: float = 0.0,\n        update_after_step: int = 0,\n        use_ema_warmup: bool = False,\n        inv_gamma: Union[float, int] = 1.0,\n        power: Union[float, int] = 2 / 3,\n        model_cls: Optional[Any] = None,\n        model_config: Dict[str, Any] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            parameters (Iterable[torch.nn.Parameter]): The parameters to track.\n            decay (float): The decay factor for the exponential moving average.\n            min_decay (float): The minimum decay factor for the exponential moving average.\n            update_after_step (int): The number of steps to wait before starting to update the EMA weights.\n            use_ema_warmup (bool): Whether to use EMA warmup.\n            inv_gamma (float):\n                Inverse multiplicative factor of EMA warmup. Default: 1. Only used if `use_ema_warmup` is True.\n            power (float): Exponential factor of EMA warmup. Default: 2/3. Only used if `use_ema_warmup` is True.\n            device (Optional[Union[str, torch.device]]): The device to store the EMA weights on. If None, the EMA\n                        weights will be stored on CPU.\n\n        @crowsonkb's notes on EMA Warmup:\n            If gamma=1 and power=1, implements a simple average. gamma=1, power=2/3 are good values for models you plan\n            to train for a million or more steps (reaches decay factor 0.999 at 31.6K steps, 0.9999 at 1M steps),\n            gamma=1, power=3/4 for mo",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 50, "column": 4 },
          "end": { "row": 50, "column": 4 }
        }
      }
    }
  ],
  [
    "2037",
    {
      "pageContent": "def save_pretrained(self, path):\n        if self.model_cls is None:\n            raise ValueError(\"`save_pretrained` can only be used if `model_cls` was defined at __init__.\")\n\n        if self.model_config is None:\n            raise ValueError(\"`save_pretrained` can only be used if `model_config` was defined at __init__.\")\n\n        model = self.model_cls.from_config(self.model_config)\n        state_dict = self.state_dict()\n        state_dict.pop(\"shadow_params\", None)\n\n        model.register_to_config(**state_dict)\n        self.copy_to(model.parameters())\n        model.save_pretrained(path)",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "2038",
    {
      "pageContent": "def get_decay(self, optimization_step: int) -> float:\n        \"\"\"\n        Compute the decay factor for the exponential moving average.\n        \"\"\"\n        step = max(0, optimization_step - self.update_after_step - 1)\n\n        if step <= 0:\n            return 0.0\n\n        if self.use_ema_warmup:\n            cur_decay_value = 1 - (1 + step / self.inv_gamma) ** -self.power\n        else:\n            cur_decay_value = (1 + step) / (10 + step)\n\n        cur_decay_value = min(cur_decay_value, self.decay)\n        # make sure decay is not smaller than min_decay\n        cur_decay_value = max(cur_decay_value, self.min_decay)\n        return cur_decay_value",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 156, "column": 4 },
          "end": { "row": 156, "column": 4 }
        }
      }
    }
  ],
  [
    "2039",
    {
      "pageContent": "def copy_to(self, parameters: Iterable[torch.nn.Parameter]) -> None:\n        \"\"\"\n        Copy current averaged parameters into given collection of parameters.\n\n        Args:\n            parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n                updated with the stored moving averages. If `None`, the parameters with which this\n                `ExponentialMovingAverage` was initialized will be used.\n        \"\"\"\n        parameters = list(parameters)\n        for s_param, param in zip(self.shadow_params, parameters):\n            param.data.copy_(s_param.to(param.device).data)",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 205, "column": 4 },
          "end": { "row": 205, "column": 4 }
        }
      }
    }
  ],
  [
    "2040",
    {
      "pageContent": "def to(self, device=None, dtype=None) -> None:\n        r\"\"\"Move internal buffers of the ExponentialMovingAverage to `device`.\n\n        Args:\n            device: like `device` argument to `torch.Tensor.to`\n        \"\"\"\n        # .to() on the tensors handles None correctly\n        self.shadow_params = [\n            p.to(device=device, dtype=dtype) if p.is_floating_point() else p.to(device=device)\n            for p in self.shadow_params\n        ]",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 218, "column": 4 },
          "end": { "row": 218, "column": 4 }
        }
      }
    }
  ],
  [
    "2041",
    {
      "pageContent": "def state_dict(self) -> dict:\n        r\"\"\"\n        Returns the state of the ExponentialMovingAverage as a dict. This method is used by accelerate during\n        checkpointing to save the ema state dict.\n        \"\"\"\n        # Following PyTorch conventions, references to tensors are returned:\n        # \"returns a reference to the state and not its copy!\" -\n        # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n        return {\n            \"decay\": self.decay,\n            \"min_decay\": self.min_decay,\n            \"optimization_step\": self.optimization_step,\n            \"update_after_step\": self.update_after_step,\n            \"use_ema_warmup\": self.use_ema_warmup,\n            \"inv_gamma\": self.inv_gamma,\n            \"power\": self.power,\n            \"shadow_params\": self.shadow_params,\n        }",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 230, "column": 4 },
          "end": { "row": 230, "column": 4 }
        }
      }
    }
  ],
  [
    "2042",
    {
      "pageContent": "def store(self, parameters: Iterable[torch.nn.Parameter]) -> None:\n        r\"\"\"\n        Args:\n        Save the current parameters for restoring later.\n            parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n                temporarily stored.\n        \"\"\"\n        self.temp_stored_params = [param.detach().cpu().clone() for param in parameters]",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 249, "column": 4 },
          "end": { "row": 249, "column": 4 }
        }
      }
    }
  ],
  [
    "2043",
    {
      "pageContent": "def restore(self, parameters: Iterable[torch.nn.Parameter]) -> None:\n        r\"\"\"\n        Args:\n        Restore the parameters stored with the `store` method. Useful to validate the model with EMA parameters without:\n        affecting the original optimization process. Store the parameters before the `copy_to()` method. After\n        validation (or model saving), use this to restore the former parameters.\n            parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n                updated with the stored parameters. If `None`, the parameters with which this\n                `ExponentialMovingAverage` was initialized will be used.\n        \"\"\"\n        if self.temp_stored_params is None:\n            raise RuntimeError(\"This ExponentialMovingAverage has no `store()`ed weights \" \"to `restore()`\")\n        for c_param, param in zip(self.temp_stored_params, parameters):\n            param.data.copy_(c_param.data)\n\n        # Better memory-wise.\n        self.temp_stored_params = None",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 258, "column": 4 },
          "end": { "row": 258, "column": 4 }
        }
      }
    }
  ],
  [
    "2044",
    {
      "pageContent": "def load_state_dict(self, state_dict: dict) -> None:\n        r\"\"\"\n        Args:\n        Loads the ExponentialMovingAverage state. This method is used by accelerate during checkpointing to save the\n        ema state dict.\n            state_dict (dict): EMA state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        # deepcopy, to be consistent with module API\n        state_dict = copy.deepcopy(state_dict)\n\n        self.decay = state_dict.get(\"decay\", self.decay)\n        if self.decay < 0.0 or self.decay > 1.0:\n            raise ValueError(\"Decay must be between 0 and 1\")\n\n        self.min_decay = state_dict.get(\"min_decay\", self.min_decay)\n        if not isinstance(self.min_decay, float):\n            raise ValueError(\"Invalid min_decay\")\n\n        self.optimization_step = state_dict.get(\"optimization_step\", self.optimization_step)\n        if not isinstance(self.optimization_step, int):\n            raise ValueError(\"Invalid optimization_step\")\n\n        self.update_after_step = state_dict.get(\"update_after_step\", self.update_after_step)\n        if not isinstance(self.update_after_step, int):\n            raise ValueError(\"Invalid update_after_step\")\n\n        self.use_ema_warmup = state_dict.get(\"use_ema_warmup\", self.use_ema_warmup)\n        if not isinstance(self.use_ema_warmup, bool):\n            raise ValueError(\"Invalid use_ema_warmup\")\n\n        self.inv_gamma = state_dict.get(\"inv_gamma\", self.inv_gamma)\n        if not isinstance(self.inv_gamma, (float, int)):\n            raise ValueError(\"Invalid inv_gamma\")\n\n        self.power",
      "metadata": {
        "source": "src/diffusers/training_utils.py",
        "range": {
          "start": { "row": 276, "column": 4 },
          "end": { "row": 276, "column": 4 }
        }
      }
    }
  ],
  [
    "2045",
    {
      "pageContent": "def torch_all_close(a, b, *args, **kwargs):\n    if not is_torch_available():\n        raise ValueError(\"PyTorch needs to be installed to use this function.\")\n    if not torch.allclose(a, b, *args, **kwargs):\n        assert False, f\"Max diff is absolute {(a - b).abs().max()}. Diff tensor is {(a - b).abs()}.\"\n    return True",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 51, "column": 0 },
          "end": { "row": 51, "column": 0 }
        }
      }
    }
  ],
  [
    "2046",
    {
      "pageContent": "def print_tensor_test(tensor, filename=\"test_corrections.txt\", expected_tensor_name=\"expected_slice\"):\n    test_name = os.environ.get(\"PYTEST_CURRENT_TEST\")\n    if not torch.is_tensor(tensor):\n        tensor = torch.from_numpy(tensor)\n\n    tensor_str = str(tensor.detach().cpu().flatten().to(torch.float32)).replace(\"\\n\", \"\")\n    # format is usually:\n    # expected_slice = np.array([-0.5713, -0.3018, -0.9814, 0.04663, -0.879, 0.76, -1.734, 0.1044, 1.161])\n    output_str = tensor_str.replace(\"tensor\", f\"{expected_tensor_name} = np.array\")\n    test_file, test_class, test_fn = test_name.split(\"::\")\n    test_fn = test_fn.split()[0]\n    with open(filename, \"a\") as f:\n        print(\";\".join([test_file, test_class, test_fn, output_str]), file=f)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 59, "column": 0 },
          "end": { "row": 59, "column": 0 }
        }
      }
    }
  ],
  [
    "2047",
    {
      "pageContent": "def get_tests_dir(append_path=None):\n    \"\"\"\n    Args:\n        append_path: optional path to append to the tests dir path\n    Return:\n        The full path to the `tests` dir, so that the tests can be invoked from anywhere. Optionally `append_path` is\n        joined after the `tests` dir the former is provided.\n    \"\"\"\n    # this function caller's __file__\n    caller__file__ = inspect.stack()[1][1]\n    tests_dir = os.path.abspath(os.path.dirname(caller__file__))\n\n    while not tests_dir.endswith(\"tests\"):\n        tests_dir = os.path.dirname(tests_dir)\n\n    if append_path:\n        return os.path.join(tests_dir, append_path)\n    else:\n        return tests_dir",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 74, "column": 0 },
          "end": { "row": 74, "column": 0 }
        }
      }
    }
  ],
  [
    "2048",
    {
      "pageContent": "def parse_flag_from_env(key, default=False):\n    try:\n        value = os.environ[key]\n    except KeyError:\n        # KEY isn't set, default to `default`.\n        _value = default\n    else:\n        # KEY is set, convert it to True or False.\n        try:\n            _value = strtobool(value)\n        except ValueError:\n            # More values are supported, but let's keep the message simple.\n            raise ValueError(f\"If set, {key} must be yes or no.\")\n    return _value",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 95, "column": 0 },
          "end": { "row": 95, "column": 0 }
        }
      }
    }
  ],
  [
    "2049",
    {
      "pageContent": "def floats_tensor(shape, scale=1.0, rng=None, name=None):\n    \"\"\"Creates a random float32 tensor\"\"\"\n    if rng is None:\n        rng = global_rng\n\n    total_dims = 1\n    for dim in shape:\n        total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n        values.append(rng.random() * scale)\n\n    return torch.tensor(data=values, dtype=torch.float).view(shape).contiguous()",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 115, "column": 0 },
          "end": { "row": 115, "column": 0 }
        }
      }
    }
  ],
  [
    "2050",
    {
      "pageContent": "def slow(test_case):\n    \"\"\"\n    Decorator marking a test as slow.\n\n    Slow tests are skipped by default. Set the RUN_SLOW environment variable to a truthy value to run them.\n\n    \"\"\"\n    return unittest.skipUnless(_run_slow_tests, \"test is slow\")(test_case)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 131, "column": 0 },
          "end": { "row": 131, "column": 0 }
        }
      }
    }
  ],
  [
    "2051",
    {
      "pageContent": "def nightly(test_case):\n    \"\"\"\n    Decorator marking a test that runs nightly in the diffusers CI.\n\n    Slow tests are skipped by default. Set the RUN_NIGHTLY environment variable to a truthy value to run them.\n\n    \"\"\"\n    return unittest.skipUnless(_run_nightly_tests, \"test is nightly\")(test_case)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 141, "column": 0 },
          "end": { "row": 141, "column": 0 }
        }
      }
    }
  ],
  [
    "2052",
    {
      "pageContent": "def require_torch(test_case):\n    \"\"\"\n    Decorator marking a test that requires PyTorch. These tests are skipped when PyTorch isn't installed.\n    \"\"\"\n    return unittest.skipUnless(is_torch_available(), \"test requires PyTorch\")(test_case)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 151, "column": 0 },
          "end": { "row": 151, "column": 0 }
        }
      }
    }
  ],
  [
    "2053",
    {
      "pageContent": "def require_torch_gpu(test_case):\n    \"\"\"Decorator marking a test that requires CUDA and PyTorch.\"\"\"\n    return unittest.skipUnless(is_torch_available() and torch_device == \"cuda\", \"test requires PyTorch+CUDA\")(\n        test_case\n    )",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 158, "column": 0 },
          "end": { "row": 158, "column": 0 }
        }
      }
    }
  ],
  [
    "2054",
    {
      "pageContent": "def require_flax(test_case):\n    \"\"\"\n    Decorator marking a test that requires JAX & Flax. These tests are skipped when one / both are not installed\n    \"\"\"\n    return unittest.skipUnless(is_flax_available(), \"test requires JAX & Flax\")(test_case)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 170, "column": 0 },
          "end": { "row": 170, "column": 0 }
        }
      }
    }
  ],
  [
    "2055",
    {
      "pageContent": "def require_compel(test_case):\n    \"\"\"\n    Decorator marking a test that requires compel: https://github.com/damian0815/compel. These tests are skipped when\n    the library is not installed.\n    \"\"\"\n    return unittest.skipUnless(is_compel_available(), \"test requires compel\")(test_case)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 177, "column": 0 },
          "end": { "row": 177, "column": 0 }
        }
      }
    }
  ],
  [
    "2056",
    {
      "pageContent": "def require_onnxruntime(test_case):\n    \"\"\"\n    Decorator marking a test that requires onnxruntime. These tests are skipped when onnxruntime isn't installed.\n    \"\"\"\n    return unittest.skipUnless(is_onnx_available(), \"test requires onnxruntime\")(test_case)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 185, "column": 0 },
          "end": { "row": 185, "column": 0 }
        }
      }
    }
  ],
  [
    "2057",
    {
      "pageContent": "def load_numpy(arry: Union[str, np.ndarray], local_path: Optional[str] = None) -> np.ndarray:\n    if isinstance(arry, str):\n        # local_path = \"/home/patrick_huggingface_co/\"\n        if local_path is not None:\n            # local_path can be passed to correct images of tests\n            return os.path.join(local_path, \"/\".join([arry.split(\"/\")[-5], arry.split(\"/\")[-2], arry.split(\"/\")[-1]]))\n        elif arry.startswith(\"http://\") or arry.startswith(\"https://\"):\n            response = requests.get(arry)\n            response.raise_for_status()\n            arry = np.load(BytesIO(response.content))\n        elif os.path.isfile(arry):\n            arry = np.load(arry)\n        else:\n            raise ValueError(\n                f\"Incorrect path or url, URLs must start with `http://` or `https://`, and {arry} is not a valid path\"\n            )\n    elif isinstance(arry, np.ndarray):\n        pass\n    else:\n        raise ValueError(\n            \"Incorrect format used for numpy ndarray. Should be an url linking to an image, a local path, or a\"\n            \" ndarray.\"\n        )\n\n    return arry",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 192, "column": 0 },
          "end": { "row": 192, "column": 0 }
        }
      }
    }
  ],
  [
    "2058",
    {
      "pageContent": "def load_pt(url: str):\n    response = requests.get(url)\n    response.raise_for_status()\n    arry = torch.load(BytesIO(response.content))\n    return arry",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 219, "column": 0 },
          "end": { "row": 219, "column": 0 }
        }
      }
    }
  ],
  [
    "2059",
    {
      "pageContent": "def load_image(image: Union[str, PIL.Image.Image]) -> PIL.Image.Image:\n    \"\"\"\n    Args:\n    Loads `image` to a PIL Image.\n        image (`str` or `PIL.Image.Image`):\n            The image to convert to the PIL Image format.\n    Returns:\n        `PIL.Image.Image`: A PIL Image.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n            image = PIL.Image.open(requests.get(image, stream=True).raw)\n        elif os.path.isfile(image):\n            image = PIL.Image.open(image)\n        else:\n            raise ValueError(\n                f\"Incorrect path or url, URLs must start with `http://` or `https://`, and {image} is not a valid path\"\n            )\n    elif isinstance(image, PIL.Image.Image):\n        image = image\n    else:\n        raise ValueError(\n            \"Incorrect format used for image. Should be an url linking to an image, a local path, or a PIL image.\"\n        )\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 226, "column": 0 },
          "end": { "row": 226, "column": 0 }
        }
      }
    }
  ],
  [
    "2060",
    {
      "pageContent": "def load_hf_numpy(path) -> np.ndarray:\n    if not path.startswith(\"http://\") or path.startswith(\"https://\"):\n        path = os.path.join(\n            \"https://huggingface.co/datasets/fusing/diffusers-testing/resolve/main\", urllib.parse.quote(path)\n        )\n\n    return load_numpy(path)",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 255, "column": 0 },
          "end": { "row": 255, "column": 0 }
        }
      }
    }
  ],
  [
    "2061",
    {
      "pageContent": "def pytest_addoption_shared(parser):\n    \"\"\"\n    This function is to be called from `conftest.py` via `pytest_addoption` wrapper that has to be defined there.\n\n    It allows loading both `conftest.py` files at once without causing a failure due to adding the same `pytest`\n    option.\n\n    \"\"\"\n    option = \"--make-reports\"\n    if option not in pytest_opt_registered:\n        parser.addoption(\n            option,\n            action=\"store\",\n            default=False,\n            help=\"generate report files. The value of this option is used as a prefix to report names\",\n        )\n        pytest_opt_registered[option] = 1",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 270, "column": 0 },
          "end": { "row": 270, "column": 0 }
        }
      }
    }
  ],
  [
    "2062",
    {
      "pageContent": "def pytest_terminal_summary_main(tr, id):\n    \"\"\"\n    Generate multiple reports at the end of test suite run - each report goes into a dedicated file in the current\n    directory. The report files are prefixed with the test suite name.\n\n    This function emulates --duration and -rA pytest arguments.\n\n    This function is to be called from `conftest.py` via `pytest_terminal_summary` wrapper that has to be defined\n    there.\n\n    Args:\n    - tr: `terminalreporter` passed from `conftest.py`\n    - id: unique id like `tests` or `examples` that will be incorporated into the final reports filenames - this is\n      needed as some jobs have multiple runs of pytest, so we can't have them overwrite each other.\n\n    NB: this functions taps into a private _pytest API and while unlikely, it could break should\n    pytest do internal changes - also it calls default internal methods of terminalreporter which\n    can be hijacked by various `pytest-` plugins and interfere.\n\n    \"\"\"\n    from _pytest.config import create_terminal_writer\n\n    if not len(id):\n        id = \"tests\"\n\n    config = tr.config\n    orig_writer = config.get_terminal_writer()\n    orig_tbstyle = config.option.tbstyle\n    orig_reportchars = tr.reportchars\n\n    dir = \"reports\"\n    Path(dir).mkdir(parents=True, exist_ok=True)\n    report_files = {\n        k: f\"{dir}/{id}_{k}.txt\"\n        for k in [\n            \"durations\",\n            \"errors\",\n            \"failures_long\",\n            \"failures_short\",\n            \"failures_line\",\n            \"passes\",\n            \"stats\",\n            \"summary_short\",\n            \"warnings\",\n  ",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 289, "column": 0 },
          "end": { "row": 289, "column": 0 }
        }
      }
    }
  ],
  [
    "2063",
    {
      "pageContent": "def summary_failures_short(tr):\n        # expecting that the reports were --tb=long (default) so we chop them off here to the last frame\n        reports = tr.getreports(\"failed\")\n        if not reports:\n            return\n        tr.write_sep(\"=\", \"FAILURES SHORT STACK\")\n        for rep in reports:\n            msg = tr._getfailureheadline(rep)\n            tr.write_sep(\"_\", msg, red=True, bold=True)\n            # chop off the optional leading extra frames, leaving only the last one\n            longrepr = re.sub(r\".*_ _ _ (_ ){10,}_ _ \", \"\", rep.longreprtext, 0, re.M | re.S)\n            tr._tw.line(longrepr)\n            # note: not printing out any rep.sections to keep the report short",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 355, "column": 4 },
          "end": { "row": 355, "column": 4 }
        }
      }
    }
  ],
  [
    "2064",
    {
      "pageContent": "class CaptureLogger:\n    \"\"\"\n    Args:\n    Context manager to capture `logging` streams\n        logger: 'logging` logger object\n    Returns:\n        The captured output is available via `self.out`\n    Example:\n    ```python\n    >>> from diffusers import logging\n    >>> from diffusers.testing_utils import CaptureLogger\n\n    >>> msg = \"Testing 1, 2, 3\"\n    >>> logging.set_verbosity_info()\n    >>> logger = logging.get_logger(\"diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.py\")\n    >>> with CaptureLogger(logger) as cl:\n    ...     logger.info(msg)\n    >>> assert cl.out, msg + \"\\n\"\n    ```\n    \"\"\"\n\n    def __init__(self, logger):\n        self.logger = logger\n        self.io = StringIO()\n        self.sh = logging.StreamHandler(self.io)\n        self.out = \"\"\n\n    def __enter__(self):\n        self.logger.addHandler(self.sh)\n        return self\n\n    def __exit__(self, *exc):\n        self.logger.removeHandler(self.sh)\n        self.out = self.io.getvalue()\n\n    def __repr__(self):\n        return f\"captured: {self.out}\\n\"",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 418, "column": 0 },
          "end": { "row": 418, "column": 0 }
        }
      }
    }
  ],
  [
    "2065",
    {
      "pageContent": "def __init__(self, logger):\n        self.logger = logger\n        self.io = StringIO()\n        self.sh = logging.StreamHandler(self.io)\n        self.out = \"\"",
      "metadata": {
        "source": "src/diffusers/utils/testing_utils.py",
        "range": {
          "start": { "row": 439, "column": 4 },
          "end": { "row": 439, "column": 4 }
        }
      }
    }
  ],
  [
    "2066",
    {
      "pageContent": "class FlaxModelMixin(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2067",
    {
      "pageContent": "class FlaxUNet2DConditionModel(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2068",
    {
      "pageContent": "class FlaxAutoencoderKL(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "2069",
    {
      "pageContent": "class FlaxDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "2070",
    {
      "pageContent": "class FlaxDDIMScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "2071",
    {
      "pageContent": "class FlaxDDPMScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "2072",
    {
      "pageContent": "class FlaxDPMSolverMultistepScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 94, "column": 0 },
          "end": { "row": 94, "column": 0 }
        }
      }
    }
  ],
  [
    "2073",
    {
      "pageContent": "class FlaxKarrasVeScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 109, "column": 0 },
          "end": { "row": 109, "column": 0 }
        }
      }
    }
  ],
  [
    "2074",
    {
      "pageContent": "class FlaxLMSDiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 124, "column": 0 },
          "end": { "row": 124, "column": 0 }
        }
      }
    }
  ],
  [
    "2075",
    {
      "pageContent": "class FlaxPNDMScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 139, "column": 0 },
          "end": { "row": 139, "column": 0 }
        }
      }
    }
  ],
  [
    "2076",
    {
      "pageContent": "class FlaxSchedulerMixin(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 154, "column": 0 },
          "end": { "row": 154, "column": 0 }
        }
      }
    }
  ],
  [
    "2077",
    {
      "pageContent": "class FlaxScoreSdeVeScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_objects.py",
        "range": {
          "start": { "row": 169, "column": 0 },
          "end": { "row": 169, "column": 0 }
        }
      }
    }
  ],
  [
    "2078",
    {
      "pageContent": "def _get_default_logging_level():\n    \"\"\"\n    If DIFFUSERS_VERBOSITY env var is set to one of the valid choices return that as the new default level. If it is\n    not - fall back to `_default_log_level`\n    \"\"\"\n    env_level_str = os.getenv(\"DIFFUSERS_VERBOSITY\", None)\n    if env_level_str:\n        if env_level_str in log_levels:\n            return log_levels[env_level_str]\n        else:\n            logging.getLogger().warning(\n                f\"Unknown option DIFFUSERS_VERBOSITY={env_level_str}, \"\n                f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n            )\n    return _default_log_level",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 51, "column": 0 },
          "end": { "row": 51, "column": 0 }
        }
      }
    }
  ],
  [
    "2079",
    {
      "pageContent": "def _configure_library_root_logger() -> None:\n    global _default_handler\n\n    with _lock:\n        if _default_handler:\n            # This library has already configured the library root logger.\n            return\n        _default_handler = logging.StreamHandler()  # Set sys.stderr as stream.\n        _default_handler.flush = sys.stderr.flush\n\n        # Apply our default configuration to the library root logger.\n        library_root_logger = _get_library_root_logger()\n        library_root_logger.addHandler(_default_handler)\n        library_root_logger.setLevel(_get_default_logging_level())\n        library_root_logger.propagate = False",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 76, "column": 0 },
          "end": { "row": 76, "column": 0 }
        }
      }
    }
  ],
  [
    "2080",
    {
      "pageContent": "def _reset_library_root_logger() -> None:\n    global _default_handler\n\n    with _lock:\n        if not _default_handler:\n            return\n\n        library_root_logger = _get_library_root_logger()\n        library_root_logger.removeHandler(_default_handler)\n        library_root_logger.setLevel(logging.NOTSET)\n        _default_handler = None",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 93, "column": 0 },
          "end": { "row": 93, "column": 0 }
        }
      }
    }
  ],
  [
    "2081",
    {
      "pageContent": "def get_logger(name: Optional[str] = None) -> logging.Logger:\n    \"\"\"\n    Return a logger with the specified name.\n\n    This function is not supposed to be directly accessed unless you are writing a custom diffusers module.\n    \"\"\"\n\n    if name is None:\n        name = _get_library_name()\n\n    _configure_library_root_logger()\n    return logging.getLogger(name)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 110, "column": 0 },
          "end": { "row": 110, "column": 0 }
        }
      }
    }
  ],
  [
    "2082",
    {
      "pageContent": "def get_verbosity() -> int:\n    \"\"\"\n    Return the current level for the  Diffusers' root logger as an int.\n\n    Returns:\n        `int`: The logging level.\n\n    <Tip>\n\n     Diffusers has following logging levels:\n\n    - 50: `diffusers.logging.CRITICAL` or `diffusers.logging.FATAL`\n    - 40: `diffusers.logging.ERROR`\n    - 30: `diffusers.logging.WARNING` or `diffusers.logging.WARN`\n    - 20: `diffusers.logging.INFO`\n    - 10: `diffusers.logging.DEBUG`\n\n    </Tip>\"\"\"\n\n    _configure_library_root_logger()\n    return _get_library_root_logger().getEffectiveLevel()",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 124, "column": 0 },
          "end": { "row": 124, "column": 0 }
        }
      }
    }
  ],
  [
    "2083",
    {
      "pageContent": "def set_verbosity(verbosity: int) -> None:\n    \"\"\"\n    Set the verbosity level for the  Diffusers' root logger.\n\n    Args:\n        verbosity (`int`):\n            Logging level, e.g., one of:\n\n            - `diffusers.logging.CRITICAL` or `diffusers.logging.FATAL`\n            - `diffusers.logging.ERROR`\n            - `diffusers.logging.WARNING` or `diffusers.logging.WARN`\n            - `diffusers.logging.INFO`\n            - `diffusers.logging.DEBUG`\n    \"\"\"\n\n    _configure_library_root_logger()\n    _get_library_root_logger().setLevel(verbosity)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 147, "column": 0 },
          "end": { "row": 147, "column": 0 }
        }
      }
    }
  ],
  [
    "2084",
    {
      "pageContent": "def disable_default_handler() -> None:\n    \"\"\"Disable the default handler of the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert _default_handler is not None\n    _get_library_root_logger().removeHandler(_default_handler)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 186, "column": 0 },
          "end": { "row": 186, "column": 0 }
        }
      }
    }
  ],
  [
    "2085",
    {
      "pageContent": "def enable_default_handler() -> None:\n    \"\"\"Enable the default handler of the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert _default_handler is not None\n    _get_library_root_logger().addHandler(_default_handler)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 195, "column": 0 },
          "end": { "row": 195, "column": 0 }
        }
      }
    }
  ],
  [
    "2086",
    {
      "pageContent": "def add_handler(handler: logging.Handler) -> None:\n    \"\"\"adds a handler to the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert handler is not None\n    _get_library_root_logger().addHandler(handler)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 204, "column": 0 },
          "end": { "row": 204, "column": 0 }
        }
      }
    }
  ],
  [
    "2087",
    {
      "pageContent": "def remove_handler(handler: logging.Handler) -> None:\n    \"\"\"removes given handler from the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert handler is not None and handler not in _get_library_root_logger().handlers\n    _get_library_root_logger().removeHandler(handler)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 213, "column": 0 },
          "end": { "row": 213, "column": 0 }
        }
      }
    }
  ],
  [
    "2088",
    {
      "pageContent": "def disable_propagation() -> None:\n    \"\"\"\n    Disable propagation of the library log outputs. Note that log propagation is disabled by default.\n    \"\"\"\n\n    _configure_library_root_logger()\n    _get_library_root_logger().propagate = False",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 222, "column": 0 },
          "end": { "row": 222, "column": 0 }
        }
      }
    }
  ],
  [
    "2089",
    {
      "pageContent": "def enable_propagation() -> None:\n    \"\"\"\n    Enable propagation of the library log outputs. Please disable the HuggingFace Diffusers' default handler to prevent\n    double logging if the root logger has been configured.\n    \"\"\"\n\n    _configure_library_root_logger()\n    _get_library_root_logger().propagate = True",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 231, "column": 0 },
          "end": { "row": 231, "column": 0 }
        }
      }
    }
  ],
  [
    "2090",
    {
      "pageContent": "def enable_explicit_format() -> None:\n    \"\"\"\n    Enable explicit formatting for every HuggingFace Diffusers' logger. The explicit formatter is as follows:\n    ```\n        [LEVELNAME|FILENAME|LINE NUMBER] TIME >> MESSAGE\n    ```\n    All handlers currently bound to the root logger are affected by this method.\n    \"\"\"\n    handlers = _get_library_root_logger().handlers\n\n    for handler in handlers:\n        formatter = logging.Formatter(\"[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s\")\n        handler.setFormatter(formatter)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 241, "column": 0 },
          "end": { "row": 241, "column": 0 }
        }
      }
    }
  ],
  [
    "2091",
    {
      "pageContent": "def reset_format() -> None:\n    \"\"\"\n    Resets the formatting for HuggingFace Diffusers' loggers.\n\n    All handlers currently bound to the root logger are affected by this method.\n    \"\"\"\n    handlers = _get_library_root_logger().handlers\n\n    for handler in handlers:\n        handler.setFormatter(None)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 256, "column": 0 },
          "end": { "row": 256, "column": 0 }
        }
      }
    }
  ],
  [
    "2092",
    {
      "pageContent": "def warning_advice(self, *args, **kwargs):\n    \"\"\"\n    This method is identical to `logger.warning()`, but if env var DIFFUSERS_NO_ADVISORY_WARNINGS=1 is set, this\n    warning will not be printed\n    \"\"\"\n    no_advisory_warnings = os.getenv(\"DIFFUSERS_NO_ADVISORY_WARNINGS\", False)\n    if no_advisory_warnings:\n        return\n    self.warning(*args, **kwargs)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 268, "column": 0 },
          "end": { "row": 268, "column": 0 }
        }
      }
    }
  ],
  [
    "2093",
    {
      "pageContent": "class EmptyTqdm:\n    \"\"\"Dummy tqdm which doesn't do anything.\"\"\"\n\n    def __init__(self, *args, **kwargs):  # pylint: disable=unused-argument\n        self._iterator = args[0] if args else None\n\n    def __iter__(self):\n        return iter(self._iterator)\n\n    def __getattr__(self, _):\n        \"\"\"Return empty function.\"\"\"\n\n        def empty_fn(*args, **kwargs):  # pylint: disable=unused-argument\n            return\n\n        return empty_fn\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        return",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 282, "column": 0 },
          "end": { "row": 282, "column": 0 }
        }
      }
    }
  ],
  [
    "2094",
    {
      "pageContent": "def __getattr__(self, _):\n        \"\"\"Return empty function.\"\"\"\n\n        def empty_fn(*args, **kwargs):  # pylint: disable=unused-argument\n            return\n\n        return empty_fn",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 291, "column": 4 },
          "end": { "row": 291, "column": 4 }
        }
      }
    }
  ],
  [
    "2095",
    {
      "pageContent": "class _tqdm_cls:\n    def __call__(self, *args, **kwargs):\n        if _tqdm_active:\n            return tqdm_lib.tqdm(*args, **kwargs)\n        else:\n            return EmptyTqdm(*args, **kwargs)\n\n    def set_lock(self, *args, **kwargs):\n        self._lock = None\n        if _tqdm_active:\n            return tqdm_lib.tqdm.set_lock(*args, **kwargs)\n\n    def get_lock(self):\n        if _tqdm_active:\n            return tqdm_lib.tqdm.get_lock()",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 306, "column": 0 },
          "end": { "row": 306, "column": 0 }
        }
      }
    }
  ],
  [
    "2096",
    {
      "pageContent": "def __call__(self, *args, **kwargs):\n        if _tqdm_active:\n            return tqdm_lib.tqdm(*args, **kwargs)\n        else:\n            return EmptyTqdm(*args, **kwargs)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 307, "column": 4 },
          "end": { "row": 307, "column": 4 }
        }
      }
    }
  ],
  [
    "2097",
    {
      "pageContent": "def set_lock(self, *args, **kwargs):\n        self._lock = None\n        if _tqdm_active:\n            return tqdm_lib.tqdm.set_lock(*args, **kwargs)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 313, "column": 4 },
          "end": { "row": 313, "column": 4 }
        }
      }
    }
  ],
  [
    "2098",
    {
      "pageContent": "def is_progress_bar_enabled() -> bool:\n    \"\"\"Return a boolean indicating whether tqdm progress bars are enabled.\"\"\"\n    global _tqdm_active\n    return bool(_tqdm_active)",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 326, "column": 0 },
          "end": { "row": 326, "column": 0 }
        }
      }
    }
  ],
  [
    "2099",
    {
      "pageContent": "def enable_progress_bar():\n    \"\"\"Enable tqdm progress bar.\"\"\"\n    global _tqdm_active\n    _tqdm_active = True",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 332, "column": 0 },
          "end": { "row": 332, "column": 0 }
        }
      }
    }
  ],
  [
    "2100",
    {
      "pageContent": "def disable_progress_bar():\n    \"\"\"Disable tqdm progress bar.\"\"\"\n    global _tqdm_active\n    _tqdm_active = False",
      "metadata": {
        "source": "src/diffusers/utils/logging.py",
        "range": {
          "start": { "row": 338, "column": 0 },
          "end": { "row": 338, "column": 0 }
        }
      }
    }
  ],
  [
    "2101",
    {
      "pageContent": "class AltDiffusionImg2ImgPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2102",
    {
      "pageContent": "class AltDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2103",
    {
      "pageContent": "class CycleDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "2104",
    {
      "pageContent": "class LDMTextToImagePipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "2105",
    {
      "pageContent": "class PaintByExamplePipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "2106",
    {
      "pageContent": "class SemanticStableDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "2107",
    {
      "pageContent": "class StableDiffusionAttendAndExcitePipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 94, "column": 0 },
          "end": { "row": 94, "column": 0 }
        }
      }
    }
  ],
  [
    "2108",
    {
      "pageContent": "class StableDiffusionControlNetPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 109, "column": 0 },
          "end": { "row": 109, "column": 0 }
        }
      }
    }
  ],
  [
    "2109",
    {
      "pageContent": "class StableDiffusionDepth2ImgPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 124, "column": 0 },
          "end": { "row": 124, "column": 0 }
        }
      }
    }
  ],
  [
    "2110",
    {
      "pageContent": "class StableDiffusionImageVariationPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 139, "column": 0 },
          "end": { "row": 139, "column": 0 }
        }
      }
    }
  ],
  [
    "2111",
    {
      "pageContent": "class StableDiffusionImg2ImgPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 154, "column": 0 },
          "end": { "row": 154, "column": 0 }
        }
      }
    }
  ],
  [
    "2112",
    {
      "pageContent": "class StableDiffusionInpaintPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 169, "column": 0 },
          "end": { "row": 169, "column": 0 }
        }
      }
    }
  ],
  [
    "2113",
    {
      "pageContent": "class StableDiffusionInpaintPipelineLegacy(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 184, "column": 0 },
          "end": { "row": 184, "column": 0 }
        }
      }
    }
  ],
  [
    "2114",
    {
      "pageContent": "class StableDiffusionInstructPix2PixPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 199, "column": 0 },
          "end": { "row": 199, "column": 0 }
        }
      }
    }
  ],
  [
    "2115",
    {
      "pageContent": "class StableDiffusionLatentUpscalePipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 214, "column": 0 },
          "end": { "row": 214, "column": 0 }
        }
      }
    }
  ],
  [
    "2116",
    {
      "pageContent": "class StableDiffusionPanoramaPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 229, "column": 0 },
          "end": { "row": 229, "column": 0 }
        }
      }
    }
  ],
  [
    "2117",
    {
      "pageContent": "class StableDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 244, "column": 0 },
          "end": { "row": 244, "column": 0 }
        }
      }
    }
  ],
  [
    "2118",
    {
      "pageContent": "class StableDiffusionPipelineSafe(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 259, "column": 0 },
          "end": { "row": 259, "column": 0 }
        }
      }
    }
  ],
  [
    "2119",
    {
      "pageContent": "class StableDiffusionPix2PixZeroPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 274, "column": 0 },
          "end": { "row": 274, "column": 0 }
        }
      }
    }
  ],
  [
    "2120",
    {
      "pageContent": "class StableDiffusionSAGPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 289, "column": 0 },
          "end": { "row": 289, "column": 0 }
        }
      }
    }
  ],
  [
    "2121",
    {
      "pageContent": "class StableDiffusionUpscalePipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 304, "column": 0 },
          "end": { "row": 304, "column": 0 }
        }
      }
    }
  ],
  [
    "2122",
    {
      "pageContent": "class StableUnCLIPImg2ImgPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 319, "column": 0 },
          "end": { "row": 319, "column": 0 }
        }
      }
    }
  ],
  [
    "2123",
    {
      "pageContent": "class StableUnCLIPPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 334, "column": 0 },
          "end": { "row": 334, "column": 0 }
        }
      }
    }
  ],
  [
    "2124",
    {
      "pageContent": "class UnCLIPImageVariationPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 349, "column": 0 },
          "end": { "row": 349, "column": 0 }
        }
      }
    }
  ],
  [
    "2125",
    {
      "pageContent": "class UnCLIPPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 364, "column": 0 },
          "end": { "row": 364, "column": 0 }
        }
      }
    }
  ],
  [
    "2126",
    {
      "pageContent": "class VersatileDiffusionDualGuidedPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 379, "column": 0 },
          "end": { "row": 379, "column": 0 }
        }
      }
    }
  ],
  [
    "2127",
    {
      "pageContent": "class VersatileDiffusionImageVariationPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 394, "column": 0 },
          "end": { "row": 394, "column": 0 }
        }
      }
    }
  ],
  [
    "2128",
    {
      "pageContent": "class VersatileDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 409, "column": 0 },
          "end": { "row": 409, "column": 0 }
        }
      }
    }
  ],
  [
    "2129",
    {
      "pageContent": "class VersatileDiffusionTextToImagePipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 424, "column": 0 },
          "end": { "row": 424, "column": 0 }
        }
      }
    }
  ],
  [
    "2130",
    {
      "pageContent": "class VQDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "range": {
          "start": { "row": 439, "column": 0 },
          "end": { "row": 439, "column": 0 }
        }
      }
    }
  ],
  [
    "2131",
    {
      "pageContent": "def deprecate(*args, take_from: Optional[Union[Dict, Any]] = None, standard_warn=True):\n    from .. import __version__\n\n    deprecated_kwargs = take_from\n    values = ()\n    if not isinstance(args[0], tuple):\n        args = (args,)\n\n    for attribute, version_name, message in args:\n        if version.parse(version.parse(__version__).base_version) >= version.parse(version_name):\n            raise ValueError(\n                f\"The deprecation tuple {(attribute, version_name, message)} should be removed since diffusers'\"\n                f\" version {__version__} is >= {version_name}\"\n            )\n\n        warning = None\n        if isinstance(deprecated_kwargs, dict) and attribute in deprecated_kwargs:\n            values += (deprecated_kwargs.pop(attribute),)\n            warning = f\"The `{attribute}` argument is deprecated and will be removed in version {version_name}.\"\n        elif hasattr(deprecated_kwargs, attribute):\n            values += (getattr(deprecated_kwargs, attribute),)\n            warning = f\"The `{attribute}` attribute is deprecated and will be removed in version {version_name}.\"\n        elif deprecated_kwargs is None:\n            warning = f\"`{attribute}` is deprecated and will be removed in version {version_name}.\"\n\n        if warning is not None:\n            warning = warning + \" \" if standard_warn else \"\"\n            warnings.warn(warning + message, FutureWarning, stacklevel=2)\n\n    if isinstance(deprecated_kwargs, dict) and len(deprecated_kwargs) > 0:\n        call_frame = inspect.getouterframes(inspect.currentframe())[1]\n        filename = call_frame.filenam",
      "metadata": {
        "source": "src/diffusers/utils/deprecation_utils.py",
        "range": {
          "start": { "row": 7, "column": 0 },
          "end": { "row": 7, "column": 0 }
        }
      }
    }
  ],
  [
    "2132",
    {
      "pageContent": "class StableDiffusionKDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\", \"k_diffusion\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\", \"k_diffusion\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"k_diffusion\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"k_diffusion\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2133",
    {
      "pageContent": "class FlaxStableDiffusionImg2ImgPipeline(metaclass=DummyObject):\n    _backends = [\"flax\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_and_transformers_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2134",
    {
      "pageContent": "class FlaxStableDiffusionInpaintPipeline(metaclass=DummyObject):\n    _backends = [\"flax\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_and_transformers_objects.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2135",
    {
      "pageContent": "class FlaxStableDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"flax\", \"transformers\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\", \"transformers\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\", \"transformers\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\", \"transformers\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_flax_and_transformers_objects.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "2136",
    {
      "pageContent": "def is_tensor(x):\n    \"\"\"\n    Tests if `x` is a `torch.Tensor` or `np.ndarray`.\n    \"\"\"\n    if is_torch_available():\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n\n    return isinstance(x, np.ndarray)",
      "metadata": {
        "source": "src/diffusers/utils/outputs.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "2137",
    {
      "pageContent": "class BaseOutput(OrderedDict):\n    \"\"\"\n    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a\n    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular\n    python dictionary.\n\n    <Tip warning={true}>\n\n    You can't unpack a `BaseOutput` directly. Use the [`~utils.BaseOutput.to_tuple`] method to convert it to a tuple\n    before.\n\n    </Tip>\n    \"\"\"\n\n    def __post_init__(self):\n        class_fields = fields(self)\n\n        # Safety and consistency checks\n        if not len(class_fields):\n            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n\n        first_field = getattr(self, class_fields[0].name)\n        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n\n        if other_fields_are_none and isinstance(first_field, dict):\n            for key, value in first_field.items():\n                self[key] = value\n        else:\n            for field in class_fields:\n                v = getattr(self, field.name)\n                if v is not None:\n                    self[field.name] = v\n\n    def __delitem__(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n\n    def setdefault(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n\n    def pop(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n\n    de",
      "metadata": {
        "source": "src/diffusers/utils/outputs.py",
        "range": {
          "start": { "row": 39, "column": 0 },
          "end": { "row": 39, "column": 0 }
        }
      }
    }
  ],
  [
    "2138",
    {
      "pageContent": "def __post_init__(self):\n        class_fields = fields(self)\n\n        # Safety and consistency checks\n        if not len(class_fields):\n            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n\n        first_field = getattr(self, class_fields[0].name)\n        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n\n        if other_fields_are_none and isinstance(first_field, dict):\n            for key, value in first_field.items():\n                self[key] = value\n        else:\n            for field in class_fields:\n                v = getattr(self, field.name)\n                if v is not None:\n                    self[field.name] = v",
      "metadata": {
        "source": "src/diffusers/utils/outputs.py",
        "range": {
          "start": { "row": 53, "column": 4 },
          "end": { "row": 53, "column": 4 }
        }
      }
    }
  ],
  [
    "2139",
    {
      "pageContent": "def __getitem__(self, k):\n        if isinstance(k, str):\n            inner_dict = {k: v for (k, v) in self.items()}\n            return inner_dict[k]\n        else:\n            return self.to_tuple()[k]",
      "metadata": {
        "source": "src/diffusers/utils/outputs.py",
        "range": {
          "start": { "row": 84, "column": 4 },
          "end": { "row": 84, "column": 4 }
        }
      }
    }
  ],
  [
    "2140",
    {
      "pageContent": "def __setattr__(self, name, value):\n        if name in self.keys() and value is not None:\n            # Don't call self.__setitem__ to avoid recursion errors\n            super().__setitem__(name, value)\n        super().__setattr__(name, value)",
      "metadata": {
        "source": "src/diffusers/utils/outputs.py",
        "range": {
          "start": { "row": 91, "column": 4 },
          "end": { "row": 91, "column": 4 }
        }
      }
    }
  ],
  [
    "2141",
    {
      "pageContent": "def __setitem__(self, key, value):\n        # Will raise a KeyException if needed\n        super().__setitem__(key, value)\n        # Don't call self.__setattr__ to avoid recursion errors\n        super().__setattr__(key, value)",
      "metadata": {
        "source": "src/diffusers/utils/outputs.py",
        "range": {
          "start": { "row": 97, "column": 4 },
          "end": { "row": 97, "column": 4 }
        }
      }
    }
  ],
  [
    "2142",
    {
      "pageContent": "def to_tuple(self) -> Tuple[Any]:\n        \"\"\"\n        Convert self to a tuple containing all the attributes/keys that are not `None`.\n        \"\"\"\n        return tuple(self[k] for k in self.keys())",
      "metadata": {
        "source": "src/diffusers/utils/outputs.py",
        "range": {
          "start": { "row": 103, "column": 4 },
          "end": { "row": 103, "column": 4 }
        }
      }
    }
  ],
  [
    "2143",
    {
      "pageContent": "def apply_forward_hook(method):\n    \"\"\"\n    Decorator that applies a registered CpuOffload hook to an arbitrary function rather than `forward`. This is useful\n    for cases where a PyTorch module provides functions other than `forward` that should trigger a move to the\n    appropriate acceleration device. This is the case for `encode` and `decode` in [`AutoencoderKL`].\n\n    This decorator looks inside the internal `_hf_hook` property to find a registered offload hook.\n\n    :param method: The method to decorate. This method should be a method of a PyTorch module.\n    \"\"\"\n    if not is_accelerate_available():\n        return method\n    accelerate_version = version.parse(accelerate.__version__).base_version\n    if version.parse(accelerate_version) < version.parse(\"0.17.0\"):\n        return method\n\n    def wrapper(self, *args, **kwargs):\n        if hasattr(self, \"_hf_hook\") and hasattr(self._hf_hook, \"pre_forward\"):\n            self._hf_hook.pre_forward(self)\n        return method(self, *args, **kwargs)\n\n    return wrapper",
      "metadata": {
        "source": "src/diffusers/utils/accelerate_utils.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "2144",
    {
      "pageContent": "def wrapper(self, *args, **kwargs):\n        if hasattr(self, \"_hf_hook\") and hasattr(self._hf_hook, \"pre_forward\"):\n            self._hf_hook.pre_forward(self)\n        return method(self, *args, **kwargs)",
      "metadata": {
        "source": "src/diffusers/utils/accelerate_utils.py",
        "range": {
          "start": { "row": 42, "column": 4 },
          "end": { "row": 42, "column": 4 }
        }
      }
    }
  ],
  [
    "2145",
    {
      "pageContent": "class AutoencoderKL(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2146",
    {
      "pageContent": "class ControlNetModel(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2147",
    {
      "pageContent": "class ModelMixin(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "2148",
    {
      "pageContent": "class PriorTransformer(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "2149",
    {
      "pageContent": "class Transformer2DModel(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "2150",
    {
      "pageContent": "class UNet1DModel(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "2151",
    {
      "pageContent": "class UNet2DConditionModel(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 94, "column": 0 },
          "end": { "row": 94, "column": 0 }
        }
      }
    }
  ],
  [
    "2152",
    {
      "pageContent": "class UNet2DModel(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 109, "column": 0 },
          "end": { "row": 109, "column": 0 }
        }
      }
    }
  ],
  [
    "2153",
    {
      "pageContent": "class VQModel(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 124, "column": 0 },
          "end": { "row": 124, "column": 0 }
        }
      }
    }
  ],
  [
    "2154",
    {
      "pageContent": "class AudioPipelineOutput(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 167, "column": 0 },
          "end": { "row": 167, "column": 0 }
        }
      }
    }
  ],
  [
    "2155",
    {
      "pageContent": "class DanceDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 182, "column": 0 },
          "end": { "row": 182, "column": 0 }
        }
      }
    }
  ],
  [
    "2156",
    {
      "pageContent": "class DDIMPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 197, "column": 0 },
          "end": { "row": 197, "column": 0 }
        }
      }
    }
  ],
  [
    "2157",
    {
      "pageContent": "class DDPMPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 212, "column": 0 },
          "end": { "row": 212, "column": 0 }
        }
      }
    }
  ],
  [
    "2158",
    {
      "pageContent": "class DiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 227, "column": 0 },
          "end": { "row": 227, "column": 0 }
        }
      }
    }
  ],
  [
    "2159",
    {
      "pageContent": "class DiTPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 242, "column": 0 },
          "end": { "row": 242, "column": 0 }
        }
      }
    }
  ],
  [
    "2160",
    {
      "pageContent": "class ImagePipelineOutput(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 257, "column": 0 },
          "end": { "row": 257, "column": 0 }
        }
      }
    }
  ],
  [
    "2161",
    {
      "pageContent": "class KarrasVePipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 272, "column": 0 },
          "end": { "row": 272, "column": 0 }
        }
      }
    }
  ],
  [
    "2162",
    {
      "pageContent": "class LDMPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 287, "column": 0 },
          "end": { "row": 287, "column": 0 }
        }
      }
    }
  ],
  [
    "2163",
    {
      "pageContent": "class LDMSuperResolutionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 302, "column": 0 },
          "end": { "row": 302, "column": 0 }
        }
      }
    }
  ],
  [
    "2164",
    {
      "pageContent": "class PNDMPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 317, "column": 0 },
          "end": { "row": 317, "column": 0 }
        }
      }
    }
  ],
  [
    "2165",
    {
      "pageContent": "class RePaintPipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 332, "column": 0 },
          "end": { "row": 332, "column": 0 }
        }
      }
    }
  ],
  [
    "2166",
    {
      "pageContent": "class ScoreSdeVePipeline(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 347, "column": 0 },
          "end": { "row": 347, "column": 0 }
        }
      }
    }
  ],
  [
    "2167",
    {
      "pageContent": "class DDIMInverseScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 362, "column": 0 },
          "end": { "row": 362, "column": 0 }
        }
      }
    }
  ],
  [
    "2168",
    {
      "pageContent": "class DDIMScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 377, "column": 0 },
          "end": { "row": 377, "column": 0 }
        }
      }
    }
  ],
  [
    "2169",
    {
      "pageContent": "class DDPMScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 392, "column": 0 },
          "end": { "row": 392, "column": 0 }
        }
      }
    }
  ],
  [
    "2170",
    {
      "pageContent": "class DEISMultistepScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 407, "column": 0 },
          "end": { "row": 407, "column": 0 }
        }
      }
    }
  ],
  [
    "2171",
    {
      "pageContent": "class DPMSolverMultistepScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 422, "column": 0 },
          "end": { "row": 422, "column": 0 }
        }
      }
    }
  ],
  [
    "2172",
    {
      "pageContent": "class DPMSolverSinglestepScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 437, "column": 0 },
          "end": { "row": 437, "column": 0 }
        }
      }
    }
  ],
  [
    "2173",
    {
      "pageContent": "class EulerAncestralDiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 452, "column": 0 },
          "end": { "row": 452, "column": 0 }
        }
      }
    }
  ],
  [
    "2174",
    {
      "pageContent": "class EulerDiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 467, "column": 0 },
          "end": { "row": 467, "column": 0 }
        }
      }
    }
  ],
  [
    "2175",
    {
      "pageContent": "class HeunDiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 482, "column": 0 },
          "end": { "row": 482, "column": 0 }
        }
      }
    }
  ],
  [
    "2176",
    {
      "pageContent": "class IPNDMScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 497, "column": 0 },
          "end": { "row": 497, "column": 0 }
        }
      }
    }
  ],
  [
    "2177",
    {
      "pageContent": "class KarrasVeScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 512, "column": 0 },
          "end": { "row": 512, "column": 0 }
        }
      }
    }
  ],
  [
    "2178",
    {
      "pageContent": "class KDPM2AncestralDiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 527, "column": 0 },
          "end": { "row": 527, "column": 0 }
        }
      }
    }
  ],
  [
    "2179",
    {
      "pageContent": "class KDPM2DiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 542, "column": 0 },
          "end": { "row": 542, "column": 0 }
        }
      }
    }
  ],
  [
    "2180",
    {
      "pageContent": "class PNDMScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 557, "column": 0 },
          "end": { "row": 557, "column": 0 }
        }
      }
    }
  ],
  [
    "2181",
    {
      "pageContent": "class RePaintScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 572, "column": 0 },
          "end": { "row": 572, "column": 0 }
        }
      }
    }
  ],
  [
    "2182",
    {
      "pageContent": "class SchedulerMixin(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 587, "column": 0 },
          "end": { "row": 587, "column": 0 }
        }
      }
    }
  ],
  [
    "2183",
    {
      "pageContent": "class ScoreSdeVeScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 602, "column": 0 },
          "end": { "row": 602, "column": 0 }
        }
      }
    }
  ],
  [
    "2184",
    {
      "pageContent": "class UnCLIPScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 617, "column": 0 },
          "end": { "row": 617, "column": 0 }
        }
      }
    }
  ],
  [
    "2185",
    {
      "pageContent": "class UniPCMultistepScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 632, "column": 0 },
          "end": { "row": 632, "column": 0 }
        }
      }
    }
  ],
  [
    "2186",
    {
      "pageContent": "class VQDiffusionScheduler(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 647, "column": 0 },
          "end": { "row": 647, "column": 0 }
        }
      }
    }
  ],
  [
    "2187",
    {
      "pageContent": "class EMAModel(metaclass=DummyObject):\n    _backends = [\"torch\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_pt_objects.py",
        "range": {
          "start": { "row": 662, "column": 0 },
          "end": { "row": 662, "column": 0 }
        }
      }
    }
  ],
  [
    "2188",
    {
      "pageContent": "def get_diffusers_versions():\n    url = \"https://pypi.org/pypi/diffusers/json\"\n    releases = json.loads(request.urlopen(url).read())[\"releases\"].keys()\n    return sorted(releases, key=StrictVersion)",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "2189",
    {
      "pageContent": "def init_hf_modules():\n    \"\"\"\n    Creates the cache directory for modules with an init, and adds it to the Python path.\n    \"\"\"\n    # This function has already been executed if HF_MODULES_CACHE already is in the Python path.\n    if HF_MODULES_CACHE in sys.path:\n        return\n\n    sys.path.append(HF_MODULES_CACHE)\n    os.makedirs(HF_MODULES_CACHE, exist_ok=True)\n    init_path = Path(HF_MODULES_CACHE) / \"__init__.py\"\n    if not init_path.exists():\n        init_path.touch()",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "2190",
    {
      "pageContent": "def create_dynamic_module(name: Union[str, os.PathLike]):\n    \"\"\"\n    Creates a dynamic module in the cache directory for modules.\n    \"\"\"\n    init_hf_modules()\n    dynamic_module_path = Path(HF_MODULES_CACHE) / name\n    # If the parent module does not exist yet, recursively create it.\n    if not dynamic_module_path.parent.exists():\n        create_dynamic_module(dynamic_module_path.parent)\n    os.makedirs(dynamic_module_path, exist_ok=True)\n    init_path = dynamic_module_path / \"__init__.py\"\n    if not init_path.exists():\n        init_path.touch()",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 63, "column": 0 },
          "end": { "row": 63, "column": 0 }
        }
      }
    }
  ],
  [
    "2191",
    {
      "pageContent": "def get_relative_imports(module_file):\n    \"\"\"\n    Get the list of modules that are relatively imported in a module file.\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n    \"\"\"\n    with open(module_file, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n\n    # Imports of the form `import .xxx`\n    relative_imports = re.findall(\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n    # Imports of the form `from .xxx import yyy`\n    relative_imports += re.findall(\"^\\s*from\\s+\\.(\\S+)\\s+import\", content, flags=re.MULTILINE)\n    # Unique-ify\n    return list(set(relative_imports))",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 78, "column": 0 },
          "end": { "row": 78, "column": 0 }
        }
      }
    }
  ],
  [
    "2192",
    {
      "pageContent": "def get_relative_import_files(module_file):\n    \"\"\"\n    Get the list of all files that are needed for a given module. Note that this function recurses through the relative\n    imports (if a imports b and b imports c, it will return module files for b and c).\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n    \"\"\"\n    no_change = False\n    files_to_check = [module_file]\n    all_relative_imports = []\n\n    # Let's recurse through all relative imports\n    while not no_change:\n        new_imports = []\n        for f in files_to_check:\n            new_imports.extend(get_relative_imports(f))\n\n        module_path = Path(module_file).parent\n        new_import_files = [str(module_path / m) for m in new_imports]\n        new_import_files = [f for f in new_import_files if f not in all_relative_imports]\n        files_to_check = [f\"{f}.py\" for f in new_import_files]\n\n        no_change = len(new_import_files) == 0\n        all_relative_imports.extend(files_to_check)\n\n    return all_relative_imports",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 96, "column": 0 },
          "end": { "row": 96, "column": 0 }
        }
      }
    }
  ],
  [
    "2193",
    {
      "pageContent": "def check_imports(filename):\n    \"\"\"\n    Check if the current Python environment contains all the libraries that are imported in a file.\n    \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n\n    # Imports of the form `import xxx`\n    imports = re.findall(\"^\\s*import\\s+(\\S+)\\s*$\", content, flags=re.MULTILINE)\n    # Imports of the form `from xxx import yyy`\n    imports += re.findall(\"^\\s*from\\s+(\\S+)\\s+import\", content, flags=re.MULTILINE)\n    # Only keep the top-level module\n    imports = [imp.split(\".\")[0] for imp in imports if not imp.startswith(\".\")]\n\n    # Unique-ify and test we got them all\n    imports = list(set(imports))\n    missing_packages = []\n    for imp in imports:\n        try:\n            importlib.import_module(imp)\n        except ImportError:\n            missing_packages.append(imp)\n\n    if len(missing_packages) > 0:\n        raise ImportError(\n            \"This modeling file requires the following packages that were not found in your environment: \"\n            f\"{', '.join(missing_packages)}. Run `pip install {' '.join(missing_packages)}`\"\n        )\n\n    return get_relative_imports(filename)",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 125, "column": 0 },
          "end": { "row": 125, "column": 0 }
        }
      }
    }
  ],
  [
    "2194",
    {
      "pageContent": "def get_class_in_module(class_name, module_path):\n    \"\"\"\n    Import a module on the cache directory for modules and extract a class from it.\n    \"\"\"\n    module_path = module_path.replace(os.path.sep, \".\")\n    module = importlib.import_module(module_path)\n\n    if class_name is None:\n        return find_pipeline_class(module)\n    return getattr(module, class_name)",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 157, "column": 0 },
          "end": { "row": 157, "column": 0 }
        }
      }
    }
  ],
  [
    "2195",
    {
      "pageContent": "def find_pipeline_class(loaded_module):\n    \"\"\"\n    Retrieve pipeline class that inherits from `DiffusionPipeline`. Note that there has to be exactly one class\n    inheriting from `DiffusionPipeline`.\n    \"\"\"\n    from ..pipelines import DiffusionPipeline\n\n    cls_members = dict(inspect.getmembers(loaded_module, inspect.isclass))\n\n    pipeline_class = None\n    for cls_name, cls in cls_members.items():\n        if (\n            cls_name != DiffusionPipeline.__name__\n            and issubclass(cls, DiffusionPipeline)\n            and cls.__module__.split(\".\")[0] != \"diffusers\"\n        ):\n            if pipeline_class is not None:\n                raise ValueError(\n                    f\"Multiple classes that inherit from {DiffusionPipeline.__name__} have been found:\"\n                    f\" {pipeline_class.__name__}, and {cls_name}. Please make sure to define only one in\"\n                    f\" {loaded_module}.\"\n                )\n            pipeline_class = cls\n\n    return pipeline_class",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 169, "column": 0 },
          "end": { "row": 169, "column": 0 }
        }
      }
    }
  ],
  [
    "2196",
    {
      "pageContent": "def get_cached_module_file(\n    pretrained_model_name_or_path: Union[str, os.PathLike],\n    module_file: str,\n    cache_dir: Optional[Union[str, os.PathLike]] = None,\n    force_download: bool = False,\n    resume_download: bool = False,\n    proxies: Optional[Dict[str, str]] = None,\n    use_auth_token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    local_files_only: bool = False,\n):\n    \"\"\"\n    Prepares Downloads a module from a local folder or a distant repo and returns its path inside the cached\n    Transformers module.\n\n    Args:\n        pretrained_model_name_or_path (`str` or `os.PathLike`):\n            This can be either:\n\n            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n              huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced\n              under a user or organization name, like `dbmdz/bert-base-german-cased`.\n            - a path to a *directory* containing a configuration file saved using the\n              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n\n        module_file (`str`):\n            The name of the module file containing the class to look for.\n        cache_dir (`str` or `os.PathLike`, *optional*):\n            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n            cache should not be used.\n        force_download (`bool`, *optional*, defaults to `False`):\n            Whether or not to force to (re-)download the configur",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 196, "column": 0 },
          "end": { "row": 196, "column": 0 }
        }
      }
    }
  ],
  [
    "2197",
    {
      "pageContent": "def get_class_from_dynamic_module(\n    pretrained_model_name_or_path: Union[str, os.PathLike],\n    module_file: str,\n    class_name: Optional[str] = None,\n    cache_dir: Optional[Union[str, os.PathLike]] = None,\n    force_download: bool = False,\n    resume_download: bool = False,\n    proxies: Optional[Dict[str, str]] = None,\n    use_auth_token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    local_files_only: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Extracts a class from a module file, present in the local folder or repository of a model.\n\n    <Tip warning={true}>\n\n    Calling this function will execute the code in the module file found locally or downloaded from the Hub. It should\n    therefore only be called on trusted repos.\n\n    </Tip>\n\n    Args:\n        pretrained_model_name_or_path (`str` or `os.PathLike`):\n            This can be either:\n\n            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n              huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced\n              under a user or organization name, like `dbmdz/bert-base-german-cased`.\n            - a path to a *directory* containing a configuration file saved using the\n              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n\n        module_file (`str`):\n            The name of the module file containing the class to look for.\n        class_name (`str`):\n            The name of the class to import in the module.\n        cache_dir (`str` or `os",
      "metadata": {
        "source": "src/diffusers/utils/dynamic_modules_utils.py",
        "range": {
          "start": { "row": 368, "column": 0 },
          "end": { "row": 368, "column": 0 }
        }
      }
    }
  ],
  [
    "2198",
    {
      "pageContent": "def replace_example_docstring(example_docstring):\n    def docstring_decorator(fn):\n        func_doc = fn.__doc__\n        lines = func_doc.split(\"\\n\")\n        i = 0\n        while i < len(lines) and re.search(r\"^\\s*Examples?:\\s*$\", lines[i]) is None:\n            i += 1\n        if i < len(lines):\n            lines[i] = example_docstring\n            func_doc = \"\\n\".join(lines)\n        else:\n            raise ValueError(\n                f\"The function {fn} should have an empty 'Examples:' in its docstring as placeholder, \"\n                f\"current docstring is:\\n{func_doc}\"\n            )\n        fn.__doc__ = func_doc\n        return fn\n\n    return docstring_decorator",
      "metadata": {
        "source": "src/diffusers/utils/doc_utils.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2199",
    {
      "pageContent": "def docstring_decorator(fn):\n        func_doc = fn.__doc__\n        lines = func_doc.split(\"\\n\")\n        i = 0\n        while i < len(lines) and re.search(r\"^\\s*Examples?:\\s*$\", lines[i]) is None:\n            i += 1\n        if i < len(lines):\n            lines[i] = example_docstring\n            func_doc = \"\\n\".join(lines)\n        else:\n            raise ValueError(\n                f\"The function {fn} should have an empty 'Examples:' in its docstring as placeholder, \"\n                f\"current docstring is:\\n{func_doc}\"\n            )\n        fn.__doc__ = func_doc\n        return fn",
      "metadata": {
        "source": "src/diffusers/utils/doc_utils.py",
        "range": {
          "start": { "row": 20, "column": 4 },
          "end": { "row": 20, "column": 4 }
        }
      }
    }
  ],
  [
    "2200",
    {
      "pageContent": "def http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:\n    \"\"\"\n    Formats a user-agent string with basic info about a request.\n    \"\"\"\n    ua = f\"diffusers/{__version__}; python/{sys.version.split()[0]}; session_id/{SESSION_ID}\"\n    if DISABLE_TELEMETRY or HF_HUB_OFFLINE:\n        return ua + \"; telemetry/off\"\n    if is_torch_available():\n        ua += f\"; torch/{_torch_version}\"\n    if is_flax_available():\n        ua += f\"; jax/{_jax_version}\"\n        ua += f\"; flax/{_flax_version}\"\n    if is_onnx_available():\n        ua += f\"; onnxruntime/{_onnxruntime_version}\"\n    # CI will set this value to True\n    if os.environ.get(\"DIFFUSERS_IS_CI\", \"\").upper() in ENV_VARS_TRUE_VALUES:\n        ua += \"; is_ci/true\"\n    if isinstance(user_agent, dict):\n        ua += \"; \" + \"; \".join(f\"{k}/{v}\" for k, v in user_agent.items())\n    elif isinstance(user_agent, str):\n        ua += \"; \" + user_agent\n    return ua",
      "metadata": {
        "source": "src/diffusers/utils/hub_utils.py",
        "range": {
          "start": { "row": 53, "column": 0 },
          "end": { "row": 53, "column": 0 }
        }
      }
    }
  ],
  [
    "2201",
    {
      "pageContent": "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"",
      "metadata": {
        "source": "src/diffusers/utils/hub_utils.py",
        "range": {
          "start": { "row": 77, "column": 0 },
          "end": { "row": 77, "column": 0 }
        }
      }
    }
  ],
  [
    "2202",
    {
      "pageContent": "def create_model_card(args, model_name):\n    if not is_jinja_available():\n        raise ValueError(\n            \"Modelcard rendering is based on Jinja templates.\"\n            \" Please make sure to have `jinja` installed before using `create_model_card`.\"\n            \" To install it, please run `pip install Jinja2`.\"\n        )\n\n    if hasattr(args, \"local_rank\") and args.local_rank not in [-1, 0]:\n        return\n\n    hub_token = args.hub_token if hasattr(args, \"hub_token\") else None\n    repo_name = get_full_repo_name(model_name, token=hub_token)\n\n    model_card = ModelCard.from_template(\n        card_data=ModelCardData(  # Card metadata object that will be converted to YAML block\n            language=\"en\",\n            license=\"apache-2.0\",\n            library_name=\"diffusers\",\n            tags=[],\n            datasets=args.dataset_name,\n            metrics=[],\n        ),\n        template_path=MODEL_CARD_TEMPLATE_PATH,\n        model_name=model_name,\n        repo_name=repo_name,\n        dataset_name=args.dataset_name if hasattr(args, \"dataset_name\") else None,\n        learning_rate=args.learning_rate,\n        train_batch_size=args.train_batch_size,\n        eval_batch_size=args.eval_batch_size,\n        gradient_accumulation_steps=(\n            args.gradient_accumulation_steps if hasattr(args, \"gradient_accumulation_steps\") else None\n        ),\n        adam_beta1=args.adam_beta1 if hasattr(args, \"adam_beta1\") else None,\n        adam_beta2=args.adam_beta2 if hasattr(args, \"adam_beta2\") else None,\n        adam_weight_decay=args.adam_weight_decay if hasattr(args, \"adam_weight_decay",
      "metadata": {
        "source": "src/diffusers/utils/hub_utils.py",
        "range": {
          "start": { "row": 87, "column": 0 },
          "end": { "row": 87, "column": 0 }
        }
      }
    }
  ],
  [
    "2203",
    {
      "pageContent": "def extract_commit_hash(resolved_file: Optional[str], commit_hash: Optional[str] = None):\n    \"\"\"\n    Extracts the commit hash from a resolved filename toward a cache file.\n    \"\"\"\n    if resolved_file is None or commit_hash is not None:\n        return commit_hash\n    resolved_file = str(Path(resolved_file).as_posix())\n    search = re.search(r\"snapshots/([^/]+)/\", resolved_file)\n    if search is None:\n        return None\n    commit_hash = search.groups()[0]\n    return commit_hash if REGEX_COMMIT_HASH.match(commit_hash) else None",
      "metadata": {
        "source": "src/diffusers/utils/hub_utils.py",
        "range": {
          "start": { "row": 136, "column": 0 },
          "end": { "row": 136, "column": 0 }
        }
      }
    }
  ],
  [
    "2204",
    {
      "pageContent": "def move_cache(old_cache_dir: Optional[str] = None, new_cache_dir: Optional[str] = None) -> None:\n    if new_cache_dir is None:\n        new_cache_dir = DIFFUSERS_CACHE\n    if old_cache_dir is None:\n        old_cache_dir = old_diffusers_cache\n\n    old_cache_dir = Path(old_cache_dir).expanduser()\n    new_cache_dir = Path(new_cache_dir).expanduser()\n    for old_blob_path in old_cache_dir.glob(\"**/blobs/*\"):\n        if old_blob_path.is_file() and not old_blob_path.is_symlink():\n            new_blob_path = new_cache_dir / old_blob_path.relative_to(old_cache_dir)\n            new_blob_path.parent.mkdir(parents=True, exist_ok=True)\n            os.replace(old_blob_path, new_blob_path)\n            try:\n                os.symlink(new_blob_path, old_blob_path)\n            except OSError:\n                logger.warning(\n                    \"Could not create symlink between old cache and new cache. If you use an older version of diffusers again, files will be re-downloaded.\"\n                )\n    # At this point, old_cache_dir contains symlinks to the new cache (it can still be used).",
      "metadata": {
        "source": "src/diffusers/utils/hub_utils.py",
        "range": {
          "start": { "row": 160, "column": 0 },
          "end": { "row": 160, "column": 0 }
        }
      }
    }
  ],
  [
    "2205",
    {
      "pageContent": "class OnnxStableDiffusionImg2ImgPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\", \"onnx\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2206",
    {
      "pageContent": "class OnnxStableDiffusionInpaintPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\", \"onnx\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2207",
    {
      "pageContent": "class OnnxStableDiffusionInpaintPipelineLegacy(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\", \"onnx\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py",
        "range": {
          "start": { "row": 34, "column": 0 },
          "end": { "row": 34, "column": 0 }
        }
      }
    }
  ],
  [
    "2208",
    {
      "pageContent": "class OnnxStableDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\", \"onnx\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "2209",
    {
      "pageContent": "class OnnxStableDiffusionUpscalePipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\", \"onnx\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "2210",
    {
      "pageContent": "class StableDiffusionOnnxPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"transformers\", \"onnx\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"transformers\", \"onnx\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "2211",
    {
      "pageContent": "def requires_backends(obj, backends):\n    if not isinstance(backends, (list, tuple)):\n        backends = [backends]\n\n    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n    checks = (BACKENDS_MAPPING[backend] for backend in backends)\n    failed = [msg.format(name) for available, msg in checks if not available()]\n    if failed:\n        raise ImportError(\"\".join(failed))\n\n    if name in [\n        \"VersatileDiffusionTextToImagePipeline\",\n        \"VersatileDiffusionPipeline\",\n        \"VersatileDiffusionDualGuidedPipeline\",\n        \"StableDiffusionImageVariationPipeline\",\n        \"UnCLIPPipeline\",\n    ] and is_transformers_version(\"<\", \"4.25.0\"):\n        raise ImportError(\n            f\"You need to install `transformers>=4.25` in order to use {name}: \\n```\\n pip install\"\n            \" --upgrade transformers \\n```\"\n        )\n\n    if name in [\"StableDiffusionDepth2ImgPipeline\", \"StableDiffusionPix2PixZeroPipeline\"] and is_transformers_version(\n        \"<\", \"4.26.0\"\n    ):\n        raise ImportError(\n            f\"You need to install `transformers>=4.26` in order to use {name}: \\n```\\n pip install\"\n            \" --upgrade transformers \\n```\"\n        )",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 407, "column": 0 },
          "end": { "row": 407, "column": 0 }
        }
      }
    }
  ],
  [
    "2212",
    {
      "pageContent": "class DummyObject(type):\n    \"\"\"\n    Metaclass for the dummy objects. Any class inheriting from it will return the ImportError generated by\n    `requires_backend` each time a user tries to access any method of that class.\n    \"\"\"\n\n    def __getattr__(cls, key):\n        if key.startswith(\"_\"):\n            return super().__getattr__(cls, key)\n        requires_backends(cls, cls._backends)",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 438, "column": 0 },
          "end": { "row": 438, "column": 0 }
        }
      }
    }
  ],
  [
    "2213",
    {
      "pageContent": "def __getattr__(cls, key):\n        if key.startswith(\"_\"):\n            return super().__getattr__(cls, key)\n        requires_backends(cls, cls._backends)",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 444, "column": 4 },
          "end": { "row": 444, "column": 4 }
        }
      }
    }
  ],
  [
    "2214",
    {
      "pageContent": "def compare_versions(library_or_version: Union[str, Version], operation: str, requirement_version: str):\n    \"\"\"\n    Args:\n    Compares a library version to some requirement using a given operation.\n        library_or_version (`str` or `packaging.version.Version`):\n            A library name or a version to check.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`.\n        requirement_version (`str`):\n            The version to compare the library version against\n    \"\"\"\n    if operation not in STR_OPERATION_TO_FUNC.keys():\n        raise ValueError(f\"`operation` must be one of {list(STR_OPERATION_TO_FUNC.keys())}, received {operation}\")\n    operation = STR_OPERATION_TO_FUNC[operation]\n    if isinstance(library_or_version, str):\n        library_or_version = parse(importlib_metadata.version(library_or_version))\n    return operation(library_or_version, parse(requirement_version))",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 451, "column": 0 },
          "end": { "row": 451, "column": 0 }
        }
      }
    }
  ],
  [
    "2215",
    {
      "pageContent": "def is_torch_version(operation: str, version: str):\n    \"\"\"\n    Args:\n    Compares the current PyTorch version to a given reference with an operation.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`\n        version (`str`):\n            A string version of PyTorch\n    \"\"\"\n    return compare_versions(parse(_torch_version), operation, version)",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 471, "column": 0 },
          "end": { "row": 471, "column": 0 }
        }
      }
    }
  ],
  [
    "2216",
    {
      "pageContent": "def is_transformers_version(operation: str, version: str):\n    \"\"\"\n    Args:\n    Compares the current Transformers version to a given reference with an operation.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`\n        version (`str`):\n            A version string\n    \"\"\"\n    if not _transformers_available:\n        return False\n    return compare_versions(parse(_transformers_version), operation, version)",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 483, "column": 0 },
          "end": { "row": 483, "column": 0 }
        }
      }
    }
  ],
  [
    "2217",
    {
      "pageContent": "def is_accelerate_version(operation: str, version: str):\n    \"\"\"\n    Args:\n    Compares the current Accelerate version to a given reference with an operation.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`\n        version (`str`):\n            A version string\n    \"\"\"\n    if not _accelerate_available:\n        return False\n    return compare_versions(parse(_accelerate_version), operation, version)",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 497, "column": 0 },
          "end": { "row": 497, "column": 0 }
        }
      }
    }
  ],
  [
    "2218",
    {
      "pageContent": "def is_k_diffusion_version(operation: str, version: str):\n    \"\"\"\n    Args:\n    Compares the current k-diffusion version to a given reference with an operation.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`\n        version (`str`):\n            A version string\n    \"\"\"\n    if not _k_diffusion_available:\n        return False\n    return compare_versions(parse(_k_diffusion_version), operation, version)",
      "metadata": {
        "source": "src/diffusers/utils/import_utils.py",
        "range": {
          "start": { "row": 511, "column": 0 },
          "end": { "row": 511, "column": 0 }
        }
      }
    }
  ],
  [
    "2219",
    {
      "pageContent": "class LMSDiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"torch\", \"scipy\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"scipy\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"scipy\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"scipy\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_scipy_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2220",
    {
      "pageContent": "class AudioDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"torch\", \"librosa\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"librosa\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"librosa\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"librosa\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_librosa_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2221",
    {
      "pageContent": "class Mel(metaclass=DummyObject):\n    _backends = [\"torch\", \"librosa\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"torch\", \"librosa\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"librosa\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"torch\", \"librosa\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_torch_and_librosa_objects.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2222",
    {
      "pageContent": "def check_min_version(min_version):\n    if version.parse(__version__) < version.parse(min_version):\n        if \"dev\" in min_version:\n            error_message = (\n                \"This example requires a source install from HuggingFace diffusers (see \"\n                \"`https://huggingface.co/docs/diffusers/installation#install-from-source`),\"\n            )\n        else:\n            error_message = f\"This example requires a minimum version of {min_version},\"\n        error_message += f\" but the version found is {__version__}.\\n\"\n        raise ImportError(error_message)",
      "metadata": {
        "source": "src/diffusers/utils/__init__.py",
        "range": {
          "start": { "row": 98, "column": 0 },
          "end": { "row": 98, "column": 0 }
        }
      }
    }
  ],
  [
    "2223",
    {
      "pageContent": "class OnnxRuntimeModel(metaclass=DummyObject):\n    _backends = [\"onnx\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"onnx\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"onnx\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"onnx\"])",
      "metadata": {
        "source": "src/diffusers/utils/dummy_onnx_objects.py",
        "range": {
          "start": { "row": 4, "column": 0 },
          "end": { "row": 4, "column": 0 }
        }
      }
    }
  ],
  [
    "2224",
    {
      "pageContent": "def randn_tensor(\n    shape: Union[Tuple, List],\n    generator: Optional[Union[List[\"torch.Generator\"], \"torch.Generator\"]] = None,\n    device: Optional[\"torch.device\"] = None,\n    dtype: Optional[\"torch.dtype\"] = None,\n    layout: Optional[\"torch.layout\"] = None,\n):\n    \"\"\"This is a helper function that allows to create random tensors on the desired `device` with the desired `dtype`. When\n    passing a list of generators one can seed each batched size individually. If CPU generators are passed the tensor\n    will always be created on CPU.\n    \"\"\"\n    # device on which tensor is created defaults to device\n    rand_device = device\n    batch_size = shape[0]\n\n    layout = layout or torch.strided\n    device = device or torch.device(\"cpu\")\n\n    if generator is not None:\n        gen_device_type = generator.device.type if not isinstance(generator, list) else generator[0].device.type\n        if gen_device_type != device.type and gen_device_type == \"cpu\":\n            rand_device = \"cpu\"\n            if device != \"mps\":\n                logger.info(\n                    f\"The passed generator was created on 'cpu' even though a tensor on {device} was expected.\"\n                    f\" Tensors will be created on 'cpu' and then moved to {device}. Note that one can probably\"\n                    f\" slighly speed up this function by passing a generator that was created on the {device} device.\"\n                )\n        elif gen_device_type != device.type and gen_device_type == \"cuda\":\n            raise ValueError(f\"Cannot generate a {device} tensor from a generator of type {gen_device_type}.\")",
      "metadata": {
        "source": "src/diffusers/utils/torch_utils.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "2225",
    {
      "pageContent": "class SchedulerType(Enum):\n    LINEAR = \"linear\"\n    COSINE = \"cosine\"\n    COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n    POLYNOMIAL = \"polynomial\"\n    CONSTANT = \"constant\"\n    CONSTANT_WITH_WARMUP = \"constant_with_warmup\"",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "2226",
    {
      "pageContent": "def get_constant_schedule(optimizer: Optimizer, last_epoch: int = -1):\n    \"\"\"\n    Create a schedule with a constant learning rate, using the learning rate set in optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "2227",
    {
      "pageContent": "def get_constant_schedule_with_warmup(optimizer: Optimizer, num_warmup_steps: int, last_epoch: int = -1):\n    \"\"\"\n    Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate\n    increases linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step: int):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1.0, num_warmup_steps))\n        return 1.0\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 54, "column": 0 },
          "end": { "row": 54, "column": 0 }
        }
      }
    }
  ],
  [
    "2228",
    {
      "pageContent": "def lr_lambda(current_step: int):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1.0, num_warmup_steps))\n        return 1.0",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 71, "column": 4 },
          "end": { "row": 71, "column": 4 }
        }
      }
    }
  ],
  [
    "2229",
    {
      "pageContent": "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n    \"\"\"\n    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step: int):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "2230",
    {
      "pageContent": "def lr_lambda(current_step: int):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n        )",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 98, "column": 4 },
          "end": { "row": 98, "column": 4 }
        }
      }
    }
  ],
  [
    "2231",
    {
      "pageContent": "def get_cosine_schedule_with_warmup(\n    optimizer: Optimizer, num_warmup_steps: int, num_training_steps: int, num_cycles: float = 0.5, last_epoch: int = -1\n):\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_periods (`float`, *optional*, defaults to 0.5):\n            The number of periods of the cosine function in a schedule (the default is to just decrease from the max\n            value to 0 following a half-cosine).\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 108, "column": 0 },
          "end": { "row": 108, "column": 0 }
        }
      }
    }
  ],
  [
    "2232",
    {
      "pageContent": "def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 133, "column": 4 },
          "end": { "row": 133, "column": 4 }
        }
      }
    }
  ],
  [
    "2233",
    {
      "pageContent": "def get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer: Optimizer, num_warmup_steps: int, num_training_steps: int, num_cycles: int = 1, last_epoch: int = -1\n):\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases\n    linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_cycles (`int`, *optional*, defaults to 1):\n            The number of hard restarts to use.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        if progress >= 1.0:\n            return 0.0\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 142, "column": 0 },
          "end": { "row": 142, "column": 0 }
        }
      }
    }
  ],
  [
    "2234",
    {
      "pageContent": "def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        if progress >= 1.0:\n            return 0.0\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 166, "column": 4 },
          "end": { "row": 166, "column": 4 }
        }
      }
    }
  ],
  [
    "2235",
    {
      "pageContent": "def get_polynomial_decay_schedule_with_warmup(\n    optimizer, num_warmup_steps, num_training_steps, lr_end=1e-7, power=1.0, last_epoch=-1\n):\n    \"\"\"\n    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the\n    optimizer to end lr defined by *lr_end*, after a warmup period during which it increases linearly from 0 to the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        lr_end (`float`, *optional*, defaults to 1e-7):\n            The end LR.\n        power (`float`, *optional*, defaults to 1.0):\n            Power factor.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Note: *power* defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT\n    implementation at\n    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\n    \"\"\"\n\n    lr_init = optimizer.defaults[\"lr\"]\n    if not (lr_init > lr_end):\n        raise ValueError(f\"lr_end ({lr_end}) must be be smaller than initial lr ({lr_init})\")\n\n    def lr_lambda(current_step: int):\n        if current_step < num_warmup_steps:\n            return ",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 177, "column": 0 },
          "end": { "row": 177, "column": 0 }
        }
      }
    }
  ],
  [
    "2236",
    {
      "pageContent": "def lr_lambda(current_step: int):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        elif current_step > num_training_steps:\n            return lr_end / lr_init  # as LambdaLR multiplies by lr_init\n        else:\n            lr_range = lr_init - lr_end\n            decay_steps = num_training_steps - num_warmup_steps\n            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps\n            decay = lr_range * pct_remaining**power + lr_end\n            return decay / lr_init",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 212, "column": 4 },
          "end": { "row": 212, "column": 4 }
        }
      }
    }
  ],
  [
    "2237",
    {
      "pageContent": "def get_scheduler(\n    name: Union[str, SchedulerType],\n    optimizer: Optimizer,\n    num_warmup_steps: Optional[int] = None,\n    num_training_steps: Optional[int] = None,\n    num_cycles: int = 1,\n    power: float = 1.0,\n):\n    \"\"\"\n    Unified API to get any scheduler from its name.\n\n    Args:\n        name (`str` or `SchedulerType`):\n            The name of the scheduler to use.\n        optimizer (`torch.optim.Optimizer`):\n            The optimizer that will be used during training.\n        num_warmup_steps (`int`, *optional*):\n            The number of warmup steps to do. This is not required by all schedulers (hence the argument being\n            optional), the function will raise an error if it's unset and the scheduler type requires it.\n        num_training_steps (`int``, *optional*):\n            The number of training steps to do. This is not required by all schedulers (hence the argument being\n            optional), the function will raise an error if it's unset and the scheduler type requires it.\n        num_cycles (`int`, *optional*):\n            The number of hard restarts used in `COSINE_WITH_RESTARTS` scheduler.\n        power (`float`, *optional*, defaults to 1.0):\n            Power factor. See `POLYNOMIAL` scheduler\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n    \"\"\"\n    name = SchedulerType(name)\n    schedule_func = TYPE_TO_SCHEDULER_FUNCTION[name]\n    if name == SchedulerType.CONSTANT:\n        return schedule_func(optimizer)\n\n    # All other schedulers require `num_warmup_steps`\n    if",
      "metadata": {
        "source": "src/diffusers/optimization.py",
        "range": {
          "start": { "row": 237, "column": 0 },
          "end": { "row": 237, "column": 0 }
        }
      }
    }
  ],
  [
    "2238",
    {
      "pageContent": "class EnvironmentCommand(BaseDiffusersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        download_parser = parser.add_parser(\"env\")\n        download_parser.set_defaults(func=info_command_factory)\n\n    def run(self):\n        hub_version = huggingface_hub.__version__\n\n        pt_version = \"not installed\"\n        pt_cuda_available = \"NA\"\n        if is_torch_available():\n            import torch\n\n            pt_version = torch.__version__\n            pt_cuda_available = torch.cuda.is_available()\n\n        transformers_version = \"not installed\"\n        if is_transformers_available():\n            import transformers\n\n            transformers_version = transformers.__version__\n\n        accelerate_version = \"not installed\"\n        if is_accelerate_available():\n            import accelerate\n\n            accelerate_version = accelerate.__version__\n\n        xformers_version = \"not installed\"\n        if is_xformers_available():\n            import xformers\n\n            xformers_version = xformers.__version__\n\n        info = {\n            \"`diffusers` version\": version,\n            \"Platform\": platform.platform(),\n            \"Python version\": platform.python_version(),\n            \"PyTorch version (GPU?)\": f\"{pt_version} ({pt_cuda_available})\",\n            \"Huggingface_hub version\": hub_version,\n            \"Transformers version\": transformers_version,\n            \"Accelerate version\": accelerate_version,\n            \"xFormers version\": xformers_version,\n            \"Using GPU in script?\": \"<fill in>\",\n            \"Using distributed or parallel se",
      "metadata": {
        "source": "src/diffusers/commands/env.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "2239",
    {
      "pageContent": "def run(self):\n        hub_version = huggingface_hub.__version__\n\n        pt_version = \"not installed\"\n        pt_cuda_available = \"NA\"\n        if is_torch_available():\n            import torch\n\n            pt_version = torch.__version__\n            pt_cuda_available = torch.cuda.is_available()\n\n        transformers_version = \"not installed\"\n        if is_transformers_available():\n            import transformers\n\n            transformers_version = transformers.__version__\n\n        accelerate_version = \"not installed\"\n        if is_accelerate_available():\n            import accelerate\n\n            accelerate_version = accelerate.__version__\n\n        xformers_version = \"not installed\"\n        if is_xformers_available():\n            import xformers\n\n            xformers_version = xformers.__version__\n\n        info = {\n            \"`diffusers` version\": version,\n            \"Platform\": platform.platform(),\n            \"Python version\": platform.python_version(),\n            \"PyTorch version (GPU?)\": f\"{pt_version} ({pt_cuda_available})\",\n            \"Huggingface_hub version\": hub_version,\n            \"Transformers version\": transformers_version,\n            \"Accelerate version\": accelerate_version,\n            \"xFormers version\": xformers_version,\n            \"Using GPU in script?\": \"<fill in>\",\n            \"Using distributed or parallel set-up in script?\": \"<fill in>\",\n        }\n\n        print(\"\\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\\n\")\n        print(self.format_dict(info))\n\n        return info",
      "metadata": {
        "source": "src/diffusers/commands/env.py",
        "range": {
          "start": { "row": 34, "column": 4 },
          "end": { "row": 34, "column": 4 }
        }
      }
    }
  ],
  [
    "2240",
    {
      "pageContent": "def main():\n    parser = ArgumentParser(\"Diffusers CLI tool\", usage=\"diffusers-cli <command> [<args>]\")\n    commands_parser = parser.add_subparsers(help=\"diffusers-cli command helpers\")\n\n    # Register commands\n    EnvironmentCommand.register_subcommand(commands_parser)\n\n    # Let's go\n    args = parser.parse_args()\n\n    if not hasattr(args, \"func\"):\n        parser.print_help()\n        exit(1)\n\n    # Run\n    service = args.func(args)\n    service.run()",
      "metadata": {
        "source": "src/diffusers/commands/diffusers_cli.py",
        "range": {
          "start": { "row": 20, "column": 0 },
          "end": { "row": 20, "column": 0 }
        }
      }
    }
  ],
  [
    "2241",
    {
      "pageContent": "class BaseDiffusersCLICommand(ABC):\n    @staticmethod\n    @abstractmethod\n    def register_subcommand(parser: ArgumentParser):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def run(self):\n        raise NotImplementedError()",
      "metadata": {
        "source": "src/diffusers/commands/__init__.py",
        "range": {
          "start": { "row": 18, "column": 0 },
          "end": { "row": 18, "column": 0 }
        }
      }
    }
  ],
  [
    "2242",
    {
      "pageContent": "class ValueGuidedRLPipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n    Pipeline for sampling actions from a diffusion model trained to predict sequences of states.\n\n    Original implementation inspired by this repository: https://github.com/jannerm/diffuser.\n\n    Parameters:\n        value_function ([`UNet1DModel`]): A specialized UNet for fine-tuning trajectories base on reward.\n        unet ([`UNet1DModel`]): U-Net architecture to denoise the encoded trajectories.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded trajectories. Default for this\n            application is [`DDPMScheduler`].\n        env: An environment following the OpenAI gym API to act in. For now only Hopper has pretrained models.\n    \"\"\"\n\n    def __init__(\n        self,\n        value_function: UNet1DModel,\n        unet: UNet1DModel,\n        scheduler: DDPMScheduler,\n        env,\n    ):\n        super().__init__()\n        self.value_function = value_function\n        self.unet = unet\n        self.scheduler = scheduler\n        self.env = env\n        self.data = env.get_dataset()\n        self.means = dict()\n        for key in self.data.keys():\n            try:\n                self.means[key] = self.data[key].mean()\n            except:  # noqa: E722\n                pass\n        self.stds = dict()\n        for key in self.d",
      "metadata": {
        "source": "src/diffusers/experimental/rl/value_guided_sampling.py",
        "range": {
          "start": { "row": 24, "column": 0 },
          "end": { "row": 24, "column": 0 }
        }
      }
    }
  ],
  [
    "2243",
    {
      "pageContent": "def __init__(\n        self,\n        value_function: UNet1DModel,\n        unet: UNet1DModel,\n        scheduler: DDPMScheduler,\n        env,\n    ):\n        super().__init__()\n        self.value_function = value_function\n        self.unet = unet\n        self.scheduler = scheduler\n        self.env = env\n        self.data = env.get_dataset()\n        self.means = dict()\n        for key in self.data.keys():\n            try:\n                self.means[key] = self.data[key].mean()\n            except:  # noqa: E722\n                pass\n        self.stds = dict()\n        for key in self.data.keys():\n            try:\n                self.stds[key] = self.data[key].std()\n            except:  # noqa: E722\n                pass\n        self.state_dim = env.observation_space.shape[0]\n        self.action_dim = env.action_space.shape[0]",
      "metadata": {
        "source": "src/diffusers/experimental/rl/value_guided_sampling.py",
        "range": {
          "start": { "row": 41, "column": 4 },
          "end": { "row": 41, "column": 4 }
        }
      }
    }
  ],
  [
    "2244",
    {
      "pageContent": "def to_torch(self, x_in):\n        if type(x_in) is dict:\n            return {k: self.to_torch(v) for k, v in x_in.items()}\n        elif torch.is_tensor(x_in):\n            return x_in.to(self.unet.device)\n        return torch.tensor(x_in, device=self.unet.device)",
      "metadata": {
        "source": "src/diffusers/experimental/rl/value_guided_sampling.py",
        "range": {
          "start": { "row": 75, "column": 4 },
          "end": { "row": 75, "column": 4 }
        }
      }
    }
  ],
  [
    "2245",
    {
      "pageContent": "def reset_x0(self, x_in, cond, act_dim):\n        for key, val in cond.items():\n            x_in[:, key, act_dim:] = val.clone()\n        return x_in",
      "metadata": {
        "source": "src/diffusers/experimental/rl/value_guided_sampling.py",
        "range": {
          "start": { "row": 82, "column": 4 },
          "end": { "row": 82, "column": 4 }
        }
      }
    }
  ],
  [
    "2246",
    {
      "pageContent": "def run_diffusion(self, x, conditions, n_guide_steps, scale):\n        batch_size = x.shape[0]\n        y = None\n        for i in tqdm.tqdm(self.scheduler.timesteps):\n            # create batch of timesteps to pass into model\n            timesteps = torch.full((batch_size,), i, device=self.unet.device, dtype=torch.long)\n            for _ in range(n_guide_steps):\n                with torch.enable_grad():\n                    x.requires_grad_()\n\n                    # permute to match dimension for pre-trained models\n                    y = self.value_function(x.permute(0, 2, 1), timesteps).sample\n                    grad = torch.autograd.grad([y.sum()], [x])[0]\n\n                    posterior_variance = self.scheduler._get_variance(i)\n                    model_std = torch.exp(0.5 * posterior_variance)\n                    grad = model_std * grad\n\n                grad[timesteps < 2] = 0\n                x = x.detach()\n                x = x + scale * grad\n                x = self.reset_x0(x, conditions, self.action_dim)\n\n            prev_x = self.unet(x.permute(0, 2, 1), timesteps).sample.permute(0, 2, 1)\n\n            # TODO: verify deprecation of this kwarg\n            x = self.scheduler.step(prev_x, i, x, predict_epsilon=False)[\"prev_sample\"]\n\n            # apply conditions to the trajectory (set the initial state)\n            x = self.reset_x0(x, conditions, self.action_dim)\n            x = self.to_torch(x)\n        return x, y",
      "metadata": {
        "source": "src/diffusers/experimental/rl/value_guided_sampling.py",
        "range": {
          "start": { "row": 87, "column": 4 },
          "end": { "row": 87, "column": 4 }
        }
      }
    }
  ],
  [
    "2247",
    {
      "pageContent": "def __call__(self, obs, batch_size=64, planning_horizon=32, n_guide_steps=2, scale=0.1):\n        # normalize the observations and create  batch dimension\n        obs = self.normalize(obs, \"observations\")\n        obs = obs[None].repeat(batch_size, axis=0)\n\n        conditions = {0: self.to_torch(obs)}\n        shape = (batch_size, planning_horizon, self.state_dim + self.action_dim)\n\n        # generate initial noise and apply our conditions (to make the trajectories start at current state)\n        x1 = randn_tensor(shape, device=self.unet.device)\n        x = self.reset_x0(x1, conditions, self.action_dim)\n        x = self.to_torch(x)\n\n        # run the diffusion process\n        x, y = self.run_diffusion(x, conditions, n_guide_steps, scale)\n\n        # sort output trajectories by value\n        sorted_idx = y.argsort(0, descending=True).squeeze()\n        sorted_values = x[sorted_idx]\n        actions = sorted_values[:, :, : self.action_dim]\n        actions = actions.detach().cpu().numpy()\n        denorm_actions = self.de_normalize(actions, key=\"actions\")\n\n        # select the action with the highest value\n        if y is not None:\n            selected_index = 0\n        else:\n            # if we didn't run value guiding, select a random action\n            selected_index = np.random.randint(0, batch_size)\n\n        denorm_actions = denorm_actions[selected_index, 0]\n        return denorm_actions",
      "metadata": {
        "source": "src/diffusers/experimental/rl/value_guided_sampling.py",
        "range": {
          "start": { "row": 120, "column": 4 },
          "end": { "row": 120, "column": 4 }
        }
      }
    }
  ],
  [
    "2248",
    {
      "pageContent": "class FrozenDict(OrderedDict):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        for key, value in self.items():\n            setattr(self, key, value)\n\n        self.__frozen = True\n\n    def __delitem__(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n\n    def setdefault(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n\n    def pop(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n\n    def update(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n\n    def __setattr__(self, name, value):\n        if hasattr(self, \"__frozen\") and self.__frozen:\n            raise Exception(f\"You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.\")\n        super().__setattr__(name, value)\n\n    def __setitem__(self, name, value):\n        if hasattr(self, \"__frozen\") and self.__frozen:\n            raise Exception(f\"You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.\")\n        super().__setitem__(name, value)",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 49, "column": 0 },
          "end": { "row": 49, "column": 0 }
        }
      }
    }
  ],
  [
    "2249",
    {
      "pageContent": "def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        for key, value in self.items():\n            setattr(self, key, value)\n\n        self.__frozen = True",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 50, "column": 4 },
          "end": { "row": 50, "column": 4 }
        }
      }
    }
  ],
  [
    "2250",
    {
      "pageContent": "def __setattr__(self, name, value):\n        if hasattr(self, \"__frozen\") and self.__frozen:\n            raise Exception(f\"You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.\")\n        super().__setattr__(name, value)",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 70, "column": 4 },
          "end": { "row": 70, "column": 4 }
        }
      }
    }
  ],
  [
    "2251",
    {
      "pageContent": "def __setitem__(self, name, value):\n        if hasattr(self, \"__frozen\") and self.__frozen:\n            raise Exception(f\"You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.\")\n        super().__setitem__(name, value)",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 75, "column": 4 },
          "end": { "row": 75, "column": 4 }
        }
      }
    }
  ],
  [
    "2252",
    {
      "pageContent": "class ConfigMixin:\n    r\"\"\"\n    Base class for all configuration classes. Stores all configuration parameters under `self.config` Also handles all\n    methods for loading/downloading/saving classes inheriting from [`ConfigMixin`] with\n        - [`~ConfigMixin.from_config`]\n        - [`~ConfigMixin.save_config`]\n\n    Class attributes:\n        - **config_name** (`str`) -- A filename under which the config should stored when calling\n          [`~ConfigMixin.save_config`] (should be overridden by parent class).\n        - **ignore_for_config** (`List[str]`) -- A list of attributes that should not be saved in the config (should be\n          overridden by subclass).\n        - **has_compatibles** (`bool`) -- Whether the class has compatible classes (should be overridden by subclass).\n        - **_deprecated_kwargs** (`List[str]`) -- Keyword arguments that are deprecated. Note that the init function\n          should only have a `kwargs` argument if at least one argument is deprecated (should be overridden by\n          subclass).\n    \"\"\"\n    config_name = None\n    ignore_for_config = []\n    has_compatibles = False\n\n    _deprecated_kwargs = []\n\n    def register_to_config(self, **kwargs):\n        if self.config_name is None:\n            raise NotImplementedError(f\"Make sure that {self.__class__} has defined a class name `config_name`\")\n        # Special case for `kwargs` used in deprecation warning added to schedulers\n        # TODO: remove this when we remove the deprecation warning, and the `kwargs` argument,\n        # or solve in a more general way.\n        kwargs.pop(\"kwargs\", None",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 81, "column": 0 },
          "end": { "row": 81, "column": 0 }
        }
      }
    }
  ],
  [
    "2253",
    {
      "pageContent": "def register_to_config(self, **kwargs):\n        if self.config_name is None:\n            raise NotImplementedError(f\"Make sure that {self.__class__} has defined a class name `config_name`\")\n        # Special case for `kwargs` used in deprecation warning added to schedulers\n        # TODO: remove this when we remove the deprecation warning, and the `kwargs` argument,\n        # or solve in a more general way.\n        kwargs.pop(\"kwargs\", None)\n        for key, value in kwargs.items():\n            try:\n                setattr(self, key, value)\n            except AttributeError as err:\n                logger.error(f\"Can't set {key} with value {value} for {self}\")\n                raise err\n\n        if not hasattr(self, \"_internal_dict\"):\n            internal_dict = kwargs\n        else:\n            previous_dict = dict(self._internal_dict)\n            internal_dict = {**self._internal_dict, **kwargs}\n            logger.debug(f\"Updating config from {previous_dict} to {internal_dict}\")\n\n        self._internal_dict = FrozenDict(internal_dict)",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 104, "column": 4 },
          "end": { "row": 104, "column": 4 }
        }
      }
    }
  ],
  [
    "2254",
    {
      "pageContent": "def save_config(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n        \"\"\"\n        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n        [`~ConfigMixin.from_config`] class method.\n\n        Args:\n            save_directory (`str` or `os.PathLike`):\n                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        # If we save using the predefined names, we can load using `from_config`\n        output_config_file = os.path.join(save_directory, self.config_name)\n\n        self.to_json_file(output_config_file)\n        logger.info(f\"Configuration saved in {output_config_file}\")",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 127, "column": 4 },
          "end": { "row": 127, "column": 4 }
        }
      }
    }
  ],
  [
    "2255",
    {
      "pageContent": "def to_json_string(self) -> str:\n        \"\"\"\n        Serializes this instance to a JSON string.\n\n        Returns:\n            `str`: String containing all the attributes that make up this configuration instance in JSON format.\n        \"\"\"\n        config_dict = self._internal_dict if hasattr(self, \"_internal_dict\") else {}\n        config_dict[\"_class_name\"] = self.__class__.__name__\n        config_dict[\"_diffusers_version\"] = __version__\n\n        def to_json_saveable(value):\n            if isinstance(value, np.ndarray):\n                value = value.tolist()\n            elif isinstance(value, PosixPath):\n                value = str(value)\n            return value\n\n        config_dict = {k: to_json_saveable(v) for k, v in config_dict.items()}\n        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 522, "column": 4 },
          "end": { "row": 522, "column": 4 }
        }
      }
    }
  ],
  [
    "2256",
    {
      "pageContent": "def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n        \"\"\"\n        Save this instance to a JSON file.\n\n        Args:\n            json_file_path (`str` or `os.PathLike`):\n                Path to the JSON file in which this configuration instance's parameters will be saved.\n        \"\"\"\n        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n            writer.write(self.to_json_string())",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 543, "column": 4 },
          "end": { "row": 543, "column": 4 }
        }
      }
    }
  ],
  [
    "2257",
    {
      "pageContent": "def register_to_config(init):\n    r\"\"\"\n    Decorator to apply on the init of classes inheriting from [`ConfigMixin`] so that all the arguments are\n    automatically sent to `self.register_for_config`. To ignore a specific argument accepted by the init but that\n    shouldn't be registered in the config, use the `ignore_for_config` class variable\n\n    Warning: Once decorated, all private arguments (beginning with an underscore) are trashed and not sent to the init!\n    \"\"\"\n\n    @functools.wraps(init)\n    def inner_init(self, *args, **kwargs):\n        # Ignore private kwargs in the init.\n        init_kwargs = {k: v for k, v in kwargs.items() if not k.startswith(\"_\")}\n        config_init_kwargs = {k: v for k, v in kwargs.items() if k.startswith(\"_\")}\n        if not isinstance(self, ConfigMixin):\n            raise RuntimeError(\n                f\"`@register_for_config` was applied to {self.__class__.__name__} init method, but this class does \"\n                \"not inherit from `ConfigMixin`.\"\n            )\n\n        ignore = getattr(self, \"ignore_for_config\", [])\n        # Get positional arguments aligned with kwargs\n        new_kwargs = {}\n        signature = inspect.signature(init)\n        parameters = {\n            name: p.default for i, (name, p) in enumerate(signature.parameters.items()) if i > 0 and name not in ignore\n        }\n        for arg, name in zip(args, parameters.keys()):\n            new_kwargs[name] = arg\n\n        # Then add all kwargs\n        new_kwargs.update(\n            {\n                k: init_kwargs.get(k, default)\n                for k, default in paramete",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 555, "column": 0 },
          "end": { "row": 555, "column": 0 }
        }
      }
    }
  ],
  [
    "2258",
    {
      "pageContent": "def flax_register_to_config(cls):\n    original_init = cls.__init__\n\n    @functools.wraps(original_init)\n    def init(self, *args, **kwargs):\n        if not isinstance(self, ConfigMixin):\n            raise RuntimeError(\n                f\"`@register_for_config` was applied to {self.__class__.__name__} init method, but this class does \"\n                \"not inherit from `ConfigMixin`.\"\n            )\n\n        # Ignore private kwargs in the init. Retrieve all passed attributes\n        init_kwargs = {k: v for k, v in kwargs.items()}\n\n        # Retrieve default values\n        fields = dataclasses.fields(self)\n        default_kwargs = {}\n        for field in fields:\n            # ignore flax specific attributes\n            if field.name in self._flax_internal_args:\n                continue\n            if type(field.default) == dataclasses._MISSING_TYPE:\n                default_kwargs[field.name] = None\n            else:\n                default_kwargs[field.name] = getattr(self, field.name)\n\n        # Make sure init_kwargs override default kwargs\n        new_kwargs = {**default_kwargs, **init_kwargs}\n        # dtype should be part of `init_kwargs`, but not `new_kwargs`\n        if \"dtype\" in new_kwargs:\n            new_kwargs.pop(\"dtype\")\n\n        # Get positional arguments aligned with kwargs\n        for i, arg in enumerate(args):\n            name = fields[i].name\n            new_kwargs[name] = arg\n\n        getattr(self, \"register_to_config\")(**new_kwargs)\n        original_init(self, *args, **kwargs)\n\n    cls.__init__ = init\n    return cls",
      "metadata": {
        "source": "src/diffusers/configuration_utils.py",
        "range": {
          "start": { "row": 600, "column": 0 },
          "end": { "row": 600, "column": 0 }
        }
      }
    }
  ],
  [
    "2259",
    {
      "pageContent": "def get_timestep_embedding(\n    timesteps: torch.Tensor,\n    embedding_dim: int,\n    flip_sin_to_cos: bool = False,\n    downscale_freq_shift: float = 1,\n    scale: float = 1,\n    max_period: int = 10000,\n):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param embedding_dim: the dimension of the output. :param max_period: controls the minimum frequency of the\n    embeddings. :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n\n    half_dim = embedding_dim // 2\n    exponent = -math.log(max_period) * torch.arange(\n        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n    )\n    exponent = exponent / (half_dim - downscale_freq_shift)\n\n    emb = torch.exp(exponent)\n    emb = timesteps[:, None].float() * emb[None, :]\n\n    # scale embeddings\n    emb = scale * emb\n\n    # concat sine and cosine embeddings\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n\n    # flip sine and cosine embeddings\n    if flip_sin_to_cos:\n        emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n\n    # zero pad\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "2260",
    {
      "pageContent": "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):\n    \"\"\"\n    grid_size: int of the grid height and width return: pos_embed: [grid_size*grid_size, embed_dim] or\n    [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token and extra_tokens > 0:\n        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n    return pos_embed",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "2261",
    {
      "pageContent": "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    if embed_dim % 2 != 0:\n        raise ValueError(\"embed_dim must be divisible by 2\")\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 81, "column": 0 },
          "end": { "row": 81, "column": 0 }
        }
      }
    }
  ],
  [
    "2262",
    {
      "pageContent": "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)\n    \"\"\"\n    if embed_dim % 2 != 0:\n        raise ValueError(\"embed_dim must be divisible by 2\")\n\n    omega = np.arange(embed_dim // 2, dtype=np.float64)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 93, "column": 0 },
          "end": { "row": 93, "column": 0 }
        }
      }
    }
  ],
  [
    "2263",
    {
      "pageContent": "class PatchEmbed(nn.Module):\n    \"\"\"2D Image to Patch Embedding\"\"\"\n\n    def __init__(\n        self,\n        height=224,\n        width=224,\n        patch_size=16,\n        in_channels=3,\n        embed_dim=768,\n        layer_norm=False,\n        flatten=True,\n        bias=True,\n    ):\n        super().__init__()\n\n        num_patches = (height // patch_size) * (width // patch_size)\n        self.flatten = flatten\n        self.layer_norm = layer_norm\n\n        self.proj = nn.Conv2d(\n            in_channels, embed_dim, kernel_size=(patch_size, patch_size), stride=patch_size, bias=bias\n        )\n        if layer_norm:\n            self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-6)\n        else:\n            self.norm = None\n\n        pos_embed = get_2d_sincos_pos_embed(embed_dim, int(num_patches**0.5))\n        self.register_buffer(\"pos_embed\", torch.from_numpy(pos_embed).float().unsqueeze(0), persistent=False)\n\n    def forward(self, latent):\n        latent = self.proj(latent)\n        if self.flatten:\n            latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC\n        if self.layer_norm:\n            latent = self.norm(latent)\n        return latent + self.pos_embed",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 114, "column": 0 },
          "end": { "row": 114, "column": 0 }
        }
      }
    }
  ],
  [
    "2264",
    {
      "pageContent": "def __init__(\n        self,\n        height=224,\n        width=224,\n        patch_size=16,\n        in_channels=3,\n        embed_dim=768,\n        layer_norm=False,\n        flatten=True,\n        bias=True,\n    ):\n        super().__init__()\n\n        num_patches = (height // patch_size) * (width // patch_size)\n        self.flatten = flatten\n        self.layer_norm = layer_norm\n\n        self.proj = nn.Conv2d(\n            in_channels, embed_dim, kernel_size=(patch_size, patch_size), stride=patch_size, bias=bias\n        )\n        if layer_norm:\n            self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-6)\n        else:\n            self.norm = None\n\n        pos_embed = get_2d_sincos_pos_embed(embed_dim, int(num_patches**0.5))\n        self.register_buffer(\"pos_embed\", torch.from_numpy(pos_embed).float().unsqueeze(0), persistent=False)",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 117, "column": 4 },
          "end": { "row": 117, "column": 4 }
        }
      }
    }
  ],
  [
    "2265",
    {
      "pageContent": "def forward(self, latent):\n        latent = self.proj(latent)\n        if self.flatten:\n            latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC\n        if self.layer_norm:\n            latent = self.norm(latent)\n        return latent + self.pos_embed",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 145, "column": 4 },
          "end": { "row": 145, "column": 4 }
        }
      }
    }
  ],
  [
    "2266",
    {
      "pageContent": "class TimestepEmbedding(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        time_embed_dim: int,\n        act_fn: str = \"silu\",\n        out_dim: int = None,\n        post_act_fn: Optional[str] = None,\n        cond_proj_dim=None,\n    ):\n        super().__init__()\n\n        self.linear_1 = nn.Linear(in_channels, time_embed_dim)\n\n        if cond_proj_dim is not None:\n            self.cond_proj = nn.Linear(cond_proj_dim, in_channels, bias=False)\n        else:\n            self.cond_proj = None\n\n        if act_fn == \"silu\":\n            self.act = nn.SiLU()\n        elif act_fn == \"mish\":\n            self.act = nn.Mish()\n        elif act_fn == \"gelu\":\n            self.act = nn.GELU()\n        else:\n            raise ValueError(f\"{act_fn} does not exist. Make sure to define one of 'silu', 'mish', or 'gelu'\")\n\n        if out_dim is not None:\n            time_embed_dim_out = out_dim\n        else:\n            time_embed_dim_out = time_embed_dim\n        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim_out)\n\n        if post_act_fn is None:\n            self.post_act = None\n        elif post_act_fn == \"silu\":\n            self.post_act = nn.SiLU()\n        elif post_act_fn == \"mish\":\n            self.post_act = nn.Mish()\n        elif post_act_fn == \"gelu\":\n            self.post_act = nn.GELU()\n        else:\n            raise ValueError(f\"{post_act_fn} does not exist. Make sure to define one of 'silu', 'mish', or 'gelu'\")\n\n    def forward(self, sample, condition=None):\n        if condition is not None:\n            sample = sample + self.cond_proj(condition)\n     ",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 154, "column": 0 },
          "end": { "row": 154, "column": 0 }
        }
      }
    }
  ],
  [
    "2267",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        time_embed_dim: int,\n        act_fn: str = \"silu\",\n        out_dim: int = None,\n        post_act_fn: Optional[str] = None,\n        cond_proj_dim=None,\n    ):\n        super().__init__()\n\n        self.linear_1 = nn.Linear(in_channels, time_embed_dim)\n\n        if cond_proj_dim is not None:\n            self.cond_proj = nn.Linear(cond_proj_dim, in_channels, bias=False)\n        else:\n            self.cond_proj = None\n\n        if act_fn == \"silu\":\n            self.act = nn.SiLU()\n        elif act_fn == \"mish\":\n            self.act = nn.Mish()\n        elif act_fn == \"gelu\":\n            self.act = nn.GELU()\n        else:\n            raise ValueError(f\"{act_fn} does not exist. Make sure to define one of 'silu', 'mish', or 'gelu'\")\n\n        if out_dim is not None:\n            time_embed_dim_out = out_dim\n        else:\n            time_embed_dim_out = time_embed_dim\n        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim_out)\n\n        if post_act_fn is None:\n            self.post_act = None\n        elif post_act_fn == \"silu\":\n            self.post_act = nn.SiLU()\n        elif post_act_fn == \"mish\":\n            self.post_act = nn.Mish()\n        elif post_act_fn == \"gelu\":\n            self.post_act = nn.GELU()\n        else:\n            raise ValueError(f\"{post_act_fn} does not exist. Make sure to define one of 'silu', 'mish', or 'gelu'\")",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 155, "column": 4 },
          "end": { "row": 155, "column": 4 }
        }
      }
    }
  ],
  [
    "2268",
    {
      "pageContent": "def forward(self, sample, condition=None):\n        if condition is not None:\n            sample = sample + self.cond_proj(condition)\n        sample = self.linear_1(sample)\n\n        if self.act is not None:\n            sample = self.act(sample)\n\n        sample = self.linear_2(sample)\n\n        if self.post_act is not None:\n            sample = self.post_act(sample)\n        return sample",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 199, "column": 4 },
          "end": { "row": 199, "column": 4 }
        }
      }
    }
  ],
  [
    "2269",
    {
      "pageContent": "class Timesteps(nn.Module):\n    def __init__(self, num_channels: int, flip_sin_to_cos: bool, downscale_freq_shift: float):\n        super().__init__()\n        self.num_channels = num_channels\n        self.flip_sin_to_cos = flip_sin_to_cos\n        self.downscale_freq_shift = downscale_freq_shift\n\n    def forward(self, timesteps):\n        t_emb = get_timestep_embedding(\n            timesteps,\n            self.num_channels,\n            flip_sin_to_cos=self.flip_sin_to_cos,\n            downscale_freq_shift=self.downscale_freq_shift,\n        )\n        return t_emb",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 214, "column": 0 },
          "end": { "row": 214, "column": 0 }
        }
      }
    }
  ],
  [
    "2270",
    {
      "pageContent": "def __init__(self, num_channels: int, flip_sin_to_cos: bool, downscale_freq_shift: float):\n        super().__init__()\n        self.num_channels = num_channels\n        self.flip_sin_to_cos = flip_sin_to_cos\n        self.downscale_freq_shift = downscale_freq_shift",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 215, "column": 4 },
          "end": { "row": 215, "column": 4 }
        }
      }
    }
  ],
  [
    "2271",
    {
      "pageContent": "def forward(self, timesteps):\n        t_emb = get_timestep_embedding(\n            timesteps,\n            self.num_channels,\n            flip_sin_to_cos=self.flip_sin_to_cos,\n            downscale_freq_shift=self.downscale_freq_shift,\n        )\n        return t_emb",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 221, "column": 4 },
          "end": { "row": 221, "column": 4 }
        }
      }
    }
  ],
  [
    "2272",
    {
      "pageContent": "class GaussianFourierProjection(nn.Module):\n    \"\"\"Gaussian Fourier embeddings for noise levels.\"\"\"\n\n    def __init__(\n        self, embedding_size: int = 256, scale: float = 1.0, set_W_to_weight=True, log=True, flip_sin_to_cos=False\n    ):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n        self.log = log\n        self.flip_sin_to_cos = flip_sin_to_cos\n\n        if set_W_to_weight:\n            # to delete later\n            self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n\n            self.weight = self.W\n\n    def forward(self, x):\n        if self.log:\n            x = torch.log(x)\n\n        x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi\n\n        if self.flip_sin_to_cos:\n            out = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)\n        else:\n            out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n        return out",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 231, "column": 0 },
          "end": { "row": 231, "column": 0 }
        }
      }
    }
  ],
  [
    "2273",
    {
      "pageContent": "def __init__(\n        self, embedding_size: int = 256, scale: float = 1.0, set_W_to_weight=True, log=True, flip_sin_to_cos=False\n    ):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n        self.log = log\n        self.flip_sin_to_cos = flip_sin_to_cos\n\n        if set_W_to_weight:\n            # to delete later\n            self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n\n            self.weight = self.W",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 234, "column": 4 },
          "end": { "row": 234, "column": 4 }
        }
      }
    }
  ],
  [
    "2274",
    {
      "pageContent": "def forward(self, x):\n        if self.log:\n            x = torch.log(x)\n\n        x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi\n\n        if self.flip_sin_to_cos:\n            out = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)\n        else:\n            out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n        return out",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 248, "column": 4 },
          "end": { "row": 248, "column": 4 }
        }
      }
    }
  ],
  [
    "2275",
    {
      "pageContent": "class ImagePositionalEmbeddings(nn.Module):\n    \"\"\"\n    Converts latent image classes into vector embeddings. Sums the vector embeddings with positional embeddings for the\n    height and width of the latent space.\n\n    For more details, see figure 10 of the dall-e paper: https://arxiv.org/abs/2102.12092\n\n    For VQ-diffusion:\n\n    Output vector embeddings are used as input for the transformer.\n\n    Note that the vector embeddings for the transformer are different than the vector embeddings from the VQVAE.\n\n    Args:\n        num_embed (`int`):\n            Number of embeddings for the latent pixels embeddings.\n        height (`int`):\n            Height of the latent image i.e. the number of height embeddings.\n        width (`int`):\n            Width of the latent image i.e. the number of width embeddings.\n        embed_dim (`int`):\n            Dimension of the produced vector embeddings. Used for the latent pixel, height, and width embeddings.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embed: int,\n        height: int,\n        width: int,\n        embed_dim: int,\n    ):\n        super().__init__()\n\n        self.height = height\n        self.width = width\n        self.num_embed = num_embed\n        self.embed_dim = embed_dim\n\n        self.emb = nn.Embedding(self.num_embed, embed_dim)\n        self.height_emb = nn.Embedding(self.height, embed_dim)\n        self.width_emb = nn.Embedding(self.width, embed_dim)\n\n    def forward(self, index):\n        emb = self.emb(index)\n\n        height_emb = self.height_emb(torch.arange(self.height, device=index.device).view(1, self.height))\n\n",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 261, "column": 0 },
          "end": { "row": 261, "column": 0 }
        }
      }
    }
  ],
  [
    "2276",
    {
      "pageContent": "def __init__(\n        self,\n        num_embed: int,\n        height: int,\n        width: int,\n        embed_dim: int,\n    ):\n        super().__init__()\n\n        self.height = height\n        self.width = width\n        self.num_embed = num_embed\n        self.embed_dim = embed_dim\n\n        self.emb = nn.Embedding(self.num_embed, embed_dim)\n        self.height_emb = nn.Embedding(self.height, embed_dim)\n        self.width_emb = nn.Embedding(self.width, embed_dim)",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "2277",
    {
      "pageContent": "def forward(self, index):\n        emb = self.emb(index)\n\n        height_emb = self.height_emb(torch.arange(self.height, device=index.device).view(1, self.height))\n\n        # 1 x H x D -> 1 x H x 1 x D\n        height_emb = height_emb.unsqueeze(2)\n\n        width_emb = self.width_emb(torch.arange(self.width, device=index.device).view(1, self.width))\n\n        # 1 x W x D -> 1 x 1 x W x D\n        width_emb = width_emb.unsqueeze(1)\n\n        pos_emb = height_emb + width_emb\n\n        # 1 x H x W x D -> 1 x L xD\n        pos_emb = pos_emb.view(1, self.height * self.width, -1)\n\n        emb = emb + pos_emb[:, : emb.shape[1], :]\n\n        return emb",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 303, "column": 4 },
          "end": { "row": 303, "column": 4 }
        }
      }
    }
  ],
  [
    "2278",
    {
      "pageContent": "class LabelEmbedding(nn.Module):\n    \"\"\"\n    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n\n    Args:\n        num_classes (`int`): The number of classes.\n        hidden_size (`int`): The size of the vector embeddings.\n        dropout_prob (`float`): The probability of dropping a label.\n    \"\"\"\n\n    def __init__(self, num_classes, hidden_size, dropout_prob):\n        super().__init__()\n        use_cfg_embedding = dropout_prob > 0\n        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n        self.num_classes = num_classes\n        self.dropout_prob = dropout_prob\n\n    def token_drop(self, labels, force_drop_ids=None):\n        \"\"\"\n        Drops labels to enable classifier-free guidance.\n        \"\"\"\n        if force_drop_ids is None:\n            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n        else:\n            drop_ids = torch.tensor(force_drop_ids == 1)\n        labels = torch.where(drop_ids, self.num_classes, labels)\n        return labels\n\n    def forward(self, labels, force_drop_ids=None):\n        use_dropout = self.dropout_prob > 0\n        if (self.training and use_dropout) or (force_drop_ids is not None):\n            labels = self.token_drop(labels, force_drop_ids)\n        embeddings = self.embedding_table(labels)\n        return embeddings",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 326, "column": 0 },
          "end": { "row": 326, "column": 0 }
        }
      }
    }
  ],
  [
    "2279",
    {
      "pageContent": "def __init__(self, num_classes, hidden_size, dropout_prob):\n        super().__init__()\n        use_cfg_embedding = dropout_prob > 0\n        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n        self.num_classes = num_classes\n        self.dropout_prob = dropout_prob",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 336, "column": 4 },
          "end": { "row": 336, "column": 4 }
        }
      }
    }
  ],
  [
    "2280",
    {
      "pageContent": "def token_drop(self, labels, force_drop_ids=None):\n        \"\"\"\n        Drops labels to enable classifier-free guidance.\n        \"\"\"\n        if force_drop_ids is None:\n            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n        else:\n            drop_ids = torch.tensor(force_drop_ids == 1)\n        labels = torch.where(drop_ids, self.num_classes, labels)\n        return labels",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 343, "column": 4 },
          "end": { "row": 343, "column": 4 }
        }
      }
    }
  ],
  [
    "2281",
    {
      "pageContent": "def forward(self, labels, force_drop_ids=None):\n        use_dropout = self.dropout_prob > 0\n        if (self.training and use_dropout) or (force_drop_ids is not None):\n            labels = self.token_drop(labels, force_drop_ids)\n        embeddings = self.embedding_table(labels)\n        return embeddings",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 354, "column": 4 },
          "end": { "row": 354, "column": 4 }
        }
      }
    }
  ],
  [
    "2282",
    {
      "pageContent": "class CombinedTimestepLabelEmbeddings(nn.Module):\n    def __init__(self, num_classes, embedding_dim, class_dropout_prob=0.1):\n        super().__init__()\n\n        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=1)\n        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n        self.class_embedder = LabelEmbedding(num_classes, embedding_dim, class_dropout_prob)\n\n    def forward(self, timestep, class_labels, hidden_dtype=None):\n        timesteps_proj = self.time_proj(timestep)\n        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=hidden_dtype))  # (N, D)\n\n        class_labels = self.class_embedder(class_labels)  # (N, D)\n\n        conditioning = timesteps_emb + class_labels  # (N, D)\n\n        return conditioning",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 362, "column": 0 },
          "end": { "row": 362, "column": 0 }
        }
      }
    }
  ],
  [
    "2283",
    {
      "pageContent": "def __init__(self, num_classes, embedding_dim, class_dropout_prob=0.1):\n        super().__init__()\n\n        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=1)\n        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n        self.class_embedder = LabelEmbedding(num_classes, embedding_dim, class_dropout_prob)",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 363, "column": 4 },
          "end": { "row": 363, "column": 4 }
        }
      }
    }
  ],
  [
    "2284",
    {
      "pageContent": "def forward(self, timestep, class_labels, hidden_dtype=None):\n        timesteps_proj = self.time_proj(timestep)\n        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=hidden_dtype))  # (N, D)\n\n        class_labels = self.class_embedder(class_labels)  # (N, D)\n\n        conditioning = timesteps_emb + class_labels  # (N, D)\n\n        return conditioning",
      "metadata": {
        "source": "src/diffusers/models/embeddings.py",
        "range": {
          "start": { "row": 370, "column": 4 },
          "end": { "row": 370, "column": 4 }
        }
      }
    }
  ],
  [
    "2285",
    {
      "pageContent": "class FlaxDecoderOutput(BaseOutput):\n    \"\"\"\n    Output of decoding method.\n\n    Args:\n        sample (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)`):\n            Decoded output sample of the model. Output of the last layer of the model.\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n\n    sample: jnp.ndarray",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "2286",
    {
      "pageContent": "class FlaxAutoencoderKLOutput(BaseOutput):\n    \"\"\"\n    Output of AutoencoderKL encoding method.\n\n    Args:\n        latent_dist (`FlaxDiagonalGaussianDistribution`):\n            Encoded outputs of `Encoder` represented as the mean and logvar of `FlaxDiagonalGaussianDistribution`.\n            `FlaxDiagonalGaussianDistribution` allows for sampling latents from the distribution.\n    \"\"\"\n\n    latent_dist: \"FlaxDiagonalGaussianDistribution\"",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 47, "column": 0 },
          "end": { "row": 47, "column": 0 }
        }
      }
    }
  ],
  [
    "2287",
    {
      "pageContent": "class FlaxUpsample2D(nn.Module):\n    \"\"\"\n    Flax implementation of 2D Upsample layer\n\n    Args:\n        in_channels (`int`):\n            Input channels\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n\n    in_channels: int\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.conv = nn.Conv(\n            self.in_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n    def __call__(self, hidden_states):\n        batch, height, width, channels = hidden_states.shape\n        hidden_states = jax.image.resize(\n            hidden_states,\n            shape=(batch, height * 2, width * 2, channels),\n            method=\"nearest\",\n        )\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 60, "column": 0 },
          "end": { "row": 60, "column": 0 }
        }
      }
    }
  ],
  [
    "2288",
    {
      "pageContent": "def setup(self):\n        self.conv = nn.Conv(\n            self.in_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 74, "column": 4 },
          "end": { "row": 74, "column": 4 }
        }
      }
    }
  ],
  [
    "2289",
    {
      "pageContent": "def __call__(self, hidden_states):\n        batch, height, width, channels = hidden_states.shape\n        hidden_states = jax.image.resize(\n            hidden_states,\n            shape=(batch, height * 2, width * 2, channels),\n            method=\"nearest\",\n        )\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 83, "column": 4 },
          "end": { "row": 83, "column": 4 }
        }
      }
    }
  ],
  [
    "2290",
    {
      "pageContent": "class FlaxDownsample2D(nn.Module):\n    \"\"\"\n    Flax implementation of 2D Downsample layer\n\n    Args:\n        in_channels (`int`):\n            Input channels\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n\n    in_channels: int\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.conv = nn.Conv(\n            self.in_channels,\n            kernel_size=(3, 3),\n            strides=(2, 2),\n            padding=\"VALID\",\n            dtype=self.dtype,\n        )\n\n    def __call__(self, hidden_states):\n        pad = ((0, 0), (0, 1), (0, 1), (0, 0))  # pad height and width dim\n        hidden_states = jnp.pad(hidden_states, pad_width=pad)\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 94, "column": 0 },
          "end": { "row": 94, "column": 0 }
        }
      }
    }
  ],
  [
    "2291",
    {
      "pageContent": "def setup(self):\n        self.conv = nn.Conv(\n            self.in_channels,\n            kernel_size=(3, 3),\n            strides=(2, 2),\n            padding=\"VALID\",\n            dtype=self.dtype,\n        )",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 108, "column": 4 },
          "end": { "row": 108, "column": 4 }
        }
      }
    }
  ],
  [
    "2292",
    {
      "pageContent": "def __call__(self, hidden_states):\n        pad = ((0, 0), (0, 1), (0, 1), (0, 0))  # pad height and width dim\n        hidden_states = jnp.pad(hidden_states, pad_width=pad)\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 117, "column": 4 },
          "end": { "row": 117, "column": 4 }
        }
      }
    }
  ],
  [
    "2293",
    {
      "pageContent": "class FlaxResnetBlock2D(nn.Module):\n    \"\"\"\n    Flax implementation of 2D Resnet Block.\n\n    Args:\n        in_channels (`int`):\n            Input channels\n        out_channels (`int`):\n            Output channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        groups (:obj:`int`, *optional*, defaults to `32`):\n            The number of groups to use for group norm.\n        use_nin_shortcut (:obj:`bool`, *optional*, defaults to `None`):\n            Whether to use `nin_shortcut`. This activates a new layer inside ResNet block\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n\n    in_channels: int\n    out_channels: int = None\n    dropout: float = 0.0\n    groups: int = 32\n    use_nin_shortcut: bool = None\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        out_channels = self.in_channels if self.out_channels is None else self.out_channels\n\n        self.norm1 = nn.GroupNorm(num_groups=self.groups, epsilon=1e-6)\n        self.conv1 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        self.norm2 = nn.GroupNorm(num_groups=self.groups, epsilon=1e-6)\n        self.dropout_layer = nn.Dropout(self.dropout)\n        self.conv2 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        use_nin_shortcut = self.in_channels !",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 124, "column": 0 },
          "end": { "row": 124, "column": 0 }
        }
      }
    }
  ],
  [
    "2294",
    {
      "pageContent": "def setup(self):\n        out_channels = self.in_channels if self.out_channels is None else self.out_channels\n\n        self.norm1 = nn.GroupNorm(num_groups=self.groups, epsilon=1e-6)\n        self.conv1 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        self.norm2 = nn.GroupNorm(num_groups=self.groups, epsilon=1e-6)\n        self.dropout_layer = nn.Dropout(self.dropout)\n        self.conv2 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        use_nin_shortcut = self.in_channels != out_channels if self.use_nin_shortcut is None else self.use_nin_shortcut\n\n        self.conv_shortcut = None\n        if use_nin_shortcut:\n            self.conv_shortcut = nn.Conv(\n                out_channels,\n                kernel_size=(1, 1),\n                strides=(1, 1),\n                padding=\"VALID\",\n                dtype=self.dtype,\n            )",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 150, "column": 4 },
          "end": { "row": 150, "column": 4 }
        }
      }
    }
  ],
  [
    "2295",
    {
      "pageContent": "def __call__(self, hidden_states, deterministic=True):\n        residual = hidden_states\n        hidden_states = self.norm1(hidden_states)\n        hidden_states = nn.swish(hidden_states)\n        hidden_states = self.conv1(hidden_states)\n\n        hidden_states = self.norm2(hidden_states)\n        hidden_states = nn.swish(hidden_states)\n        hidden_states = self.dropout_layer(hidden_states, deterministic)\n        hidden_states = self.conv2(hidden_states)\n\n        if self.conv_shortcut is not None:\n            residual = self.conv_shortcut(residual)\n\n        return hidden_states + residual",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 184, "column": 4 },
          "end": { "row": 184, "column": 4 }
        }
      }
    }
  ],
  [
    "2296",
    {
      "pageContent": "class FlaxAttentionBlock(nn.Module):\n    r\"\"\"\n    Flax Convolutional based multi-head attention block for diffusion-based VAE.\n\n    Parameters:\n        channels (:obj:`int`):\n            Input channels\n        num_head_channels (:obj:`int`, *optional*, defaults to `None`):\n            Number of attention heads\n        num_groups (:obj:`int`, *optional*, defaults to `32`):\n            The number of groups to use for group norm\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n\n    \"\"\"\n    channels: int\n    num_head_channels: int = None\n    num_groups: int = 32\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.num_heads = self.channels // self.num_head_channels if self.num_head_channels is not None else 1\n\n        dense = partial(nn.Dense, self.channels, dtype=self.dtype)\n\n        self.group_norm = nn.GroupNorm(num_groups=self.num_groups, epsilon=1e-6)\n        self.query, self.key, self.value = dense(), dense(), dense()\n        self.proj_attn = dense()\n\n    def transpose_for_scores(self, projection):\n        new_projection_shape = projection.shape[:-1] + (self.num_heads, -1)\n        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D)\n        new_projection = projection.reshape(new_projection_shape)\n        # (B, T, H, D) -> (B, H, T, D)\n        new_projection = jnp.transpose(new_projection, (0, 2, 1, 3))\n        return new_projection\n\n    def __call__(self, hidden_states):\n        residual = hidden_states\n        batch, height, width, channels = hidden_states.shape\n\n        hidden_states = self.gro",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 201, "column": 0 },
          "end": { "row": 201, "column": 0 }
        }
      }
    }
  ],
  [
    "2297",
    {
      "pageContent": "def setup(self):\n        self.num_heads = self.channels // self.num_head_channels if self.num_head_channels is not None else 1\n\n        dense = partial(nn.Dense, self.channels, dtype=self.dtype)\n\n        self.group_norm = nn.GroupNorm(num_groups=self.num_groups, epsilon=1e-6)\n        self.query, self.key, self.value = dense(), dense(), dense()\n        self.proj_attn = dense()",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 221, "column": 4 },
          "end": { "row": 221, "column": 4 }
        }
      }
    }
  ],
  [
    "2298",
    {
      "pageContent": "def transpose_for_scores(self, projection):\n        new_projection_shape = projection.shape[:-1] + (self.num_heads, -1)\n        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D)\n        new_projection = projection.reshape(new_projection_shape)\n        # (B, T, H, D) -> (B, H, T, D)\n        new_projection = jnp.transpose(new_projection, (0, 2, 1, 3))\n        return new_projection",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 230, "column": 4 },
          "end": { "row": 230, "column": 4 }
        }
      }
    }
  ],
  [
    "2299",
    {
      "pageContent": "def __call__(self, hidden_states):\n        residual = hidden_states\n        batch, height, width, channels = hidden_states.shape\n\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.reshape((batch, height * width, channels))\n\n        query = self.query(hidden_states)\n        key = self.key(hidden_states)\n        value = self.value(hidden_states)\n\n        # transpose\n        query = self.transpose_for_scores(query)\n        key = self.transpose_for_scores(key)\n        value = self.transpose_for_scores(value)\n\n        # compute attentions\n        scale = 1 / math.sqrt(math.sqrt(self.channels / self.num_heads))\n        attn_weights = jnp.einsum(\"...qc,...kc->...qk\", query * scale, key * scale)\n        attn_weights = nn.softmax(attn_weights, axis=-1)\n\n        # attend to values\n        hidden_states = jnp.einsum(\"...kc,...qk->...qc\", value, attn_weights)\n\n        hidden_states = jnp.transpose(hidden_states, (0, 2, 1, 3))\n        new_hidden_states_shape = hidden_states.shape[:-2] + (self.channels,)\n        hidden_states = hidden_states.reshape(new_hidden_states_shape)\n\n        hidden_states = self.proj_attn(hidden_states)\n        hidden_states = hidden_states.reshape((batch, height, width, channels))\n        hidden_states = hidden_states + residual\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 238, "column": 4 },
          "end": { "row": 238, "column": 4 }
        }
      }
    }
  ],
  [
    "2300",
    {
      "pageContent": "class FlaxDownEncoderBlock2D(nn.Module):\n    r\"\"\"\n    Flax Resnet blocks-based Encoder block for diffusion-based VAE.\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        out_channels (:obj:`int`):\n            Output channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of Resnet layer block\n        resnet_groups (:obj:`int`, *optional*, defaults to `32`):\n            The number of groups to use for the Resnet block group norm\n        add_downsample (:obj:`bool`, *optional*, defaults to `True`):\n            Whether to add downsample layer\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    out_channels: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    resnet_groups: int = 32\n    add_downsample: bool = True\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        resnets = []\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout=self.dropout,\n                groups=self.resnet_groups,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n        self.resnets = resnets\n\n        if self.add_downsample:\n            self.downsamplers_0 = FlaxDownsample2D(self.out_channels, dtype=self.",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 273, "column": 0 },
          "end": { "row": 273, "column": 0 }
        }
      }
    }
  ],
  [
    "2301",
    {
      "pageContent": "def setup(self):\n        resnets = []\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout=self.dropout,\n                groups=self.resnet_groups,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n        self.resnets = resnets\n\n        if self.add_downsample:\n            self.downsamplers_0 = FlaxDownsample2D(self.out_channels, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 301, "column": 4 },
          "end": { "row": 301, "column": 4 }
        }
      }
    }
  ],
  [
    "2302",
    {
      "pageContent": "def __call__(self, hidden_states, deterministic=True):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, deterministic=deterministic)\n\n        if self.add_downsample:\n            hidden_states = self.downsamplers_0(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 319, "column": 4 },
          "end": { "row": 319, "column": 4 }
        }
      }
    }
  ],
  [
    "2303",
    {
      "pageContent": "class FlaxUpDecoderBlock2D(nn.Module):\n    r\"\"\"\n    Flax Resnet blocks-based Decoder block for diffusion-based VAE.\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        out_channels (:obj:`int`):\n            Output channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of Resnet layer block\n        resnet_groups (:obj:`int`, *optional*, defaults to `32`):\n            The number of groups to use for the Resnet block group norm\n        add_upsample (:obj:`bool`, *optional*, defaults to `True`):\n            Whether to add upsample layer\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    out_channels: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    resnet_groups: int = 32\n    add_upsample: bool = True\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        resnets = []\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout=self.dropout,\n                groups=self.resnet_groups,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n        self.resnets = resnets\n\n        if self.add_upsample:\n            self.upsamplers_0 = FlaxUpsample2D(self.out_channels, dtype=self.dtype)\n\n    de",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 329, "column": 0 },
          "end": { "row": 329, "column": 0 }
        }
      }
    }
  ],
  [
    "2304",
    {
      "pageContent": "def setup(self):\n        resnets = []\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout=self.dropout,\n                groups=self.resnet_groups,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n        self.resnets = resnets\n\n        if self.add_upsample:\n            self.upsamplers_0 = FlaxUpsample2D(self.out_channels, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 357, "column": 4 },
          "end": { "row": 357, "column": 4 }
        }
      }
    }
  ],
  [
    "2305",
    {
      "pageContent": "def __call__(self, hidden_states, deterministic=True):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, deterministic=deterministic)\n\n        if self.add_upsample:\n            hidden_states = self.upsamplers_0(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 375, "column": 4 },
          "end": { "row": 375, "column": 4 }
        }
      }
    }
  ],
  [
    "2306",
    {
      "pageContent": "class FlaxUNetMidBlock2D(nn.Module):\n    r\"\"\"\n    Flax Unet Mid-Block module.\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of Resnet layer block\n        resnet_groups (:obj:`int`, *optional*, defaults to `32`):\n            The number of groups to use for the Resnet and Attention block group norm\n        attn_num_head_channels (:obj:`int`, *optional*, defaults to `1`):\n            Number of attention heads for each attention block\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    resnet_groups: int = 32\n    attn_num_head_channels: int = 1\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        resnet_groups = self.resnet_groups if self.resnet_groups is not None else min(self.in_channels // 4, 32)\n\n        # there is always at least one resnet\n        resnets = [\n            FlaxResnetBlock2D(\n                in_channels=self.in_channels,\n                out_channels=self.in_channels,\n                dropout=self.dropout,\n                groups=resnet_groups,\n                dtype=self.dtype,\n            )\n        ]\n\n        attentions = []\n\n        for _ in range(self.num_layers):\n            attn_block = FlaxAttentionBlock(\n                channels=self.in_channels,\n                num_head_channels=self.attn_num_head_channels,\n           ",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 385, "column": 0 },
          "end": { "row": 385, "column": 0 }
        }
      }
    }
  ],
  [
    "2307",
    {
      "pageContent": "def setup(self):\n        resnet_groups = self.resnet_groups if self.resnet_groups is not None else min(self.in_channels // 4, 32)\n\n        # there is always at least one resnet\n        resnets = [\n            FlaxResnetBlock2D(\n                in_channels=self.in_channels,\n                out_channels=self.in_channels,\n                dropout=self.dropout,\n                groups=resnet_groups,\n                dtype=self.dtype,\n            )\n        ]\n\n        attentions = []\n\n        for _ in range(self.num_layers):\n            attn_block = FlaxAttentionBlock(\n                channels=self.in_channels,\n                num_head_channels=self.attn_num_head_channels,\n                num_groups=resnet_groups,\n                dtype=self.dtype,\n            )\n            attentions.append(attn_block)\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=self.in_channels,\n                out_channels=self.in_channels,\n                dropout=self.dropout,\n                groups=resnet_groups,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n        self.resnets = resnets\n        self.attentions = attentions",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 410, "column": 4 },
          "end": { "row": 410, "column": 4 }
        }
      }
    }
  ],
  [
    "2308",
    {
      "pageContent": "def __call__(self, hidden_states, deterministic=True):\n        hidden_states = self.resnets[0](hidden_states, deterministic=deterministic)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            hidden_states = attn(hidden_states)\n            hidden_states = resnet(hidden_states, deterministic=deterministic)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 447, "column": 4 },
          "end": { "row": 447, "column": 4 }
        }
      }
    }
  ],
  [
    "2309",
    {
      "pageContent": "class FlaxEncoder(nn.Module):\n    r\"\"\"\n    Flax Implementation of VAE Encoder.\n\n    This model is a Flax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)\n    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to\n    general usage and behavior.\n\n    Finally, this model supports inherent JAX features such as:\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\n    Parameters:\n        in_channels (:obj:`int`, *optional*, defaults to 3):\n            Input channels\n        out_channels (:obj:`int`, *optional*, defaults to 3):\n            Output channels\n        down_block_types (:obj:`Tuple[str]`, *optional*, defaults to `(DownEncoderBlock2D)`):\n            DownEncoder block type\n        block_out_channels (:obj:`Tuple[str]`, *optional*, defaults to `(64,)`):\n            Tuple containing the number of output channels for each block\n        layers_per_block (:obj:`int`, *optional*, defaults to `2`):\n            Number of Resnet layer for each block\n        norm_num_groups (:obj:`int`, *optional*, defaults to `32`):\n            norm num group\n        act_fn (:obj:`str`, *optional*, defaults to `silu`):\n            Activation function\n        d",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 456, "column": 0 },
          "end": { "row": 456, "column": 0 }
        }
      }
    }
  ],
  [
    "2310",
    {
      "pageContent": "def setup(self):\n        block_out_channels = self.block_out_channels\n        # in\n        self.conv_in = nn.Conv(\n            block_out_channels[0],\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        # downsampling\n        down_blocks = []\n        output_channel = block_out_channels[0]\n        for i, _ in enumerate(self.down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n            is_final_block = i == len(block_out_channels) - 1\n\n            down_block = FlaxDownEncoderBlock2D(\n                in_channels=input_channel,\n                out_channels=output_channel,\n                num_layers=self.layers_per_block,\n                resnet_groups=self.norm_num_groups,\n                add_downsample=not is_final_block,\n                dtype=self.dtype,\n            )\n            down_blocks.append(down_block)\n        self.down_blocks = down_blocks\n\n        # middle\n        self.mid_block = FlaxUNetMidBlock2D(\n            in_channels=block_out_channels[-1],\n            resnet_groups=self.norm_num_groups,\n            attn_num_head_channels=None,\n            dtype=self.dtype,\n        )\n\n        # end\n        conv_out_channels = 2 * self.out_channels if self.double_z else self.out_channels\n        self.conv_norm_out = nn.GroupNorm(num_groups=self.norm_num_groups, epsilon=1e-6)\n        self.conv_out = nn.Conv(\n            conv_out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1)",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 500, "column": 4 },
          "end": { "row": 500, "column": 4 }
        }
      }
    }
  ],
  [
    "2311",
    {
      "pageContent": "def __call__(self, sample, deterministic: bool = True):\n        # in\n        sample = self.conv_in(sample)\n\n        # downsampling\n        for block in self.down_blocks:\n            sample = block(sample, deterministic=deterministic)\n\n        # middle\n        sample = self.mid_block(sample, deterministic=deterministic)\n\n        # end\n        sample = self.conv_norm_out(sample)\n        sample = nn.swish(sample)\n        sample = self.conv_out(sample)\n\n        return sample",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 549, "column": 4 },
          "end": { "row": 549, "column": 4 }
        }
      }
    }
  ],
  [
    "2312",
    {
      "pageContent": "class FlaxDecoder(nn.Module):\n    r\"\"\"\n    Flax Implementation of VAE Decoder.\n\n    This model is a Flax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)\n    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to\n    general usage and behavior.\n\n    Finally, this model supports inherent JAX features such as:\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\n    Parameters:\n        in_channels (:obj:`int`, *optional*, defaults to 3):\n            Input channels\n        out_channels (:obj:`int`, *optional*, defaults to 3):\n            Output channels\n        up_block_types (:obj:`Tuple[str]`, *optional*, defaults to `(UpDecoderBlock2D)`):\n            UpDecoder block type\n        block_out_channels (:obj:`Tuple[str]`, *optional*, defaults to `(64,)`):\n            Tuple containing the number of output channels for each block\n        layers_per_block (:obj:`int`, *optional*, defaults to `2`):\n            Number of Resnet layer for each block\n        norm_num_groups (:obj:`int`, *optional*, defaults to `32`):\n            norm num group\n        act_fn (:obj:`str`, *optional*, defaults to `silu`):\n            Activation function\n        double_",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 568, "column": 0 },
          "end": { "row": 568, "column": 0 }
        }
      }
    }
  ],
  [
    "2313",
    {
      "pageContent": "def setup(self):\n        block_out_channels = self.block_out_channels\n\n        # z to block_in\n        self.conv_in = nn.Conv(\n            block_out_channels[-1],\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        # middle\n        self.mid_block = FlaxUNetMidBlock2D(\n            in_channels=block_out_channels[-1],\n            resnet_groups=self.norm_num_groups,\n            attn_num_head_channels=None,\n            dtype=self.dtype,\n        )\n\n        # upsampling\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        output_channel = reversed_block_out_channels[0]\n        up_blocks = []\n        for i, _ in enumerate(self.up_block_types):\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n\n            is_final_block = i == len(block_out_channels) - 1\n\n            up_block = FlaxUpDecoderBlock2D(\n                in_channels=prev_output_channel,\n                out_channels=output_channel,\n                num_layers=self.layers_per_block + 1,\n                resnet_groups=self.norm_num_groups,\n                add_upsample=not is_final_block,\n                dtype=self.dtype,\n            )\n            up_blocks.append(up_block)\n            prev_output_channel = output_channel\n\n        self.up_blocks = up_blocks\n\n        # end\n        self.conv_norm_out = nn.GroupNorm(num_groups=self.norm_num_groups, epsilon=1e-6)\n        self.conv_out = nn.Conv(\n            self.out_channels,\n            kernel_size=(3",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 611, "column": 4 },
          "end": { "row": 611, "column": 4 }
        }
      }
    }
  ],
  [
    "2314",
    {
      "pageContent": "def __call__(self, sample, deterministic: bool = True):\n        # z to block_in\n        sample = self.conv_in(sample)\n\n        # middle\n        sample = self.mid_block(sample, deterministic=deterministic)\n\n        # upsampling\n        for block in self.up_blocks:\n            sample = block(sample, deterministic=deterministic)\n\n        sample = self.conv_norm_out(sample)\n        sample = nn.swish(sample)\n        sample = self.conv_out(sample)\n\n        return sample",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 664, "column": 4 },
          "end": { "row": 664, "column": 4 }
        }
      }
    }
  ],
  [
    "2315",
    {
      "pageContent": "class FlaxDiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        # Last axis to account for channels-last\n        self.mean, self.logvar = jnp.split(parameters, 2, axis=-1)\n        self.logvar = jnp.clip(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = jnp.exp(0.5 * self.logvar)\n        self.var = jnp.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = jnp.zeros_like(self.mean)\n\n    def sample(self, key):\n        return self.mean + self.std * jax.random.normal(key, self.mean.shape)\n\n    def kl(self, other=None):\n        if self.deterministic:\n            return jnp.array([0.0])\n\n        if other is None:\n            return 0.5 * jnp.sum(self.mean**2 + self.var - 1.0 - self.logvar, axis=[1, 2, 3])\n\n        return 0.5 * jnp.sum(\n            jnp.square(self.mean - other.mean) / other.var + self.var / other.var - 1.0 - self.logvar + other.logvar,\n            axis=[1, 2, 3],\n        )\n\n    def nll(self, sample, axis=[1, 2, 3]):\n        if self.deterministic:\n            return jnp.array([0.0])\n\n        logtwopi = jnp.log(2.0 * jnp.pi)\n        return 0.5 * jnp.sum(logtwopi + self.logvar + jnp.square(sample - self.mean) / self.var, axis=axis)\n\n    def mode(self):\n        return self.mean",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 682, "column": 0 },
          "end": { "row": 682, "column": 0 }
        }
      }
    }
  ],
  [
    "2316",
    {
      "pageContent": "def __init__(self, parameters, deterministic=False):\n        # Last axis to account for channels-last\n        self.mean, self.logvar = jnp.split(parameters, 2, axis=-1)\n        self.logvar = jnp.clip(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = jnp.exp(0.5 * self.logvar)\n        self.var = jnp.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = jnp.zeros_like(self.mean)",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 683, "column": 4 },
          "end": { "row": 683, "column": 4 }
        }
      }
    }
  ],
  [
    "2317",
    {
      "pageContent": "def kl(self, other=None):\n        if self.deterministic:\n            return jnp.array([0.0])\n\n        if other is None:\n            return 0.5 * jnp.sum(self.mean**2 + self.var - 1.0 - self.logvar, axis=[1, 2, 3])\n\n        return 0.5 * jnp.sum(\n            jnp.square(self.mean - other.mean) / other.var + self.var / other.var - 1.0 - self.logvar + other.logvar,\n            axis=[1, 2, 3],\n        )",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 696, "column": 4 },
          "end": { "row": 696, "column": 4 }
        }
      }
    }
  ],
  [
    "2318",
    {
      "pageContent": "def nll(self, sample, axis=[1, 2, 3]):\n        if self.deterministic:\n            return jnp.array([0.0])\n\n        logtwopi = jnp.log(2.0 * jnp.pi)\n        return 0.5 * jnp.sum(logtwopi + self.logvar + jnp.square(sample - self.mean) / self.var, axis=axis)",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 708, "column": 4 },
          "end": { "row": 708, "column": 4 }
        }
      }
    }
  ],
  [
    "2319",
    {
      "pageContent": "class FlaxAutoencoderKL(nn.Module, FlaxModelMixin, ConfigMixin):\n    r\"\"\"\n    Flax Implementation of Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational\n    Bayes by Diederik P. Kingma and Max Welling.\n\n    This model is a Flax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)\n    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to\n    general usage and behavior.\n\n    Finally, this model supports inherent JAX features such as:\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\n    Parameters:\n        in_channels (:obj:`int`, *optional*, defaults to 3):\n            Input channels\n        out_channels (:obj:`int`, *optional*, defaults to 3):\n            Output channels\n        down_block_types (:obj:`Tuple[str]`, *optional*, defaults to `(DownEncoderBlock2D)`):\n            DownEncoder block type\n        up_block_types (:obj:`Tuple[str]`, *optional*, defaults to `(UpDecoderBlock2D)`):\n            UpDecoder block type\n        block_out_channels (:obj:`Tuple[str]`, *optional*, defaults to `(64,)`):\n            Tuple containing the number of output channels for each block\n        layers_per_block (:obj:",
      "metadata": {
        "source": "src/diffusers/models/vae_flax.py",
        "range": {
          "start": { "row": 720, "column": 0 },
          "end": { "row": 720, "column": 0 }
        }
      }
    }
  ],
  [
    "2320",
    {
      "pageContent": "def get_parameter_device(parameter: torch.nn.Module):\n    try:\n        return next(parameter.parameters()).device\n    except StopIteration:\n        # For torch.nn.DataParallel compatibility in PyTorch 1.5\n\n        def find_tensor_attributes(module: torch.nn.Module) -> List[Tuple[str, Tensor]]:\n            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n            return tuples\n\n        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n        first_tuple = next(gen)\n        return first_tuple[1].device",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 64, "column": 0 },
          "end": { "row": 64, "column": 0 }
        }
      }
    }
  ],
  [
    "2321",
    {
      "pageContent": "def get_parameter_dtype(parameter: torch.nn.Module):\n    try:\n        return next(parameter.parameters()).dtype\n    except StopIteration:\n        # For torch.nn.DataParallel compatibility in PyTorch 1.5\n\n        def find_tensor_attributes(module: torch.nn.Module) -> List[Tuple[str, Tensor]]:\n            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n            return tuples\n\n        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n        first_tuple = next(gen)\n        return first_tuple[1].dtype",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "2322",
    {
      "pageContent": "def load_state_dict(checkpoint_file: Union[str, os.PathLike], variant: Optional[str] = None):\n    \"\"\"\n    Reads a checkpoint file, returning properly formatted errors if they arise.\n    \"\"\"\n    try:\n        if os.path.basename(checkpoint_file) == _add_variant(WEIGHTS_NAME, variant):\n            return torch.load(checkpoint_file, map_location=\"cpu\")\n        else:\n            return safetensors.torch.load_file(checkpoint_file, device=\"cpu\")\n    except Exception as e:\n        try:\n            with open(checkpoint_file) as f:\n                if f.read().startswith(\"version\"):\n                    raise OSError(\n                        \"You seem to have cloned a repository without having git-lfs installed. Please install \"\n                        \"git-lfs and run `git lfs install` followed by `git lfs pull` in the folder \"\n                        \"you cloned.\"\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unable to locate the file {checkpoint_file} which is necessary to load this pretrained \"\n                        \"model. Make sure you have saved the model properly.\"\n                    ) from e\n        except (UnicodeDecodeError, ValueError):\n            raise OSError(\n                f\"Unable to load weights from checkpoint file for '{checkpoint_file}' \"\n                f\"at '{checkpoint_file}'. \"\n                \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\n            )",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 94, "column": 0 },
          "end": { "row": 94, "column": 0 }
        }
      }
    }
  ],
  [
    "2323",
    {
      "pageContent": "def _load_state_dict_into_model(model_to_load, state_dict):\n    # Convert old format to new format if needed from a PyTorch state_dict\n    # copy state_dict so _load_from_state_dict can modify it\n    state_dict = state_dict.copy()\n    error_msgs = []\n\n    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n    # so we need to apply the function recursively.\n    def load(module: torch.nn.Module, prefix=\"\"):\n        args = (state_dict, prefix, {}, True, [], [], error_msgs)\n        module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    load(model_to_load)\n\n    return error_msgs",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 125, "column": 0 },
          "end": { "row": 125, "column": 0 }
        }
      }
    }
  ],
  [
    "2324",
    {
      "pageContent": "def load(module: torch.nn.Module, prefix=\"\"):\n        args = (state_dict, prefix, {}, True, [], [], error_msgs)\n        module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 133, "column": 4 },
          "end": { "row": 133, "column": 4 }
        }
      }
    }
  ],
  [
    "2325",
    {
      "pageContent": "def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n    if variant is not None:\n        splits = weights_name.split(\".\")\n        splits = splits[:-1] + [variant] + splits[-1:]\n        weights_name = \".\".join(splits)\n\n    return weights_name",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 146, "column": 0 },
          "end": { "row": 146, "column": 0 }
        }
      }
    }
  ],
  [
    "2326",
    {
      "pageContent": "class ModelMixin(torch.nn.Module):\n    r\"\"\"\n    Base class for all models.\n\n    [`ModelMixin`] takes care of storing the configuration of the models and handles methods for loading, downloading\n    and saving models.\n\n        - **config_name** ([`str`]) -- A filename under which the model should be stored when calling\n          [`~models.ModelMixin.save_pretrained`].\n    \"\"\"\n    config_name = CONFIG_NAME\n    _automatically_saved_args = [\"_diffusers_version\", \"_class_name\", \"_name_or_path\"]\n    _supports_gradient_checkpointing = False\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def is_gradient_checkpointing(self) -> bool:\n        \"\"\"\n        Whether gradient checkpointing is activated for this model or not.\n\n        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n        activations\".\n        \"\"\"\n        return any(hasattr(m, \"gradient_checkpointing\") and m.gradient_checkpointing for m in self.modules())\n\n    def enable_gradient_checkpointing(self):\n        \"\"\"\n        Activates gradient checkpointing for the current model.\n\n        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n        activations\".\n        \"\"\"\n        if not self._supports_gradient_checkpointing:\n            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")\n        self.apply(partial(self._set_gradient_checkpointing, value=True))\n\n    def disable_gradient_checkpointing(self):\n        \"\"\"\n        Deactivates gradient checkpoin",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 155, "column": 0 },
          "end": { "row": 155, "column": 0 }
        }
      }
    }
  ],
  [
    "2327",
    {
      "pageContent": "def enable_gradient_checkpointing(self):\n        \"\"\"\n        Activates gradient checkpointing for the current model.\n\n        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n        activations\".\n        \"\"\"\n        if not self._supports_gradient_checkpointing:\n            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")\n        self.apply(partial(self._set_gradient_checkpointing, value=True))",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 182, "column": 4 },
          "end": { "row": 182, "column": 4 }
        }
      }
    }
  ],
  [
    "2328",
    {
      "pageContent": "def disable_gradient_checkpointing(self):\n        \"\"\"\n        Deactivates gradient checkpointing for the current model.\n\n        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n        activations\".\n        \"\"\"\n        if self._supports_gradient_checkpointing:\n            self.apply(partial(self._set_gradient_checkpointing, value=False))",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 193, "column": 4 },
          "end": { "row": 193, "column": 4 }
        }
      }
    }
  ],
  [
    "2329",
    {
      "pageContent": "def set_use_memory_efficient_attention_xformers(\n        self, valid: bool, attention_op: Optional[Callable] = None\n    ) -> None:\n        # Recursively walk through all the children.\n        # Any children which exposes the set_use_memory_efficient_attention_xformers method\n        # gets the message\n        def fn_recursive_set_mem_eff(module: torch.nn.Module):\n            if hasattr(module, \"set_use_memory_efficient_attention_xformers\"):\n                module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n\n            for child in module.children():\n                fn_recursive_set_mem_eff(child)\n\n        for module in self.children():\n            if isinstance(module, torch.nn.Module):\n                fn_recursive_set_mem_eff(module)",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 203, "column": 4 },
          "end": { "row": 203, "column": 4 }
        }
      }
    }
  ],
  [
    "2330",
    {
      "pageContent": "def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):\n        r\"\"\"\n        Enable memory efficient attention as implemented in xformers.\n\n        When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n        time. Speed up at training time is not guaranteed.\n\n        Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n        is used.\n\n        Parameters:\n            attention_op (`Callable`, *optional*):\n                Override the default `None` operator for use as `op` argument to the\n                [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n                function of xFormers.\n\n        Examples:\n\n        ```py\n        >>> import torch\n        >>> from diffusers import UNet2DConditionModel\n        >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n        >>> model = UNet2DConditionModel.from_pretrained(\n        ...     \"stabilityai/stable-diffusion-2-1\", subfolder=\"unet\", torch_dtype=torch.float16\n        ... )\n        >>> model = model.to(\"cuda\")\n        >>> model.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n        ```\n        \"\"\"\n        self.set_use_memory_efficient_attention_xformers(True, attention_op)",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 220, "column": 4 },
          "end": { "row": 220, "column": 4 }
        }
      }
    }
  ],
  [
    "2331",
    {
      "pageContent": "def disable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Disable memory efficient attention as implemented in xformers.\n        \"\"\"\n        self.set_use_memory_efficient_attention_xformers(False)",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 252, "column": 4 },
          "end": { "row": 252, "column": 4 }
        }
      }
    }
  ],
  [
    "2332",
    {
      "pageContent": "def save_pretrained(\n        self,\n        save_directory: Union[str, os.PathLike],\n        is_main_process: bool = True,\n        save_function: Callable = None,\n        safe_serialization: bool = False,\n        variant: Optional[str] = None,\n    ):\n        \"\"\"\n        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n        `[`~models.ModelMixin.from_pretrained`]` class method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n            is_main_process (`bool`, *optional*, defaults to `True`):\n                Whether the process calling this is the main process or not. Useful when in distributed training like\n                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n                the main process to avoid race conditions.\n            save_function (`Callable`):\n                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n                need to replace `torch.save` by another method. Can be configured with the environment variable\n                `DIFFUSERS_SAVE_MODE`.\n            safe_serialization (`bool`, *optional*, defaults to `False`):\n                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n            variant (`str`, *optional*):\n                If specified, weights are saved in the format pytorch_model.<variant>.bin.\n        \"\"\"\n        if sa",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 258, "column": 4 },
          "end": { "row": 258, "column": 4 }
        }
      }
    }
  ],
  [
    "2333",
    {
      "pageContent": "def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n        \"\"\"\n        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\n        Args:\n            only_trainable (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of trainable parameters\n\n            exclude_embeddings (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of non-embeddings parameters\n\n        Returns:\n            `int`: The number of parameters.\n        \"\"\"\n\n        if exclude_embeddings:\n            embedding_param_names = [\n                f\"{name}.weight\"\n                for name, module_type in self.named_modules()\n                if isinstance(module_type, torch.nn.Embedding)\n            ]\n            non_embedding_parameters = [\n                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n            ]\n            return sum(p.numel() for p in non_embedding_parameters if p.requires_grad or not only_trainable)\n        else:\n            return sum(p.numel() for p in self.parameters() if p.requires_grad or not only_trainable)",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 740, "column": 4 },
          "end": { "row": 740, "column": 4 }
        }
      }
    }
  ],
  [
    "2334",
    {
      "pageContent": "def _get_model_file(\n    pretrained_model_name_or_path,\n    *,\n    weights_name,\n    subfolder,\n    cache_dir,\n    force_download,\n    proxies,\n    resume_download,\n    local_files_only,\n    use_auth_token,\n    user_agent,\n    revision,\n    commit_hash=None,\n):\n    pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n    if os.path.isfile(pretrained_model_name_or_path):\n        return pretrained_model_name_or_path\n    elif os.path.isdir(pretrained_model_name_or_path):\n        if os.path.isfile(os.path.join(pretrained_model_name_or_path, weights_name)):\n            # Load from a PyTorch checkpoint\n            model_file = os.path.join(pretrained_model_name_or_path, weights_name)\n            return model_file\n        elif subfolder is not None and os.path.isfile(\n            os.path.join(pretrained_model_name_or_path, subfolder, weights_name)\n        ):\n            model_file = os.path.join(pretrained_model_name_or_path, subfolder, weights_name)\n            return model_file\n        else:\n            raise EnvironmentError(\n                f\"Error no file named {weights_name} found in directory {pretrained_model_name_or_path}.\"\n            )\n    else:\n        # 1. First check if deprecated way of loading from branches is used\n        if (\n            revision in DEPRECATED_REVISION_ARGS\n            and (weights_name == WEIGHTS_NAME or weights_name == SAFETENSORS_WEIGHTS_NAME)\n            and version.parse(version.parse(__version__).base_version) >= version.parse(\"0.17.0\")\n        ):\n            try:\n                model_file = hf_hub_download(\n                 ",
      "metadata": {
        "source": "src/diffusers/models/modeling_utils.py",
        "range": {
          "start": { "row": 769, "column": 0 },
          "end": { "row": 769, "column": 0 }
        }
      }
    }
  ],
  [
    "2335",
    {
      "pageContent": "class DecoderOutput(BaseOutput):\n    \"\"\"\n    Output of decoding method.\n\n    Args:\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            Decoded output sample of the model. Output of the last layer of the model.\n    \"\"\"\n\n    sample: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "2336",
    {
      "pageContent": "class Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels=3,\n        out_channels=3,\n        down_block_types=(\"DownEncoderBlock2D\",),\n        block_out_channels=(64,),\n        layers_per_block=2,\n        norm_num_groups=32,\n        act_fn=\"silu\",\n        double_z=True,\n    ):\n        super().__init__()\n        self.layers_per_block = layers_per_block\n\n        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)\n\n        self.mid_block = None\n        self.down_blocks = nn.ModuleList([])\n\n        # down\n        output_channel = block_out_channels[0]\n        for i, down_block_type in enumerate(down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n            is_final_block = i == len(block_out_channels) - 1\n\n            down_block = get_down_block(\n                down_block_type,\n                num_layers=self.layers_per_block,\n                in_channels=input_channel,\n                out_channels=output_channel,\n                add_downsample=not is_final_block,\n                resnet_eps=1e-6,\n                downsample_padding=0,\n                resnet_act_fn=act_fn,\n                resnet_groups=norm_num_groups,\n                attn_num_head_channels=None,\n                temb_channels=None,\n            )\n            self.down_blocks.append(down_block)\n\n        # mid\n        self.mid_block = UNetMidBlock2D(\n            in_channels=block_out_channels[-1],\n            resnet_eps=1e-6,\n            resnet_act_fn=act_fn,\n            output_scale_f",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "2337",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels=3,\n        out_channels=3,\n        down_block_types=(\"DownEncoderBlock2D\",),\n        block_out_channels=(64,),\n        layers_per_block=2,\n        norm_num_groups=32,\n        act_fn=\"silu\",\n        double_z=True,\n    ):\n        super().__init__()\n        self.layers_per_block = layers_per_block\n\n        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)\n\n        self.mid_block = None\n        self.down_blocks = nn.ModuleList([])\n\n        # down\n        output_channel = block_out_channels[0]\n        for i, down_block_type in enumerate(down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n            is_final_block = i == len(block_out_channels) - 1\n\n            down_block = get_down_block(\n                down_block_type,\n                num_layers=self.layers_per_block,\n                in_channels=input_channel,\n                out_channels=output_channel,\n                add_downsample=not is_final_block,\n                resnet_eps=1e-6,\n                downsample_padding=0,\n                resnet_act_fn=act_fn,\n                resnet_groups=norm_num_groups,\n                attn_num_head_channels=None,\n                temb_channels=None,\n            )\n            self.down_blocks.append(down_block)\n\n        # mid\n        self.mid_block = UNetMidBlock2D(\n            in_channels=block_out_channels[-1],\n            resnet_eps=1e-6,\n            resnet_act_fn=act_fn,\n            output_scale_factor=1,\n            resnet_ti",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 38, "column": 4 },
          "end": { "row": 38, "column": 4 }
        }
      }
    }
  ],
  [
    "2338",
    {
      "pageContent": "def forward(self, x):\n        sample = x\n        sample = self.conv_in(sample)\n\n        # down\n        for down_block in self.down_blocks:\n            sample = down_block(sample)\n\n        # middle\n        sample = self.mid_block(sample)\n\n        # post-process\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_act(sample)\n        sample = self.conv_out(sample)\n\n        return sample",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 98, "column": 4 },
          "end": { "row": 98, "column": 4 }
        }
      }
    }
  ],
  [
    "2339",
    {
      "pageContent": "class Decoder(nn.Module):\n    def __init__(\n        self,\n        in_channels=3,\n        out_channels=3,\n        up_block_types=(\"UpDecoderBlock2D\",),\n        block_out_channels=(64,),\n        layers_per_block=2,\n        norm_num_groups=32,\n        act_fn=\"silu\",\n    ):\n        super().__init__()\n        self.layers_per_block = layers_per_block\n\n        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)\n\n        self.mid_block = None\n        self.up_blocks = nn.ModuleList([])\n\n        # mid\n        self.mid_block = UNetMidBlock2D(\n            in_channels=block_out_channels[-1],\n            resnet_eps=1e-6,\n            resnet_act_fn=act_fn,\n            output_scale_factor=1,\n            resnet_time_scale_shift=\"default\",\n            attn_num_head_channels=None,\n            resnet_groups=norm_num_groups,\n            temb_channels=None,\n        )\n\n        # up\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        output_channel = reversed_block_out_channels[0]\n        for i, up_block_type in enumerate(up_block_types):\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n\n            is_final_block = i == len(block_out_channels) - 1\n\n            up_block = get_up_block(\n                up_block_type,\n                num_layers=self.layers_per_block + 1,\n                in_channels=prev_output_channel,\n                out_channels=output_channel,\n                prev_output_channel=None,\n                add_upsample=not is_final_block,\n               ",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 117, "column": 0 },
          "end": { "row": 117, "column": 0 }
        }
      }
    }
  ],
  [
    "2340",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels=3,\n        out_channels=3,\n        up_block_types=(\"UpDecoderBlock2D\",),\n        block_out_channels=(64,),\n        layers_per_block=2,\n        norm_num_groups=32,\n        act_fn=\"silu\",\n    ):\n        super().__init__()\n        self.layers_per_block = layers_per_block\n\n        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)\n\n        self.mid_block = None\n        self.up_blocks = nn.ModuleList([])\n\n        # mid\n        self.mid_block = UNetMidBlock2D(\n            in_channels=block_out_channels[-1],\n            resnet_eps=1e-6,\n            resnet_act_fn=act_fn,\n            output_scale_factor=1,\n            resnet_time_scale_shift=\"default\",\n            attn_num_head_channels=None,\n            resnet_groups=norm_num_groups,\n            temb_channels=None,\n        )\n\n        # up\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        output_channel = reversed_block_out_channels[0]\n        for i, up_block_type in enumerate(up_block_types):\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n\n            is_final_block = i == len(block_out_channels) - 1\n\n            up_block = get_up_block(\n                up_block_type,\n                num_layers=self.layers_per_block + 1,\n                in_channels=prev_output_channel,\n                out_channels=output_channel,\n                prev_output_channel=None,\n                add_upsample=not is_final_block,\n                resnet_eps=1e-6,\n            ",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 118, "column": 4 },
          "end": { "row": 118, "column": 4 }
        }
      }
    }
  ],
  [
    "2341",
    {
      "pageContent": "def forward(self, z):\n        sample = z\n        sample = self.conv_in(sample)\n\n        # middle\n        sample = self.mid_block(sample)\n\n        # up\n        for up_block in self.up_blocks:\n            sample = up_block(sample)\n\n        # post-process\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_act(sample)\n        sample = self.conv_out(sample)\n\n        return sample",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 178, "column": 4 },
          "end": { "row": 178, "column": 4 }
        }
      }
    }
  ],
  [
    "2342",
    {
      "pageContent": "class VectorQuantizer(nn.Module):\n    \"\"\"\n    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly avoids costly matrix\n    multiplications and allows for post-hoc remapping of indices.\n    \"\"\"\n\n    # NOTE: due to a bug the beta term was applied to the wrong term. for\n    # backwards compatibility we use the buggy version by default, but you can\n    # specify legacy=False to fix it.\n    def __init__(\n        self, n_e, vq_embed_dim, beta, remap=None, unknown_index=\"random\", sane_index_shape=False, legacy=True\n    ):\n        super().__init__()\n        self.n_e = n_e\n        self.vq_embed_dim = vq_embed_dim\n        self.beta = beta\n        self.legacy = legacy\n\n        self.embedding = nn.Embedding(self.n_e, self.vq_embed_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n\n        self.remap = remap\n        if self.remap is not None:\n            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n            self.re_embed = self.used.shape[0]\n            self.unknown_index = unknown_index  # \"random\" or \"extra\" or integer\n            if self.unknown_index == \"extra\":\n                self.unknown_index = self.re_embed\n                self.re_embed = self.re_embed + 1\n            print(\n                f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n                f\"Using {self.unknown_index} for unknown indices.\"\n            )\n        else:\n            self.re_embed = n_e\n\n        self.sane_index_shape = sane_index_shape\n\n    def remap_to_used(self, inds):\n        ishape = inds.shape\n      ",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 197, "column": 0 },
          "end": { "row": 197, "column": 0 }
        }
      }
    }
  ],
  [
    "2343",
    {
      "pageContent": "def __init__(\n        self, n_e, vq_embed_dim, beta, remap=None, unknown_index=\"random\", sane_index_shape=False, legacy=True\n    ):\n        super().__init__()\n        self.n_e = n_e\n        self.vq_embed_dim = vq_embed_dim\n        self.beta = beta\n        self.legacy = legacy\n\n        self.embedding = nn.Embedding(self.n_e, self.vq_embed_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n\n        self.remap = remap\n        if self.remap is not None:\n            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n            self.re_embed = self.used.shape[0]\n            self.unknown_index = unknown_index  # \"random\" or \"extra\" or integer\n            if self.unknown_index == \"extra\":\n                self.unknown_index = self.re_embed\n                self.re_embed = self.re_embed + 1\n            print(\n                f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n                f\"Using {self.unknown_index} for unknown indices.\"\n            )\n        else:\n            self.re_embed = n_e\n\n        self.sane_index_shape = sane_index_shape",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 206, "column": 4 },
          "end": { "row": 206, "column": 4 }
        }
      }
    }
  ],
  [
    "2344",
    {
      "pageContent": "def remap_to_used(self, inds):\n        ishape = inds.shape\n        assert len(ishape) > 1\n        inds = inds.reshape(ishape[0], -1)\n        used = self.used.to(inds)\n        match = (inds[:, :, None] == used[None, None, ...]).long()\n        new = match.argmax(-1)\n        unknown = match.sum(2) < 1\n        if self.unknown_index == \"random\":\n            new[unknown] = torch.randint(0, self.re_embed, size=new[unknown].shape).to(device=new.device)\n        else:\n            new[unknown] = self.unknown_index\n        return new.reshape(ishape)",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 235, "column": 4 },
          "end": { "row": 235, "column": 4 }
        }
      }
    }
  ],
  [
    "2345",
    {
      "pageContent": "def unmap_to_all(self, inds):\n        ishape = inds.shape\n        assert len(ishape) > 1\n        inds = inds.reshape(ishape[0], -1)\n        used = self.used.to(inds)\n        if self.re_embed > self.used.shape[0]:  # extra token\n            inds[inds >= self.used.shape[0]] = 0  # simply set to zero\n        back = torch.gather(used[None, :][inds.shape[0] * [0], :], 1, inds)\n        return back.reshape(ishape)",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 249, "column": 4 },
          "end": { "row": 249, "column": 4 }
        }
      }
    }
  ],
  [
    "2346",
    {
      "pageContent": "def forward(self, z):\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = z.permute(0, 2, 3, 1).contiguous()\n        z_flattened = z.view(-1, self.vq_embed_dim)\n\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n        min_encoding_indices = torch.argmin(torch.cdist(z_flattened, self.embedding.weight), dim=1)\n\n        z_q = self.embedding(min_encoding_indices).view(z.shape)\n        perplexity = None\n        min_encodings = None\n\n        # compute loss for embedding\n        if not self.legacy:\n            loss = self.beta * torch.mean((z_q.detach() - z) ** 2) + torch.mean((z_q - z.detach()) ** 2)\n        else:\n            loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean((z_q - z.detach()) ** 2)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        if self.remap is not None:\n            min_encoding_indices = min_encoding_indices.reshape(z.shape[0], -1)  # add batch axis\n            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n            min_encoding_indices = min_encoding_indices.reshape(-1, 1)  # flatten\n\n        if self.sane_index_shape:\n            min_encoding_indices = min_encoding_indices.reshape(z_q.shape[0], z_q.shape[2], z_q.shape[3])\n\n        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 259, "column": 4 },
          "end": { "row": 259, "column": 4 }
        }
      }
    }
  ],
  [
    "2347",
    {
      "pageContent": "def get_codebook_entry(self, indices, shape):\n        # shape specifying (batch, height, width, channel)\n        if self.remap is not None:\n            indices = indices.reshape(shape[0], -1)  # add batch axis\n            indices = self.unmap_to_all(indices)\n            indices = indices.reshape(-1)  # flatten again\n\n        # get quantized latent vectors\n        z_q = self.embedding(indices)\n\n        if shape is not None:\n            z_q = z_q.view(shape)\n            # reshape back to match original input shape\n            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 293, "column": 4 },
          "end": { "row": 293, "column": 4 }
        }
      }
    }
  ],
  [
    "2348",
    {
      "pageContent": "class DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(\n                self.mean, device=self.parameters.device, dtype=self.parameters.dtype\n            )\n\n    def sample(self, generator: Optional[torch.Generator] = None) -> torch.FloatTensor:\n        # make sure sample is on the same device as the parameters and has same dtype\n        sample = randn_tensor(\n            self.mean.shape, generator=generator, device=self.parameters.device, dtype=self.parameters.dtype\n        )\n        x = self.mean + self.std * sample\n        return x\n\n    def kl(self, other=None):\n        if self.deterministic:\n            return torch.Tensor([0.0])\n        else:\n            if other is None:\n                return 0.5 * torch.sum(torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar, dim=[1, 2, 3])\n            else:\n                return 0.5 * torch.sum(\n                    torch.pow(self.mean - other.mean, 2) / other.var\n                    + self.var / other.var\n                    - 1.0\n                    - self.logvar\n                    + other.logvar,\n                    dim=[1, 2, 3],\n                )\n\n    def nll(self, sample, dims=[1, 2, 3]):\n        if self.d",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 311, "column": 0 },
          "end": { "row": 311, "column": 0 }
        }
      }
    }
  ],
  [
    "2349",
    {
      "pageContent": "def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(\n                self.mean, device=self.parameters.device, dtype=self.parameters.dtype\n            )",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 312, "column": 4 },
          "end": { "row": 312, "column": 4 }
        }
      }
    }
  ],
  [
    "2350",
    {
      "pageContent": "def sample(self, generator: Optional[torch.Generator] = None) -> torch.FloatTensor:\n        # make sure sample is on the same device as the parameters and has same dtype\n        sample = randn_tensor(\n            self.mean.shape, generator=generator, device=self.parameters.device, dtype=self.parameters.dtype\n        )\n        x = self.mean + self.std * sample\n        return x",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 324, "column": 4 },
          "end": { "row": 324, "column": 4 }
        }
      }
    }
  ],
  [
    "2351",
    {
      "pageContent": "def kl(self, other=None):\n        if self.deterministic:\n            return torch.Tensor([0.0])\n        else:\n            if other is None:\n                return 0.5 * torch.sum(torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar, dim=[1, 2, 3])\n            else:\n                return 0.5 * torch.sum(\n                    torch.pow(self.mean - other.mean, 2) / other.var\n                    + self.var / other.var\n                    - 1.0\n                    - self.logvar\n                    + other.logvar,\n                    dim=[1, 2, 3],\n                )",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 332, "column": 4 },
          "end": { "row": 332, "column": 4 }
        }
      }
    }
  ],
  [
    "2352",
    {
      "pageContent": "def nll(self, sample, dims=[1, 2, 3]):\n        if self.deterministic:\n            return torch.Tensor([0.0])\n        logtwopi = np.log(2.0 * np.pi)\n        return 0.5 * torch.sum(logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var, dim=dims)",
      "metadata": {
        "source": "src/diffusers/models/vae.py",
        "range": {
          "start": { "row": 348, "column": 4 },
          "end": { "row": 348, "column": 4 }
        }
      }
    }
  ],
  [
    "2353",
    {
      "pageContent": "class Transformer2DModelOutput(BaseOutput):\n    \"\"\"\n    Args:\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` or `(batch size, num_vector_embeds - 1, num_latent_pixels)` if [`Transformer2DModel`] is discrete):\n            Hidden states conditioned on `encoder_hidden_states` input. If discrete, returns probability distributions\n            for the unnoised latent pixels.\n    \"\"\"\n\n    sample: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/models/transformer_2d.py",
        "range": {
          "start": { "row": 29, "column": 0 },
          "end": { "row": 29, "column": 0 }
        }
      }
    }
  ],
  [
    "2354",
    {
      "pageContent": "class Transformer2DModel(ModelMixin, ConfigMixin):\n    \"\"\"\n    Transformer model for image-like data. Takes either discrete (classes of vector embeddings) or continuous (actual\n    embeddings) inputs.\n\n    When input is continuous: First, project the input (aka embedding) and reshape to b, t, d. Then apply standard\n    transformer action. Finally, reshape to image.\n\n    When input is discrete: First, input (classes of latent pixels) is converted to embeddings and has positional\n    embeddings applied, see `ImagePositionalEmbeddings`. Then apply standard transformer action. Finally, predict\n    classes of unnoised image.\n\n    Note that it is assumed one of the input classes is the masked latent pixel. The predicted classes of the unnoised\n    image do not contain a prediction for the masked pixel as the unnoised image cannot be masked.\n\n    Parameters:\n        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.\n        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.\n        in_channels (`int`, *optional*):\n            Pass if the input is continuous. The number of channels in the input and output.\n        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.\n        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n        cross_attention_dim (`int`, *optional*): The number of encoder_hidden_states dimensions to use.\n        sample_size (`int`, *optional*): Pass if the input is discrete. The width of ",
      "metadata": {
        "source": "src/diffusers/models/transformer_2d.py",
        "range": {
          "start": { "row": 40, "column": 0 },
          "end": { "row": 40, "column": 0 }
        }
      }
    }
  ],
  [
    "2355",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        encoder_hidden_states=None,\n        timestep=None,\n        class_labels=None,\n        cross_attention_kwargs=None,\n        return_dict: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hidden_states ( When discrete, `torch.LongTensor` of shape `(batch size, num latent pixels)`.\n                When continous, `torch.FloatTensor` of shape `(batch size, channel, height, width)`): Input\n                hidden_states\n            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):\n                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to\n                self-attention.\n            timestep ( `torch.long`, *optional*):\n                Optional timestep to be applied as an embedding in AdaLayerNorm's. Used to indicate denoising step.\n            class_labels ( `torch.LongTensor` of shape `(batch size, num classes)`, *optional*):\n                Optional class labels to be applied as an embedding in AdaLayerZeroNorm. Used to indicate class labels\n                conditioning.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~models.transformer_2d.Transformer2DModelOutput`] or `tuple`:\n            [`~models.transformer_2d.Transformer2DModelOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element",
      "metadata": {
        "source": "src/diffusers/models/transformer_2d.py",
        "range": {
          "start": { "row": 213, "column": 4 },
          "end": { "row": 213, "column": 4 }
        }
      }
    }
  ],
  [
    "2356",
    {
      "pageContent": "class UNet2DConditionOutput(BaseOutput):\n    \"\"\"\n    Args:\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            Hidden states conditioned on `encoder_hidden_states` input. Output of last layer of model.\n    \"\"\"\n\n    sample: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_condition.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "2357",
    {
      "pageContent": "class UNet2DConditionModel(ModelMixin, ConfigMixin, UNet2DConditionLoadersMixin):\n    r\"\"\"\n    UNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a timestep\n    and returns sample shaped output.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the models (such as downloading or saving, etc.)\n\n    Parameters:\n        sample_size (`int` or `Tuple[int, int]`, *optional*, defaults to `None`):\n            Height and width of input/output sample.\n        in_channels (`int`, *optional*, defaults to 4): The number of channels in the input sample.\n        out_channels (`int`, *optional*, defaults to 4): The number of channels in the output.\n        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.\n        flip_sin_to_cos (`bool`, *optional*, defaults to `False`):\n            Whether to flip the sin to cos in the time embedding.\n        freq_shift (`int`, *optional*, defaults to 0): The frequency shift to apply to the time embedding.\n        down_block_types (`Tuple[str]`, *optional*, defaults to `(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\")`):\n            The tuple of downsample blocks to use.\n        mid_block_type (`str`, *optional*, defaults to `\"UNetMidBlock2DCrossAttn\"`):\n            The mid block type. Choose from `UNetMidBlock2DCrossAttn` or `UNetMidBlock2DSimpleCrossAttn`, will skip the\n            mid block layer if `None`.\n        up_block_",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_condition.py",
        "range": {
          "start": { "row": 52, "column": 0 },
          "end": { "row": 52, "column": 0 }
        }
      }
    }
  ],
  [
    "2358",
    {
      "pageContent": "def set_attn_processor(self, processor: Union[AttnProcessor, Dict[str, AttnProcessor]]):\n        r\"\"\"\n        Parameters:\n            `processor (`dict` of `AttnProcessor` or `AttnProcessor`):\n                The instantiated processor class or a dictionary of processor classes that will be set as the processor\n                of **all** `CrossAttention` layers.\n            In case `processor` is a dict, the key needs to define the path to the corresponding cross attention processor. This is strongly recommended when setting trainablae attention processors.:\n\n        \"\"\"\n        count = len(self.attn_processors.keys())\n\n        if isinstance(processor, dict) and len(processor) != count:\n            raise ValueError(\n                f\"A dict of processors was passed, but the number of processors {len(processor)} does not match the\"\n                f\" number of attention layers: {count}. Please make sure to pass {count} processor classes.\"\n            )\n\n        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):\n            if hasattr(module, \"set_processor\"):\n                if not isinstance(processor, dict):\n                    module.set_processor(processor)\n                else:\n                    module.set_processor(processor.pop(f\"{name}.processor\"))\n\n            for sub_name, child in module.named_children():\n                fn_recursive_attn_processor(f\"{name}.{sub_name}\", child, processor)\n\n        for name, module in self.named_children():\n            fn_recursive_attn_processor(name, module, processor)",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_condition.py",
        "range": {
          "start": { "row": 387, "column": 4 },
          "end": { "row": 387, "column": 4 }
        }
      }
    }
  ],
  [
    "2359",
    {
      "pageContent": "def set_attention_slice(self, slice_size):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int` or `list(int)`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                `\"max\"`, maxium amount of memory will be saved by running only one slice at a time. If a number is\n                provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n                must be a multiple of `slice_size`.\n        \"\"\"\n        sliceable_head_dims = []\n\n        def fn_recursive_retrieve_slicable_dims(module: torch.nn.Module):\n            if hasattr(module, \"set_attention_slice\"):\n                sliceable_head_dims.append(module.sliceable_head_dim)\n\n            for child in module.children():\n                fn_recursive_retrieve_slicable_dims(child)\n\n        # retrieve number of attention layers\n        for module in self.children():\n            fn_recursive_retrieve_slicable_dims(module)\n\n        num_slicable_layers = len(sliceable_head_dims)\n\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = [dim // 2 for dim in sliceable_head_dims]\n        el",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_condition.py",
        "range": {
          "start": { "row": 417, "column": 4 },
          "end": { "row": 417, "column": 4 }
        }
      }
    }
  ],
  [
    "2360",
    {
      "pageContent": "def forward(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[torch.Tensor, float, int],\n        encoder_hidden_states: torch.Tensor,\n        class_labels: Optional[torch.Tensor] = None,\n        timestep_cond: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        down_block_additional_residuals: Optional[Tuple[torch.Tensor]] = None,\n        mid_block_additional_residual: Optional[torch.Tensor] = None,\n        return_dict: bool = True,\n    ) -> Union[UNet2DConditionOutput, Tuple]:\n        r\"\"\"\n        Args:\n            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n            timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps\n            encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n                `self.processor` in\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n\n        Returns:\n            [`~models.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:\n            [`~models.unet_2d_condition.UNet2D",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_condition.py",
        "range": {
          "start": { "row": 486, "column": 4 },
          "end": { "row": 486, "column": 4 }
        }
      }
    }
  ],
  [
    "2361",
    {
      "pageContent": "def rename_key(key):\n    regex = r\"\\w+[.]\\d+\"\n    pats = re.findall(regex, key)\n    for pat in pats:\n        key = key.replace(pat, \"_\".join(pat.split(\".\")))\n    return key",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_pytorch_utils.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "2362",
    {
      "pageContent": "def rename_key_and_reshape_tensor(pt_tuple_key, pt_tensor, random_flax_state_dict):\n    \"\"\"Rename PT weight names to corresponding Flax weight names and reshape tensor if necessary\"\"\"\n\n    # conv norm or layer norm\n    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"scale\",)\n    if (\n        any(\"norm\" in str_ for str_ in pt_tuple_key)\n        and (pt_tuple_key[-1] == \"bias\")\n        and (pt_tuple_key[:-1] + (\"bias\",) not in random_flax_state_dict)\n        and (pt_tuple_key[:-1] + (\"scale\",) in random_flax_state_dict)\n    ):\n        renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"scale\",)\n        return renamed_pt_tuple_key, pt_tensor\n    elif pt_tuple_key[-1] in [\"weight\", \"gamma\"] and pt_tuple_key[:-1] + (\"scale\",) in random_flax_state_dict:\n        renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"scale\",)\n        return renamed_pt_tuple_key, pt_tensor\n\n    # embedding\n    if pt_tuple_key[-1] == \"weight\" and pt_tuple_key[:-1] + (\"embedding\",) in random_flax_state_dict:\n        pt_tuple_key = pt_tuple_key[:-1] + (\"embedding\",)\n        return renamed_pt_tuple_key, pt_tensor\n\n    # conv layer\n    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"kernel\",)\n    if pt_tuple_key[-1] == \"weight\" and pt_tensor.ndim == 4:\n        pt_tensor = pt_tensor.transpose(2, 3, 1, 0)\n        return renamed_pt_tuple_key, pt_tensor\n\n    # linear layer\n    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"kernel\",)\n    if pt_tuple_key[-1] == \"weight\":\n        pt_tensor = pt_tensor.T\n        return renamed_pt_tuple_key, pt_tensor\n\n    # old PyTorch layer norm weight\n    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"w",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_pytorch_utils.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "2363",
    {
      "pageContent": "def convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model, init_key=42):\n    # Step 1: Convert pytorch tensor to numpy\n    pt_state_dict = {k: v.numpy() for k, v in pt_state_dict.items()}\n\n    # Step 2: Since the model is stateless, get random Flax params\n    random_flax_params = flax_model.init_weights(PRNGKey(init_key))\n\n    random_flax_state_dict = flatten_dict(random_flax_params)\n    flax_state_dict = {}\n\n    # Need to change some parameters name to match Flax names\n    for pt_key, pt_tensor in pt_state_dict.items():\n        renamed_pt_key = rename_key(pt_key)\n        pt_tuple_key = tuple(renamed_pt_key.split(\".\"))\n\n        # Correctly rename weight parameters\n        flax_key, flax_tensor = rename_key_and_reshape_tensor(pt_tuple_key, pt_tensor, random_flax_state_dict)\n\n        if flax_key in random_flax_state_dict:\n            if flax_tensor.shape != random_flax_state_dict[flax_key].shape:\n                raise ValueError(\n                    f\"PyTorch checkpoint seems to be incorrect. Weight {pt_key} was expected to be of shape \"\n                    f\"{random_flax_state_dict[flax_key].shape}, but is {flax_tensor.shape}.\"\n                )\n\n        # also add unexpected weight so that warning is thrown\n        flax_state_dict[flax_key] = jnp.asarray(flax_tensor)\n\n    return unflatten_dict(flax_state_dict)",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_pytorch_utils.py",
        "range": {
          "start": { "row": 89, "column": 0 },
          "end": { "row": 89, "column": 0 }
        }
      }
    }
  ],
  [
    "2364",
    {
      "pageContent": "def get_sinusoidal_embeddings(\n    timesteps: jnp.ndarray,\n    embedding_dim: int,\n    freq_shift: float = 1,\n    min_timescale: float = 1,\n    max_timescale: float = 1.0e4,\n    flip_sin_to_cos: bool = False,\n    scale: float = 1.0,\n) -> jnp.ndarray:\n    \"\"\"Returns the positional encoding (same as Tensor2Tensor).\n\n    Args:\n        timesteps: a 1-D Tensor of N indices, one per batch element.\n        These may be fractional.\n        embedding_dim: The number of output channels.\n        min_timescale: The smallest time unit (should probably be 0.0).\n        max_timescale: The largest time unit.\n    Returns:\n        a Tensor of timing signals [N, num_channels]\n    \"\"\"\n    assert timesteps.ndim == 1, \"Timesteps should be a 1d-array\"\n    assert embedding_dim % 2 == 0, f\"Embedding dimension {embedding_dim} should be even\"\n    num_timescales = float(embedding_dim // 2)\n    log_timescale_increment = math.log(max_timescale / min_timescale) / (num_timescales - freq_shift)\n    inv_timescales = min_timescale * jnp.exp(jnp.arange(num_timescales, dtype=jnp.float32) * -log_timescale_increment)\n    emb = jnp.expand_dims(timesteps, 1) * jnp.expand_dims(inv_timescales, 0)\n\n    # scale embeddings\n    scaled_time = scale * emb\n\n    if flip_sin_to_cos:\n        signal = jnp.concatenate([jnp.cos(scaled_time), jnp.sin(scaled_time)], axis=1)\n    else:\n        signal = jnp.concatenate([jnp.sin(scaled_time), jnp.cos(scaled_time)], axis=1)\n    signal = jnp.reshape(signal, [jnp.shape(timesteps)[0], embedding_dim])\n    return signal",
      "metadata": {
        "source": "src/diffusers/models/embeddings_flax.py",
        "range": {
          "start": { "row": 19, "column": 0 },
          "end": { "row": 19, "column": 0 }
        }
      }
    }
  ],
  [
    "2365",
    {
      "pageContent": "class FlaxTimestepEmbedding(nn.Module):\n    r\"\"\"\n    Time step Embedding Module. Learns embeddings for input time steps.\n\n    Args:\n        time_embed_dim (`int`, *optional*, defaults to `32`):\n                Time step embedding dimension\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n                Parameters `dtype`\n    \"\"\"\n    time_embed_dim: int = 32\n    dtype: jnp.dtype = jnp.float32\n\n    @nn.compact\n    def __call__(self, temb):\n        temb = nn.Dense(self.time_embed_dim, dtype=self.dtype, name=\"linear_1\")(temb)\n        temb = nn.silu(temb)\n        temb = nn.Dense(self.time_embed_dim, dtype=self.dtype, name=\"linear_2\")(temb)\n        return temb",
      "metadata": {
        "source": "src/diffusers/models/embeddings_flax.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "2366",
    {
      "pageContent": "class FlaxTimesteps(nn.Module):\n    r\"\"\"\n    Wrapper Module for sinusoidal Time step Embeddings as described in https://arxiv.org/abs/2006.11239\n\n    Args:\n        dim (`int`, *optional*, defaults to `32`):\n                Time step embedding dimension\n    \"\"\"\n    dim: int = 32\n    flip_sin_to_cos: bool = False\n    freq_shift: float = 1\n\n    @nn.compact\n    def __call__(self, timesteps):\n        return get_sinusoidal_embeddings(\n            timesteps, embedding_dim=self.dim, flip_sin_to_cos=self.flip_sin_to_cos, freq_shift=self.freq_shift\n        )",
      "metadata": {
        "source": "src/diffusers/models/embeddings_flax.py",
        "range": {
          "start": { "row": 78, "column": 0 },
          "end": { "row": 78, "column": 0 }
        }
      }
    }
  ],
  [
    "2367",
    {
      "pageContent": "class AutoencoderKLOutput(BaseOutput):\n    \"\"\"\n    Output of AutoencoderKL encoding method.\n\n    Args:\n        latent_dist (`DiagonalGaussianDistribution`):\n            Encoded outputs of `Encoder` represented as the mean and logvar of `DiagonalGaussianDistribution`.\n            `DiagonalGaussianDistribution` allows for sampling latents from the distribution.\n    \"\"\"\n\n    latent_dist: \"DiagonalGaussianDistribution\"",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "2368",
    {
      "pageContent": "class AutoencoderKL(ModelMixin, ConfigMixin):\n    r\"\"\"Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational Bayes by Diederik P. Kingma\n    and Max Welling.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the model (such as downloading or saving, etc.)\n\n    Parameters:\n        in_channels (int, *optional*, defaults to 3): Number of channels in the input image.\n        out_channels (int,  *optional*, defaults to 3): Number of channels in the output.\n        down_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"DownEncoderBlock2D\",)`): Tuple of downsample block types.\n        up_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"UpDecoderBlock2D\",)`): Tuple of upsample block types.\n        block_out_channels (`Tuple[int]`, *optional*, defaults to :\n            obj:`(64,)`): Tuple of block output channels.\n        act_fn (`str`, *optional*, defaults to `\"silu\"`): The activation function to use.\n        latent_channels (`int`, *optional*, defaults to 4): Number of channels in the latent space.\n        sample_size (`int`, *optional*, defaults to `32`): TODO\n        scaling_factor (`float`, *optional*, defaults to 0.18215):\n            The component-wise standard deviation of the trained latent space computed using the first batch of the\n            training set. This is used to scale the latent space to have unit variance when training the diffusion\n            model. The latents are scaled with the formula `z",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 39, "column": 0 },
          "end": { "row": 39, "column": 0 }
        }
      }
    }
  ],
  [
    "2369",
    {
      "pageContent": "def enable_tiling(self, use_tiling: bool = True):\n        r\"\"\"\n        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n        compute decoding and encoding in several steps. This is useful to save a large amount of memory and to allow\n        the processing of larger images.\n        \"\"\"\n        self.use_tiling = use_tiling",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 123, "column": 4 },
          "end": { "row": 123, "column": 4 }
        }
      }
    }
  ],
  [
    "2370",
    {
      "pageContent": "def disable_tiling(self):\n        r\"\"\"\n        Disable tiled VAE decoding. If `enable_vae_tiling` was previously invoked, this method will go back to\n        computing decoding in one step.\n        \"\"\"\n        self.enable_tiling(False)",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 131, "column": 4 },
          "end": { "row": 131, "column": 4 }
        }
      }
    }
  ],
  [
    "2371",
    {
      "pageContent": "def enable_slicing(self):\n        r\"\"\"\n        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n        \"\"\"\n        self.use_slicing = True",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 138, "column": 4 },
          "end": { "row": 138, "column": 4 }
        }
      }
    }
  ],
  [
    "2372",
    {
      "pageContent": "def disable_slicing(self):\n        r\"\"\"\n        Disable sliced VAE decoding. If `enable_slicing` was previously invoked, this method will go back to computing\n        decoding in one step.\n        \"\"\"\n        self.use_slicing = False",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 145, "column": 4 },
          "end": { "row": 145, "column": 4 }
        }
      }
    }
  ],
  [
    "2373",
    {
      "pageContent": "def _decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\n        if self.use_tiling and (z.shape[-1] > self.tile_latent_min_size or z.shape[-2] > self.tile_latent_min_size):\n            return self.tiled_decode(z, return_dict=return_dict)\n\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n\n        if not return_dict:\n            return (dec,)\n\n        return DecoderOutput(sample=dec)",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 166, "column": 4 },
          "end": { "row": 166, "column": 4 }
        }
      }
    }
  ],
  [
    "2374",
    {
      "pageContent": "def blend_v(self, a, b, blend_extent):\n        for y in range(blend_extent):\n            b[:, :, y, :] = a[:, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, y, :] * (y / blend_extent)\n        return b",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 191, "column": 4 },
          "end": { "row": 191, "column": 4 }
        }
      }
    }
  ],
  [
    "2375",
    {
      "pageContent": "def blend_h(self, a, b, blend_extent):\n        for x in range(blend_extent):\n            b[:, :, :, x] = a[:, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, x] * (x / blend_extent)\n        return b",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 196, "column": 4 },
          "end": { "row": 196, "column": 4 }
        }
      }
    }
  ],
  [
    "2376",
    {
      "pageContent": "def tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True) -> AutoencoderKLOutput:\n        r\"\"\"Encode a batch of images using a tiled encoder.\n        Args:\n        When this option is enabled, the VAE will split the input tensor into tiles to compute encoding in several\n        steps. This is useful to keep memory use constant regardless of image size. The end result of tiled encoding is:\n        different from non-tiled encoding due to each tile using a different encoder. To avoid tiling artifacts, the\n        tiles overlap and are blended together to form a smooth output. You may still see tile-sized changes in the\n        look of the output, but they should be much less noticeable.\n            x (`torch.FloatTensor`): Input batch of images. return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`AutoencoderKLOutput`] instead of a plain tuple.\n        \"\"\"\n        overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor))\n        blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)\n        row_limit = self.tile_latent_min_size - blend_extent\n\n        # Split the image into 512x512 tiles and encode them separately.\n        rows = []\n        for i in range(0, x.shape[2], overlap_size):\n            row = []\n            for j in range(0, x.shape[3], overlap_size):\n                tile = x[:, :, i : i + self.tile_sample_min_size, j : j + self.tile_sample_min_size]\n                tile = self.encoder(tile)\n                tile = self.quant_conv(tile)\n                row.append(tile)\n ",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 201, "column": 4 },
          "end": { "row": 201, "column": 4 }
        }
      }
    }
  ],
  [
    "2377",
    {
      "pageContent": "def tiled_decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\n        r\"\"\"Decode a batch of images using a tiled decoder.\n        Args:\n        When this option is enabled, the VAE will split the input tensor into tiles to compute decoding in several\n        steps. This is useful to keep memory use constant regardless of image size. The end result of tiled decoding is:\n        different from non-tiled decoding due to each tile using a different decoder. To avoid tiling artifacts, the\n        tiles overlap and are blended together to form a smooth output. You may still see tile-sized changes in the\n        look of the output, but they should be much less noticeable.\n            z (`torch.FloatTensor`): Input batch of latent vectors. return_dict (`bool`, *optional*, defaults to\n            `True`):\n                Whether or not to return a [`DecoderOutput`] instead of a plain tuple.\n        \"\"\"\n        overlap_size = int(self.tile_latent_min_size * (1 - self.tile_overlap_factor))\n        blend_extent = int(self.tile_sample_min_size * self.tile_overlap_factor)\n        row_limit = self.tile_sample_min_size - blend_extent\n\n        # Split z into overlapping 64x64 tiles and decode them separately.\n        # The tiles have an overlap to avoid seams between tiles.\n        rows = []\n        for i in range(0, z.shape[2], overlap_size):\n            row = []\n            for j in range(0, z.shape[3], overlap_size):\n                tile = z[:, :, i : i + self.tile_latent_min_size, j : j + self.tile_latent_min_size]\n                til",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 247, "column": 4 },
          "end": { "row": 247, "column": 4 }
        }
      }
    }
  ],
  [
    "2378",
    {
      "pageContent": "def forward(\n        self,\n        sample: torch.FloatTensor,\n        sample_posterior: bool = False,\n        return_dict: bool = True,\n        generator: Optional[torch.Generator] = None,\n    ) -> Union[DecoderOutput, torch.FloatTensor]:\n        r\"\"\"\n        Args:\n            sample (`torch.FloatTensor`): Input sample.\n            sample_posterior (`bool`, *optional*, defaults to `False`):\n                Whether to sample from the posterior.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`DecoderOutput`] instead of a plain tuple.\n        \"\"\"\n        x = sample\n        posterior = self.encode(x).latent_dist\n        if sample_posterior:\n            z = posterior.sample(generator=generator)\n        else:\n            z = posterior.mode()\n        dec = self.decode(z).sample\n\n        if not return_dict:\n            return (dec,)\n\n        return DecoderOutput(sample=dec)",
      "metadata": {
        "source": "src/diffusers/models/autoencoder_kl.py",
        "range": {
          "start": { "row": 293, "column": 4 },
          "end": { "row": 293, "column": 4 }
        }
      }
    }
  ],
  [
    "2379",
    {
      "pageContent": "class CrossAttention(nn.Module):\n    r\"\"\"\n    A cross attention layer.\n\n    Parameters:\n        query_dim (`int`): The number of channels in the query.\n        cross_attention_dim (`int`, *optional*):\n            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n        heads (`int`,  *optional*, defaults to 8): The number of heads to use for multi-head attention.\n        dim_head (`int`,  *optional*, defaults to 64): The number of channels in each head.\n        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n        bias (`bool`, *optional*, defaults to False):\n            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim: int,\n        cross_attention_dim: Optional[int] = None,\n        heads: int = 8,\n        dim_head: int = 64,\n        dropout: float = 0.0,\n        bias=False,\n        upcast_attention: bool = False,\n        upcast_softmax: bool = False,\n        cross_attention_norm: bool = False,\n        added_kv_proj_dim: Optional[int] = None,\n        norm_num_groups: Optional[int] = None,\n        processor: Optional[\"AttnProcessor\"] = None,\n    ):\n        super().__init__()\n        inner_dim = dim_head * heads\n        cross_attention_dim = cross_attention_dim if cross_attention_dim is not None else query_dim\n        self.upcast_attention = upcast_attention\n        self.upcast_softmax = upcast_softmax\n        self.cross_attention_norm = cross_attention_norm\n\n        self.scale = dim_head**-0.5\n\n        s",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 33, "column": 0 },
          "end": { "row": 33, "column": 0 }
        }
      }
    }
  ],
  [
    "2380",
    {
      "pageContent": "def __init__(\n        self,\n        query_dim: int,\n        cross_attention_dim: Optional[int] = None,\n        heads: int = 8,\n        dim_head: int = 64,\n        dropout: float = 0.0,\n        bias=False,\n        upcast_attention: bool = False,\n        upcast_softmax: bool = False,\n        cross_attention_norm: bool = False,\n        added_kv_proj_dim: Optional[int] = None,\n        norm_num_groups: Optional[int] = None,\n        processor: Optional[\"AttnProcessor\"] = None,\n    ):\n        super().__init__()\n        inner_dim = dim_head * heads\n        cross_attention_dim = cross_attention_dim if cross_attention_dim is not None else query_dim\n        self.upcast_attention = upcast_attention\n        self.upcast_softmax = upcast_softmax\n        self.cross_attention_norm = cross_attention_norm\n\n        self.scale = dim_head**-0.5\n\n        self.heads = heads\n        # for slice_size > 0 the attention score computation\n        # is split across the batch axis to save memory\n        # You can set slice_size with `set_attention_slice`\n        self.sliceable_head_dim = heads\n\n        self.added_kv_proj_dim = added_kv_proj_dim\n\n        if norm_num_groups is not None:\n            self.group_norm = nn.GroupNorm(num_channels=inner_dim, num_groups=norm_num_groups, eps=1e-5, affine=True)\n        else:\n            self.group_norm = None\n\n        if cross_attention_norm:\n            self.norm_cross = nn.LayerNorm(cross_attention_dim)\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=bias)\n        self.to_k = nn.Linear(cross_attention_dim, inner_dim, bias=bias)\n        self.to_v = nn.Li",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 48, "column": 4 },
          "end": { "row": 48, "column": 4 }
        }
      }
    }
  ],
  [
    "2381",
    {
      "pageContent": "def set_use_memory_efficient_attention_xformers(\n        self, use_memory_efficient_attention_xformers: bool, attention_op: Optional[Callable] = None\n    ):\n        is_lora = hasattr(self, \"processor\") and isinstance(\n            self.processor, (LoRACrossAttnProcessor, LoRAXFormersCrossAttnProcessor)\n        )\n\n        if use_memory_efficient_attention_xformers:\n            if self.added_kv_proj_dim is not None:\n                # TODO(Anton, Patrick, Suraj, William) - currently xformers doesn't work for UnCLIP\n                # which uses this type of cross attention ONLY because the attention mask of format\n                # [0, ..., -10.000, ..., 0, ...,] is not supported\n                raise NotImplementedError(\n                    \"Memory efficient attention with `xformers` is currently not supported when\"\n                    \" `self.added_kv_proj_dim` is defined.\"\n                )\n            elif not is_xformers_available():\n                raise ModuleNotFoundError(\n                    (\n                        \"Refer to https://github.com/facebookresearch/xformers for more information on how to install\"\n                        \" xformers\"\n                    ),\n                    name=\"xformers\",\n                )\n            elif not torch.cuda.is_available():\n                raise ValueError(\n                    \"torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is\"\n                    \" only available for GPU \"\n                )\n            else:\n                try:\n                    # Make sure we can run the memo",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 107, "column": 4 },
          "end": { "row": 107, "column": 4 }
        }
      }
    }
  ],
  [
    "2382",
    {
      "pageContent": "def set_attention_slice(self, slice_size):\n        if slice_size is not None and slice_size > self.sliceable_head_dim:\n            raise ValueError(f\"slice_size {slice_size} has to be smaller or equal to {self.sliceable_head_dim}.\")\n\n        if slice_size is not None and self.added_kv_proj_dim is not None:\n            processor = SlicedAttnAddedKVProcessor(slice_size)\n        elif slice_size is not None:\n            processor = SlicedAttnProcessor(slice_size)\n        elif self.added_kv_proj_dim is not None:\n            processor = CrossAttnAddedKVProcessor()\n        else:\n            processor = CrossAttnProcessor()\n\n        self.set_processor(processor)",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 172, "column": 4 },
          "end": { "row": 172, "column": 4 }
        }
      }
    }
  ],
  [
    "2383",
    {
      "pageContent": "def set_processor(self, processor: \"AttnProcessor\"):\n        # if current processor is in `self._modules` and if passed `processor` is not, we need to\n        # pop `processor` from `self._modules`\n        if (\n            hasattr(self, \"processor\")\n            and isinstance(self.processor, torch.nn.Module)\n            and not isinstance(processor, torch.nn.Module)\n        ):\n            logger.info(f\"You are removing possibly trained weights of {self.processor} with {processor}\")\n            self._modules.pop(\"processor\")\n\n        self.processor = processor",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 187, "column": 4 },
          "end": { "row": 187, "column": 4 }
        }
      }
    }
  ],
  [
    "2384",
    {
      "pageContent": "def forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None, **cross_attention_kwargs):\n        # The `CrossAttention` class can call different attention processors / attention functions\n        # here we simply pass along all tensors to the selected processor class\n        # For standard processors that are defined here, `**cross_attention_kwargs` is empty\n        return self.processor(\n            self,\n            hidden_states,\n            encoder_hidden_states=encoder_hidden_states,\n            attention_mask=attention_mask,\n            **cross_attention_kwargs,\n        )",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 200, "column": 4 },
          "end": { "row": 200, "column": 4 }
        }
      }
    }
  ],
  [
    "2385",
    {
      "pageContent": "def batch_to_head_dim(self, tensor):\n        head_size = self.heads\n        batch_size, seq_len, dim = tensor.shape\n        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n        return tensor",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 212, "column": 4 },
          "end": { "row": 212, "column": 4 }
        }
      }
    }
  ],
  [
    "2386",
    {
      "pageContent": "def head_to_batch_dim(self, tensor):\n        head_size = self.heads\n        batch_size, seq_len, dim = tensor.shape\n        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n        return tensor",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 219, "column": 4 },
          "end": { "row": 219, "column": 4 }
        }
      }
    }
  ],
  [
    "2387",
    {
      "pageContent": "def get_attention_scores(self, query, key, attention_mask=None):\n        dtype = query.dtype\n        if self.upcast_attention:\n            query = query.float()\n            key = key.float()\n\n        if attention_mask is None:\n            baddbmm_input = torch.empty(\n                query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device\n            )\n            beta = 0\n        else:\n            baddbmm_input = attention_mask\n            beta = 1\n\n        attention_scores = torch.baddbmm(\n            baddbmm_input,\n            query,\n            key.transpose(-1, -2),\n            beta=beta,\n            alpha=self.scale,\n        )\n\n        if self.upcast_softmax:\n            attention_scores = attention_scores.float()\n\n        attention_probs = attention_scores.softmax(dim=-1)\n        attention_probs = attention_probs.to(dtype)\n\n        return attention_probs",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 226, "column": 4 },
          "end": { "row": 226, "column": 4 }
        }
      }
    }
  ],
  [
    "2388",
    {
      "pageContent": "def prepare_attention_mask(self, attention_mask, target_length, batch_size=None):\n        if batch_size is None:\n            deprecate(\n                \"batch_size=None\",\n                \"0.0.15\",\n                (\n                    \"Not passing the `batch_size` parameter to `prepare_attention_mask` can lead to incorrect\"\n                    \" attention mask preparation and is deprecated behavior. Please make sure to pass `batch_size` to\"\n                    \" `prepare_attention_mask` when preparing the attention_mask.\"\n                ),\n            )\n            batch_size = 1\n\n        head_size = self.heads\n        if attention_mask is None:\n            return attention_mask\n\n        if attention_mask.shape[-1] != target_length:\n            if attention_mask.device.type == \"mps\":\n                # HACK: MPS: Does not support padding by greater than dimension of input tensor.\n                # Instead, we can manually construct the padding tensor.\n                padding_shape = (attention_mask.shape[0], attention_mask.shape[1], target_length)\n                padding = torch.zeros(padding_shape, dtype=attention_mask.dtype, device=attention_mask.device)\n                attention_mask = torch.cat([attention_mask, padding], dim=2)\n            else:\n                attention_mask = F.pad(attention_mask, (0, target_length), value=0.0)\n\n        if attention_mask.shape[0] < batch_size * head_size:\n            attention_mask = attention_mask.repeat_interleave(head_size, dim=0)\n        return attention_mask",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 257, "column": 4 },
          "end": { "row": 257, "column": 4 }
        }
      }
    }
  ],
  [
    "2389",
    {
      "pageContent": "class CrossAttnProcessor:\n    def __call__(\n        self,\n        attn: CrossAttention,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 289, "column": 0 },
          "end": { "row": 289, "column": 0 }
        }
      }
    }
  ],
  [
    "2390",
    {
      "pageContent": "def __call__(\n        self,\n        attn: CrossAttention,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 290, "column": 4 },
          "end": { "row": 290, "column": 4 }
        }
      }
    }
  ],
  [
    "2391",
    {
      "pageContent": "class LoRALinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, rank=4):\n        super().__init__()\n\n        if rank > min(in_features, out_features):\n            raise ValueError(f\"LoRA rank {rank} must be less or equal than {min(in_features, out_features)}\")\n\n        self.down = nn.Linear(in_features, rank, bias=False)\n        self.up = nn.Linear(rank, out_features, bias=False)\n\n        nn.init.normal_(self.down.weight, std=1 / rank)\n        nn.init.zeros_(self.up.weight)\n\n    def forward(self, hidden_states):\n        orig_dtype = hidden_states.dtype\n        dtype = self.down.weight.dtype\n\n        down_hidden_states = self.down(hidden_states.to(dtype))\n        up_hidden_states = self.up(down_hidden_states)\n\n        return up_hidden_states.to(orig_dtype)",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 325, "column": 0 },
          "end": { "row": 325, "column": 0 }
        }
      }
    }
  ],
  [
    "2392",
    {
      "pageContent": "def __init__(self, in_features, out_features, rank=4):\n        super().__init__()\n\n        if rank > min(in_features, out_features):\n            raise ValueError(f\"LoRA rank {rank} must be less or equal than {min(in_features, out_features)}\")\n\n        self.down = nn.Linear(in_features, rank, bias=False)\n        self.up = nn.Linear(rank, out_features, bias=False)\n\n        nn.init.normal_(self.down.weight, std=1 / rank)\n        nn.init.zeros_(self.up.weight)",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 326, "column": 4 },
          "end": { "row": 326, "column": 4 }
        }
      }
    }
  ],
  [
    "2393",
    {
      "pageContent": "def forward(self, hidden_states):\n        orig_dtype = hidden_states.dtype\n        dtype = self.down.weight.dtype\n\n        down_hidden_states = self.down(hidden_states.to(dtype))\n        up_hidden_states = self.up(down_hidden_states)\n\n        return up_hidden_states.to(orig_dtype)",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 338, "column": 4 },
          "end": { "row": 338, "column": 4 }
        }
      }
    }
  ],
  [
    "2394",
    {
      "pageContent": "class LoRACrossAttnProcessor(nn.Module):\n    def __init__(self, hidden_size, cross_attention_dim=None, rank=4):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.rank = rank\n\n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n\n    def __call__(\n        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)\n        query = attn.head_to_batch_dim(query)\n\n        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n\n        key = attn.to_k(encoder_hidden_states) + scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + scale * self.to_v_lora(encoder_hidden_states)\n\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_di",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 348, "column": 0 },
          "end": { "row": 348, "column": 0 }
        }
      }
    }
  ],
  [
    "2395",
    {
      "pageContent": "def __init__(self, hidden_size, cross_attention_dim=None, rank=4):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.rank = rank\n\n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank)",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 349, "column": 4 },
          "end": { "row": 349, "column": 4 }
        }
      }
    }
  ],
  [
    "2396",
    {
      "pageContent": "def __call__(\n        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)\n        query = attn.head_to_batch_dim(query)\n\n        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n\n        key = attn.to_k(encoder_hidden_states) + scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + scale * self.to_v_lora(encoder_hidden_states)\n\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 361, "column": 4 },
          "end": { "row": 361, "column": 4 }
        }
      }
    }
  ],
  [
    "2397",
    {
      "pageContent": "class CrossAttnAddedKVProcessor:\n    def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        residual = hidden_states\n        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)\n        batch_size, sequence_length, _ = hidden_states.shape\n        encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        query = attn.head_to_batch_dim(query)\n\n        key = attn.to_k(hidden_states)\n        value = attn.to_v(hidden_states)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)\n        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)\n        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)\n        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)\n\n        key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)\n        value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        ",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 390, "column": 0 },
          "end": { "row": 390, "column": 0 }
        }
      }
    }
  ],
  [
    "2398",
    {
      "pageContent": "def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        residual = hidden_states\n        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)\n        batch_size, sequence_length, _ = hidden_states.shape\n        encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        query = attn.head_to_batch_dim(query)\n\n        key = attn.to_k(hidden_states)\n        value = attn.to_v(hidden_states)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)\n        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)\n        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)\n        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)\n\n        key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)\n        value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 391, "column": 4 },
          "end": { "row": 391, "column": 4 }
        }
      }
    }
  ],
  [
    "2399",
    {
      "pageContent": "class XFormersCrossAttnProcessor:\n    def __init__(self, attention_op: Optional[Callable] = None):\n        self.attention_op = attention_op\n\n    def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, _ = hidden_states.shape\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        hidden_states = xformers.ops.memory_efficient_attention(\n            query, key, value, attn_bias=attention_mask, op=self.attention_op\n        )\n        hidden_states = hidden_states.to(query.dtype)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 432, "column": 0 },
          "end": { "row": 432, "column": 0 }
        }
      }
    }
  ],
  [
    "2400",
    {
      "pageContent": "def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, _ = hidden_states.shape\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        hidden_states = xformers.ops.memory_efficient_attention(\n            query, key, value, attn_bias=attention_mask, op=self.attention_op\n        )\n        hidden_states = hidden_states.to(query.dtype)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 436, "column": 4 },
          "end": { "row": 436, "column": 4 }
        }
      }
    }
  ],
  [
    "2401",
    {
      "pageContent": "class AttnProcessor2_0:\n    def __init__(self):\n        if not hasattr(F, \"scaled_dot_product_attention\"):\n            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n\n    def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, inner_dim = hidden_states.shape\n\n        if attention_mask is not None:\n            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n            # scaled_dot_product_attention expects attention_mask shape to be\n            # (batch, heads, source_length, target_length)\n            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        head_dim = inner_dim // attn.heads\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropo",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 468, "column": 0 },
          "end": { "row": 468, "column": 0 }
        }
      }
    }
  ],
  [
    "2402",
    {
      "pageContent": "def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, inner_dim = hidden_states.shape\n\n        if attention_mask is not None:\n            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n            # scaled_dot_product_attention expects attention_mask shape to be\n            # (batch, heads, source_length, target_length)\n            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        head_dim = inner_dim // attn.heads\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n        )\n\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        hidden_states = hidden_states.to(query.dtype)\n\n        # linear proj\n        hid",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 473, "column": 4 },
          "end": { "row": 473, "column": 4 }
        }
      }
    }
  ],
  [
    "2403",
    {
      "pageContent": "class LoRAXFormersCrossAttnProcessor(nn.Module):\n    def __init__(self, hidden_size, cross_attention_dim, rank=4, attention_op: Optional[Callable] = None):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.rank = rank\n        self.attention_op = attention_op\n\n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n\n    def __call__(\n        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)\n        query = attn.head_to_batch_dim(query).contiguous()\n\n        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n\n        key = attn.to_k(encoder_hidden_states) + scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + scale * self.to_v_lora(encoder_hidden_states)\n\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        hidden_states = xformers.ops.memory_efficient_atten",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 512, "column": 0 },
          "end": { "row": 512, "column": 0 }
        }
      }
    }
  ],
  [
    "2404",
    {
      "pageContent": "def __init__(self, hidden_size, cross_attention_dim, rank=4, attention_op: Optional[Callable] = None):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.rank = rank\n        self.attention_op = attention_op\n\n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank)",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 513, "column": 4 },
          "end": { "row": 513, "column": 4 }
        }
      }
    }
  ],
  [
    "2405",
    {
      "pageContent": "def __call__(\n        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)\n        query = attn.head_to_batch_dim(query).contiguous()\n\n        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n\n        key = attn.to_k(encoder_hidden_states) + scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + scale * self.to_v_lora(encoder_hidden_states)\n\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        hidden_states = xformers.ops.memory_efficient_attention(\n            query, key, value, attn_bias=attention_mask, op=self.attention_op\n        )\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 526, "column": 4 },
          "end": { "row": 526, "column": 4 }
        }
      }
    }
  ],
  [
    "2406",
    {
      "pageContent": "class SlicedAttnProcessor:\n    def __init__(self, slice_size):\n        self.slice_size = slice_size\n\n    def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, _ = hidden_states.shape\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states)\n        dim = query.shape[-1]\n        query = attn.head_to_batch_dim(query)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        batch_size_attention = query.shape[0]\n        hidden_states = torch.zeros(\n            (batch_size_attention, sequence_length, dim // attn.heads), device=query.device, dtype=query.dtype\n        )\n\n        for i in range(hidden_states.shape[0] // self.slice_size):\n            start_idx = i * self.slice_size\n            end_idx = (i + 1) * self.slice_size\n\n            query_slice = query[start_idx:end_idx]\n            key_slice = key[start_idx:end_idx]\n            attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None\n\n            attn_slice = attn.get_attention_scores(query_slice, key_slice, attn_mask_slice)\n\n            attn_slice = torch.bmm(attn_slice, value[sta",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 556, "column": 0 },
          "end": { "row": 556, "column": 0 }
        }
      }
    }
  ],
  [
    "2407",
    {
      "pageContent": "def __call__(self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n        batch_size, sequence_length, _ = hidden_states.shape\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        query = attn.to_q(hidden_states)\n        dim = query.shape[-1]\n        query = attn.head_to_batch_dim(query)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.cross_attention_norm:\n            encoder_hidden_states = attn.norm_cross(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        batch_size_attention = query.shape[0]\n        hidden_states = torch.zeros(\n            (batch_size_attention, sequence_length, dim // attn.heads), device=query.device, dtype=query.dtype\n        )\n\n        for i in range(hidden_states.shape[0] // self.slice_size):\n            start_idx = i * self.slice_size\n            end_idx = (i + 1) * self.slice_size\n\n            query_slice = query[start_idx:end_idx]\n            key_slice = key[start_idx:end_idx]\n            attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None\n\n            attn_slice = attn.get_attention_scores(query_slice, key_slice, attn_mask_slice)\n\n            attn_slice = torch.bmm(attn_slice, value[start_idx:end_idx])\n\n            hidden_states[start_idx:end_idx] = attn_slice\n\n        hidden_states = attn",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 560, "column": 4 },
          "end": { "row": 560, "column": 4 }
        }
      }
    }
  ],
  [
    "2408",
    {
      "pageContent": "class SlicedAttnAddedKVProcessor:\n    def __init__(self, slice_size):\n        self.slice_size = slice_size\n\n    def __call__(self, attn: \"CrossAttention\", hidden_states, encoder_hidden_states=None, attention_mask=None):\n        residual = hidden_states\n        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)\n        encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n\n        batch_size, sequence_length, _ = hidden_states.shape\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        dim = query.shape[-1]\n        query = attn.head_to_batch_dim(query)\n\n        key = attn.to_k(hidden_states)\n        value = attn.to_v(hidden_states)\n        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)\n        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)\n\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)\n        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)\n\n        key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)\n        value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)\n\n        batch_size_attention = query.shape[0]\n        hidden_states = torch.zeros(\n            (batch_size_attention, seque",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 608, "column": 0 },
          "end": { "row": 608, "column": 0 }
        }
      }
    }
  ],
  [
    "2409",
    {
      "pageContent": "def __call__(self, attn: \"CrossAttention\", hidden_states, encoder_hidden_states=None, attention_mask=None):\n        residual = hidden_states\n        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)\n        encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n\n        batch_size, sequence_length, _ = hidden_states.shape\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        dim = query.shape[-1]\n        query = attn.head_to_batch_dim(query)\n\n        key = attn.to_k(hidden_states)\n        value = attn.to_v(hidden_states)\n        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)\n        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)\n\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)\n        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)\n\n        key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)\n        value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)\n\n        batch_size_attention = query.shape[0]\n        hidden_states = torch.zeros(\n            (batch_size_attention, sequence_length, dim // attn.heads), device=query.device, dtype=query.dtype\n        )\n\n        for i in range(hidden_",
      "metadata": {
        "source": "src/diffusers/models/cross_attention.py",
        "range": {
          "start": { "row": 612, "column": 4 },
          "end": { "row": 612, "column": 4 }
        }
      }
    }
  ],
  [
    "2410",
    {
      "pageContent": "class DualTransformer2DModel(nn.Module):\n    \"\"\"\n    Dual transformer wrapper that combines two `Transformer2DModel`s for mixed inference.\n\n    Parameters:\n        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.\n        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.\n        in_channels (`int`, *optional*):\n            Pass if the input is continuous. The number of channels in the input and output.\n        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.\n        dropout (`float`, *optional*, defaults to 0.1): The dropout probability to use.\n        cross_attention_dim (`int`, *optional*): The number of encoder_hidden_states dimensions to use.\n        sample_size (`int`, *optional*): Pass if the input is discrete. The width of the latent images.\n            Note that this is fixed at training time as it is used for learning a number of position embeddings. See\n            `ImagePositionalEmbeddings`.\n        num_vector_embeds (`int`, *optional*):\n            Pass if the input is discrete. The number of classes of the vector embeddings of the latent pixels.\n            Includes the class for the masked latent pixel.\n        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n        num_embeds_ada_norm ( `int`, *optional*): Pass if at least one of the norm_layers is `AdaLayerNorm`.\n            The number of diffusion steps used during training. Note that this is fixed a",
      "metadata": {
        "source": "src/diffusers/models/dual_transformer_2d.py",
        "range": {
          "start": { "row": 20, "column": 0 },
          "end": { "row": 20, "column": 0 }
        }
      }
    }
  ],
  [
    "2411",
    {
      "pageContent": "def __init__(\n        self,\n        num_attention_heads: int = 16,\n        attention_head_dim: int = 88,\n        in_channels: Optional[int] = None,\n        num_layers: int = 1,\n        dropout: float = 0.0,\n        norm_num_groups: int = 32,\n        cross_attention_dim: Optional[int] = None,\n        attention_bias: bool = False,\n        sample_size: Optional[int] = None,\n        num_vector_embeds: Optional[int] = None,\n        activation_fn: str = \"geglu\",\n        num_embeds_ada_norm: Optional[int] = None,\n    ):\n        super().__init__()\n        self.transformers = nn.ModuleList(\n            [\n                Transformer2DModel(\n                    num_attention_heads=num_attention_heads,\n                    attention_head_dim=attention_head_dim,\n                    in_channels=in_channels,\n                    num_layers=num_layers,\n                    dropout=dropout,\n                    norm_num_groups=norm_num_groups,\n                    cross_attention_dim=cross_attention_dim,\n                    attention_bias=attention_bias,\n                    sample_size=sample_size,\n                    num_vector_embeds=num_vector_embeds,\n                    activation_fn=activation_fn,\n                    num_embeds_ada_norm=num_embeds_ada_norm,\n                )\n                for _ in range(2)\n            ]\n        )\n\n        # Variables that can be set by a pipeline:\n\n        # The ratio of transformer1 to transformer2's output states to be combined during inference\n        self.mix_ratio = 0.5\n\n        # The shape of `encoder_hidden_states` is expected to be\n        # `(bat",
      "metadata": {
        "source": "src/diffusers/models/dual_transformer_2d.py",
        "range": {
          "start": { "row": 47, "column": 4 },
          "end": { "row": 47, "column": 4 }
        }
      }
    }
  ],
  [
    "2412",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        encoder_hidden_states,\n        timestep=None,\n        attention_mask=None,\n        cross_attention_kwargs=None,\n        return_dict: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hidden_states ( When discrete, `torch.LongTensor` of shape `(batch size, num latent pixels)`.\n                When continuous, `torch.FloatTensor` of shape `(batch size, channel, height, width)`): Input\n                hidden_states\n            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):\n                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to\n                self-attention.\n            timestep ( `torch.long`, *optional*):\n                Optional timestep to be applied as an embedding in AdaLayerNorm's. Used to indicate denoising step.\n            attention_mask (`torch.FloatTensor`, *optional*):\n                Optional attention mask to be applied in CrossAttention\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~models.transformer_2d.Transformer2DModelOutput`] or `tuple`:\n            [`~models.transformer_2d.Transformer2DModelOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        input_states = hidden_states\n\n        encoded_states = []\n        token",
      "metadata": {
        "source": "src/diffusers/models/dual_transformer_2d.py",
        "range": {
          "start": { "row": 96, "column": 4 },
          "end": { "row": 96, "column": 4 }
        }
      }
    }
  ],
  [
    "2413",
    {
      "pageContent": "class FlaxUNet2DConditionOutput(BaseOutput):\n    \"\"\"\n    Args:\n        sample (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)`):\n            Hidden states conditioned on `encoder_hidden_states` input. Output of last layer of model.\n    \"\"\"\n\n    sample: jnp.ndarray",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_condition_flax.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "2414",
    {
      "pageContent": "class FlaxUNet2DConditionModel(nn.Module, FlaxModelMixin, ConfigMixin):\n    r\"\"\"\n    FlaxUNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a\n    timestep and returns sample shaped output.\n\n    This model inherits from [`FlaxModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the models (such as downloading or saving, etc.)\n\n    Also, this model is a Flax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)\n    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to\n    general usage and behavior.\n\n    Finally, this model supports inherent JAX features such as:\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\n    Parameters:\n        sample_size (`int`, *optional*):\n            The size of the input sample.\n        in_channels (`int`, *optional*, defaults to 4):\n            The number of channels in the input sample.\n        out_channels (`int`, *optional*, defaults to 4):\n            The number of channels in the output.\n        down_block_types (`Tuple[str]`, *optional*, defaults to `(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\"",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_condition_flax.py",
        "range": {
          "start": { "row": 46, "column": 0 },
          "end": { "row": 46, "column": 0 }
        }
      }
    }
  ],
  [
    "2415",
    {
      "pageContent": "class AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other. Originally ported from here, but adapted\n    to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    Uses three q, k, v linear layers to compute attention.\n\n    Parameters:\n        channels (`int`): The number of channels in the input and output.\n        num_head_channels (`int`, *optional*):\n            The number of channels in each head. If None, then `num_heads` = 1.\n        norm_num_groups (`int`, *optional*, defaults to 32): The number of groups to use for group norm.\n        rescale_output_factor (`float`, *optional*, defaults to 1.0): The factor to rescale the output by.\n        eps (`float`, *optional*, defaults to 1e-5): The epsilon value to use for group norm.\n    \"\"\"\n\n    # IMPORTANT;TODO(Patrick, William) - this class will be deprecated soon. Do not use it anymore\n\n    def __init__(\n        self,\n        channels: int,\n        num_head_channels: Optional[int] = None,\n        norm_num_groups: int = 32,\n        rescale_output_factor: float = 1.0,\n        eps: float = 1e-5,\n    ):\n        super().__init__()\n        self.channels = channels\n\n        self.num_heads = channels // num_head_channels if num_head_channels is not None else 1\n        self.num_head_size = num_head_channels\n        self.group_norm = nn.GroupNorm(num_channels=channels, num_groups=norm_num_groups, eps=eps, affine=True)\n\n        # define q,k,v as linear layers\n        self.query = nn.Linea",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 32, "column": 0 },
          "end": { "row": 32, "column": 0 }
        }
      }
    }
  ],
  [
    "2416",
    {
      "pageContent": "def __init__(\n        self,\n        channels: int,\n        num_head_channels: Optional[int] = None,\n        norm_num_groups: int = 32,\n        rescale_output_factor: float = 1.0,\n        eps: float = 1e-5,\n    ):\n        super().__init__()\n        self.channels = channels\n\n        self.num_heads = channels // num_head_channels if num_head_channels is not None else 1\n        self.num_head_size = num_head_channels\n        self.group_norm = nn.GroupNorm(num_channels=channels, num_groups=norm_num_groups, eps=eps, affine=True)\n\n        # define q,k,v as linear layers\n        self.query = nn.Linear(channels, channels)\n        self.key = nn.Linear(channels, channels)\n        self.value = nn.Linear(channels, channels)\n\n        self.rescale_output_factor = rescale_output_factor\n        self.proj_attn = nn.Linear(channels, channels, 1)\n\n        self._use_memory_efficient_attention_xformers = False\n        self._attention_op = None",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 50, "column": 4 },
          "end": { "row": 50, "column": 4 }
        }
      }
    }
  ],
  [
    "2417",
    {
      "pageContent": "def reshape_heads_to_batch_dim(self, tensor):\n        batch_size, seq_len, dim = tensor.shape\n        head_size = self.num_heads\n        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n        return tensor",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 76, "column": 4 },
          "end": { "row": 76, "column": 4 }
        }
      }
    }
  ],
  [
    "2418",
    {
      "pageContent": "def reshape_batch_dim_to_heads(self, tensor):\n        batch_size, seq_len, dim = tensor.shape\n        head_size = self.num_heads\n        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n        return tensor",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 83, "column": 4 },
          "end": { "row": 83, "column": 4 }
        }
      }
    }
  ],
  [
    "2419",
    {
      "pageContent": "def set_use_memory_efficient_attention_xformers(\n        self, use_memory_efficient_attention_xformers: bool, attention_op: Optional[Callable] = None\n    ):\n        if use_memory_efficient_attention_xformers:\n            if not is_xformers_available():\n                raise ModuleNotFoundError(\n                    (\n                        \"Refer to https://github.com/facebookresearch/xformers for more information on how to install\"\n                        \" xformers\"\n                    ),\n                    name=\"xformers\",\n                )\n            elif not torch.cuda.is_available():\n                raise ValueError(\n                    \"torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is\"\n                    \" only available for GPU \"\n                )\n            else:\n                try:\n                    # Make sure we can run the memory efficient attention\n                    _ = xformers.ops.memory_efficient_attention(\n                        torch.randn((1, 2, 40), device=\"cuda\"),\n                        torch.randn((1, 2, 40), device=\"cuda\"),\n                        torch.randn((1, 2, 40), device=\"cuda\"),\n                    )\n                except Exception as e:\n                    raise e\n        self._use_memory_efficient_attention_xformers = use_memory_efficient_attention_xformers\n        self._attention_op = attention_op",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 90, "column": 4 },
          "end": { "row": 90, "column": 4 }
        }
      }
    }
  ],
  [
    "2420",
    {
      "pageContent": "def forward(self, hidden_states):\n        residual = hidden_states\n        batch, channel, height, width = hidden_states.shape\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n\n        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n\n        # proj to q, k, v\n        query_proj = self.query(hidden_states)\n        key_proj = self.key(hidden_states)\n        value_proj = self.value(hidden_states)\n\n        scale = 1 / math.sqrt(self.channels / self.num_heads)\n\n        query_proj = self.reshape_heads_to_batch_dim(query_proj)\n        key_proj = self.reshape_heads_to_batch_dim(key_proj)\n        value_proj = self.reshape_heads_to_batch_dim(value_proj)\n\n        if self._use_memory_efficient_attention_xformers:\n            # Memory efficient attention\n            hidden_states = xformers.ops.memory_efficient_attention(\n                query_proj, key_proj, value_proj, attn_bias=None, op=self._attention_op\n            )\n            hidden_states = hidden_states.to(query_proj.dtype)\n        else:\n            attention_scores = torch.baddbmm(\n                torch.empty(\n                    query_proj.shape[0],\n                    query_proj.shape[1],\n                    key_proj.shape[1],\n                    dtype=query_proj.dtype,\n                    device=query_proj.device,\n                ),\n                query_proj,\n                key_proj.transpose(-1, -2),\n                beta=0,\n                alpha=scale,\n            )\n            attention_probs = torch.softmax(attention_scores.float(), dim=-1).type(attention_scores",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 120, "column": 4 },
          "end": { "row": 120, "column": 4 }
        }
      }
    }
  ],
  [
    "2421",
    {
      "pageContent": "class BasicTransformerBlock(nn.Module):\n    r\"\"\"\n    A basic Transformer block.\n\n    Parameters:\n        dim (`int`): The number of channels in the input and output.\n        num_attention_heads (`int`): The number of heads to use for multi-head attention.\n        attention_head_dim (`int`): The number of channels in each head.\n        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.\n        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n        num_embeds_ada_norm (:\n            obj: `int`, *optional*): The number of diffusion steps used during training. See `Transformer2DModel`.\n        attention_bias (:\n            obj: `bool`, *optional*, defaults to `False`): Configure if the attentions should contain a bias parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_attention_heads: int,\n        attention_head_dim: int,\n        dropout=0.0,\n        cross_attention_dim: Optional[int] = None,\n        activation_fn: str = \"geglu\",\n        num_embeds_ada_norm: Optional[int] = None,\n        attention_bias: bool = False,\n        only_cross_attention: bool = False,\n        upcast_attention: bool = False,\n        norm_elementwise_affine: bool = True,\n        norm_type: str = \"layer_norm\",\n        final_dropout: bool = False,\n    ):\n        super().__init__()\n        self.only_cross_attention = only_cross_attention\n\n        self.use_ada_layer_norm_zero = ",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 176, "column": 0 },
          "end": { "row": 176, "column": 0 }
        }
      }
    }
  ],
  [
    "2422",
    {
      "pageContent": "def __init__(\n        self,\n        dim: int,\n        num_attention_heads: int,\n        attention_head_dim: int,\n        dropout=0.0,\n        cross_attention_dim: Optional[int] = None,\n        activation_fn: str = \"geglu\",\n        num_embeds_ada_norm: Optional[int] = None,\n        attention_bias: bool = False,\n        only_cross_attention: bool = False,\n        upcast_attention: bool = False,\n        norm_elementwise_affine: bool = True,\n        norm_type: str = \"layer_norm\",\n        final_dropout: bool = False,\n    ):\n        super().__init__()\n        self.only_cross_attention = only_cross_attention\n\n        self.use_ada_layer_norm_zero = (num_embeds_ada_norm is not None) and norm_type == \"ada_norm_zero\"\n        self.use_ada_layer_norm = (num_embeds_ada_norm is not None) and norm_type == \"ada_norm\"\n\n        if norm_type in (\"ada_norm\", \"ada_norm_zero\") and num_embeds_ada_norm is None:\n            raise ValueError(\n                f\"`norm_type` is set to {norm_type}, but `num_embeds_ada_norm` is not defined. Please make sure to\"\n                f\" define `num_embeds_ada_norm` if setting `norm_type` to {norm_type}.\"\n            )\n\n        # 1. Self-Attn\n        self.attn1 = CrossAttention(\n            query_dim=dim,\n            heads=num_attention_heads,\n            dim_head=attention_head_dim,\n            dropout=dropout,\n            bias=attention_bias,\n            cross_attention_dim=cross_attention_dim if only_cross_attention else None,\n            upcast_attention=upcast_attention,\n        )\n\n        self.ff = FeedForward(dim, dropout=dropout, activation_fn=activation_",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 193, "column": 4 },
          "end": { "row": 193, "column": 4 }
        }
      }
    }
  ],
  [
    "2423",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        encoder_hidden_states=None,\n        timestep=None,\n        attention_mask=None,\n        cross_attention_kwargs=None,\n        class_labels=None,\n    ):\n        if self.use_ada_layer_norm:\n            norm_hidden_states = self.norm1(hidden_states, timestep)\n        elif self.use_ada_layer_norm_zero:\n            norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(\n                hidden_states, timestep, class_labels, hidden_dtype=hidden_states.dtype\n            )\n        else:\n            norm_hidden_states = self.norm1(hidden_states)\n\n        # 1. Self-Attention\n        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}\n        attn_output = self.attn1(\n            norm_hidden_states,\n            encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None,\n            attention_mask=attention_mask,\n            **cross_attention_kwargs,\n        )\n        if self.use_ada_layer_norm_zero:\n            attn_output = gate_msa.unsqueeze(1) * attn_output\n        hidden_states = attn_output + hidden_states\n\n        if self.attn2 is not None:\n            norm_hidden_states = (\n                self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n            )\n\n            # 2. Cross-Attention\n            attn_output = self.attn2(\n                norm_hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                attention_mask=attention_mask,\n                **cross_attenti",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 270, "column": 4 },
          "end": { "row": 270, "column": 4 }
        }
      }
    }
  ],
  [
    "2424",
    {
      "pageContent": "class FeedForward(nn.Module):\n    r\"\"\"\n    A feed-forward layer.\n\n    Parameters:\n        dim (`int`): The number of channels in the input.\n        dim_out (`int`, *optional*): The number of channels in the output. If not given, defaults to `dim`.\n        mult (`int`, *optional*, defaults to 4): The multiplier to use for the hidden dimension.\n        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n        final_dropout (`bool` *optional*, defaults to False): Apply a final dropout.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        dim_out: Optional[int] = None,\n        mult: int = 4,\n        dropout: float = 0.0,\n        activation_fn: str = \"geglu\",\n        final_dropout: bool = False,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = dim_out if dim_out is not None else dim\n\n        if activation_fn == \"gelu\":\n            act_fn = GELU(dim, inner_dim)\n        if activation_fn == \"gelu-approximate\":\n            act_fn = GELU(dim, inner_dim, approximate=\"tanh\")\n        elif activation_fn == \"geglu\":\n            act_fn = GEGLU(dim, inner_dim)\n        elif activation_fn == \"geglu-approximate\":\n            act_fn = ApproximateGELU(dim, inner_dim)\n\n        self.net = nn.ModuleList([])\n        # project in\n        self.net.append(act_fn)\n        # project dropout\n        self.net.append(nn.Dropout(dropout))\n        # project out\n        self.net.append(nn.Linear(inner_dim, dim_out))\n",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 330, "column": 0 },
          "end": { "row": 330, "column": 0 }
        }
      }
    }
  ],
  [
    "2425",
    {
      "pageContent": "def __init__(\n        self,\n        dim: int,\n        dim_out: Optional[int] = None,\n        mult: int = 4,\n        dropout: float = 0.0,\n        activation_fn: str = \"geglu\",\n        final_dropout: bool = False,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = dim_out if dim_out is not None else dim\n\n        if activation_fn == \"gelu\":\n            act_fn = GELU(dim, inner_dim)\n        if activation_fn == \"gelu-approximate\":\n            act_fn = GELU(dim, inner_dim, approximate=\"tanh\")\n        elif activation_fn == \"geglu\":\n            act_fn = GEGLU(dim, inner_dim)\n        elif activation_fn == \"geglu-approximate\":\n            act_fn = ApproximateGELU(dim, inner_dim)\n\n        self.net = nn.ModuleList([])\n        # project in\n        self.net.append(act_fn)\n        # project dropout\n        self.net.append(nn.Dropout(dropout))\n        # project out\n        self.net.append(nn.Linear(inner_dim, dim_out))\n        # FF as used in Vision Transformer, MLP-Mixer, etc. have a final dropout\n        if final_dropout:\n            self.net.append(nn.Dropout(dropout))",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 343, "column": 4 },
          "end": { "row": 343, "column": 4 }
        }
      }
    }
  ],
  [
    "2426",
    {
      "pageContent": "def forward(self, hidden_states):\n        for module in self.net:\n            hidden_states = module(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 376, "column": 4 },
          "end": { "row": 376, "column": 4 }
        }
      }
    }
  ],
  [
    "2427",
    {
      "pageContent": "class GELU(nn.Module):\n    r\"\"\"\n    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out)\n        self.approximate = approximate\n\n    def gelu(self, gate):\n        if gate.device.type != \"mps\":\n            return F.gelu(gate, approximate=self.approximate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n\n    def forward(self, hidden_states):\n        hidden_states = self.proj(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 382, "column": 0 },
          "end": { "row": 382, "column": 0 }
        }
      }
    }
  ],
  [
    "2428",
    {
      "pageContent": "def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out)\n        self.approximate = approximate",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 387, "column": 4 },
          "end": { "row": 387, "column": 4 }
        }
      }
    }
  ],
  [
    "2429",
    {
      "pageContent": "def gelu(self, gate):\n        if gate.device.type != \"mps\":\n            return F.gelu(gate, approximate=self.approximate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 392, "column": 4 },
          "end": { "row": 392, "column": 4 }
        }
      }
    }
  ],
  [
    "2430",
    {
      "pageContent": "def forward(self, hidden_states):\n        hidden_states = self.proj(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 398, "column": 4 },
          "end": { "row": 398, "column": 4 }
        }
      }
    }
  ],
  [
    "2431",
    {
      "pageContent": "class GEGLU(nn.Module):\n    r\"\"\"\n    A variant of the gated linear unit activation function from https://arxiv.org/abs/2002.05202.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def gelu(self, gate):\n        if gate.device.type != \"mps\":\n            return F.gelu(gate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32)).to(dtype=gate.dtype)\n\n    def forward(self, hidden_states):\n        hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n        return hidden_states * self.gelu(gate)",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 404, "column": 0 },
          "end": { "row": 404, "column": 0 }
        }
      }
    }
  ],
  [
    "2432",
    {
      "pageContent": "def gelu(self, gate):\n        if gate.device.type != \"mps\":\n            return F.gelu(gate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32)).to(dtype=gate.dtype)",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 417, "column": 4 },
          "end": { "row": 417, "column": 4 }
        }
      }
    }
  ],
  [
    "2433",
    {
      "pageContent": "class ApproximateGELU(nn.Module):\n    \"\"\"\n    The approximate form of Gaussian Error Linear Unit (GELU)\n\n    For more details, see section 2: https://arxiv.org/abs/1606.08415\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out)\n\n    def forward(self, x):\n        x = self.proj(x)\n        return x * torch.sigmoid(1.702 * x)",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 428, "column": 0 },
          "end": { "row": 428, "column": 0 }
        }
      }
    }
  ],
  [
    "2434",
    {
      "pageContent": "class AdaLayerNorm(nn.Module):\n    \"\"\"\n    Norm layer modified to incorporate timestep embeddings.\n    \"\"\"\n\n    def __init__(self, embedding_dim, num_embeddings):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n        self.silu = nn.SiLU()\n        self.linear = nn.Linear(embedding_dim, embedding_dim * 2)\n        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n\n    def forward(self, x, timestep):\n        emb = self.linear(self.silu(self.emb(timestep)))\n        scale, shift = torch.chunk(emb, 2)\n        x = self.norm(x) * (1 + scale) + shift\n        return x",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 444, "column": 0 },
          "end": { "row": 444, "column": 0 }
        }
      }
    }
  ],
  [
    "2435",
    {
      "pageContent": "def __init__(self, embedding_dim, num_embeddings):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n        self.silu = nn.SiLU()\n        self.linear = nn.Linear(embedding_dim, embedding_dim * 2)\n        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False)",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 449, "column": 4 },
          "end": { "row": 449, "column": 4 }
        }
      }
    }
  ],
  [
    "2436",
    {
      "pageContent": "def forward(self, x, timestep):\n        emb = self.linear(self.silu(self.emb(timestep)))\n        scale, shift = torch.chunk(emb, 2)\n        x = self.norm(x) * (1 + scale) + shift\n        return x",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 456, "column": 4 },
          "end": { "row": 456, "column": 4 }
        }
      }
    }
  ],
  [
    "2437",
    {
      "pageContent": "class AdaLayerNormZero(nn.Module):\n    \"\"\"\n    Norm layer adaptive layer norm zero (adaLN-Zero).\n    \"\"\"\n\n    def __init__(self, embedding_dim, num_embeddings):\n        super().__init__()\n\n        self.emb = CombinedTimestepLabelEmbeddings(num_embeddings, embedding_dim)\n\n        self.silu = nn.SiLU()\n        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=True)\n        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-6)\n\n    def forward(self, x, timestep, class_labels, hidden_dtype=None):\n        emb = self.linear(self.silu(self.emb(timestep, class_labels, hidden_dtype=hidden_dtype)))\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = emb.chunk(6, dim=1)\n        x = self.norm(x) * (1 + scale_msa[:, None]) + shift_msa[:, None]\n        return x, gate_msa, shift_mlp, scale_mlp, gate_mlp",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 463, "column": 0 },
          "end": { "row": 463, "column": 0 }
        }
      }
    }
  ],
  [
    "2438",
    {
      "pageContent": "def __init__(self, embedding_dim, num_embeddings):\n        super().__init__()\n\n        self.emb = CombinedTimestepLabelEmbeddings(num_embeddings, embedding_dim)\n\n        self.silu = nn.SiLU()\n        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=True)\n        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-6)",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 468, "column": 4 },
          "end": { "row": 468, "column": 4 }
        }
      }
    }
  ],
  [
    "2439",
    {
      "pageContent": "def forward(self, x, timestep, class_labels, hidden_dtype=None):\n        emb = self.linear(self.silu(self.emb(timestep, class_labels, hidden_dtype=hidden_dtype)))\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = emb.chunk(6, dim=1)\n        x = self.norm(x) * (1 + scale_msa[:, None]) + shift_msa[:, None]\n        return x, gate_msa, shift_mlp, scale_mlp, gate_mlp",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 477, "column": 4 },
          "end": { "row": 477, "column": 4 }
        }
      }
    }
  ],
  [
    "2440",
    {
      "pageContent": "class AdaGroupNorm(nn.Module):\n    \"\"\"\n    GroupNorm layer modified to incorporate timestep embeddings.\n    \"\"\"\n\n    def __init__(\n        self, embedding_dim: int, out_dim: int, num_groups: int, act_fn: Optional[str] = None, eps: float = 1e-5\n    ):\n        super().__init__()\n        self.num_groups = num_groups\n        self.eps = eps\n        self.act = None\n        if act_fn == \"swish\":\n            self.act = lambda x: F.silu(x)\n        elif act_fn == \"mish\":\n            self.act = nn.Mish()\n        elif act_fn == \"silu\":\n            self.act = nn.SiLU()\n        elif act_fn == \"gelu\":\n            self.act = nn.GELU()\n\n        self.linear = nn.Linear(embedding_dim, out_dim * 2)\n\n    def forward(self, x, emb):\n        if self.act:\n            emb = self.act(emb)\n        emb = self.linear(emb)\n        emb = emb[:, :, None, None]\n        scale, shift = emb.chunk(2, dim=1)\n\n        x = F.group_norm(x, self.num_groups, eps=self.eps)\n        x = x * (1 + scale) + shift\n        return x",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 484, "column": 0 },
          "end": { "row": 484, "column": 0 }
        }
      }
    }
  ],
  [
    "2441",
    {
      "pageContent": "def __init__(\n        self, embedding_dim: int, out_dim: int, num_groups: int, act_fn: Optional[str] = None, eps: float = 1e-5\n    ):\n        super().__init__()\n        self.num_groups = num_groups\n        self.eps = eps\n        self.act = None\n        if act_fn == \"swish\":\n            self.act = lambda x: F.silu(x)\n        elif act_fn == \"mish\":\n            self.act = nn.Mish()\n        elif act_fn == \"silu\":\n            self.act = nn.SiLU()\n        elif act_fn == \"gelu\":\n            self.act = nn.GELU()\n\n        self.linear = nn.Linear(embedding_dim, out_dim * 2)",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 489, "column": 4 },
          "end": { "row": 489, "column": 4 }
        }
      }
    }
  ],
  [
    "2442",
    {
      "pageContent": "def forward(self, x, emb):\n        if self.act:\n            emb = self.act(emb)\n        emb = self.linear(emb)\n        emb = emb[:, :, None, None]\n        scale, shift = emb.chunk(2, dim=1)\n\n        x = F.group_norm(x, self.num_groups, eps=self.eps)\n        x = x * (1 + scale) + shift\n        return x",
      "metadata": {
        "source": "src/diffusers/models/attention.py",
        "range": {
          "start": { "row": 507, "column": 4 },
          "end": { "row": 507, "column": 4 }
        }
      }
    }
  ],
  [
    "2443",
    {
      "pageContent": "def load_flax_checkpoint_in_pytorch_model(pt_model, model_file):\r\n    try:\r\n        with open(model_file, \"rb\") as flax_state_f:\r\n            flax_state = from_bytes(None, flax_state_f.read())\r\n    except UnpicklingError as e:\r\n        try:\r\n            with open(model_file) as f:\r\n                if f.read().startswith(\"version\"):\r\n                    raise OSError(\r\n                        \"You seem to have cloned a repository without having git-lfs installed. Please\"\r\n                        \" install git-lfs and run `git lfs install` followed by `git lfs pull` in the\"\r\n                        \" folder you cloned.\"\r\n                    )\r\n                else:\r\n                    raise ValueError from e\r\n        except (UnicodeDecodeError, ValueError):\r\n            raise EnvironmentError(f\"Unable to convert {model_file} to Flax deserializable object. \")\r\n\r\n    return load_flax_weights_in_pytorch_model(pt_model, flax_state)",
      "metadata": {
        "source": "src/diffusers/models/modeling_pytorch_flax_utils.py",
        "range": {
          "start": { "row": 36, "column": 0 },
          "end": { "row": 36, "column": 0 }
        }
      }
    }
  ],
  [
    "2444",
    {
      "pageContent": "def load_flax_weights_in_pytorch_model(pt_model, flax_state):\r\n    \"\"\"Load flax checkpoints in a PyTorch model\"\"\"\r\n\r\n    try:\r\n        import torch  # noqa: F401\r\n    except ImportError:\r\n        logger.error(\r\n            \"Loading Flax weights in PyTorch requires both PyTorch and Flax to be installed. Please see\"\r\n            \" https://pytorch.org/ and https://flax.readthedocs.io/en/latest/installation.html for installation\"\r\n            \" instructions.\"\r\n        )\r\n        raise\r\n\r\n    # check if we have bf16 weights\r\n    is_type_bf16 = flatten_dict(jax.tree_util.tree_map(lambda x: x.dtype == jnp.bfloat16, flax_state)).values()\r\n    if any(is_type_bf16):\r\n        # convert all weights to fp32 if they are bf16 since torch.from_numpy can-not handle bf16\r\n\r\n        # and bf16 is not fully supported in PT yet.\r\n        logger.warning(\r\n            \"Found ``bfloat16`` weights in Flax model. Casting all ``bfloat16`` weights to ``float32`` \"\r\n            \"before loading those in PyTorch model.\"\r\n        )\r\n        flax_state = jax.tree_util.tree_map(\r\n            lambda params: params.astype(np.float32) if params.dtype == jnp.bfloat16 else params, flax_state\r\n        )\r\n\r\n    pt_model.base_model_prefix = \"\"\r\n\r\n    flax_state_dict = flatten_dict(flax_state, sep=\".\")\r\n    pt_model_dict = pt_model.state_dict()\r\n\r\n    # keep track of unexpected & missing keys\r\n    unexpected_keys = []\r\n    missing_keys = set(pt_model_dict.keys())\r\n\r\n    for flax_key_tuple, flax_tensor in flax_state_dict.items():\r\n        flax_key_tuple_array = flax_key_tuple.split(\".\")\r\n\r\n        if flax_key_tuple_a",
      "metadata": {
        "source": "src/diffusers/models/modeling_pytorch_flax_utils.py",
        "range": {
          "start": { "row": 57, "column": 0 },
          "end": { "row": 57, "column": 0 }
        }
      }
    }
  ],
  [
    "2445",
    {
      "pageContent": "class FlaxUpsample2D(nn.Module):\n    out_channels: int\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.conv = nn.Conv(\n            self.out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n    def __call__(self, hidden_states):\n        batch, height, width, channels = hidden_states.shape\n        hidden_states = jax.image.resize(\n            hidden_states,\n            shape=(batch, height * 2, width * 2, channels),\n            method=\"nearest\",\n        )\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 18, "column": 0 },
          "end": { "row": 18, "column": 0 }
        }
      }
    }
  ],
  [
    "2446",
    {
      "pageContent": "def setup(self):\n        self.conv = nn.Conv(\n            self.out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 22, "column": 4 },
          "end": { "row": 22, "column": 4 }
        }
      }
    }
  ],
  [
    "2447",
    {
      "pageContent": "def __call__(self, hidden_states):\n        batch, height, width, channels = hidden_states.shape\n        hidden_states = jax.image.resize(\n            hidden_states,\n            shape=(batch, height * 2, width * 2, channels),\n            method=\"nearest\",\n        )\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 31, "column": 4 },
          "end": { "row": 31, "column": 4 }
        }
      }
    }
  ],
  [
    "2448",
    {
      "pageContent": "class FlaxDownsample2D(nn.Module):\n    out_channels: int\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.conv = nn.Conv(\n            self.out_channels,\n            kernel_size=(3, 3),\n            strides=(2, 2),\n            padding=((1, 1), (1, 1)),  # padding=\"VALID\",\n            dtype=self.dtype,\n        )\n\n    def __call__(self, hidden_states):\n        # pad = ((0, 0), (0, 1), (0, 1), (0, 0))  # pad height and width dim\n        # hidden_states = jnp.pad(hidden_states, pad_width=pad)\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "2449",
    {
      "pageContent": "def setup(self):\n        self.conv = nn.Conv(\n            self.out_channels,\n            kernel_size=(3, 3),\n            strides=(2, 2),\n            padding=((1, 1), (1, 1)),  # padding=\"VALID\",\n            dtype=self.dtype,\n        )",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 46, "column": 4 },
          "end": { "row": 46, "column": 4 }
        }
      }
    }
  ],
  [
    "2450",
    {
      "pageContent": "def __call__(self, hidden_states):\n        # pad = ((0, 0), (0, 1), (0, 1), (0, 0))  # pad height and width dim\n        # hidden_states = jnp.pad(hidden_states, pad_width=pad)\n        hidden_states = self.conv(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 55, "column": 4 },
          "end": { "row": 55, "column": 4 }
        }
      }
    }
  ],
  [
    "2451",
    {
      "pageContent": "class FlaxResnetBlock2D(nn.Module):\n    in_channels: int\n    out_channels: int = None\n    dropout_prob: float = 0.0\n    use_nin_shortcut: bool = None\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        out_channels = self.in_channels if self.out_channels is None else self.out_channels\n\n        self.norm1 = nn.GroupNorm(num_groups=32, epsilon=1e-5)\n        self.conv1 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        self.time_emb_proj = nn.Dense(out_channels, dtype=self.dtype)\n\n        self.norm2 = nn.GroupNorm(num_groups=32, epsilon=1e-5)\n        self.dropout = nn.Dropout(self.dropout_prob)\n        self.conv2 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        use_nin_shortcut = self.in_channels != out_channels if self.use_nin_shortcut is None else self.use_nin_shortcut\n\n        self.conv_shortcut = None\n        if use_nin_shortcut:\n            self.conv_shortcut = nn.Conv(\n                out_channels,\n                kernel_size=(1, 1),\n                strides=(1, 1),\n                padding=\"VALID\",\n                dtype=self.dtype,\n            )\n\n    def __call__(self, hidden_states, temb, deterministic=True):\n        residual = hidden_states\n        hidden_states = self.norm1(hidden_states)\n        hidden_states = nn.swish(hidden_states)\n        hidden_states = self.conv1(hidden_states)\n\n       ",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 62, "column": 0 },
          "end": { "row": 62, "column": 0 }
        }
      }
    }
  ],
  [
    "2452",
    {
      "pageContent": "def setup(self):\n        out_channels = self.in_channels if self.out_channels is None else self.out_channels\n\n        self.norm1 = nn.GroupNorm(num_groups=32, epsilon=1e-5)\n        self.conv1 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        self.time_emb_proj = nn.Dense(out_channels, dtype=self.dtype)\n\n        self.norm2 = nn.GroupNorm(num_groups=32, epsilon=1e-5)\n        self.dropout = nn.Dropout(self.dropout_prob)\n        self.conv2 = nn.Conv(\n            out_channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=((1, 1), (1, 1)),\n            dtype=self.dtype,\n        )\n\n        use_nin_shortcut = self.in_channels != out_channels if self.use_nin_shortcut is None else self.use_nin_shortcut\n\n        self.conv_shortcut = None\n        if use_nin_shortcut:\n            self.conv_shortcut = nn.Conv(\n                out_channels,\n                kernel_size=(1, 1),\n                strides=(1, 1),\n                padding=\"VALID\",\n                dtype=self.dtype,\n            )",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 69, "column": 4 },
          "end": { "row": 69, "column": 4 }
        }
      }
    }
  ],
  [
    "2453",
    {
      "pageContent": "def __call__(self, hidden_states, temb, deterministic=True):\n        residual = hidden_states\n        hidden_states = self.norm1(hidden_states)\n        hidden_states = nn.swish(hidden_states)\n        hidden_states = self.conv1(hidden_states)\n\n        temb = self.time_emb_proj(nn.swish(temb))\n        temb = jnp.expand_dims(jnp.expand_dims(temb, 1), 1)\n        hidden_states = hidden_states + temb\n\n        hidden_states = self.norm2(hidden_states)\n        hidden_states = nn.swish(hidden_states)\n        hidden_states = self.dropout(hidden_states, deterministic)\n        hidden_states = self.conv2(hidden_states)\n\n        if self.conv_shortcut is not None:\n            residual = self.conv_shortcut(residual)\n\n        return hidden_states + residual",
      "metadata": {
        "source": "src/diffusers/models/resnet_flax.py",
        "range": {
          "start": { "row": 105, "column": 4 },
          "end": { "row": 105, "column": 4 }
        }
      }
    }
  ],
  [
    "2454",
    {
      "pageContent": "class Upsample1D(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n\n    Parameters:\n            channels: channels in the inputs and outputs.\n            use_conv: a bool determining if a convolution is applied.\n            use_conv_transpose:\n            out_channels:\n    \"\"\"\n\n    def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_conv_transpose = use_conv_transpose\n        self.name = name\n\n        self.conv = None\n        if use_conv_transpose:\n            self.conv = nn.ConvTranspose1d(channels, self.out_channels, 4, 2, 1)\n        elif use_conv:\n            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, padding=1)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.use_conv_transpose:\n            return self.conv(x)\n\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n\n        if self.use_conv:\n            x = self.conv(x)\n\n        return x",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 10, "column": 0 },
          "end": { "row": 10, "column": 0 }
        }
      }
    }
  ],
  [
    "2455",
    {
      "pageContent": "def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_conv_transpose = use_conv_transpose\n        self.name = name\n\n        self.conv = None\n        if use_conv_transpose:\n            self.conv = nn.ConvTranspose1d(channels, self.out_channels, 4, 2, 1)\n        elif use_conv:\n            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, padding=1)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 21, "column": 4 },
          "end": { "row": 21, "column": 4 }
        }
      }
    }
  ],
  [
    "2456",
    {
      "pageContent": "def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.use_conv_transpose:\n            return self.conv(x)\n\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n\n        if self.use_conv:\n            x = self.conv(x)\n\n        return x",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 35, "column": 4 },
          "end": { "row": 35, "column": 4 }
        }
      }
    }
  ],
  [
    "2457",
    {
      "pageContent": "class Downsample1D(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    Parameters:\n        channels: channels in the inputs and outputs.\n        use_conv: a bool determining if a convolution is applied.\n        out_channels:\n        padding:\n    \"\"\"\n\n    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.padding = padding\n        stride = 2\n        self.name = name\n\n        if use_conv:\n            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, stride=stride, padding=padding)\n        else:\n            assert self.channels == self.out_channels\n            self.conv = nn.AvgPool1d(kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.conv(x)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 48, "column": 0 },
          "end": { "row": 48, "column": 0 }
        }
      }
    }
  ],
  [
    "2458",
    {
      "pageContent": "def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.padding = padding\n        stride = 2\n        self.name = name\n\n        if use_conv:\n            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, stride=stride, padding=padding)\n        else:\n            assert self.channels == self.out_channels\n            self.conv = nn.AvgPool1d(kernel_size=stride, stride=stride)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 59, "column": 4 },
          "end": { "row": 59, "column": 4 }
        }
      }
    }
  ],
  [
    "2459",
    {
      "pageContent": "class Upsample2D(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n\n    Parameters:\n        channels: channels in the inputs and outputs.\n        use_conv: a bool determining if a convolution is applied.\n        use_conv_transpose:\n        out_channels:\n    \"\"\"\n\n    def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_conv_transpose = use_conv_transpose\n        self.name = name\n\n        conv = None\n        if use_conv_transpose:\n            conv = nn.ConvTranspose2d(channels, self.out_channels, 4, 2, 1)\n        elif use_conv:\n            conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=1)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.conv = conv\n        else:\n            self.Conv2d_0 = conv\n\n    def forward(self, hidden_states, output_size=None):\n        assert hidden_states.shape[1] == self.channels\n\n        if self.use_conv_transpose:\n            return self.conv(hidden_states)\n\n        # Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16\n        # TODO(Suraj): Remove this cast once the issue is fixed in PyTorch\n        # https://github.com/pytorch/pytorch/issues/86679\n        dtype = hidden_states.dtype\n        if dtype == torch.bfloat16:\n            hidden_states = hidden_states.to(torch.float32)\n\n        # upsa",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 79, "column": 0 },
          "end": { "row": 79, "column": 0 }
        }
      }
    }
  ],
  [
    "2460",
    {
      "pageContent": "def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_conv_transpose = use_conv_transpose\n        self.name = name\n\n        conv = None\n        if use_conv_transpose:\n            conv = nn.ConvTranspose2d(channels, self.out_channels, 4, 2, 1)\n        elif use_conv:\n            conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=1)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.conv = conv\n        else:\n            self.Conv2d_0 = conv",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 90, "column": 4 },
          "end": { "row": 90, "column": 4 }
        }
      }
    }
  ],
  [
    "2461",
    {
      "pageContent": "def forward(self, hidden_states, output_size=None):\n        assert hidden_states.shape[1] == self.channels\n\n        if self.use_conv_transpose:\n            return self.conv(hidden_states)\n\n        # Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16\n        # TODO(Suraj): Remove this cast once the issue is fixed in PyTorch\n        # https://github.com/pytorch/pytorch/issues/86679\n        dtype = hidden_states.dtype\n        if dtype == torch.bfloat16:\n            hidden_states = hidden_states.to(torch.float32)\n\n        # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n        if hidden_states.shape[0] >= 64:\n            hidden_states = hidden_states.contiguous()\n\n        # if `output_size` is passed we force the interpolation output\n        # size and do not make use of `scale_factor=2`\n        if output_size is None:\n            hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n        else:\n            hidden_states = F.interpolate(hidden_states, size=output_size, mode=\"nearest\")\n\n        # If the input is bfloat16, we cast back to bfloat16\n        if dtype == torch.bfloat16:\n            hidden_states = hidden_states.to(dtype)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if self.use_conv:\n            if self.name == \"conv\":\n                hidden_states = self.conv(hidden_states)\n            else:\n                hidden_states = self.Conv2d_0(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 110, "column": 4 },
          "end": { "row": 110, "column": 4 }
        }
      }
    }
  ],
  [
    "2462",
    {
      "pageContent": "class Downsample2D(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    Parameters:\n        channels: channels in the inputs and outputs.\n        use_conv: a bool determining if a convolution is applied.\n        out_channels:\n        padding:\n    \"\"\"\n\n    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.padding = padding\n        stride = 2\n        self.name = name\n\n        if use_conv:\n            conv = nn.Conv2d(self.channels, self.out_channels, 3, stride=stride, padding=padding)\n        else:\n            assert self.channels == self.out_channels\n            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.Conv2d_0 = conv\n            self.conv = conv\n        elif name == \"Conv2d_0\":\n            self.conv = conv\n        else:\n            self.conv = conv\n\n    def forward(self, hidden_states):\n        assert hidden_states.shape[1] == self.channels\n        if self.use_conv and self.padding == 0:\n            pad = (0, 1, 0, 1)\n            hidden_states = F.pad(hidden_states, pad, mode=\"constant\", value=0)\n\n        assert hidden_states.shape[1] == self.channels\n        hidden_states = self.conv(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 148, "column": 0 },
          "end": { "row": 148, "column": 0 }
        }
      }
    }
  ],
  [
    "2463",
    {
      "pageContent": "def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.padding = padding\n        stride = 2\n        self.name = name\n\n        if use_conv:\n            conv = nn.Conv2d(self.channels, self.out_channels, 3, stride=stride, padding=padding)\n        else:\n            assert self.channels == self.out_channels\n            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.Conv2d_0 = conv\n            self.conv = conv\n        elif name == \"Conv2d_0\":\n            self.conv = conv\n        else:\n            self.conv = conv",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 159, "column": 4 },
          "end": { "row": 159, "column": 4 }
        }
      }
    }
  ],
  [
    "2464",
    {
      "pageContent": "def forward(self, hidden_states):\n        assert hidden_states.shape[1] == self.channels\n        if self.use_conv and self.padding == 0:\n            pad = (0, 1, 0, 1)\n            hidden_states = F.pad(hidden_states, pad, mode=\"constant\", value=0)\n\n        assert hidden_states.shape[1] == self.channels\n        hidden_states = self.conv(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 183, "column": 4 },
          "end": { "row": 183, "column": 4 }
        }
      }
    }
  ],
  [
    "2465",
    {
      "pageContent": "class FirUpsample2D(nn.Module):\n    def __init__(self, channels=None, out_channels=None, use_conv=False, fir_kernel=(1, 3, 3, 1)):\n        super().__init__()\n        out_channels = out_channels if out_channels else channels\n        if use_conv:\n            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.use_conv = use_conv\n        self.fir_kernel = fir_kernel\n        self.out_channels = out_channels\n\n    def _upsample_2d(self, hidden_states, weight=None, kernel=None, factor=2, gain=1):\n        \"\"\"Fused `upsample_2d()` followed by `Conv2d()`.\n\n        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more\n        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of\n        arbitrary order.\n\n        Args:\n            hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n            weight: Weight tensor of the shape `[filterH, filterW, inChannels,\n                outChannels]`. Grouped convolution can be performed by `inChannels = x.shape[0] // numGroups`.\n            kernel: FIR filter of the shape `[firH, firW]` or `[firN]`\n                (separable). The default is `[1] * factor`, which corresponds to nearest-neighbor upsampling.\n            factor: Integer upsampling factor (default: 2).\n            gain: Scaling factor for signal magnitude (default: 1.0).\n\n        Returns:\n            output: Tensor of the shape `[N, C, H * factor, W * factor]` or `[N, H * factor, W * factor, C]`, and same\n     ",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 195, "column": 0 },
          "end": { "row": 195, "column": 0 }
        }
      }
    }
  ],
  [
    "2466",
    {
      "pageContent": "def __init__(self, channels=None, out_channels=None, use_conv=False, fir_kernel=(1, 3, 3, 1)):\n        super().__init__()\n        out_channels = out_channels if out_channels else channels\n        if use_conv:\n            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.use_conv = use_conv\n        self.fir_kernel = fir_kernel\n        self.out_channels = out_channels",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 196, "column": 4 },
          "end": { "row": 196, "column": 4 }
        }
      }
    }
  ],
  [
    "2467",
    {
      "pageContent": "def _upsample_2d(self, hidden_states, weight=None, kernel=None, factor=2, gain=1):\n        \"\"\"Fused `upsample_2d()` followed by `Conv2d()`.\n\n        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more\n        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of\n        arbitrary order.\n\n        Args:\n            hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n            weight: Weight tensor of the shape `[filterH, filterW, inChannels,\n                outChannels]`. Grouped convolution can be performed by `inChannels = x.shape[0] // numGroups`.\n            kernel: FIR filter of the shape `[firH, firW]` or `[firN]`\n                (separable). The default is `[1] * factor`, which corresponds to nearest-neighbor upsampling.\n            factor: Integer upsampling factor (default: 2).\n            gain: Scaling factor for signal magnitude (default: 1.0).\n\n        Returns:\n            output: Tensor of the shape `[N, C, H * factor, W * factor]` or `[N, H * factor, W * factor, C]`, and same\n            datatype as `hidden_states`.\n        \"\"\"\n\n        assert isinstance(factor, int) and factor >= 1\n\n        # Setup filter kernel.\n        if kernel is None:\n            kernel = [1] * factor\n\n        # setup kernel\n        kernel = torch.tensor(kernel, dtype=torch.float32)\n        if kernel.ndim == 1:\n            kernel = torch.outer(kernel, kernel)\n        kernel /= torch.sum(kernel)\n\n        kernel = kernel * (gain * (factor**2))\n\n        if self.",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 205, "column": 4 },
          "end": { "row": 205, "column": 4 }
        }
      }
    }
  ],
  [
    "2468",
    {
      "pageContent": "def forward(self, hidden_states):\n        if self.use_conv:\n            height = self._upsample_2d(hidden_states, self.Conv2d_0.weight, kernel=self.fir_kernel)\n            height = height + self.Conv2d_0.bias.reshape(1, -1, 1, 1)\n        else:\n            height = self._upsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)\n\n        return height",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "2469",
    {
      "pageContent": "class FirDownsample2D(nn.Module):\n    def __init__(self, channels=None, out_channels=None, use_conv=False, fir_kernel=(1, 3, 3, 1)):\n        super().__init__()\n        out_channels = out_channels if out_channels else channels\n        if use_conv:\n            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.fir_kernel = fir_kernel\n        self.use_conv = use_conv\n        self.out_channels = out_channels\n\n    def _downsample_2d(self, hidden_states, weight=None, kernel=None, factor=2, gain=1):\n        \"\"\"Fused `Conv2d()` followed by `downsample_2d()`.\n        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more\n        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of\n        arbitrary order.\n\n        Args:\n            hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n            weight:\n                Weight tensor of the shape `[filterH, filterW, inChannels, outChannels]`. Grouped convolution can be\n                performed by `inChannels = x.shape[0] // numGroups`.\n            kernel: FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] *\n            factor`, which corresponds to average pooling.\n            factor: Integer downsampling factor (default: 2).\n            gain: Scaling factor for signal magnitude (default: 1.0).\n\n        Returns:\n            output: Tensor of the shape `[N, C, H // factor, W // factor]` or `[N, H // factor, W // factor, C]`, and",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 295, "column": 0 },
          "end": { "row": 295, "column": 0 }
        }
      }
    }
  ],
  [
    "2470",
    {
      "pageContent": "def __init__(self, channels=None, out_channels=None, use_conv=False, fir_kernel=(1, 3, 3, 1)):\n        super().__init__()\n        out_channels = out_channels if out_channels else channels\n        if use_conv:\n            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.fir_kernel = fir_kernel\n        self.use_conv = use_conv\n        self.out_channels = out_channels",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 296, "column": 4 },
          "end": { "row": 296, "column": 4 }
        }
      }
    }
  ],
  [
    "2471",
    {
      "pageContent": "def _downsample_2d(self, hidden_states, weight=None, kernel=None, factor=2, gain=1):\n        \"\"\"Fused `Conv2d()` followed by `downsample_2d()`.\n        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more\n        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of\n        arbitrary order.\n\n        Args:\n            hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n            weight:\n                Weight tensor of the shape `[filterH, filterW, inChannels, outChannels]`. Grouped convolution can be\n                performed by `inChannels = x.shape[0] // numGroups`.\n            kernel: FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] *\n            factor`, which corresponds to average pooling.\n            factor: Integer downsampling factor (default: 2).\n            gain: Scaling factor for signal magnitude (default: 1.0).\n\n        Returns:\n            output: Tensor of the shape `[N, C, H // factor, W // factor]` or `[N, H // factor, W // factor, C]`, and\n            same datatype as `x`.\n        \"\"\"\n\n        assert isinstance(factor, int) and factor >= 1\n        if kernel is None:\n            kernel = [1] * factor\n\n        # setup kernel\n        kernel = torch.tensor(kernel, dtype=torch.float32)\n        if kernel.ndim == 1:\n            kernel = torch.outer(kernel, kernel)\n        kernel /= torch.sum(kernel)\n\n        kernel = kernel * gain\n\n        if self.use_conv:\n            _, _, convH, convW = weight.s",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 305, "column": 4 },
          "end": { "row": 305, "column": 4 }
        }
      }
    }
  ],
  [
    "2472",
    {
      "pageContent": "def forward(self, hidden_states):\n        if self.use_conv:\n            downsample_input = self._downsample_2d(hidden_states, weight=self.Conv2d_0.weight, kernel=self.fir_kernel)\n            hidden_states = downsample_input + self.Conv2d_0.bias.reshape(1, -1, 1, 1)\n        else:\n            hidden_states = self._downsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 359, "column": 4 },
          "end": { "row": 359, "column": 4 }
        }
      }
    }
  ],
  [
    "2473",
    {
      "pageContent": "class KDownsample2D(nn.Module):\n    def __init__(self, pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]])\n        self.pad = kernel_1d.shape[1] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d.T @ kernel_1d, persistent=False)\n\n    def forward(self, x):\n        x = F.pad(x, (self.pad,) * 4, self.pad_mode)\n        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0], self.kernel.shape[1]])\n        indices = torch.arange(x.shape[1], device=x.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv2d(x, weight, stride=2)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 370, "column": 0 },
          "end": { "row": 370, "column": 0 }
        }
      }
    }
  ],
  [
    "2474",
    {
      "pageContent": "def __init__(self, pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]])\n        self.pad = kernel_1d.shape[1] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d.T @ kernel_1d, persistent=False)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 371, "column": 4 },
          "end": { "row": 371, "column": 4 }
        }
      }
    }
  ],
  [
    "2475",
    {
      "pageContent": "def forward(self, x):\n        x = F.pad(x, (self.pad,) * 4, self.pad_mode)\n        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0], self.kernel.shape[1]])\n        indices = torch.arange(x.shape[1], device=x.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv2d(x, weight, stride=2)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 378, "column": 4 },
          "end": { "row": 378, "column": 4 }
        }
      }
    }
  ],
  [
    "2476",
    {
      "pageContent": "class KUpsample2D(nn.Module):\n    def __init__(self, pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]]) * 2\n        self.pad = kernel_1d.shape[1] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d.T @ kernel_1d, persistent=False)\n\n    def forward(self, x):\n        x = F.pad(x, ((self.pad + 1) // 2,) * 4, self.pad_mode)\n        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0], self.kernel.shape[1]])\n        indices = torch.arange(x.shape[1], device=x.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv_transpose2d(x, weight, stride=2, padding=self.pad * 2 + 1)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 386, "column": 0 },
          "end": { "row": 386, "column": 0 }
        }
      }
    }
  ],
  [
    "2477",
    {
      "pageContent": "def __init__(self, pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]]) * 2\n        self.pad = kernel_1d.shape[1] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d.T @ kernel_1d, persistent=False)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 387, "column": 4 },
          "end": { "row": 387, "column": 4 }
        }
      }
    }
  ],
  [
    "2478",
    {
      "pageContent": "def forward(self, x):\n        x = F.pad(x, ((self.pad + 1) // 2,) * 4, self.pad_mode)\n        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0], self.kernel.shape[1]])\n        indices = torch.arange(x.shape[1], device=x.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv_transpose2d(x, weight, stride=2, padding=self.pad * 2 + 1)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 394, "column": 4 },
          "end": { "row": 394, "column": 4 }
        }
      }
    }
  ],
  [
    "2479",
    {
      "pageContent": "class ResnetBlock2D(nn.Module):\n    r\"\"\"\n    A Resnet block.\n\n    Parameters:\n        in_channels (`int`): The number of channels in the input.\n        out_channels (`int`, *optional*, default to be `None`):\n            The number of output channels for the first conv2d layer. If None, same as `in_channels`.\n        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.\n        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.\n        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.\n        groups_out (`int`, *optional*, default to None):\n            The number of groups to use for the second normalization layer. if set to None, same as `groups`.\n        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.\n        non_linearity (`str`, *optional*, default to `\"swish\"`): the activation function to use.\n        time_embedding_norm (`str`, *optional*, default to `\"default\"` ): Time scale shift config.\n            By default, apply timestep embedding conditioning with a simple shift mechanism. Choose \"scale_shift\" or\n            \"ada_group\" for a stronger conditioning with scale and shift.\n        kernal (`torch.FloatTensor`, optional, default to None): FIR filter, see\n            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].\n        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.\n        use_in_shortcut (`bool`, *optional*, def",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 402, "column": 0 },
          "end": { "row": 402, "column": 0 }
        }
      }
    }
  ],
  [
    "2480",
    {
      "pageContent": "def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        conv_shortcut=False,\n        dropout=0.0,\n        temb_channels=512,\n        groups=32,\n        groups_out=None,\n        pre_norm=True,\n        eps=1e-6,\n        non_linearity=\"swish\",\n        time_embedding_norm=\"default\",  # default, scale_shift, ada_group\n        kernel=None,\n        output_scale_factor=1.0,\n        use_in_shortcut=None,\n        up=False,\n        down=False,\n        conv_shortcut_bias: bool = True,\n        conv_2d_out_channels: Optional[int] = None,\n    ):\n        super().__init__()\n        self.pre_norm = pre_norm\n        self.pre_norm = True\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n        self.up = up\n        self.down = down\n        self.output_scale_factor = output_scale_factor\n        self.time_embedding_norm = time_embedding_norm\n\n        if groups_out is None:\n            groups_out = groups\n\n        if self.time_embedding_norm == \"ada_group\":\n            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)\n        else:\n            self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n\n        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n\n        if temb_channels is not None:\n            if self.time_embedding_norm == \"default\":\n                self.time_emb_proj = torch.nn.Linear(tem",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 433, "column": 4 },
          "end": { "row": 433, "column": 4 }
        }
      }
    }
  ],
  [
    "2481",
    {
      "pageContent": "def forward(self, input_tensor, temb):\n        hidden_states = input_tensor\n\n        if self.time_embedding_norm == \"ada_group\":\n            hidden_states = self.norm1(hidden_states, temb)\n        else:\n            hidden_states = self.norm1(hidden_states)\n\n        hidden_states = self.nonlinearity(hidden_states)\n\n        if self.upsample is not None:\n            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n            if hidden_states.shape[0] >= 64:\n                input_tensor = input_tensor.contiguous()\n                hidden_states = hidden_states.contiguous()\n            input_tensor = self.upsample(input_tensor)\n            hidden_states = self.upsample(hidden_states)\n        elif self.downsample is not None:\n            input_tensor = self.downsample(input_tensor)\n            hidden_states = self.downsample(hidden_states)\n\n        hidden_states = self.conv1(hidden_states)\n\n        if self.time_emb_proj is not None:\n            temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n\n        if temb is not None and self.time_embedding_norm == \"default\":\n            hidden_states = hidden_states + temb\n\n        if self.time_embedding_norm == \"ada_group\":\n            hidden_states = self.norm2(hidden_states, temb)\n        else:\n            hidden_states = self.norm2(hidden_states)\n\n        if temb is not None and self.time_embedding_norm == \"scale_shift\":\n            scale, shift = torch.chunk(temb, 2, dim=1)\n            hidden_states = hidden_states * (1 + scale) + shift\n\n        hidden_stat",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 533, "column": 4 },
          "end": { "row": 533, "column": 4 }
        }
      }
    }
  ],
  [
    "2482",
    {
      "pageContent": "def rearrange_dims(tensor):\n    if len(tensor.shape) == 2:\n        return tensor[:, :, None]\n    if len(tensor.shape) == 3:\n        return tensor[:, :, None, :]\n    elif len(tensor.shape) == 4:\n        return tensor[:, :, 0, :]\n    else:\n        raise ValueError(f\"`len(tensor)`: {len(tensor)} has to be 2, 3 or 4.\")",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 590, "column": 0 },
          "end": { "row": 590, "column": 0 }
        }
      }
    }
  ],
  [
    "2483",
    {
      "pageContent": "class Conv1dBlock(nn.Module):\n    \"\"\"\n    Conv1d --> GroupNorm --> Mish\n    \"\"\"\n\n    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n        super().__init__()\n\n        self.conv1d = nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.group_norm = nn.GroupNorm(n_groups, out_channels)\n        self.mish = nn.Mish()\n\n    def forward(self, x):\n        x = self.conv1d(x)\n        x = rearrange_dims(x)\n        x = self.group_norm(x)\n        x = rearrange_dims(x)\n        x = self.mish(x)\n        return x",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 601, "column": 0 },
          "end": { "row": 601, "column": 0 }
        }
      }
    }
  ],
  [
    "2484",
    {
      "pageContent": "def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n        super().__init__()\n\n        self.conv1d = nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.group_norm = nn.GroupNorm(n_groups, out_channels)\n        self.mish = nn.Mish()",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 606, "column": 4 },
          "end": { "row": 606, "column": 4 }
        }
      }
    }
  ],
  [
    "2485",
    {
      "pageContent": "def forward(self, x):\n        x = self.conv1d(x)\n        x = rearrange_dims(x)\n        x = self.group_norm(x)\n        x = rearrange_dims(x)\n        x = self.mish(x)\n        return x",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 613, "column": 4 },
          "end": { "row": 613, "column": 4 }
        }
      }
    }
  ],
  [
    "2486",
    {
      "pageContent": "class ResidualTemporalBlock1D(nn.Module):\n    def __init__(self, inp_channels, out_channels, embed_dim, kernel_size=5):\n        super().__init__()\n        self.conv_in = Conv1dBlock(inp_channels, out_channels, kernel_size)\n        self.conv_out = Conv1dBlock(out_channels, out_channels, kernel_size)\n\n        self.time_emb_act = nn.Mish()\n        self.time_emb = nn.Linear(embed_dim, out_channels)\n\n        self.residual_conv = (\n            nn.Conv1d(inp_channels, out_channels, 1) if inp_channels != out_channels else nn.Identity()\n        )\n\n    def forward(self, x, t):\n        \"\"\"\n        Args:\n            x : [ batch_size x inp_channels x horizon ]\n            t : [ batch_size x embed_dim ]\n\n        returns:\n            out : [ batch_size x out_channels x horizon ]\n        \"\"\"\n        t = self.time_emb_act(t)\n        t = self.time_emb(t)\n        out = self.conv_in(x) + rearrange_dims(t)\n        out = self.conv_out(out)\n        return out + self.residual_conv(x)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 623, "column": 0 },
          "end": { "row": 623, "column": 0 }
        }
      }
    }
  ],
  [
    "2487",
    {
      "pageContent": "def __init__(self, inp_channels, out_channels, embed_dim, kernel_size=5):\n        super().__init__()\n        self.conv_in = Conv1dBlock(inp_channels, out_channels, kernel_size)\n        self.conv_out = Conv1dBlock(out_channels, out_channels, kernel_size)\n\n        self.time_emb_act = nn.Mish()\n        self.time_emb = nn.Linear(embed_dim, out_channels)\n\n        self.residual_conv = (\n            nn.Conv1d(inp_channels, out_channels, 1) if inp_channels != out_channels else nn.Identity()\n        )",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 624, "column": 4 },
          "end": { "row": 624, "column": 4 }
        }
      }
    }
  ],
  [
    "2488",
    {
      "pageContent": "def forward(self, x, t):\n        \"\"\"\n        Args:\n            x : [ batch_size x inp_channels x horizon ]\n            t : [ batch_size x embed_dim ]\n\n        returns:\n            out : [ batch_size x out_channels x horizon ]\n        \"\"\"\n        t = self.time_emb_act(t)\n        t = self.time_emb(t)\n        out = self.conv_in(x) + rearrange_dims(t)\n        out = self.conv_out(out)\n        return out + self.residual_conv(x)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 636, "column": 4 },
          "end": { "row": 636, "column": 4 }
        }
      }
    }
  ],
  [
    "2489",
    {
      "pageContent": "def upsample_2d(hidden_states, kernel=None, factor=2, gain=1):\n    r\"\"\"Upsample2D a batch of 2D images with the given filter.\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and upsamples each image with the given\n    filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified\n    `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is\n    a: multiple of the upsampling factor.\n\n    Args:\n        hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n        kernel: FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to nearest-neighbor upsampling.\n        factor: Integer upsampling factor (default: 2).\n        gain: Scaling factor for signal magnitude (default: 1.0).\n\n    Returns:\n        output: Tensor of the shape `[N, C, H * factor, W * factor]`\n    \"\"\"\n    assert isinstance(factor, int) and factor >= 1\n    if kernel is None:\n        kernel = [1] * factor\n\n    kernel = torch.tensor(kernel, dtype=torch.float32)\n    if kernel.ndim == 1:\n        kernel = torch.outer(kernel, kernel)\n    kernel /= torch.sum(kernel)\n\n    kernel = kernel * (gain * (factor**2))\n    pad_value = kernel.shape[0] - factor\n    output = upfirdn2d_native(\n        hidden_states,\n        kernel.to(device=hidden_states.device),\n        up=factor,\n        pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2),\n    )\n    return output",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 652, "column": 0 },
          "end": { "row": 652, "column": 0 }
        }
      }
    }
  ],
  [
    "2490",
    {
      "pageContent": "def downsample_2d(hidden_states, kernel=None, factor=2, gain=1):\n    r\"\"\"Downsample2D a batch of 2D images with the given filter.\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and downsamples each image with the\n    given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the\n    specified `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its\n    shape is a multiple of the downsampling factor.\n\n    Args:\n        hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n        kernel: FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to average pooling.\n        factor: Integer downsampling factor (default: 2).\n        gain: Scaling factor for signal magnitude (default: 1.0).\n\n    Returns:\n        output: Tensor of the shape `[N, C, H // factor, W // factor]`\n    \"\"\"\n\n    assert isinstance(factor, int) and factor >= 1\n    if kernel is None:\n        kernel = [1] * factor\n\n    kernel = torch.tensor(kernel, dtype=torch.float32)\n    if kernel.ndim == 1:\n        kernel = torch.outer(kernel, kernel)\n    kernel /= torch.sum(kernel)\n\n    kernel = kernel * gain\n    pad_value = kernel.shape[0] - factor\n    output = upfirdn2d_native(\n        hidden_states, kernel.to(device=hidden_states.device), down=factor, pad=((pad_value + 1) // 2, pad_value // 2)\n    )\n    return output",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 689, "column": 0 },
          "end": { "row": 689, "column": 0 }
        }
      }
    }
  ],
  [
    "2491",
    {
      "pageContent": "def upfirdn2d_native(tensor, kernel, up=1, down=1, pad=(0, 0)):\n    up_x = up_y = up\n    down_x = down_y = down\n    pad_x0 = pad_y0 = pad[0]\n    pad_x1 = pad_y1 = pad[1]\n\n    _, channel, in_h, in_w = tensor.shape\n    tensor = tensor.reshape(-1, in_h, in_w, 1)\n\n    _, in_h, in_w, minor = tensor.shape\n    kernel_h, kernel_w = kernel.shape\n\n    out = tensor.view(-1, in_h, 1, in_w, 1, minor)\n    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n\n    out = F.pad(out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)])\n    out = out.to(tensor.device)  # Move back to mps if necessary\n    out = out[\n        :,\n        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n        :,\n    ]\n\n    out = out.permute(0, 3, 1, 2)\n    out = out.reshape([-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1])\n    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n    out = F.conv2d(out, w)\n    out = out.reshape(\n        -1,\n        minor,\n        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n    )\n    out = out.permute(0, 2, 3, 1)\n    out = out[:, ::down_y, ::down_x, :]\n\n    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n\n    return out.view(-1, channel, out_h, out_w)",
      "metadata": {
        "source": "src/diffusers/models/resnet.py",
        "range": {
          "start": { "row": 724, "column": 0 },
          "end": { "row": 724, "column": 0 }
        }
      }
    }
  ],
  [
    "2492",
    {
      "pageContent": "class VQEncoderOutput(BaseOutput):\n    \"\"\"\n    Output of VQModel encoding method.\n\n    Args:\n        latents (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            Encoded output sample of the model. Output of the last layer of the model.\n    \"\"\"\n\n    latents: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/models/vq_model.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "2493",
    {
      "pageContent": "class VQModel(ModelMixin, ConfigMixin):\n    r\"\"\"VQ-VAE model from the paper Neural Discrete Representation Learning by Aaron van den Oord, Oriol Vinyals and Koray\n    Kavukcuoglu.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the model (such as downloading or saving, etc.)\n\n    Parameters:\n        in_channels (int, *optional*, defaults to 3): Number of channels in the input image.\n        out_channels (int,  *optional*, defaults to 3): Number of channels in the output.\n        down_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"DownEncoderBlock2D\",)`): Tuple of downsample block types.\n        up_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"UpDecoderBlock2D\",)`): Tuple of upsample block types.\n        block_out_channels (`Tuple[int]`, *optional*, defaults to :\n            obj:`(64,)`): Tuple of block output channels.\n        act_fn (`str`, *optional*, defaults to `\"silu\"`): The activation function to use.\n        latent_channels (`int`, *optional*, defaults to `3`): Number of channels in the latent space.\n        sample_size (`int`, *optional*, defaults to `32`): TODO\n        num_vq_embeddings (`int`, *optional*, defaults to `256`): Number of codebook vectors in the VQ-VAE.\n        vq_embed_dim (`int`, *optional*): Hidden dim of codebook vectors in the VQ-VAE.\n        scaling_factor (`float`, *optional*, defaults to `0.18215`):\n            The component-wise standard deviation of the trained latent space computed using the first ba",
      "metadata": {
        "source": "src/diffusers/models/vq_model.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "2494",
    {
      "pageContent": "def encode(self, x: torch.FloatTensor, return_dict: bool = True) -> VQEncoderOutput:\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n\n        if not return_dict:\n            return (h,)\n\n        return VQEncoderOutput(latents=h)",
      "metadata": {
        "source": "src/diffusers/models/vq_model.py",
        "range": {
          "start": { "row": 116, "column": 4 },
          "end": { "row": 116, "column": 4 }
        }
      }
    }
  ],
  [
    "2495",
    {
      "pageContent": "def decode(\n        self, h: torch.FloatTensor, force_not_quantize: bool = False, return_dict: bool = True\n    ) -> Union[DecoderOutput, torch.FloatTensor]:\n        # also go through quantization layer\n        if not force_not_quantize:\n            quant, emb_loss, info = self.quantize(h)\n        else:\n            quant = h\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n\n        if not return_dict:\n            return (dec,)\n\n        return DecoderOutput(sample=dec)",
      "metadata": {
        "source": "src/diffusers/models/vq_model.py",
        "range": {
          "start": { "row": 125, "column": 4 },
          "end": { "row": 125, "column": 4 }
        }
      }
    }
  ],
  [
    "2496",
    {
      "pageContent": "def forward(self, sample: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\n        r\"\"\"\n        Args:\n            sample (`torch.FloatTensor`): Input sample.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`DecoderOutput`] instead of a plain tuple.\n        \"\"\"\n        x = sample\n        h = self.encode(x).latents\n        dec = self.decode(h).sample\n\n        if not return_dict:\n            return (dec,)\n\n        return DecoderOutput(sample=dec)",
      "metadata": {
        "source": "src/diffusers/models/vq_model.py",
        "range": {
          "start": { "row": 141, "column": 4 },
          "end": { "row": 141, "column": 4 }
        }
      }
    }
  ],
  [
    "2497",
    {
      "pageContent": "class UNet2DOutput(BaseOutput):\n    \"\"\"\n    Args:\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            Hidden states output. Output of last layer of model.\n    \"\"\"\n\n    sample: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/models/unet_2d.py",
        "range": {
          "start": { "row": 27, "column": 0 },
          "end": { "row": 27, "column": 0 }
        }
      }
    }
  ],
  [
    "2498",
    {
      "pageContent": "class UNet2DModel(ModelMixin, ConfigMixin):\n    r\"\"\"\n    UNet2DModel is a 2D UNet model that takes in a noisy sample and a timestep and returns sample shaped output.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the model (such as downloading or saving, etc.)\n\n    Parameters:\n        sample_size (`int` or `Tuple[int, int]`, *optional*, defaults to `None`):\n            Height and width of input/output sample.\n        in_channels (`int`, *optional*, defaults to 3): Number of channels in the input image.\n        out_channels (`int`, *optional*, defaults to 3): Number of channels in the output.\n        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.\n        time_embedding_type (`str`, *optional*, defaults to `\"positional\"`): Type of time embedding to use.\n        freq_shift (`int`, *optional*, defaults to 0): Frequency shift for fourier time embedding.\n        flip_sin_to_cos (`bool`, *optional*, defaults to :\n            obj:`True`): Whether to flip sin to cos for fourier time embedding.\n        down_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\")`): Tuple of downsample block\n            types.\n        mid_block_type (`str`, *optional*, defaults to `\"UNetMidBlock2D\"`):\n            The mid block type. Choose from `UNetMidBlock2D` or `UnCLIPUNetMidBlock2D`.\n        up_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"Attn",
      "metadata": {
        "source": "src/diffusers/models/unet_2d.py",
        "range": {
          "start": { "row": 37, "column": 0 },
          "end": { "row": 37, "column": 0 }
        }
      }
    }
  ],
  [
    "2499",
    {
      "pageContent": "def forward(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[torch.Tensor, float, int],\n        class_labels: Optional[torch.Tensor] = None,\n        return_dict: bool = True,\n    ) -> Union[UNet2DOutput, Tuple]:\n        r\"\"\"\n        Args:\n            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n            timestep (`torch.FloatTensor` or `float` or `int): (batch) timesteps\n            class_labels (`torch.FloatTensor`, *optional*, defaults to `None`):\n                Optional class labels for conditioning. Their embeddings will be summed with the timestep embeddings.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~models.unet_2d.UNet2DOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~models.unet_2d.UNet2DOutput`] or `tuple`: [`~models.unet_2d.UNet2DOutput`] if `return_dict` is True,\n            otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        # 0. center input if necessary\n        if self.config.center_input_sample:\n            sample = 2 * sample - 1.0\n\n        # 1. time\n        timesteps = timestep\n        if not torch.is_tensor(timesteps):\n            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n            timesteps = timesteps[None].to(sample.device)\n\n        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n        timesteps = timesteps * torc",
      "metadata": {
        "source": "src/diffusers/models/unet_2d.py",
        "range": {
          "start": { "row": 217, "column": 4 },
          "end": { "row": 217, "column": 4 }
        }
      }
    }
  ],
  [
    "2500",
    {
      "pageContent": "class FlaxCrossAttention(nn.Module):\n    r\"\"\"\n    A Flax multi-head attention module as described in: https://arxiv.org/abs/1706.03762\n\n    Parameters:\n        query_dim (:obj:`int`):\n            Input hidden states dimension\n        heads (:obj:`int`, *optional*, defaults to 8):\n            Number of heads\n        dim_head (:obj:`int`, *optional*, defaults to 64):\n            Hidden states dimension inside each head\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n\n    \"\"\"\n    query_dim: int\n    heads: int = 8\n    dim_head: int = 64\n    dropout: float = 0.0\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        inner_dim = self.dim_head * self.heads\n        self.scale = self.dim_head**-0.5\n\n        # Weights were exported with old names {to_q, to_k, to_v, to_out}\n        self.query = nn.Dense(inner_dim, use_bias=False, dtype=self.dtype, name=\"to_q\")\n        self.key = nn.Dense(inner_dim, use_bias=False, dtype=self.dtype, name=\"to_k\")\n        self.value = nn.Dense(inner_dim, use_bias=False, dtype=self.dtype, name=\"to_v\")\n\n        self.proj_attn = nn.Dense(self.query_dim, dtype=self.dtype, name=\"to_out_0\")\n\n    def reshape_heads_to_batch_dim(self, tensor):\n        batch_size, seq_len, dim = tensor.shape\n        head_size = self.heads\n        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n        tensor = jnp.transpose(tensor, (0, 2, 1, 3))\n        tensor = tensor.reshape(batch_size * head_size, seq_len",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 18, "column": 0 },
          "end": { "row": 18, "column": 0 }
        }
      }
    }
  ],
  [
    "2501",
    {
      "pageContent": "def setup(self):\n        inner_dim = self.dim_head * self.heads\n        self.scale = self.dim_head**-0.5\n\n        # Weights were exported with old names {to_q, to_k, to_v, to_out}\n        self.query = nn.Dense(inner_dim, use_bias=False, dtype=self.dtype, name=\"to_q\")\n        self.key = nn.Dense(inner_dim, use_bias=False, dtype=self.dtype, name=\"to_k\")\n        self.value = nn.Dense(inner_dim, use_bias=False, dtype=self.dtype, name=\"to_v\")\n\n        self.proj_attn = nn.Dense(self.query_dim, dtype=self.dtype, name=\"to_out_0\")",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 41, "column": 4 },
          "end": { "row": 41, "column": 4 }
        }
      }
    }
  ],
  [
    "2502",
    {
      "pageContent": "def reshape_heads_to_batch_dim(self, tensor):\n        batch_size, seq_len, dim = tensor.shape\n        head_size = self.heads\n        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n        tensor = jnp.transpose(tensor, (0, 2, 1, 3))\n        tensor = tensor.reshape(batch_size * head_size, seq_len, dim // head_size)\n        return tensor",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 52, "column": 4 },
          "end": { "row": 52, "column": 4 }
        }
      }
    }
  ],
  [
    "2503",
    {
      "pageContent": "def reshape_batch_dim_to_heads(self, tensor):\n        batch_size, seq_len, dim = tensor.shape\n        head_size = self.heads\n        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n        tensor = jnp.transpose(tensor, (0, 2, 1, 3))\n        tensor = tensor.reshape(batch_size // head_size, seq_len, dim * head_size)\n        return tensor",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 60, "column": 4 },
          "end": { "row": 60, "column": 4 }
        }
      }
    }
  ],
  [
    "2504",
    {
      "pageContent": "def __call__(self, hidden_states, context=None, deterministic=True):\n        context = hidden_states if context is None else context\n\n        query_proj = self.query(hidden_states)\n        key_proj = self.key(context)\n        value_proj = self.value(context)\n\n        query_states = self.reshape_heads_to_batch_dim(query_proj)\n        key_states = self.reshape_heads_to_batch_dim(key_proj)\n        value_states = self.reshape_heads_to_batch_dim(value_proj)\n\n        # compute attentions\n        attention_scores = jnp.einsum(\"b i d, b j d->b i j\", query_states, key_states)\n        attention_scores = attention_scores * self.scale\n        attention_probs = nn.softmax(attention_scores, axis=2)\n\n        # attend to values\n        hidden_states = jnp.einsum(\"b i j, b j d -> b i d\", attention_probs, value_states)\n        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n        hidden_states = self.proj_attn(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 68, "column": 4 },
          "end": { "row": 68, "column": 4 }
        }
      }
    }
  ],
  [
    "2505",
    {
      "pageContent": "class FlaxBasicTransformerBlock(nn.Module):\n    r\"\"\"\n    A Flax transformer block layer with `GLU` (Gated Linear Unit) activation function as described in:\n    https://arxiv.org/abs/1706.03762\n\n\n    Parameters:\n        dim (:obj:`int`):\n            Inner hidden states dimension\n        n_heads (:obj:`int`):\n            Number of heads\n        d_head (:obj:`int`):\n            Hidden states dimension inside each head\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        only_cross_attention (`bool`, defaults to `False`):\n            Whether to only apply cross attention.\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    dim: int\n    n_heads: int\n    d_head: int\n    dropout: float = 0.0\n    only_cross_attention: bool = False\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        # self attention (or cross_attention if only_cross_attention is True)\n        self.attn1 = FlaxCrossAttention(self.dim, self.n_heads, self.d_head, self.dropout, dtype=self.dtype)\n        # cross attention\n        self.attn2 = FlaxCrossAttention(self.dim, self.n_heads, self.d_head, self.dropout, dtype=self.dtype)\n        self.ff = FlaxFeedForward(dim=self.dim, dropout=self.dropout, dtype=self.dtype)\n        self.norm1 = nn.LayerNorm(epsilon=1e-5, dtype=self.dtype)\n        self.norm2 = nn.LayerNorm(epsilon=1e-5, dtype=self.dtype)\n        self.norm3 = nn.LayerNorm(epsilon=1e-5, dtype=self.dtype)\n\n    def __call__(self, hidden_states, context, deterministic=True):\n        # self attention\n     ",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 91, "column": 0 },
          "end": { "row": 91, "column": 0 }
        }
      }
    }
  ],
  [
    "2506",
    {
      "pageContent": "def setup(self):\n        # self attention (or cross_attention if only_cross_attention is True)\n        self.attn1 = FlaxCrossAttention(self.dim, self.n_heads, self.d_head, self.dropout, dtype=self.dtype)\n        # cross attention\n        self.attn2 = FlaxCrossAttention(self.dim, self.n_heads, self.d_head, self.dropout, dtype=self.dtype)\n        self.ff = FlaxFeedForward(dim=self.dim, dropout=self.dropout, dtype=self.dtype)\n        self.norm1 = nn.LayerNorm(epsilon=1e-5, dtype=self.dtype)\n        self.norm2 = nn.LayerNorm(epsilon=1e-5, dtype=self.dtype)\n        self.norm3 = nn.LayerNorm(epsilon=1e-5, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 118, "column": 4 },
          "end": { "row": 118, "column": 4 }
        }
      }
    }
  ],
  [
    "2507",
    {
      "pageContent": "def __call__(self, hidden_states, context, deterministic=True):\n        # self attention\n        residual = hidden_states\n        if self.only_cross_attention:\n            hidden_states = self.attn1(self.norm1(hidden_states), context, deterministic=deterministic)\n        else:\n            hidden_states = self.attn1(self.norm1(hidden_states), deterministic=deterministic)\n        hidden_states = hidden_states + residual\n\n        # cross attention\n        residual = hidden_states\n        hidden_states = self.attn2(self.norm2(hidden_states), context, deterministic=deterministic)\n        hidden_states = hidden_states + residual\n\n        # feed forward\n        residual = hidden_states\n        hidden_states = self.ff(self.norm3(hidden_states), deterministic=deterministic)\n        hidden_states = hidden_states + residual\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 128, "column": 4 },
          "end": { "row": 128, "column": 4 }
        }
      }
    }
  ],
  [
    "2508",
    {
      "pageContent": "class FlaxTransformer2DModel(nn.Module):\n    r\"\"\"\n    A Spatial Transformer layer with Gated Linear Unit (GLU) activation function as described in:\n    https://arxiv.org/pdf/1506.02025.pdf\n\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input number of channels\n        n_heads (:obj:`int`):\n            Number of heads\n        d_head (:obj:`int`):\n            Hidden states dimension inside each head\n        depth (:obj:`int`, *optional*, defaults to 1):\n            Number of transformers block\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        use_linear_projection (`bool`, defaults to `False`): tbd\n        only_cross_attention (`bool`, defaults to `False`): tbd\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    n_heads: int\n    d_head: int\n    depth: int = 1\n    dropout: float = 0.0\n    use_linear_projection: bool = False\n    only_cross_attention: bool = False\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.norm = nn.GroupNorm(num_groups=32, epsilon=1e-5)\n\n        inner_dim = self.n_heads * self.d_head\n        if self.use_linear_projection:\n            self.proj_in = nn.Dense(inner_dim, dtype=self.dtype)\n        else:\n            self.proj_in = nn.Conv(\n                inner_dim,\n                kernel_size=(1, 1),\n                strides=(1, 1),\n                padding=\"VALID\",\n                dtype=self.dtype,\n            )\n\n        self.transformer_blocks = [\n            FlaxBasicTransformerBlock(\n           ",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 150, "column": 0 },
          "end": { "row": 150, "column": 0 }
        }
      }
    }
  ],
  [
    "2509",
    {
      "pageContent": "def setup(self):\n        self.norm = nn.GroupNorm(num_groups=32, epsilon=1e-5)\n\n        inner_dim = self.n_heads * self.d_head\n        if self.use_linear_projection:\n            self.proj_in = nn.Dense(inner_dim, dtype=self.dtype)\n        else:\n            self.proj_in = nn.Conv(\n                inner_dim,\n                kernel_size=(1, 1),\n                strides=(1, 1),\n                padding=\"VALID\",\n                dtype=self.dtype,\n            )\n\n        self.transformer_blocks = [\n            FlaxBasicTransformerBlock(\n                inner_dim,\n                self.n_heads,\n                self.d_head,\n                dropout=self.dropout,\n                only_cross_attention=self.only_cross_attention,\n                dtype=self.dtype,\n            )\n            for _ in range(self.depth)\n        ]\n\n        if self.use_linear_projection:\n            self.proj_out = nn.Dense(inner_dim, dtype=self.dtype)\n        else:\n            self.proj_out = nn.Conv(\n                inner_dim,\n                kernel_size=(1, 1),\n                strides=(1, 1),\n                padding=\"VALID\",\n                dtype=self.dtype,\n            )",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 181, "column": 4 },
          "end": { "row": 181, "column": 4 }
        }
      }
    }
  ],
  [
    "2510",
    {
      "pageContent": "def __call__(self, hidden_states, context, deterministic=True):\n        batch, height, width, channels = hidden_states.shape\n        residual = hidden_states\n        hidden_states = self.norm(hidden_states)\n        if self.use_linear_projection:\n            hidden_states = hidden_states.reshape(batch, height * width, channels)\n            hidden_states = self.proj_in(hidden_states)\n        else:\n            hidden_states = self.proj_in(hidden_states)\n            hidden_states = hidden_states.reshape(batch, height * width, channels)\n\n        for transformer_block in self.transformer_blocks:\n            hidden_states = transformer_block(hidden_states, context, deterministic=deterministic)\n\n        if self.use_linear_projection:\n            hidden_states = self.proj_out(hidden_states)\n            hidden_states = hidden_states.reshape(batch, height, width, channels)\n        else:\n            hidden_states = hidden_states.reshape(batch, height, width, channels)\n            hidden_states = self.proj_out(hidden_states)\n\n        hidden_states = hidden_states + residual\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 219, "column": 4 },
          "end": { "row": 219, "column": 4 }
        }
      }
    }
  ],
  [
    "2511",
    {
      "pageContent": "class FlaxFeedForward(nn.Module):\n    r\"\"\"\n    Flax module that encapsulates two Linear layers separated by a non-linearity. It is the counterpart of PyTorch's\n    [`FeedForward`] class, with the following simplifications:\n    - The activation function is currently hardcoded to a gated linear unit from:\n    https://arxiv.org/abs/2002.05202\n    - `dim_out` is equal to `dim`.\n    - The number of hidden dimensions is hardcoded to `dim * 4` in [`FlaxGELU`].\n\n    Parameters:\n        dim (:obj:`int`):\n            Inner hidden states dimension\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    dim: int\n    dropout: float = 0.0\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        # The second linear layer needs to be called\n        # net_2 for now to match the index of the Sequential layer\n        self.net_0 = FlaxGEGLU(self.dim, self.dropout, self.dtype)\n        self.net_2 = nn.Dense(self.dim, dtype=self.dtype)\n\n    def __call__(self, hidden_states, deterministic=True):\n        hidden_states = self.net_0(hidden_states)\n        hidden_states = self.net_2(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 244, "column": 0 },
          "end": { "row": 244, "column": 0 }
        }
      }
    }
  ],
  [
    "2512",
    {
      "pageContent": "def setup(self):\n        # The second linear layer needs to be called\n        # net_2 for now to match the index of the Sequential layer\n        self.net_0 = FlaxGEGLU(self.dim, self.dropout, self.dtype)\n        self.net_2 = nn.Dense(self.dim, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 265, "column": 4 },
          "end": { "row": 265, "column": 4 }
        }
      }
    }
  ],
  [
    "2513",
    {
      "pageContent": "def __call__(self, hidden_states, deterministic=True):\n        hidden_states = self.net_0(hidden_states)\n        hidden_states = self.net_2(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 271, "column": 4 },
          "end": { "row": 271, "column": 4 }
        }
      }
    }
  ],
  [
    "2514",
    {
      "pageContent": "class FlaxGEGLU(nn.Module):\n    r\"\"\"\n    Flax implementation of a Linear layer followed by the variant of the gated linear unit activation function from\n    https://arxiv.org/abs/2002.05202.\n\n    Parameters:\n        dim (:obj:`int`):\n            Input hidden states dimension\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    dim: int\n    dropout: float = 0.0\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        inner_dim = self.dim * 4\n        self.proj = nn.Dense(inner_dim * 2, dtype=self.dtype)\n\n    def __call__(self, hidden_states, deterministic=True):\n        hidden_states = self.proj(hidden_states)\n        hidden_linear, hidden_gelu = jnp.split(hidden_states, 2, axis=2)\n        return hidden_linear * nn.gelu(hidden_gelu)",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 277, "column": 0 },
          "end": { "row": 277, "column": 0 }
        }
      }
    }
  ],
  [
    "2515",
    {
      "pageContent": "def __call__(self, hidden_states, deterministic=True):\n        hidden_states = self.proj(hidden_states)\n        hidden_linear, hidden_gelu = jnp.split(hidden_states, 2, axis=2)\n        return hidden_linear * nn.gelu(hidden_gelu)",
      "metadata": {
        "source": "src/diffusers/models/attention_flax.py",
        "range": {
          "start": { "row": 298, "column": 4 },
          "end": { "row": 298, "column": 4 }
        }
      }
    }
  ],
  [
    "2516",
    {
      "pageContent": "class ControlNetConditioningEmbedding(nn.Module):\n    \"\"\"\n    Quoting from https://arxiv.org/abs/2302.05543: \"Stable Diffusion uses a pre-processing method similar to VQ-GAN\n    [11] to convert the entire dataset of 512  512 images into smaller 64  64 latent images for stabilized\n    training. This requires ControlNets to convert image-based conditions to 64  64 feature space to match the\n    convolution size. We use a tiny network E() of four convolution layers with 4  4 kernels and 2  2 strides\n    (activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly with the full\n    model) to encode image-space conditions ... into feature maps ...\"\n    \"\"\"\n\n    def __init__(\n        self,\n        conditioning_embedding_channels: int,\n        conditioning_channels: int = 3,\n        block_out_channels: Tuple[int] = (16, 32, 96, 256),\n    ):\n        super().__init__()\n\n        self.conv_in = nn.Conv2d(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1)\n\n        self.blocks = nn.ModuleList([])\n\n        for i in range(len(block_out_channels) - 1):\n            channel_in = block_out_channels[i]\n            channel_out = block_out_channels[i + 1]\n            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=1))\n            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1, stride=2))\n\n        self.conv_out = zero_module(\n            nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1)\n        )\n\n    def forward(self, conditio",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 42, "column": 0 },
          "end": { "row": 42, "column": 0 }
        }
      }
    }
  ],
  [
    "2517",
    {
      "pageContent": "def __init__(\n        self,\n        conditioning_embedding_channels: int,\n        conditioning_channels: int = 3,\n        block_out_channels: Tuple[int] = (16, 32, 96, 256),\n    ):\n        super().__init__()\n\n        self.conv_in = nn.Conv2d(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1)\n\n        self.blocks = nn.ModuleList([])\n\n        for i in range(len(block_out_channels) - 1):\n            channel_in = block_out_channels[i]\n            channel_out = block_out_channels[i + 1]\n            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=1))\n            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1, stride=2))\n\n        self.conv_out = zero_module(\n            nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1)\n        )",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 52, "column": 4 },
          "end": { "row": 52, "column": 4 }
        }
      }
    }
  ],
  [
    "2518",
    {
      "pageContent": "def forward(self, conditioning):\n        embedding = self.conv_in(conditioning)\n        embedding = F.silu(embedding)\n\n        for block in self.blocks:\n            embedding = block(embedding)\n            embedding = F.silu(embedding)\n\n        embedding = self.conv_out(embedding)\n\n        return embedding",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 74, "column": 4 },
          "end": { "row": 74, "column": 4 }
        }
      }
    }
  ],
  [
    "2519",
    {
      "pageContent": "class ControlNetModel(ModelMixin, ConfigMixin):\n    _supports_gradient_checkpointing = True\n\n    @register_to_config\n    def __init__(\n        self,\n        in_channels: int = 4,\n        flip_sin_to_cos: bool = True,\n        freq_shift: int = 0,\n        down_block_types: Tuple[str] = (\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"DownBlock2D\",\n        ),\n        only_cross_attention: Union[bool, Tuple[bool]] = False,\n        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n        layers_per_block: int = 2,\n        downsample_padding: int = 1,\n        mid_block_scale_factor: float = 1,\n        act_fn: str = \"silu\",\n        norm_num_groups: Optional[int] = 32,\n        norm_eps: float = 1e-5,\n        cross_attention_dim: int = 1280,\n        attention_head_dim: Union[int, Tuple[int]] = 8,\n        use_linear_projection: bool = False,\n        class_embed_type: Optional[str] = None,\n        num_class_embeds: Optional[int] = None,\n        upcast_attention: bool = False,\n        resnet_time_scale_shift: str = \"default\",\n        projection_class_embeddings_input_dim: Optional[int] = None,\n        controlnet_conditioning_channel_order: str = \"rgb\",\n        conditioning_embedding_out_channels: Optional[Tuple[int]] = (16, 32, 96, 256),\n    ):\n        super().__init__()\n\n        # Check inputs\n        if len(block_out_channels) != len(down_block_types):\n            raise ValueError(\n                f\"Must provide the same number of `block_out_channels` as `down_block_types`. `block_out_channels`: {block_",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 87, "column": 0 },
          "end": { "row": 87, "column": 0 }
        }
      }
    }
  ],
  [
    "2520",
    {
      "pageContent": "def set_attn_processor(self, processor: Union[AttnProcessor, Dict[str, AttnProcessor]]):\n        r\"\"\"\n        Parameters:\n            `processor (`dict` of `AttnProcessor` or `AttnProcessor`):\n                The instantiated processor class or a dictionary of processor classes that will be set as the processor\n                of **all** `CrossAttention` layers.\n            In case `processor` is a dict, the key needs to define the path to the corresponding cross attention processor. This is strongly recommended when setting trainablae attention processors.:\n\n        \"\"\"\n        count = len(self.attn_processors.keys())\n\n        if isinstance(processor, dict) and len(processor) != count:\n            raise ValueError(\n                f\"A dict of processors was passed, but the number of processors {len(processor)} does not match the\"\n                f\" number of attention layers: {count}. Please make sure to pass {count} processor classes.\"\n            )\n\n        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):\n            if hasattr(module, \"set_processor\"):\n                if not isinstance(processor, dict):\n                    module.set_processor(processor)\n                else:\n                    module.set_processor(processor.pop(f\"{name}.processor\"))\n\n            for sub_name, child in module.named_children():\n                fn_recursive_attn_processor(f\"{name}.{sub_name}\", child, processor)\n\n        for name, module in self.named_children():\n            fn_recursive_attn_processor(name, module, processor)",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "2521",
    {
      "pageContent": "def set_attention_slice(self, slice_size):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int` or `list(int)`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                `\"max\"`, maxium amount of memory will be saved by running only one slice at a time. If a number is\n                provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n                must be a multiple of `slice_size`.\n        \"\"\"\n        sliceable_head_dims = []\n\n        def fn_recursive_retrieve_slicable_dims(module: torch.nn.Module):\n            if hasattr(module, \"set_attention_slice\"):\n                sliceable_head_dims.append(module.sliceable_head_dim)\n\n            for child in module.children():\n                fn_recursive_retrieve_slicable_dims(child)\n\n        # retrieve number of attention layers\n        for module in self.children():\n            fn_recursive_retrieve_slicable_dims(module)\n\n        num_slicable_layers = len(sliceable_head_dims)\n\n        if slice_size == \"auto\":\n            # half the attention head size is usually a good trade-off between\n            # speed and memory\n            slice_size = [dim // 2 for dim in sliceable_head_dims]\n        el",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 316, "column": 4 },
          "end": { "row": 316, "column": 4 }
        }
      }
    }
  ],
  [
    "2522",
    {
      "pageContent": "def forward(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[torch.Tensor, float, int],\n        encoder_hidden_states: torch.Tensor,\n        controlnet_cond: torch.FloatTensor,\n        class_labels: Optional[torch.Tensor] = None,\n        timestep_cond: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        return_dict: bool = True,\n    ) -> Union[ControlNetOutput, Tuple]:\n        # check channel order\n        channel_order = self.config.controlnet_conditioning_channel_order\n\n        if channel_order == \"rgb\":\n            # in rgb order by default\n            ...\n        elif channel_order == \"bgr\":\n            controlnet_cond = torch.flip(controlnet_cond, dims=[1])\n        else:\n            raise ValueError(f\"unknown `controlnet_conditioning_channel_order`: {channel_order}\")\n\n        # prepare attention_mask\n        if attention_mask is not None:\n            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n            attention_mask = attention_mask.unsqueeze(1)\n\n        # 1. time\n        timesteps = timestep\n        if not torch.is_tensor(timesteps):\n            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n            # This would be a good case for the `match` statement (Python 3.10+)\n            is_mps = sample.device.type == \"mps\"\n            if isinstance(timestep, float):\n                dtype = torch.float32 if is_mps else torch.float64\n            else:\n                dtyp",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 385, "column": 4 },
          "end": { "row": 385, "column": 4 }
        }
      }
    }
  ],
  [
    "2523",
    {
      "pageContent": "def zero_module(module):\n    for p in module.parameters():\n        nn.init.zeros_(p)\n    return module",
      "metadata": {
        "source": "src/diffusers/models/controlnet.py",
        "range": {
          "start": { "row": 502, "column": 0 },
          "end": { "row": 502, "column": 0 }
        }
      }
    }
  ],
  [
    "2524",
    {
      "pageContent": "class FlaxModelMixin:\n    r\"\"\"\n    Base class for all flax models.\n\n    [`FlaxModelMixin`] takes care of storing the configuration of the models and handles methods for loading,\n    downloading and saving models.\n    \"\"\"\n    config_name = CONFIG_NAME\n    _automatically_saved_args = [\"_diffusers_version\", \"_class_name\", \"_name_or_path\"]\n    _flax_internal_args = [\"name\", \"parent\", \"dtype\"]\n\n    @classmethod\n    def _from_config(cls, config, **kwargs):\n        \"\"\"\n        All context managers that the model should be initialized under go here.\n        \"\"\"\n        return cls(config, **kwargs)\n\n    def _cast_floating_to(self, params: Union[Dict, FrozenDict], dtype: jnp.dtype, mask: Any = None) -> Any:\n        \"\"\"\n        Helper method to cast floating-point values of given parameter `PyTree` to given `dtype`.\n        \"\"\"\n\n        # taken from https://github.com/deepmind/jmp/blob/3a8318abc3292be38582794dbf7b094e6583b192/jmp/_src/policy.py#L27\n        def conditional_cast(param):\n            if isinstance(param, jnp.ndarray) and jnp.issubdtype(param.dtype, jnp.floating):\n                param = param.astype(dtype)\n            return param\n\n        if mask is None:\n            return jax.tree_map(conditional_cast, params)\n\n        flat_params = flatten_dict(params)\n        flat_mask, _ = jax.tree_flatten(mask)\n\n        for masked, key in zip(flat_mask, flat_params.keys()):\n            if masked:\n                param = flat_params[key]\n                flat_params[key] = conditional_cast(param)\n\n        return unflatten_dict(flat_params)\n\n    def to_bf16(self, params: Union[Dict, F",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_utils.py",
        "range": {
          "start": { "row": 44, "column": 0 },
          "end": { "row": 44, "column": 0 }
        }
      }
    }
  ],
  [
    "2525",
    {
      "pageContent": "def _cast_floating_to(self, params: Union[Dict, FrozenDict], dtype: jnp.dtype, mask: Any = None) -> Any:\n        \"\"\"\n        Helper method to cast floating-point values of given parameter `PyTree` to given `dtype`.\n        \"\"\"\n\n        # taken from https://github.com/deepmind/jmp/blob/3a8318abc3292be38582794dbf7b094e6583b192/jmp/_src/policy.py#L27\n        def conditional_cast(param):\n            if isinstance(param, jnp.ndarray) and jnp.issubdtype(param.dtype, jnp.floating):\n                param = param.astype(dtype)\n            return param\n\n        if mask is None:\n            return jax.tree_map(conditional_cast, params)\n\n        flat_params = flatten_dict(params)\n        flat_mask, _ = jax.tree_flatten(mask)\n\n        for masked, key in zip(flat_mask, flat_params.keys()):\n            if masked:\n                param = flat_params[key]\n                flat_params[key] = conditional_cast(param)\n\n        return unflatten_dict(flat_params)",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_utils.py",
        "range": {
          "start": { "row": 62, "column": 4 },
          "end": { "row": 62, "column": 4 }
        }
      }
    }
  ],
  [
    "2526",
    {
      "pageContent": "def to_bf16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n        r\"\"\"\n        Cast the floating-point `params` to `jax.numpy.bfloat16`. This returns a new `params` tree and does not cast\n        the `params` in place.\n\n        This method can be used on TPU to explicitly convert the model parameters to bfloat16 precision to do full\n        half-precision training or to save weights in bfloat16 for inference in order to save memory and improve speed.\n\n        Arguments:\n            params (`Union[Dict, FrozenDict]`):\n                A `PyTree` of model parameters.\n            mask (`Union[Dict, FrozenDict]`):\n                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n                you want to cast, and should be `False` for those you want to skip.\n\n        Examples:\n\n        ```python\n        >>> from diffusers import FlaxUNet2DConditionModel\n\n        >>> # load model\n        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        >>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision\n        >>> params = model.to_bf16(params)\n        >>> # If you don't want to cast certain parameters (for example layer norm bias and scale)\n        >>> # then pass the mask as follows\n        >>> from flax import traverse_util\n\n        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        >>> flat_params = traverse_util.flatten_dict(params)\n        >>> mask = {\n        ...     pa",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_utils.py",
        "range": {
          "start": { "row": 86, "column": 4 },
          "end": { "row": 86, "column": 4 }
        }
      }
    }
  ],
  [
    "2527",
    {
      "pageContent": "def to_fp32(self, params: Union[Dict, FrozenDict], mask: Any = None):\n        r\"\"\"\n        Cast the floating-point `params` to `jax.numpy.float32`. This method can be used to explicitly convert the\n        model parameters to fp32 precision. This returns a new `params` tree and does not cast the `params` in place.\n\n        Arguments:\n            params (`Union[Dict, FrozenDict]`):\n                A `PyTree` of model parameters.\n            mask (`Union[Dict, FrozenDict]`):\n                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n                you want to cast, and should be `False` for those you want to skip\n\n        Examples:\n\n        ```python\n        >>> from diffusers import FlaxUNet2DConditionModel\n\n        >>> # Download model and configuration from huggingface.co\n        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        >>> # By default, the model params will be in fp32, to illustrate the use of this method,\n        >>> # we'll first cast to fp16 and back to fp32\n        >>> params = model.to_f16(params)\n        >>> # now cast back to fp32\n        >>> params = model.to_fp32(params)\n        ```\"\"\"\n        return self._cast_floating_to(params, jnp.float32, mask)",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_utils.py",
        "range": {
          "start": { "row": 125, "column": 4 },
          "end": { "row": 125, "column": 4 }
        }
      }
    }
  ],
  [
    "2528",
    {
      "pageContent": "def to_fp16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n        r\"\"\"\n        Cast the floating-point `params` to `jax.numpy.float16`. This returns a new `params` tree and does not cast the\n        `params` in place.\n\n        This method can be used on GPU to explicitly convert the model parameters to float16 precision to do full\n        half-precision training or to save weights in float16 for inference in order to save memory and improve speed.\n\n        Arguments:\n            params (`Union[Dict, FrozenDict]`):\n                A `PyTree` of model parameters.\n            mask (`Union[Dict, FrozenDict]`):\n                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n                you want to cast, and should be `False` for those you want to skip\n\n        Examples:\n\n        ```python\n        >>> from diffusers import FlaxUNet2DConditionModel\n\n        >>> # load model\n        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        >>> # By default, the model params will be in fp32, to cast these to float16\n        >>> params = model.to_fp16(params)\n        >>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n        >>> # then pass the mask as follows\n        >>> from flax import traverse_util\n\n        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        >>> flat_params = traverse_util.flatten_dict(params)\n        >>> mask = {\n        ...     path: (path[-2] != (\"Layer",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_utils.py",
        "range": {
          "start": { "row": 152, "column": 4 },
          "end": { "row": 152, "column": 4 }
        }
      }
    }
  ],
  [
    "2529",
    {
      "pageContent": "def save_pretrained(\n        self,\n        save_directory: Union[str, os.PathLike],\n        params: Union[Dict, FrozenDict],\n        is_main_process: bool = True,\n    ):\n        \"\"\"\n        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n        `[`~FlaxModelMixin.from_pretrained`]` class method\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n            params (`Union[Dict, FrozenDict]`):\n                A `PyTree` of model parameters.\n            is_main_process (`bool`, *optional*, defaults to `True`):\n                Whether the process calling this is the main process or not. Useful when in distributed training like\n                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n                the main process to avoid race conditions.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n            return\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        model_to_save = self\n\n        # Attach architecture to the config\n        # Save the config\n        if is_main_process:\n            model_to_save.save_config(save_directory)\n\n        # save model\n        output_model_file = os.path.join(save_directory, FLAX_WEIGHTS_NAME)\n        with open(output_model_file, \"wb\") as f:\n            model_bytes = to_bytes(params)\n            f.write(model_bytes)\n\n        logg",
      "metadata": {
        "source": "src/diffusers/models/modeling_flax_utils.py",
        "range": {
          "start": { "row": 486, "column": 4 },
          "end": { "row": 486, "column": 4 }
        }
      }
    }
  ],
  [
    "2530",
    {
      "pageContent": "class DownResnetBlock1D(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        num_layers=1,\n        conv_shortcut=False,\n        temb_channels=32,\n        groups=32,\n        groups_out=None,\n        non_linearity=None,\n        time_embedding_norm=\"default\",\n        output_scale_factor=1.0,\n        add_downsample=True,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n        self.time_embedding_norm = time_embedding_norm\n        self.add_downsample = add_downsample\n        self.output_scale_factor = output_scale_factor\n\n        if groups_out is None:\n            groups_out = groups\n\n        # there will always be at least one resnet\n        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=temb_channels)]\n\n        for _ in range(num_layers):\n            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if non_linearity == \"swish\":\n            self.nonlinearity = lambda x: F.silu(x)\n        elif non_linearity == \"mish\":\n            self.nonlinearity = nn.Mish()\n        elif non_linearity == \"silu\":\n            self.nonlinearity = nn.SiLU()\n        else:\n            self.nonlinearity = None\n\n        self.downsample = None\n        if add_downsample:\n            self.downsample = Downsample1D(out_channels, use_conv=True, padding=",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 22, "column": 0 },
          "end": { "row": 22, "column": 0 }
        }
      }
    }
  ],
  [
    "2531",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        num_layers=1,\n        conv_shortcut=False,\n        temb_channels=32,\n        groups=32,\n        groups_out=None,\n        non_linearity=None,\n        time_embedding_norm=\"default\",\n        output_scale_factor=1.0,\n        add_downsample=True,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n        self.time_embedding_norm = time_embedding_norm\n        self.add_downsample = add_downsample\n        self.output_scale_factor = output_scale_factor\n\n        if groups_out is None:\n            groups_out = groups\n\n        # there will always be at least one resnet\n        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=temb_channels)]\n\n        for _ in range(num_layers):\n            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if non_linearity == \"swish\":\n            self.nonlinearity = lambda x: F.silu(x)\n        elif non_linearity == \"mish\":\n            self.nonlinearity = nn.Mish()\n        elif non_linearity == \"silu\":\n            self.nonlinearity = nn.SiLU()\n        else:\n            self.nonlinearity = None\n\n        self.downsample = None\n        if add_downsample:\n            self.downsample = Downsample1D(out_channels, use_conv=True, padding=1)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 23, "column": 4 },
          "end": { "row": 23, "column": 4 }
        }
      }
    }
  ],
  [
    "2532",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for resnet in self.resnets[1:]:\n            hidden_states = resnet(hidden_states, temb)\n\n        output_states += (hidden_states,)\n\n        if self.nonlinearity is not None:\n            hidden_states = self.nonlinearity(hidden_states)\n\n        if self.downsample is not None:\n            hidden_states = self.downsample(hidden_states)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 70, "column": 4 },
          "end": { "row": 70, "column": 4 }
        }
      }
    }
  ],
  [
    "2533",
    {
      "pageContent": "class UpResnetBlock1D(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        num_layers=1,\n        temb_channels=32,\n        groups=32,\n        groups_out=None,\n        non_linearity=None,\n        time_embedding_norm=\"default\",\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.time_embedding_norm = time_embedding_norm\n        self.add_upsample = add_upsample\n        self.output_scale_factor = output_scale_factor\n\n        if groups_out is None:\n            groups_out = groups\n\n        # there will always be at least one resnet\n        resnets = [ResidualTemporalBlock1D(2 * in_channels, out_channels, embed_dim=temb_channels)]\n\n        for _ in range(num_layers):\n            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if non_linearity == \"swish\":\n            self.nonlinearity = lambda x: F.silu(x)\n        elif non_linearity == \"mish\":\n            self.nonlinearity = nn.Mish()\n        elif non_linearity == \"silu\":\n            self.nonlinearity = nn.SiLU()\n        else:\n            self.nonlinearity = None\n\n        self.upsample = None\n        if add_upsample:\n            self.upsample = Upsample1D(out_channels, use_conv_transpose=True)\n\n    def forward(self, hidden_states, res_hidden_states_tuple=None, temb=None):\n      ",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 88, "column": 0 },
          "end": { "row": 88, "column": 0 }
        }
      }
    }
  ],
  [
    "2534",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        num_layers=1,\n        temb_channels=32,\n        groups=32,\n        groups_out=None,\n        non_linearity=None,\n        time_embedding_norm=\"default\",\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.time_embedding_norm = time_embedding_norm\n        self.add_upsample = add_upsample\n        self.output_scale_factor = output_scale_factor\n\n        if groups_out is None:\n            groups_out = groups\n\n        # there will always be at least one resnet\n        resnets = [ResidualTemporalBlock1D(2 * in_channels, out_channels, embed_dim=temb_channels)]\n\n        for _ in range(num_layers):\n            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if non_linearity == \"swish\":\n            self.nonlinearity = lambda x: F.silu(x)\n        elif non_linearity == \"mish\":\n            self.nonlinearity = nn.Mish()\n        elif non_linearity == \"silu\":\n            self.nonlinearity = nn.SiLU()\n        else:\n            self.nonlinearity = None\n\n        self.upsample = None\n        if add_upsample:\n            self.upsample = Upsample1D(out_channels, use_conv_transpose=True)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 89, "column": 4 },
          "end": { "row": 89, "column": 4 }
        }
      }
    }
  ],
  [
    "2535",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple=None, temb=None):\n        if res_hidden_states_tuple is not None:\n            res_hidden_states = res_hidden_states_tuple[-1]\n            hidden_states = torch.cat((hidden_states, res_hidden_states), dim=1)\n\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for resnet in self.resnets[1:]:\n            hidden_states = resnet(hidden_states, temb)\n\n        if self.nonlinearity is not None:\n            hidden_states = self.nonlinearity(hidden_states)\n\n        if self.upsample is not None:\n            hidden_states = self.upsample(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 134, "column": 4 },
          "end": { "row": 134, "column": 4 }
        }
      }
    }
  ],
  [
    "2536",
    {
      "pageContent": "class ValueFunctionMidBlock1D(nn.Module):\n    def __init__(self, in_channels, out_channels, embed_dim):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.embed_dim = embed_dim\n\n        self.res1 = ResidualTemporalBlock1D(in_channels, in_channels // 2, embed_dim=embed_dim)\n        self.down1 = Downsample1D(out_channels // 2, use_conv=True)\n        self.res2 = ResidualTemporalBlock1D(in_channels // 2, in_channels // 4, embed_dim=embed_dim)\n        self.down2 = Downsample1D(out_channels // 4, use_conv=True)\n\n    def forward(self, x, temb=None):\n        x = self.res1(x, temb)\n        x = self.down1(x)\n        x = self.res2(x, temb)\n        x = self.down2(x)\n        return x",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 152, "column": 0 },
          "end": { "row": 152, "column": 0 }
        }
      }
    }
  ],
  [
    "2537",
    {
      "pageContent": "def __init__(self, in_channels, out_channels, embed_dim):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.embed_dim = embed_dim\n\n        self.res1 = ResidualTemporalBlock1D(in_channels, in_channels // 2, embed_dim=embed_dim)\n        self.down1 = Downsample1D(out_channels // 2, use_conv=True)\n        self.res2 = ResidualTemporalBlock1D(in_channels // 2, in_channels // 4, embed_dim=embed_dim)\n        self.down2 = Downsample1D(out_channels // 4, use_conv=True)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 153, "column": 4 },
          "end": { "row": 153, "column": 4 }
        }
      }
    }
  ],
  [
    "2538",
    {
      "pageContent": "def forward(self, x, temb=None):\n        x = self.res1(x, temb)\n        x = self.down1(x)\n        x = self.res2(x, temb)\n        x = self.down2(x)\n        return x",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 164, "column": 4 },
          "end": { "row": 164, "column": 4 }
        }
      }
    }
  ],
  [
    "2539",
    {
      "pageContent": "class MidResTemporalBlock1D(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        embed_dim,\n        num_layers: int = 1,\n        add_downsample: bool = False,\n        add_upsample: bool = False,\n        non_linearity=None,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.add_downsample = add_downsample\n\n        # there will always be at least one resnet\n        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=embed_dim)]\n\n        for _ in range(num_layers):\n            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=embed_dim))\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if non_linearity == \"swish\":\n            self.nonlinearity = lambda x: F.silu(x)\n        elif non_linearity == \"mish\":\n            self.nonlinearity = nn.Mish()\n        elif non_linearity == \"silu\":\n            self.nonlinearity = nn.SiLU()\n        else:\n            self.nonlinearity = None\n\n        self.upsample = None\n        if add_upsample:\n            self.upsample = Downsample1D(out_channels, use_conv=True)\n\n        self.downsample = None\n        if add_downsample:\n            self.downsample = Downsample1D(out_channels, use_conv=True)\n\n        if self.upsample and self.downsample:\n            raise ValueError(\"Block cannot downsample and upsample\")\n\n    def forward(self, hidden_states, temb):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for resnet in self.resnets[1:]:\n            hidden_states = ",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 172, "column": 0 },
          "end": { "row": 172, "column": 0 }
        }
      }
    }
  ],
  [
    "2540",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels,\n        out_channels,\n        embed_dim,\n        num_layers: int = 1,\n        add_downsample: bool = False,\n        add_upsample: bool = False,\n        non_linearity=None,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.add_downsample = add_downsample\n\n        # there will always be at least one resnet\n        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=embed_dim)]\n\n        for _ in range(num_layers):\n            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=embed_dim))\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if non_linearity == \"swish\":\n            self.nonlinearity = lambda x: F.silu(x)\n        elif non_linearity == \"mish\":\n            self.nonlinearity = nn.Mish()\n        elif non_linearity == \"silu\":\n            self.nonlinearity = nn.SiLU()\n        else:\n            self.nonlinearity = None\n\n        self.upsample = None\n        if add_upsample:\n            self.upsample = Downsample1D(out_channels, use_conv=True)\n\n        self.downsample = None\n        if add_downsample:\n            self.downsample = Downsample1D(out_channels, use_conv=True)\n\n        if self.upsample and self.downsample:\n            raise ValueError(\"Block cannot downsample and upsample\")",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 173, "column": 4 },
          "end": { "row": 173, "column": 4 }
        }
      }
    }
  ],
  [
    "2541",
    {
      "pageContent": "def forward(self, hidden_states, temb):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for resnet in self.resnets[1:]:\n            hidden_states = resnet(hidden_states, temb)\n\n        if self.upsample:\n            hidden_states = self.upsample(hidden_states)\n        if self.downsample:\n            self.downsample = self.downsample(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 216, "column": 4 },
          "end": { "row": 216, "column": 4 }
        }
      }
    }
  ],
  [
    "2542",
    {
      "pageContent": "class OutConv1DBlock(nn.Module):\n    def __init__(self, num_groups_out, out_channels, embed_dim, act_fn):\n        super().__init__()\n        self.final_conv1d_1 = nn.Conv1d(embed_dim, embed_dim, 5, padding=2)\n        self.final_conv1d_gn = nn.GroupNorm(num_groups_out, embed_dim)\n        if act_fn == \"silu\":\n            self.final_conv1d_act = nn.SiLU()\n        if act_fn == \"mish\":\n            self.final_conv1d_act = nn.Mish()\n        self.final_conv1d_2 = nn.Conv1d(embed_dim, out_channels, 1)\n\n    def forward(self, hidden_states, temb=None):\n        hidden_states = self.final_conv1d_1(hidden_states)\n        hidden_states = rearrange_dims(hidden_states)\n        hidden_states = self.final_conv1d_gn(hidden_states)\n        hidden_states = rearrange_dims(hidden_states)\n        hidden_states = self.final_conv1d_act(hidden_states)\n        hidden_states = self.final_conv1d_2(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 229, "column": 0 },
          "end": { "row": 229, "column": 0 }
        }
      }
    }
  ],
  [
    "2543",
    {
      "pageContent": "def __init__(self, num_groups_out, out_channels, embed_dim, act_fn):\n        super().__init__()\n        self.final_conv1d_1 = nn.Conv1d(embed_dim, embed_dim, 5, padding=2)\n        self.final_conv1d_gn = nn.GroupNorm(num_groups_out, embed_dim)\n        if act_fn == \"silu\":\n            self.final_conv1d_act = nn.SiLU()\n        if act_fn == \"mish\":\n            self.final_conv1d_act = nn.Mish()\n        self.final_conv1d_2 = nn.Conv1d(embed_dim, out_channels, 1)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 230, "column": 4 },
          "end": { "row": 230, "column": 4 }
        }
      }
    }
  ],
  [
    "2544",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        hidden_states = self.final_conv1d_1(hidden_states)\n        hidden_states = rearrange_dims(hidden_states)\n        hidden_states = self.final_conv1d_gn(hidden_states)\n        hidden_states = rearrange_dims(hidden_states)\n        hidden_states = self.final_conv1d_act(hidden_states)\n        hidden_states = self.final_conv1d_2(hidden_states)\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 240, "column": 4 },
          "end": { "row": 240, "column": 4 }
        }
      }
    }
  ],
  [
    "2545",
    {
      "pageContent": "class OutValueFunctionBlock(nn.Module):\n    def __init__(self, fc_dim, embed_dim):\n        super().__init__()\n        self.final_block = nn.ModuleList(\n            [\n                nn.Linear(fc_dim + embed_dim, fc_dim // 2),\n                nn.Mish(),\n                nn.Linear(fc_dim // 2, 1),\n            ]\n        )\n\n    def forward(self, hidden_states, temb):\n        hidden_states = hidden_states.view(hidden_states.shape[0], -1)\n        hidden_states = torch.cat((hidden_states, temb), dim=-1)\n        for layer in self.final_block:\n            hidden_states = layer(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 250, "column": 0 },
          "end": { "row": 250, "column": 0 }
        }
      }
    }
  ],
  [
    "2546",
    {
      "pageContent": "def __init__(self, fc_dim, embed_dim):\n        super().__init__()\n        self.final_block = nn.ModuleList(\n            [\n                nn.Linear(fc_dim + embed_dim, fc_dim // 2),\n                nn.Mish(),\n                nn.Linear(fc_dim // 2, 1),\n            ]\n        )",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 251, "column": 4 },
          "end": { "row": 251, "column": 4 }
        }
      }
    }
  ],
  [
    "2547",
    {
      "pageContent": "def forward(self, hidden_states, temb):\n        hidden_states = hidden_states.view(hidden_states.shape[0], -1)\n        hidden_states = torch.cat((hidden_states, temb), dim=-1)\n        for layer in self.final_block:\n            hidden_states = layer(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 261, "column": 4 },
          "end": { "row": 261, "column": 4 }
        }
      }
    }
  ],
  [
    "2548",
    {
      "pageContent": "class Downsample1d(nn.Module):\n    def __init__(self, kernel=\"linear\", pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor(_kernels[kernel])\n        self.pad = kernel_1d.shape[0] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d)\n\n    def forward(self, hidden_states):\n        hidden_states = F.pad(hidden_states, (self.pad,) * 2, self.pad_mode)\n        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])\n        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv1d(hidden_states, weight, stride=2)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 290, "column": 0 },
          "end": { "row": 290, "column": 0 }
        }
      }
    }
  ],
  [
    "2549",
    {
      "pageContent": "def __init__(self, kernel=\"linear\", pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor(_kernels[kernel])\n        self.pad = kernel_1d.shape[0] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 291, "column": 4 },
          "end": { "row": 291, "column": 4 }
        }
      }
    }
  ],
  [
    "2550",
    {
      "pageContent": "def forward(self, hidden_states):\n        hidden_states = F.pad(hidden_states, (self.pad,) * 2, self.pad_mode)\n        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])\n        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv1d(hidden_states, weight, stride=2)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 298, "column": 4 },
          "end": { "row": 298, "column": 4 }
        }
      }
    }
  ],
  [
    "2551",
    {
      "pageContent": "class Upsample1d(nn.Module):\n    def __init__(self, kernel=\"linear\", pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor(_kernels[kernel]) * 2\n        self.pad = kernel_1d.shape[0] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d)\n\n    def forward(self, hidden_states, temb=None):\n        hidden_states = F.pad(hidden_states, ((self.pad + 1) // 2,) * 2, self.pad_mode)\n        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])\n        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv_transpose1d(hidden_states, weight, stride=2, padding=self.pad * 2 + 1)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 306, "column": 0 },
          "end": { "row": 306, "column": 0 }
        }
      }
    }
  ],
  [
    "2552",
    {
      "pageContent": "def __init__(self, kernel=\"linear\", pad_mode=\"reflect\"):\n        super().__init__()\n        self.pad_mode = pad_mode\n        kernel_1d = torch.tensor(_kernels[kernel]) * 2\n        self.pad = kernel_1d.shape[0] // 2 - 1\n        self.register_buffer(\"kernel\", kernel_1d)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 307, "column": 4 },
          "end": { "row": 307, "column": 4 }
        }
      }
    }
  ],
  [
    "2553",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        hidden_states = F.pad(hidden_states, ((self.pad + 1) // 2,) * 2, self.pad_mode)\n        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])\n        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)\n        weight[indices, indices] = self.kernel.to(weight)\n        return F.conv_transpose1d(hidden_states, weight, stride=2, padding=self.pad * 2 + 1)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 314, "column": 4 },
          "end": { "row": 314, "column": 4 }
        }
      }
    }
  ],
  [
    "2554",
    {
      "pageContent": "class SelfAttention1d(nn.Module):\n    def __init__(self, in_channels, n_head=1, dropout_rate=0.0):\n        super().__init__()\n        self.channels = in_channels\n        self.group_norm = nn.GroupNorm(1, num_channels=in_channels)\n        self.num_heads = n_head\n\n        self.query = nn.Linear(self.channels, self.channels)\n        self.key = nn.Linear(self.channels, self.channels)\n        self.value = nn.Linear(self.channels, self.channels)\n\n        self.proj_attn = nn.Linear(self.channels, self.channels, 1)\n\n        self.dropout = nn.Dropout(dropout_rate, inplace=True)\n\n    def transpose_for_scores(self, projection: torch.Tensor) -> torch.Tensor:\n        new_projection_shape = projection.size()[:-1] + (self.num_heads, -1)\n        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D) -> (B, H, T, D)\n        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)\n        return new_projection\n\n    def forward(self, hidden_states):\n        residual = hidden_states\n        batch, channel_dim, seq = hidden_states.shape\n\n        hidden_states = self.group_norm(hidden_states)\n        hidden_states = hidden_states.transpose(1, 2)\n\n        query_proj = self.query(hidden_states)\n        key_proj = self.key(hidden_states)\n        value_proj = self.value(hidden_states)\n\n        query_states = self.transpose_for_scores(query_proj)\n        key_states = self.transpose_for_scores(key_proj)\n        value_states = self.transpose_for_scores(value_proj)\n\n        scale = 1 / math.sqrt(math.sqrt(key_states.shape[-1]))\n\n        attention_scores = torch.matmul(query_states ",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 322, "column": 0 },
          "end": { "row": 322, "column": 0 }
        }
      }
    }
  ],
  [
    "2555",
    {
      "pageContent": "def __init__(self, in_channels, n_head=1, dropout_rate=0.0):\n        super().__init__()\n        self.channels = in_channels\n        self.group_norm = nn.GroupNorm(1, num_channels=in_channels)\n        self.num_heads = n_head\n\n        self.query = nn.Linear(self.channels, self.channels)\n        self.key = nn.Linear(self.channels, self.channels)\n        self.value = nn.Linear(self.channels, self.channels)\n\n        self.proj_attn = nn.Linear(self.channels, self.channels, 1)\n\n        self.dropout = nn.Dropout(dropout_rate, inplace=True)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 323, "column": 4 },
          "end": { "row": 323, "column": 4 }
        }
      }
    }
  ],
  [
    "2556",
    {
      "pageContent": "def transpose_for_scores(self, projection: torch.Tensor) -> torch.Tensor:\n        new_projection_shape = projection.size()[:-1] + (self.num_heads, -1)\n        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D) -> (B, H, T, D)\n        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)\n        return new_projection",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 337, "column": 4 },
          "end": { "row": 337, "column": 4 }
        }
      }
    }
  ],
  [
    "2557",
    {
      "pageContent": "def forward(self, hidden_states):\n        residual = hidden_states\n        batch, channel_dim, seq = hidden_states.shape\n\n        hidden_states = self.group_norm(hidden_states)\n        hidden_states = hidden_states.transpose(1, 2)\n\n        query_proj = self.query(hidden_states)\n        key_proj = self.key(hidden_states)\n        value_proj = self.value(hidden_states)\n\n        query_states = self.transpose_for_scores(query_proj)\n        key_states = self.transpose_for_scores(key_proj)\n        value_states = self.transpose_for_scores(value_proj)\n\n        scale = 1 / math.sqrt(math.sqrt(key_states.shape[-1]))\n\n        attention_scores = torch.matmul(query_states * scale, key_states.transpose(-1, -2) * scale)\n        attention_probs = torch.softmax(attention_scores, dim=-1)\n\n        # compute attention output\n        hidden_states = torch.matmul(attention_probs, value_states)\n\n        hidden_states = hidden_states.permute(0, 2, 1, 3).contiguous()\n        new_hidden_states_shape = hidden_states.size()[:-2] + (self.channels,)\n        hidden_states = hidden_states.view(new_hidden_states_shape)\n\n        # compute next hidden_states\n        hidden_states = self.proj_attn(hidden_states)\n        hidden_states = hidden_states.transpose(1, 2)\n        hidden_states = self.dropout(hidden_states)\n\n        output = hidden_states + residual\n\n        return output",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 343, "column": 4 },
          "end": { "row": 343, "column": 4 }
        }
      }
    }
  ],
  [
    "2558",
    {
      "pageContent": "class ResConvBlock(nn.Module):\n    def __init__(self, in_channels, mid_channels, out_channels, is_last=False):\n        super().__init__()\n        self.is_last = is_last\n        self.has_conv_skip = in_channels != out_channels\n\n        if self.has_conv_skip:\n            self.conv_skip = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n\n        self.conv_1 = nn.Conv1d(in_channels, mid_channels, 5, padding=2)\n        self.group_norm_1 = nn.GroupNorm(1, mid_channels)\n        self.gelu_1 = nn.GELU()\n        self.conv_2 = nn.Conv1d(mid_channels, out_channels, 5, padding=2)\n\n        if not self.is_last:\n            self.group_norm_2 = nn.GroupNorm(1, out_channels)\n            self.gelu_2 = nn.GELU()\n\n    def forward(self, hidden_states):\n        residual = self.conv_skip(hidden_states) if self.has_conv_skip else hidden_states\n\n        hidden_states = self.conv_1(hidden_states)\n        hidden_states = self.group_norm_1(hidden_states)\n        hidden_states = self.gelu_1(hidden_states)\n        hidden_states = self.conv_2(hidden_states)\n\n        if not self.is_last:\n            hidden_states = self.group_norm_2(hidden_states)\n            hidden_states = self.gelu_2(hidden_states)\n\n        output = hidden_states + residual\n        return output",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 380, "column": 0 },
          "end": { "row": 380, "column": 0 }
        }
      }
    }
  ],
  [
    "2559",
    {
      "pageContent": "def __init__(self, in_channels, mid_channels, out_channels, is_last=False):\n        super().__init__()\n        self.is_last = is_last\n        self.has_conv_skip = in_channels != out_channels\n\n        if self.has_conv_skip:\n            self.conv_skip = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n\n        self.conv_1 = nn.Conv1d(in_channels, mid_channels, 5, padding=2)\n        self.group_norm_1 = nn.GroupNorm(1, mid_channels)\n        self.gelu_1 = nn.GELU()\n        self.conv_2 = nn.Conv1d(mid_channels, out_channels, 5, padding=2)\n\n        if not self.is_last:\n            self.group_norm_2 = nn.GroupNorm(1, out_channels)\n            self.gelu_2 = nn.GELU()",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 381, "column": 4 },
          "end": { "row": 381, "column": 4 }
        }
      }
    }
  ],
  [
    "2560",
    {
      "pageContent": "def forward(self, hidden_states):\n        residual = self.conv_skip(hidden_states) if self.has_conv_skip else hidden_states\n\n        hidden_states = self.conv_1(hidden_states)\n        hidden_states = self.group_norm_1(hidden_states)\n        hidden_states = self.gelu_1(hidden_states)\n        hidden_states = self.conv_2(hidden_states)\n\n        if not self.is_last:\n            hidden_states = self.group_norm_2(hidden_states)\n            hidden_states = self.gelu_2(hidden_states)\n\n        output = hidden_states + residual\n        return output",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 398, "column": 4 },
          "end": { "row": 398, "column": 4 }
        }
      }
    }
  ],
  [
    "2561",
    {
      "pageContent": "class UNetMidBlock1D(nn.Module):\n    def __init__(self, mid_channels, in_channels, out_channels=None):\n        super().__init__()\n\n        out_channels = in_channels if out_channels is None else out_channels\n\n        # there is always at least one resnet\n        self.down = Downsample1d(\"cubic\")\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n        attentions = [\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(out_channels, out_channels // 32),\n        ]\n        self.up = Upsample1d(kernel=\"cubic\")\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n    def forward(self, hidden_states, temb=None):\n        hidden_states = self.down(hidden_states)\n        for attn, resnet in zip(self.attentions, self.resnets):\n            hidden_states = resnet(hidden_states)\n            hidden_states = attn(hidden_states)\n\n        hidden_states = self.up(hidden_states)\n\n ",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 414, "column": 0 },
          "end": { "row": 414, "column": 0 }
        }
      }
    }
  ],
  [
    "2562",
    {
      "pageContent": "def __init__(self, mid_channels, in_channels, out_channels=None):\n        super().__init__()\n\n        out_channels = in_channels if out_channels is None else out_channels\n\n        # there is always at least one resnet\n        self.down = Downsample1d(\"cubic\")\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n        attentions = [\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(out_channels, out_channels // 32),\n        ]\n        self.up = Upsample1d(kernel=\"cubic\")\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 415, "column": 4 },
          "end": { "row": 415, "column": 4 }
        }
      }
    }
  ],
  [
    "2563",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        hidden_states = self.down(hidden_states)\n        for attn, resnet in zip(self.attentions, self.resnets):\n            hidden_states = resnet(hidden_states)\n            hidden_states = attn(hidden_states)\n\n        hidden_states = self.up(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 443, "column": 4 },
          "end": { "row": 443, "column": 4 }
        }
      }
    }
  ],
  [
    "2564",
    {
      "pageContent": "class AttnDownBlock1D(nn.Module):\n    def __init__(self, out_channels, in_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        self.down = Downsample1d(\"cubic\")\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n        attentions = [\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(out_channels, out_channels // 32),\n        ]\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n    def forward(self, hidden_states, temb=None):\n        hidden_states = self.down(hidden_states)\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states)\n            hidden_states = attn(hidden_states)\n\n        return hidden_states, (hidden_states,)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 454, "column": 0 },
          "end": { "row": 454, "column": 0 }
        }
      }
    }
  ],
  [
    "2565",
    {
      "pageContent": "def __init__(self, out_channels, in_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        self.down = Downsample1d(\"cubic\")\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n        attentions = [\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(out_channels, out_channels // 32),\n        ]\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 455, "column": 4 },
          "end": { "row": 455, "column": 4 }
        }
      }
    }
  ],
  [
    "2566",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        hidden_states = self.down(hidden_states)\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states)\n            hidden_states = attn(hidden_states)\n\n        return hidden_states, (hidden_states,)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 474, "column": 4 },
          "end": { "row": 474, "column": 4 }
        }
      }
    }
  ],
  [
    "2567",
    {
      "pageContent": "class DownBlock1D(nn.Module):\n    def __init__(self, out_channels, in_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        self.down = Downsample1d(\"cubic\")\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)\n\n    def forward(self, hidden_states, temb=None):\n        hidden_states = self.down(hidden_states)\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        return hidden_states, (hidden_states,)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 484, "column": 0 },
          "end": { "row": 484, "column": 0 }
        }
      }
    }
  ],
  [
    "2568",
    {
      "pageContent": "def __init__(self, out_channels, in_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        self.down = Downsample1d(\"cubic\")\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 485, "column": 4 },
          "end": { "row": 485, "column": 4 }
        }
      }
    }
  ],
  [
    "2569",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        hidden_states = self.down(hidden_states)\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        return hidden_states, (hidden_states,)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 498, "column": 4 },
          "end": { "row": 498, "column": 4 }
        }
      }
    }
  ],
  [
    "2570",
    {
      "pageContent": "class DownBlock1DNoSkip(nn.Module):\n    def __init__(self, out_channels, in_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)\n\n    def forward(self, hidden_states, temb=None):\n        hidden_states = torch.cat([hidden_states, temb], dim=1)\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        return hidden_states, (hidden_states,)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 507, "column": 0 },
          "end": { "row": 507, "column": 0 }
        }
      }
    }
  ],
  [
    "2571",
    {
      "pageContent": "def __init__(self, out_channels, in_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 508, "column": 4 },
          "end": { "row": 508, "column": 4 }
        }
      }
    }
  ],
  [
    "2572",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        hidden_states = torch.cat([hidden_states, temb], dim=1)\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        return hidden_states, (hidden_states,)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 520, "column": 4 },
          "end": { "row": 520, "column": 4 }
        }
      }
    }
  ],
  [
    "2573",
    {
      "pageContent": "class AttnUpBlock1D(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(2 * in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n        attentions = [\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(out_channels, out_channels // 32),\n        ]\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n        self.up = Upsample1d(kernel=\"cubic\")\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None):\n        res_hidden_states = res_hidden_states_tuple[-1]\n        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states)\n            hidden_states = attn(hidden_states)\n\n        hidden_states = self.up(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 528, "column": 0 },
          "end": { "row": 528, "column": 0 }
        }
      }
    }
  ],
  [
    "2574",
    {
      "pageContent": "def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = out_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(2 * in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n        attentions = [\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(mid_channels, mid_channels // 32),\n            SelfAttention1d(out_channels, out_channels // 32),\n        ]\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n        self.up = Upsample1d(kernel=\"cubic\")",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 529, "column": 4 },
          "end": { "row": 529, "column": 4 }
        }
      }
    }
  ],
  [
    "2575",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None):\n        res_hidden_states = res_hidden_states_tuple[-1]\n        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states)\n            hidden_states = attn(hidden_states)\n\n        hidden_states = self.up(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 548, "column": 4 },
          "end": { "row": 548, "column": 4 }
        }
      }
    }
  ],
  [
    "2576",
    {
      "pageContent": "class UpBlock1D(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = in_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(2 * in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)\n        self.up = Upsample1d(kernel=\"cubic\")\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None):\n        res_hidden_states = res_hidden_states_tuple[-1]\n        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        hidden_states = self.up(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 561, "column": 0 },
          "end": { "row": 561, "column": 0 }
        }
      }
    }
  ],
  [
    "2577",
    {
      "pageContent": "def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = in_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(2 * in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)\n        self.up = Upsample1d(kernel=\"cubic\")",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 562, "column": 4 },
          "end": { "row": 562, "column": 4 }
        }
      }
    }
  ],
  [
    "2578",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None):\n        res_hidden_states = res_hidden_states_tuple[-1]\n        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        hidden_states = self.up(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 575, "column": 4 },
          "end": { "row": 575, "column": 4 }
        }
      }
    }
  ],
  [
    "2579",
    {
      "pageContent": "class UpBlock1DNoSkip(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = in_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(2 * in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels, is_last=True),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None):\n        res_hidden_states = res_hidden_states_tuple[-1]\n        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 587, "column": 0 },
          "end": { "row": 587, "column": 0 }
        }
      }
    }
  ],
  [
    "2580",
    {
      "pageContent": "def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        mid_channels = in_channels if mid_channels is None else mid_channels\n\n        resnets = [\n            ResConvBlock(2 * in_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, mid_channels),\n            ResConvBlock(mid_channels, mid_channels, out_channels, is_last=True),\n        ]\n\n        self.resnets = nn.ModuleList(resnets)",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 588, "column": 4 },
          "end": { "row": 588, "column": 4 }
        }
      }
    }
  ],
  [
    "2581",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None):\n        res_hidden_states = res_hidden_states_tuple[-1]\n        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 600, "column": 4 },
          "end": { "row": 600, "column": 4 }
        }
      }
    }
  ],
  [
    "2582",
    {
      "pageContent": "def get_down_block(down_block_type, num_layers, in_channels, out_channels, temb_channels, add_downsample):\n    if down_block_type == \"DownResnetBlock1D\":\n        return DownResnetBlock1D(\n            in_channels=in_channels,\n            num_layers=num_layers,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n        )\n    elif down_block_type == \"DownBlock1D\":\n        return DownBlock1D(out_channels=out_channels, in_channels=in_channels)\n    elif down_block_type == \"AttnDownBlock1D\":\n        return AttnDownBlock1D(out_channels=out_channels, in_channels=in_channels)\n    elif down_block_type == \"DownBlock1DNoSkip\":\n        return DownBlock1DNoSkip(out_channels=out_channels, in_channels=in_channels)\n    raise ValueError(f\"{down_block_type} does not exist.\")",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 610, "column": 0 },
          "end": { "row": 610, "column": 0 }
        }
      }
    }
  ],
  [
    "2583",
    {
      "pageContent": "def get_up_block(up_block_type, num_layers, in_channels, out_channels, temb_channels, add_upsample):\n    if up_block_type == \"UpResnetBlock1D\":\n        return UpResnetBlock1D(\n            in_channels=in_channels,\n            num_layers=num_layers,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n        )\n    elif up_block_type == \"UpBlock1D\":\n        return UpBlock1D(in_channels=in_channels, out_channels=out_channels)\n    elif up_block_type == \"AttnUpBlock1D\":\n        return AttnUpBlock1D(in_channels=in_channels, out_channels=out_channels)\n    elif up_block_type == \"UpBlock1DNoSkip\":\n        return UpBlock1DNoSkip(in_channels=in_channels, out_channels=out_channels)\n    raise ValueError(f\"{up_block_type} does not exist.\")",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 628, "column": 0 },
          "end": { "row": 628, "column": 0 }
        }
      }
    }
  ],
  [
    "2584",
    {
      "pageContent": "def get_mid_block(mid_block_type, num_layers, in_channels, mid_channels, out_channels, embed_dim, add_downsample):\n    if mid_block_type == \"MidResTemporalBlock1D\":\n        return MidResTemporalBlock1D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            embed_dim=embed_dim,\n            add_downsample=add_downsample,\n        )\n    elif mid_block_type == \"ValueFunctionMidBlock1D\":\n        return ValueFunctionMidBlock1D(in_channels=in_channels, out_channels=out_channels, embed_dim=embed_dim)\n    elif mid_block_type == \"UNetMidBlock1D\":\n        return UNetMidBlock1D(in_channels=in_channels, mid_channels=mid_channels, out_channels=out_channels)\n    raise ValueError(f\"{mid_block_type} does not exist.\")",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 646, "column": 0 },
          "end": { "row": 646, "column": 0 }
        }
      }
    }
  ],
  [
    "2585",
    {
      "pageContent": "def get_out_block(*, out_block_type, num_groups_out, embed_dim, out_channels, act_fn, fc_dim):\n    if out_block_type == \"OutConv1DBlock\":\n        return OutConv1DBlock(num_groups_out, out_channels, embed_dim, act_fn)\n    elif out_block_type == \"ValueFunction\":\n        return OutValueFunctionBlock(fc_dim, embed_dim)\n    return None",
      "metadata": {
        "source": "src/diffusers/models/unet_1d_blocks.py",
        "range": {
          "start": { "row": 662, "column": 0 },
          "end": { "row": 662, "column": 0 }
        }
      }
    }
  ],
  [
    "2586",
    {
      "pageContent": "class UNet1DOutput(BaseOutput):\n    \"\"\"\n    Args:\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, sample_size)`):\n            Hidden states output. Output of last layer of model.\n    \"\"\"\n\n    sample: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/models/unet_1d.py",
        "range": {
          "start": { "row": 28, "column": 0 },
          "end": { "row": 28, "column": 0 }
        }
      }
    }
  ],
  [
    "2587",
    {
      "pageContent": "class UNet1DModel(ModelMixin, ConfigMixin):\n    r\"\"\"\n    UNet1DModel is a 1D UNet model that takes in a noisy sample and a timestep and returns sample shaped output.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the model (such as downloading or saving, etc.)\n\n    Parameters:\n        sample_size (`int`, *optional*): Default length of sample. Should be adaptable at runtime.\n        in_channels (`int`, *optional*, defaults to 2): Number of channels in the input sample.\n        out_channels (`int`, *optional*, defaults to 2): Number of channels in the output.\n        time_embedding_type (`str`, *optional*, defaults to `\"fourier\"`): Type of time embedding to use.\n        freq_shift (`float`, *optional*, defaults to 0.0): Frequency shift for fourier time embedding.\n        flip_sin_to_cos (`bool`, *optional*, defaults to :\n            obj:`False`): Whether to flip sin to cos for fourier time embedding.\n        down_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"DownBlock1D\", \"DownBlock1DNoSkip\", \"AttnDownBlock1D\")`): Tuple of downsample block types.\n        up_block_types (`Tuple[str]`, *optional*, defaults to :\n            obj:`(\"UpBlock1D\", \"UpBlock1DNoSkip\", \"AttnUpBlock1D\")`): Tuple of upsample block types.\n        block_out_channels (`Tuple[int]`, *optional*, defaults to :\n            obj:`(32, 32, 64)`): Tuple of block output channels.\n        mid_block_type (`str`, *optional*, defaults to \"UNetMidBlock1D\"): block type for middle of UNet.\n        out_block",
      "metadata": {
        "source": "src/diffusers/models/unet_1d.py",
        "range": {
          "start": { "row": 38, "column": 0 },
          "end": { "row": 38, "column": 0 }
        }
      }
    }
  ],
  [
    "2588",
    {
      "pageContent": "def forward(\n        self,\n        sample: torch.FloatTensor,\n        timestep: Union[torch.Tensor, float, int],\n        return_dict: bool = True,\n    ) -> Union[UNet1DOutput, Tuple]:\n        r\"\"\"\n        Args:\n            sample (`torch.FloatTensor`): `(batch_size, num_channels, sample_size)` noisy inputs tensor\n            timestep (`torch.FloatTensor` or `float` or `int): (batch) timesteps\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~models.unet_1d.UNet1DOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~models.unet_1d.UNet1DOutput`] or `tuple`: [`~models.unet_1d.UNet1DOutput`] if `return_dict` is True,\n            otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n        \"\"\"\n\n        # 1. time\n        timesteps = timestep\n        if not torch.is_tensor(timesteps):\n            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n            timesteps = timesteps[None].to(sample.device)\n\n        timestep_embed = self.time_proj(timesteps)\n        if self.config.use_timestep_embedding:\n            timestep_embed = self.time_mlp(timestep_embed)\n        else:\n            timestep_embed = timestep_embed[..., None]\n            timestep_embed = timestep_embed.repeat([1, 1, sample.shape[2]]).to(sample.dtype)\n            timestep_embed = timestep_embed.broadcast_to((sample.shape[:1] + timestep_embed.shape[1:]))\n\n        # 2. down\n        down_block_res_samples = ()\n    ",
      "metadata": {
        "source": "src/diffusers/models/unet_1d.py",
        "range": {
          "start": { "row": 189, "column": 4 },
          "end": { "row": 189, "column": 4 }
        }
      }
    }
  ],
  [
    "2589",
    {
      "pageContent": "def get_down_block(\n    down_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    temb_channels,\n    add_downsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    resnet_groups=None,\n    cross_attention_dim=None,\n    downsample_padding=None,\n    dual_cross_attention=False,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    upcast_attention=False,\n    resnet_time_scale_shift=\"default\",\n):\n    down_block_type = down_block_type[7:] if down_block_type.startswith(\"UNetRes\") else down_block_type\n    if down_block_type == \"DownBlock2D\":\n        return DownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif down_block_type == \"ResnetDownsampleBlock2D\":\n        return ResnetDownsampleBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif down_block_type == \"AttnDownBlock2D\":\n        return AttnDownB",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 26, "column": 0 },
          "end": { "row": 26, "column": 0 }
        }
      }
    }
  ],
  [
    "2590",
    {
      "pageContent": "def get_up_block(\n    up_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    prev_output_channel,\n    temb_channels,\n    add_upsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    resnet_groups=None,\n    cross_attention_dim=None,\n    dual_cross_attention=False,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    upcast_attention=False,\n    resnet_time_scale_shift=\"default\",\n):\n    up_block_type = up_block_type[7:] if up_block_type.startswith(\"UNetRes\") else up_block_type\n    if up_block_type == \"UpBlock2D\":\n        return UpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif up_block_type == \"ResnetUpsampleBlock2D\":\n        return ResnetUpsampleBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif up_block_type == \"CrossAttnUpBlock2D\":\n      ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 198, "column": 0 },
          "end": { "row": 198, "column": 0 }
        }
      }
    }
  ],
  [
    "2591",
    {
      "pageContent": "class UNetMidBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        add_attention: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n    ):\n        super().__init__()\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n        self.add_attention = add_attention\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            if self.add_attention:\n                attentions.append(\n                    AttentionBlock(\n                        in_channels,\n                        num_head_channels=attn_num_head_channels,\n                        rescale_output_factor=output_scale_factor,\n                        eps=resnet_eps,\n                    ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 370, "column": 0 },
          "end": { "row": 370, "column": 0 }
        }
      }
    }
  ],
  [
    "2592",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        add_attention: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n    ):\n        super().__init__()\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n        self.add_attention = add_attention\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            if self.add_attention:\n                attentions.append(\n                    AttentionBlock(\n                        in_channels,\n                        num_head_channels=attn_num_head_channels,\n                        rescale_output_factor=output_scale_factor,\n                        eps=resnet_eps,\n                        norm_num_groups=resnet_groups,\n  ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 371, "column": 4 },
          "end": { "row": 371, "column": 4 }
        }
      }
    }
  ],
  [
    "2593",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            if attn is not None:\n                hidden_states = attn(hidden_states)\n            hidden_states = resnet(hidden_states, temb)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 439, "column": 4 },
          "end": { "row": 439, "column": 4 }
        }
      }
    }
  ],
  [
    "2594",
    {
      "pageContent": "class UNetMidBlock2DCrossAttn(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            if not dual_cross_attention:\n                attentions.append(\n                    Transformer2DModel(\n                        attn_num_head_channels,\n ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 449, "column": 0 },
          "end": { "row": 449, "column": 0 }
        }
      }
    }
  ],
  [
    "2595",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            if not dual_cross_attention:\n                attentions.append(\n                    Transformer2DModel(\n                        attn_num_head_channels,\n                        in_channels // attn_num",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 450, "column": 4 },
          "end": { "row": 450, "column": 4 }
        }
      }
    }
  ],
  [
    "2596",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            hidden_states = attn(\n                hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                cross_attention_kwargs=cross_attention_kwargs,\n            ).sample\n            hidden_states = resnet(hidden_states, temb)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 534, "column": 4 },
          "end": { "row": 534, "column": 4 }
        }
      }
    }
  ],
  [
    "2597",
    {
      "pageContent": "class UNetMidBlock2DSimpleCrossAttn(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        self.num_heads = in_channels // self.attn_num_head_channels\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            attentions.append(\n                CrossAttention(\n                    query_dim=in_channels,\n                    cross_attention_dim=in_channels,\n                    heads=self.num_",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 549, "column": 0 },
          "end": { "row": 549, "column": 0 }
        }
      }
    }
  ],
  [
    "2598",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n\n        self.num_heads = in_channels // self.attn_num_head_channels\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            attentions.append(\n                CrossAttention(\n                    query_dim=in_channels,\n                    cross_attention_dim=in_channels,\n                    heads=self.num_heads,\n                    dim_head=attn_num_head_ch",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 550, "column": 4 },
          "end": { "row": 550, "column": 4 }
        }
      }
    }
  ],
  [
    "2599",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            # attn\n            hidden_states = attn(\n                hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                attention_mask=attention_mask,\n                **cross_attention_kwargs,\n            )\n\n            # resnet\n            hidden_states = resnet(hidden_states, temb)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 623, "column": 4 },
          "end": { "row": 623, "column": 4 }
        }
      }
    }
  ],
  [
    "2600",
    {
      "pageContent": "class AttnDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_downsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n                    norm_num_groups=resnet_groups,\n                ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 643, "column": 0 },
          "end": { "row": 643, "column": 0 }
        }
      }
    }
  ],
  [
    "2601",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_downsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n                    norm_num_groups=resnet_groups,\n                )\n            )\n\n        self.attentio",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 644, "column": 4 },
          "end": { "row": 644, "column": 4 }
        }
      }
    }
  ],
  [
    "2602",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states)\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 705, "column": 4 },
          "end": { "row": 705, "column": 4 }
        }
      }
    }
  ],
  [
    "2603",
    {
      "pageContent": "class CrossAttnDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_downsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            if not dual_cross_attention:\n  ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 722, "column": 0 },
          "end": { "row": 722, "column": 0 }
        }
      }
    }
  ],
  [
    "2604",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_downsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            if not dual_cross_attention:\n                attentions.append(\n          ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 723, "column": 4 },
          "end": { "row": 723, "column": 4 }
        }
      }
    }
  ],
  [
    "2605",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        # TODO(Patrick, William) - attention mask is not used\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(attn, return_dict=False),\n                    hidden_states,\n                    encoder_hidden_states,\n                    cross_attention_kwargs,\n                )[0]\n            else:\n                hidden_states = resnet(hidden_states, temb)\n                hidden_states = attn(\n                    hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                ).sample\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states =",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 809, "column": 4 },
          "end": { "row": 809, "column": 4 }
        }
      }
    }
  ],
  [
    "2606",
    {
      "pageContent": "class DownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsample",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 853, "column": 0 },
          "end": { "row": 853, "column": 0 }
        }
      }
    }
  ],
  [
    "2607",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n        self.gradient_c",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 854, "column": 4 },
          "end": { "row": 854, "column": 4 }
        }
      }
    }
  ],
  [
    "2608",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 905, "column": 4 },
          "end": { "row": 905, "column": 4 }
        }
      }
    }
  ],
  [
    "2609",
    {
      "pageContent": "class DownEncoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n    def forward(sel",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 932, "column": 0 },
          "end": { "row": 932, "column": 0 }
        }
      }
    }
  ],
  [
    "2610",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 933, "column": 4 },
          "end": { "row": 933, "column": 4 }
        }
      }
    }
  ],
  [
    "2611",
    {
      "pageContent": "def forward(self, hidden_states):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb=None)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 981, "column": 4 },
          "end": { "row": 981, "column": 4 }
        }
      }
    }
  ],
  [
    "2612",
    {
      "pageContent": "class AttnDownEncoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n                    norm_num_groups=resnet_groups,\n                )\n            )\n\n        self.",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 992, "column": 0 },
          "end": { "row": 992, "column": 0 }
        }
      }
    }
  ],
  [
    "2613",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n                    norm_num_groups=resnet_groups,\n                )\n            )\n\n        self.attentions = nn.ModuleList(attentions)\n      ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 993, "column": 4 },
          "end": { "row": 993, "column": 4 }
        }
      }
    }
  ],
  [
    "2614",
    {
      "pageContent": "def forward(self, hidden_states):\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states, temb=None)\n            hidden_states = attn(hidden_states)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1053, "column": 4 },
          "end": { "row": 1053, "column": 4 }
        }
      }
    }
  ],
  [
    "2615",
    {
      "pageContent": "class AttnSkipDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=np.sqrt(2.0),\n        downsample_padding=1,\n        add_downsample=True,\n    ):\n        super().__init__()\n        self.attentions = nn.ModuleList([])\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min(in_channels // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            self.attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n  ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1065, "column": 0 },
          "end": { "row": 1065, "column": 0 }
        }
      }
    }
  ],
  [
    "2616",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=np.sqrt(2.0),\n        downsample_padding=1,\n        add_downsample=True,\n    ):\n        super().__init__()\n        self.attentions = nn.ModuleList([])\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min(in_channels // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            self.attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n        ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1066, "column": 4 },
          "end": { "row": 1066, "column": 4 }
        }
      }
    }
  ],
  [
    "2617",
    {
      "pageContent": "def forward(self, hidden_states, temb=None, skip_sample=None):\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states)\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            hidden_states = self.resnet_down(hidden_states, temb)\n            for downsampler in self.downsamplers:\n                skip_sample = downsampler(skip_sample)\n\n            hidden_states = self.skip_conv(skip_sample) + hidden_states\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states, skip_sample",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1135, "column": 4 },
          "end": { "row": 1135, "column": 4 }
        }
      }
    }
  ],
  [
    "2618",
    {
      "pageContent": "class SkipDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        output_scale_factor=np.sqrt(2.0),\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min(in_channels // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        if add_downsample:\n            self.resnet_down = ResnetBlock2D(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=min(out_channels // 4, 32),\n                dropout=",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1155, "column": 0 },
          "end": { "row": 1155, "column": 0 }
        }
      }
    }
  ],
  [
    "2619",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        output_scale_factor=np.sqrt(2.0),\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min(in_channels // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        if add_downsample:\n            self.resnet_down = ResnetBlock2D(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=min(out_channels // 4, 32),\n                dropout=dropout,\n                time_embeddin",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1156, "column": 4 },
          "end": { "row": 1156, "column": 4 }
        }
      }
    }
  ],
  [
    "2620",
    {
      "pageContent": "def forward(self, hidden_states, temb=None, skip_sample=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb)\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            hidden_states = self.resnet_down(hidden_states, temb)\n            for downsampler in self.downsamplers:\n                skip_sample = downsampler(skip_sample)\n\n            hidden_states = self.skip_conv(skip_sample) + hidden_states\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states, skip_sample",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1215, "column": 4 },
          "end": { "row": 1215, "column": 4 }
        }
      }
    }
  ],
  [
    "2621",
    {
      "pageContent": "class ResnetDownsampleBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    ResnetBlock2D(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        temb_channels=temb_channels,\n                        eps=resnet_eps,\n                        groups=resne",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1234, "column": 0 },
          "end": { "row": 1234, "column": 0 }
        }
      }
    }
  ],
  [
    "2622",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    ResnetBlock2D(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        temb_channels=temb_channels,\n                        eps=resnet_eps,\n                        groups=resnet_groups,\n                        dropout=drop",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1235, "column": 4 },
          "end": { "row": 1235, "column": 4 }
        }
      }
    }
  ],
  [
    "2623",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states, temb)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1295, "column": 4 },
          "end": { "row": 1295, "column": 4 }
        }
      }
    }
  ],
  [
    "2624",
    {
      "pageContent": "class SimpleCrossAttnDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_downsample=True,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n\n        resnets = []\n        attentions = []\n\n        self.attn_num_head_channels = attn_num_head_channels\n        self.num_heads = out_channels // self.attn_num_head_channels\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                CrossAttention(\n                    query_dim=out_channels,\n                    cross_atte",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1322, "column": 0 },
          "end": { "row": 1322, "column": 0 }
        }
      }
    }
  ],
  [
    "2625",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_downsample=True,\n    ):\n        super().__init__()\n\n        self.has_cross_attention = True\n\n        resnets = []\n        attentions = []\n\n        self.attn_num_head_channels = attn_num_head_channels\n        self.num_heads = out_channels // self.attn_num_head_channels\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                CrossAttention(\n                    query_dim=out_channels,\n                    cross_attention_dim=out_channels,\n                    heads",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1323, "column": 4 },
          "end": { "row": 1323, "column": 4 }
        }
      }
    }
  ],
  [
    "2626",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        output_states = ()\n        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # resnet\n            hidden_states = resnet(hidden_states, temb)\n\n            # attn\n            hidden_states = attn(\n                hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                attention_mask=attention_mask,\n                **cross_attention_kwargs,\n            )\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states, temb)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1405, "column": 4 },
          "end": { "row": 1405, "column": 4 }
        }
      }
    }
  ],
  [
    "2627",
    {
      "pageContent": "class KDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 4,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n        resnet_group_size: int = 32,\n        add_downsample=False,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    dropout=dropout,\n                    temb_channels=temb_channels,\n                    groups=groups,\n                    groups_out=groups_out,\n                    eps=resnet_eps,\n                    non_linearity=resnet_act_fn,\n                    time_embedding_norm=\"ada_group\",\n                    conv_shortcut_bias=False,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            # YiYi's comments- might be able to use FirDownsample2D, look into details later\n            self.downsamplers = nn.ModuleList([KDownsample2D()])\n        else:\n            self.downsamplers = None\n\n        self.gradient_checkpointing = False\n\n    def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            i",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1434, "column": 0 },
          "end": { "row": 1434, "column": 0 }
        }
      }
    }
  ],
  [
    "2628",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 4,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n        resnet_group_size: int = 32,\n        add_downsample=False,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    dropout=dropout,\n                    temb_channels=temb_channels,\n                    groups=groups,\n                    groups_out=groups_out,\n                    eps=resnet_eps,\n                    non_linearity=resnet_act_fn,\n                    time_embedding_norm=\"ada_group\",\n                    conv_shortcut_bias=False,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            # YiYi's comments- might be able to use FirDownsample2D, look into details later\n            self.downsamplers = nn.ModuleList([KDownsample2D()])\n        else:\n            self.downsamplers = None\n\n        self.gradient_checkpointing = False",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1435, "column": 4 },
          "end": { "row": 1435, "column": 4 }
        }
      }
    }
  ],
  [
    "2629",
    {
      "pageContent": "def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1480, "column": 4 },
          "end": { "row": 1480, "column": 4 }
        }
      }
    }
  ],
  [
    "2630",
    {
      "pageContent": "class KCrossAttnDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        cross_attention_dim: int,\n        dropout: float = 0.0,\n        num_layers: int = 4,\n        resnet_group_size: int = 32,\n        add_downsample=True,\n        attn_num_head_channels: int = 64,\n        add_self_attention: bool = False,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    dropout=dropout,\n                    temb_channels=temb_channels,\n                    groups=groups,\n                    groups_out=groups_out,\n                    eps=resnet_eps,\n                    non_linearity=resnet_act_fn,\n                    time_embedding_norm=\"ada_group\",\n                    conv_shortcut_bias=False,\n                )\n            )\n            attentions.append(\n                KAttentionBlock(\n                    out_channels,\n                    out_channels // attn_num_head_channels,\n                    attn_num_head_channels,\n                    cross_attention_dim=cross_attention_dim,\n  ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1505, "column": 0 },
          "end": { "row": 1505, "column": 0 }
        }
      }
    }
  ],
  [
    "2631",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        cross_attention_dim: int,\n        dropout: float = 0.0,\n        num_layers: int = 4,\n        resnet_group_size: int = 32,\n        add_downsample=True,\n        attn_num_head_channels: int = 64,\n        add_self_attention: bool = False,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    dropout=dropout,\n                    temb_channels=temb_channels,\n                    groups=groups,\n                    groups_out=groups_out,\n                    eps=resnet_eps,\n                    non_linearity=resnet_act_fn,\n                    time_embedding_norm=\"ada_group\",\n                    conv_shortcut_bias=False,\n                )\n            )\n            attentions.append(\n                KAttentionBlock(\n                    out_channels,\n                    out_channels // attn_num_head_channels,\n                    attn_num_head_channels,\n                    cross_attention_dim=cross_attention_dim,\n                    temb_channels=temb_channel",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1506, "column": 4 },
          "end": { "row": 1506, "column": 4 }
        }
      }
    }
  ],
  [
    "2632",
    {
      "pageContent": "def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(attn, return_dict=False),\n                    hidden_states,\n                    encoder_hidden_states,\n                    attention_mask,\n                    cross_attention_kwargs,\n                )\n            else:\n                hidden_states = resnet(hidden_states, temb)\n                hidden_states = attn(\n                    hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                    emb=temb,\n                    attention_mask=attention_mask,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                )\n\n            if self.downsamplers is None:\n                output_states += (None,)\n            else:\n                output_s",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1570, "column": 4 },
          "end": { "row": 1570, "column": 4 }
        }
      }
    }
  ],
  [
    "2633",
    {
      "pageContent": "class AttnUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_f",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1617, "column": 0 },
          "end": { "row": 1617, "column": 0 }
        }
      }
    }
  ],
  [
    "2634",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n         ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1618, "column": 4 },
          "end": { "row": 1618, "column": 4 }
        }
      }
    }
  ],
  [
    "2635",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None):\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1675, "column": 4 },
          "end": { "row": 1675, "column": 4 }
        }
      }
    }
  ],
  [
    "2636",
    {
      "pageContent": "class CrossAttnUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        prev_output_channel: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_upsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1692, "column": 0 },
          "end": { "row": 1692, "column": 0 }
        }
      }
    }
  ],
  [
    "2637",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        prev_output_channel: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_upsample=True,\n        dual_cross_attention=False,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        upcast_attention=False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=res",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1693, "column": 4 },
          "end": { "row": 1693, "column": 4 }
        }
      }
    }
  ],
  [
    "2638",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        res_hidden_states_tuple,\n        temb=None,\n        encoder_hidden_states=None,\n        cross_attention_kwargs=None,\n        upsample_size=None,\n        attention_mask=None,\n    ):\n        # TODO(Patrick, William) - attention mask is not used\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(attn, return_dict=False),\n                    hidden_states,\n                    encoder_hidden_states,\n                    cross_attention_kwargs,\n                )[0]\n            else:\n                hidden_states = resnet(hidden_states, temb)\n                hidden_states = attn(\n                    hidden_states,\n         ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1775, "column": 4 },
          "end": { "row": 1775, "column": 4 }
        }
      }
    }
  ],
  [
    "2639",
    {
      "pageContent": "class UpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])\n        else:\n            self.upsamplers = None\n\n        self.gradien",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1825, "column": 0 },
          "end": { "row": 1825, "column": 0 }
        }
      }
    }
  ],
  [
    "2640",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])\n        else:\n            self.upsamplers = None\n\n        self.gradient_checkpointing = False",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1826, "column": 4 },
          "end": { "row": 1826, "column": 4 }
        }
      }
    }
  ],
  [
    "2641",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states, upsample_size)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1873, "column": 4 },
          "end": { "row": 1873, "column": 4 }
        }
      }
    }
  ],
  [
    "2642",
    {
      "pageContent": "class UpDecoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            input_channels = in_channels if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=input_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])\n        else:\n            self.upsamplers = None\n\n    def forward(self, hidden_states):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb=None)\n\n        if self.upsamplers is not None:\n            for upsampler in se",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1899, "column": 0 },
          "end": { "row": 1899, "column": 0 }
        }
      }
    }
  ],
  [
    "2643",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            input_channels = in_channels if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=input_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])\n        else:\n            self.upsamplers = None",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1900, "column": 4 },
          "end": { "row": 1900, "column": 4 }
        }
      }
    }
  ],
  [
    "2644",
    {
      "pageContent": "def forward(self, hidden_states):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb=None)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1942, "column": 4 },
          "end": { "row": 1942, "column": 4 }
        }
      }
    }
  ],
  [
    "2645",
    {
      "pageContent": "class AttnUpDecoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            input_channels = in_channels if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=input_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n                    norm_num_groups=resnet_groups,\n                )\n            )\n\n        self.attentions = nn.ModuleList(",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1953, "column": 0 },
          "end": { "row": 1953, "column": 0 }
        }
      }
    }
  ],
  [
    "2646",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        for i in range(num_layers):\n            input_channels = in_channels if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=input_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.append(\n                AttentionBlock(\n                    out_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n                    norm_num_groups=resnet_groups,\n                )\n            )\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.Modul",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 1954, "column": 4 },
          "end": { "row": 1954, "column": 4 }
        }
      }
    }
  ],
  [
    "2647",
    {
      "pageContent": "def forward(self, hidden_states):\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states, temb=None)\n            hidden_states = attn(hidden_states)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2008, "column": 4 },
          "end": { "row": 2008, "column": 4 }
        }
      }
    }
  ],
  [
    "2648",
    {
      "pageContent": "class AttnSkipUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=np.sqrt(2.0),\n        upsample_padding=1,\n        add_upsample=True,\n    ):\n        super().__init__()\n        self.attentions = nn.ModuleList([])\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min(resnet_in_channels + res_skip_channels // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.attentions.append(\n           ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2020, "column": 0 },
          "end": { "row": 2020, "column": 0 }
        }
      }
    }
  ],
  [
    "2649",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        output_scale_factor=np.sqrt(2.0),\n        upsample_padding=1,\n        add_upsample=True,\n    ):\n        super().__init__()\n        self.attentions = nn.ModuleList([])\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min(resnet_in_channels + res_skip_channels // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.attentions.append(\n            AttentionBlock(\n                out_cha",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2021, "column": 4 },
          "end": { "row": 2021, "column": 4 }
        }
      }
    }
  ],
  [
    "2650",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None, skip_sample=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            hidden_states = resnet(hidden_states, temb)\n\n        hidden_states = self.attentions[0](hidden_states)\n\n        if skip_sample is not None:\n            skip_sample = self.upsampler(skip_sample)\n        else:\n            skip_sample = 0\n\n        if self.resnet_up is not None:\n            skip_sample_states = self.skip_norm(hidden_states)\n            skip_sample_states = self.act(skip_sample_states)\n            skip_sample_states = self.skip_conv(skip_sample_states)\n\n            skip_sample = skip_sample + skip_sample_states\n\n            hidden_states = self.resnet_up(hidden_states, temb)\n\n        return hidden_states, skip_sample",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2100, "column": 4 },
          "end": { "row": 2100, "column": 4 }
        }
      }
    }
  ],
  [
    "2651",
    {
      "pageContent": "class SkipUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        output_scale_factor=np.sqrt(2.0),\n        add_upsample=True,\n        upsample_padding=1,\n    ):\n        super().__init__()\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min((resnet_in_channels + res_skip_channels) // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.upsampler = FirUpsample2D(in_channels, out_channels=out_channels)\n        if add_upsample:\n            self.re",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2128, "column": 0 },
          "end": { "row": 2128, "column": 0 }
        }
      }
    }
  ],
  [
    "2652",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        output_scale_factor=np.sqrt(2.0),\n        add_upsample=True,\n        upsample_padding=1,\n    ):\n        super().__init__()\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min((resnet_in_channels + res_skip_channels) // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.upsampler = FirUpsample2D(in_channels, out_channels=out_channels)\n        if add_upsample:\n            self.resnet_up = ResnetBlock2D(\n           ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2129, "column": 4 },
          "end": { "row": 2129, "column": 4 }
        }
      }
    }
  ],
  [
    "2653",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None, skip_sample=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            hidden_states = resnet(hidden_states, temb)\n\n        if skip_sample is not None:\n            skip_sample = self.upsampler(skip_sample)\n        else:\n            skip_sample = 0\n\n        if self.resnet_up is not None:\n            skip_sample_states = self.skip_norm(hidden_states)\n            skip_sample_states = self.act(skip_sample_states)\n            skip_sample_states = self.skip_conv(skip_sample_states)\n\n            skip_sample = skip_sample + skip_sample_states\n\n            hidden_states = self.resnet_up(hidden_states, temb)\n\n        return hidden_states, skip_sample",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2197, "column": 4 },
          "end": { "row": 2197, "column": 4 }
        }
      }
    }
  ],
  [
    "2654",
    {
      "pageContent": "class ResnetUpsampleBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList(\n                [\n                    ResnetBlock2D(\n                        in_channels=out_channels,\n                        ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2223, "column": 0 },
          "end": { "row": 2223, "column": 0 }
        }
      }
    }
  ],
  [
    "2655",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList(\n                [\n                    ResnetBlock2D(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                 ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2224, "column": 4 },
          "end": { "row": 2224, "column": 4 }
        }
      }
    }
  ],
  [
    "2656",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states, temb)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2287, "column": 4 },
          "end": { "row": 2287, "column": 4 }
        }
      }
    }
  ],
  [
    "2657",
    {
      "pageContent": "class SimpleCrossAttnUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        prev_output_channel: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        self.num_heads = out_channels // self.attn_num_head_channels\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n          ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2313, "column": 0 },
          "end": { "row": 2313, "column": 0 }
        }
      }
    }
  ],
  [
    "2658",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        prev_output_channel: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        self.num_heads = out_channels // self.attn_num_head_channels\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n            attentions.ap",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2314, "column": 4 },
          "end": { "row": 2314, "column": 4 }
        }
      }
    }
  ],
  [
    "2659",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        res_hidden_states_tuple,\n        temb=None,\n        encoder_hidden_states=None,\n        upsample_size=None,\n        attention_mask=None,\n        cross_attention_kwargs=None,\n    ):\n        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # resnet\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            hidden_states = resnet(hidden_states, temb)\n\n            # attn\n            hidden_states = attn(\n                hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                attention_mask=attention_mask,\n                **cross_attention_kwargs,\n            )\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states, temb)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2398, "column": 4 },
          "end": { "row": 2398, "column": 4 }
        }
      }
    }
  ],
  [
    "2660",
    {
      "pageContent": "class KUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 5,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n        resnet_group_size: Optional[int] = 32,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        k_in_channels = 2 * out_channels\n        k_out_channels = in_channels\n        num_layers = num_layers - 1\n\n        for i in range(num_layers):\n            in_channels = k_in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=k_out_channels if (i == num_layers - 1) else out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=groups,\n                    groups_out=groups_out,\n                    dropout=dropout,\n                    non_linearity=resnet_act_fn,\n                    time_embedding_norm=\"ada_group\",\n                    conv_shortcut_bias=False,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([KUpsample2D()])\n        else:\n            self.upsamplers = None\n\n        self.gradient_checkpointing = False\n\n    def forward(self, hidden_states, res_hidden_states_tuple,",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2433, "column": 0 },
          "end": { "row": 2433, "column": 0 }
        }
      }
    }
  ],
  [
    "2661",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 5,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n        resnet_group_size: Optional[int] = 32,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n        k_in_channels = 2 * out_channels\n        k_out_channels = in_channels\n        num_layers = num_layers - 1\n\n        for i in range(num_layers):\n            in_channels = k_in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=k_out_channels if (i == num_layers - 1) else out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=groups,\n                    groups_out=groups_out,\n                    dropout=dropout,\n                    non_linearity=resnet_act_fn,\n                    time_embedding_norm=\"ada_group\",\n                    conv_shortcut_bias=False,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList([KUpsample2D()])\n        else:\n            self.upsamplers = None\n\n        self.gradient_checkpointing = False",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2434, "column": 4 },
          "end": { "row": 2434, "column": 4 }
        }
      }
    }
  ],
  [
    "2662",
    {
      "pageContent": "def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):\n        res_hidden_states_tuple = res_hidden_states_tuple[-1]\n        if res_hidden_states_tuple is not None:\n            hidden_states = torch.cat([hidden_states, res_hidden_states_tuple], dim=1)\n\n        for resnet in self.resnets:\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2481, "column": 4 },
          "end": { "row": 2481, "column": 4 }
        }
      }
    }
  ],
  [
    "2663",
    {
      "pageContent": "class KCrossAttnUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 4,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n        resnet_group_size: int = 32,\n        attn_num_head_channels=1,  # attention dim_head\n        cross_attention_dim: int = 768,\n        add_upsample: bool = True,\n        upcast_attention: bool = False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        is_first_block = in_channels == out_channels == temb_channels\n        is_middle_block = in_channels != out_channels\n        add_self_attention = True if is_first_block else False\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        # in_channels, and out_channels for the block (k-unet)\n        k_in_channels = out_channels if is_first_block else 2 * out_channels\n        k_out_channels = in_channels\n\n        num_layers = num_layers - 1\n\n        for i in range(num_layers):\n            in_channels = k_in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            if is_middle_block and (i == num_layers - 1):\n                conv_2d_out_channels = k_out_channels\n            else:\n                conv_2d_out_channels = None\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_chann",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2506, "column": 0 },
          "end": { "row": 2506, "column": 0 }
        }
      }
    }
  ],
  [
    "2664",
    {
      "pageContent": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 4,\n        resnet_eps: float = 1e-5,\n        resnet_act_fn: str = \"gelu\",\n        resnet_group_size: int = 32,\n        attn_num_head_channels=1,  # attention dim_head\n        cross_attention_dim: int = 768,\n        add_upsample: bool = True,\n        upcast_attention: bool = False,\n    ):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        is_first_block = in_channels == out_channels == temb_channels\n        is_middle_block = in_channels != out_channels\n        add_self_attention = True if is_first_block else False\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = attn_num_head_channels\n\n        # in_channels, and out_channels for the block (k-unet)\n        k_in_channels = out_channels if is_first_block else 2 * out_channels\n        k_out_channels = in_channels\n\n        num_layers = num_layers - 1\n\n        for i in range(num_layers):\n            in_channels = k_in_channels if i == 0 else out_channels\n            groups = in_channels // resnet_group_size\n            groups_out = out_channels // resnet_group_size\n\n            if is_middle_block and (i == num_layers - 1):\n                conv_2d_out_channels = k_out_channels\n            else:\n                conv_2d_out_channels = None\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    conv",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2507, "column": 4 },
          "end": { "row": 2507, "column": 4 }
        }
      }
    }
  ],
  [
    "2665",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        res_hidden_states_tuple,\n        temb=None,\n        encoder_hidden_states=None,\n        cross_attention_kwargs=None,\n        upsample_size=None,\n        attention_mask=None,\n    ):\n        res_hidden_states_tuple = res_hidden_states_tuple[-1]\n        if res_hidden_states_tuple is not None:\n            hidden_states = torch.cat([hidden_states, res_hidden_states_tuple], dim=1)\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(attn, return_dict=False),\n                    hidden_states,\n                    encoder_hidden_states,\n                    attention_mask,\n                    cross_attention_kwargs,\n                )[0]\n            else:\n                hidden_states = resnet(hidden_states, temb)\n                hidden_states = attn(\n                    hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                 ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2590, "column": 4 },
          "end": { "row": 2590, "column": 4 }
        }
      }
    }
  ],
  [
    "2666",
    {
      "pageContent": "class KAttentionBlock(nn.Module):\n    r\"\"\"\n    A basic Transformer block.\n\n    Parameters:\n        dim (`int`): The number of channels in the input and output.\n        num_attention_heads (`int`): The number of heads to use for multi-head attention.\n        attention_head_dim (`int`): The number of channels in each head.\n        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.\n        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n        num_embeds_ada_norm (:\n            obj: `int`, *optional*): The number of diffusion steps used during training. See `Transformer2DModel`.\n        attention_bias (:\n            obj: `bool`, *optional*, defaults to `False`): Configure if the attentions should contain a bias parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_attention_heads: int,\n        attention_head_dim: int,\n        dropout: float = 0.0,\n        cross_attention_dim: Optional[int] = None,\n        attention_bias: bool = False,\n        upcast_attention: bool = False,\n        temb_channels: int = 768,  # for ada_group_norm\n        add_self_attention: bool = False,\n        cross_attention_norm: bool = False,\n        group_size: int = 32,\n    ):\n        super().__init__()\n        self.add_self_attention = add_self_attention\n\n        # 1. Self-Attn\n        if add_self_attention:\n            self.norm1 = AdaGroupNorm(temb_channels, dim, max(1, dim ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2642, "column": 0 },
          "end": { "row": 2642, "column": 0 }
        }
      }
    }
  ],
  [
    "2667",
    {
      "pageContent": "def __init__(\n        self,\n        dim: int,\n        num_attention_heads: int,\n        attention_head_dim: int,\n        dropout: float = 0.0,\n        cross_attention_dim: Optional[int] = None,\n        attention_bias: bool = False,\n        upcast_attention: bool = False,\n        temb_channels: int = 768,  # for ada_group_norm\n        add_self_attention: bool = False,\n        cross_attention_norm: bool = False,\n        group_size: int = 32,\n    ):\n        super().__init__()\n        self.add_self_attention = add_self_attention\n\n        # 1. Self-Attn\n        if add_self_attention:\n            self.norm1 = AdaGroupNorm(temb_channels, dim, max(1, dim // group_size))\n            self.attn1 = CrossAttention(\n                query_dim=dim,\n                heads=num_attention_heads,\n                dim_head=attention_head_dim,\n                dropout=dropout,\n                bias=attention_bias,\n                cross_attention_dim=None,\n                cross_attention_norm=None,\n            )\n\n        # 2. Cross-Attn\n        self.norm2 = AdaGroupNorm(temb_channels, dim, max(1, dim // group_size))\n        self.attn2 = CrossAttention(\n            query_dim=dim,\n            cross_attention_dim=cross_attention_dim,\n            heads=num_attention_heads,\n            dim_head=attention_head_dim,\n            dropout=dropout,\n            bias=attention_bias,\n            upcast_attention=upcast_attention,\n            cross_attention_norm=cross_attention_norm,\n        )",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2659, "column": 4 },
          "end": { "row": 2659, "column": 4 }
        }
      }
    }
  ],
  [
    "2668",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        encoder_hidden_states=None,\n        emb=None,\n        attention_mask=None,\n        cross_attention_kwargs=None,\n    ):\n        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}\n\n        # 1. Self-Attention\n        if self.add_self_attention:\n            norm_hidden_states = self.norm1(hidden_states, emb)\n\n            height, weight = norm_hidden_states.shape[2:]\n            norm_hidden_states = self._to_3d(norm_hidden_states, height, weight)\n\n            attn_output = self.attn1(\n                norm_hidden_states,\n                encoder_hidden_states=None,\n                **cross_attention_kwargs,\n            )\n            attn_output = self._to_4d(attn_output, height, weight)\n\n            hidden_states = attn_output + hidden_states\n\n        # 2. Cross-Attention/None\n        norm_hidden_states = self.norm2(hidden_states, emb)\n\n        height, weight = norm_hidden_states.shape[2:]\n        norm_hidden_states = self._to_3d(norm_hidden_states, height, weight)\n        attn_output = self.attn2(\n            norm_hidden_states,\n            encoder_hidden_states=encoder_hidden_states,\n            **cross_attention_kwargs,\n        )\n        attn_output = self._to_4d(attn_output, height, weight)\n\n        hidden_states = attn_output + hidden_states\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks.py",
        "range": {
          "start": { "row": 2708, "column": 4 },
          "end": { "row": 2708, "column": 4 }
        }
      }
    }
  ],
  [
    "2669",
    {
      "pageContent": "class FlaxCrossAttnDownBlock2D(nn.Module):\n    r\"\"\"\n    Cross Attention 2D Downsizing block - original architecture from Unet transformers:\n    https://arxiv.org/abs/2103.06104\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        out_channels (:obj:`int`):\n            Output channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of attention blocks layers\n        attn_num_head_channels (:obj:`int`, *optional*, defaults to 1):\n            Number of attention heads of each spatial transformer block\n        add_downsample (:obj:`bool`, *optional*, defaults to `True`):\n            Whether to add downsampling layer before each final output\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    out_channels: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    attn_num_head_channels: int = 1\n    add_downsample: bool = True\n    use_linear_projection: bool = False\n    only_cross_attention: bool = False\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        resnets = []\n        attentions = []\n\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            ",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 21, "column": 0 },
          "end": { "row": 21, "column": 0 }
        }
      }
    }
  ],
  [
    "2670",
    {
      "pageContent": "def setup(self):\n        resnets = []\n        attentions = []\n\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n            attn_block = FlaxTransformer2DModel(\n                in_channels=self.out_channels,\n                n_heads=self.attn_num_head_channels,\n                d_head=self.out_channels // self.attn_num_head_channels,\n                depth=1,\n                use_linear_projection=self.use_linear_projection,\n                only_cross_attention=self.only_cross_attention,\n                dtype=self.dtype,\n            )\n            attentions.append(attn_block)\n\n        self.resnets = resnets\n        self.attentions = attentions\n\n        if self.add_downsample:\n            self.downsamplers_0 = FlaxDownsample2D(self.out_channels, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 52, "column": 4 },
          "end": { "row": 52, "column": 4 }
        }
      }
    }
  ],
  [
    "2671",
    {
      "pageContent": "def __call__(self, hidden_states, temb, encoder_hidden_states, deterministic=True):\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states, temb, deterministic=deterministic)\n            hidden_states = attn(hidden_states, encoder_hidden_states, deterministic=deterministic)\n            output_states += (hidden_states,)\n\n        if self.add_downsample:\n            hidden_states = self.downsamplers_0(hidden_states)\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 84, "column": 4 },
          "end": { "row": 84, "column": 4 }
        }
      }
    }
  ],
  [
    "2672",
    {
      "pageContent": "class FlaxDownBlock2D(nn.Module):\n    r\"\"\"\n    Flax 2D downsizing block\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        out_channels (:obj:`int`):\n            Output channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of attention blocks layers\n        add_downsample (:obj:`bool`, *optional*, defaults to `True`):\n            Whether to add downsampling layer before each final output\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    out_channels: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    add_downsample: bool = True\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        resnets = []\n\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n        self.resnets = resnets\n\n        if self.add_downsample:\n            self.downsamplers_0 = FlaxDownsample2D(self.out_channels, dtype=self.dtype)\n\n    def __call__(self, hidden_states, temb, deterministic=True):\n        output_states = ()\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb, deterministic=determinis",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 99, "column": 0 },
          "end": { "row": 99, "column": 0 }
        }
      }
    }
  ],
  [
    "2673",
    {
      "pageContent": "def setup(self):\n        resnets = []\n\n        for i in range(self.num_layers):\n            in_channels = self.in_channels if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=self.out_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n        self.resnets = resnets\n\n        if self.add_downsample:\n            self.downsamplers_0 = FlaxDownsample2D(self.out_channels, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 124, "column": 4 },
          "end": { "row": 124, "column": 4 }
        }
      }
    }
  ],
  [
    "2674",
    {
      "pageContent": "def __call__(self, hidden_states, temb, deterministic=True):\n        output_states = ()\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb, deterministic=deterministic)\n            output_states += (hidden_states,)\n\n        if self.add_downsample:\n            hidden_states = self.downsamplers_0(hidden_states)\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 142, "column": 4 },
          "end": { "row": 142, "column": 4 }
        }
      }
    }
  ],
  [
    "2675",
    {
      "pageContent": "class FlaxCrossAttnUpBlock2D(nn.Module):\n    r\"\"\"\n    Cross Attention 2D Upsampling block - original architecture from Unet transformers:\n    https://arxiv.org/abs/2103.06104\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        out_channels (:obj:`int`):\n            Output channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of attention blocks layers\n        attn_num_head_channels (:obj:`int`, *optional*, defaults to 1):\n            Number of attention heads of each spatial transformer block\n        add_upsample (:obj:`bool`, *optional*, defaults to `True`):\n            Whether to add upsampling layer before each final output\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    out_channels: int\n    prev_output_channel: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    attn_num_head_channels: int = 1\n    add_upsample: bool = True\n    use_linear_projection: bool = False\n    only_cross_attention: bool = False\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        resnets = []\n        attentions = []\n\n        for i in range(self.num_layers):\n            res_skip_channels = self.in_channels if (i == self.num_layers - 1) else self.out_channels\n            resnet_in_channels = self.prev_output_channel if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=resnet_in_channels + res_s",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 156, "column": 0 },
          "end": { "row": 156, "column": 0 }
        }
      }
    }
  ],
  [
    "2676",
    {
      "pageContent": "def setup(self):\n        resnets = []\n        attentions = []\n\n        for i in range(self.num_layers):\n            res_skip_channels = self.in_channels if (i == self.num_layers - 1) else self.out_channels\n            resnet_in_channels = self.prev_output_channel if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=resnet_in_channels + res_skip_channels,\n                out_channels=self.out_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n            attn_block = FlaxTransformer2DModel(\n                in_channels=self.out_channels,\n                n_heads=self.attn_num_head_channels,\n                d_head=self.out_channels // self.attn_num_head_channels,\n                depth=1,\n                use_linear_projection=self.use_linear_projection,\n                only_cross_attention=self.only_cross_attention,\n                dtype=self.dtype,\n            )\n            attentions.append(attn_block)\n\n        self.resnets = resnets\n        self.attentions = attentions\n\n        if self.add_upsample:\n            self.upsamplers_0 = FlaxUpsample2D(self.out_channels, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 188, "column": 4 },
          "end": { "row": 188, "column": 4 }
        }
      }
    }
  ],
  [
    "2677",
    {
      "pageContent": "def __call__(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, deterministic=True):\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = jnp.concatenate((hidden_states, res_hidden_states), axis=-1)\n\n            hidden_states = resnet(hidden_states, temb, deterministic=deterministic)\n            hidden_states = attn(hidden_states, encoder_hidden_states, deterministic=deterministic)\n\n        if self.add_upsample:\n            hidden_states = self.upsamplers_0(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 221, "column": 4 },
          "end": { "row": 221, "column": 4 }
        }
      }
    }
  ],
  [
    "2678",
    {
      "pageContent": "class FlaxUpBlock2D(nn.Module):\n    r\"\"\"\n    Flax 2D upsampling block\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        out_channels (:obj:`int`):\n            Output channels\n        prev_output_channel (:obj:`int`):\n            Output channels from the previous block\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of attention blocks layers\n        add_downsample (:obj:`bool`, *optional*, defaults to `True`):\n            Whether to add downsampling layer before each final output\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    out_channels: int\n    prev_output_channel: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    add_upsample: bool = True\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        resnets = []\n\n        for i in range(self.num_layers):\n            res_skip_channels = self.in_channels if (i == self.num_layers - 1) else self.out_channels\n            resnet_in_channels = self.prev_output_channel if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=resnet_in_channels + res_skip_channels,\n                out_channels=self.out_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n        self.resnets = resnets\n\n        if self.add_upsample:\n            self.upsamplers_0 = Flax",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 237, "column": 0 },
          "end": { "row": 237, "column": 0 }
        }
      }
    }
  ],
  [
    "2679",
    {
      "pageContent": "def setup(self):\n        resnets = []\n\n        for i in range(self.num_layers):\n            res_skip_channels = self.in_channels if (i == self.num_layers - 1) else self.out_channels\n            resnet_in_channels = self.prev_output_channel if i == 0 else self.out_channels\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=resnet_in_channels + res_skip_channels,\n                out_channels=self.out_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n        self.resnets = resnets\n\n        if self.add_upsample:\n            self.upsamplers_0 = FlaxUpsample2D(self.out_channels, dtype=self.dtype)",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 265, "column": 4 },
          "end": { "row": 265, "column": 4 }
        }
      }
    }
  ],
  [
    "2680",
    {
      "pageContent": "def __call__(self, hidden_states, res_hidden_states_tuple, temb, deterministic=True):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = jnp.concatenate((hidden_states, res_hidden_states), axis=-1)\n\n            hidden_states = resnet(hidden_states, temb, deterministic=deterministic)\n\n        if self.add_upsample:\n            hidden_states = self.upsamplers_0(hidden_states)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 285, "column": 4 },
          "end": { "row": 285, "column": 4 }
        }
      }
    }
  ],
  [
    "2681",
    {
      "pageContent": "class FlaxUNetMidBlock2DCrossAttn(nn.Module):\n    r\"\"\"\n    Cross Attention 2D Mid-level block - original architecture from Unet transformers: https://arxiv.org/abs/2103.06104\n\n    Parameters:\n        in_channels (:obj:`int`):\n            Input channels\n        dropout (:obj:`float`, *optional*, defaults to 0.0):\n            Dropout rate\n        num_layers (:obj:`int`, *optional*, defaults to 1):\n            Number of attention blocks layers\n        attn_num_head_channels (:obj:`int`, *optional*, defaults to 1):\n            Number of attention heads of each spatial transformer block\n        dtype (:obj:`jnp.dtype`, *optional*, defaults to jnp.float32):\n            Parameters `dtype`\n    \"\"\"\n    in_channels: int\n    dropout: float = 0.0\n    num_layers: int = 1\n    attn_num_head_channels: int = 1\n    use_linear_projection: bool = False\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        # there is always at least one resnet\n        resnets = [\n            FlaxResnetBlock2D(\n                in_channels=self.in_channels,\n                out_channels=self.in_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n        ]\n\n        attentions = []\n\n        for _ in range(self.num_layers):\n            attn_block = FlaxTransformer2DModel(\n                in_channels=self.in_channels,\n                n_heads=self.attn_num_head_channels,\n                d_head=self.in_channels // self.attn_num_head_channels,\n                depth=1,\n                use_linear_projection=self.use_linear_projection,\n                dtype=self.dt",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 300, "column": 0 },
          "end": { "row": 300, "column": 0 }
        }
      }
    }
  ],
  [
    "2682",
    {
      "pageContent": "def setup(self):\n        # there is always at least one resnet\n        resnets = [\n            FlaxResnetBlock2D(\n                in_channels=self.in_channels,\n                out_channels=self.in_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n        ]\n\n        attentions = []\n\n        for _ in range(self.num_layers):\n            attn_block = FlaxTransformer2DModel(\n                in_channels=self.in_channels,\n                n_heads=self.attn_num_head_channels,\n                d_head=self.in_channels // self.attn_num_head_channels,\n                depth=1,\n                use_linear_projection=self.use_linear_projection,\n                dtype=self.dtype,\n            )\n            attentions.append(attn_block)\n\n            res_block = FlaxResnetBlock2D(\n                in_channels=self.in_channels,\n                out_channels=self.in_channels,\n                dropout_prob=self.dropout,\n                dtype=self.dtype,\n            )\n            resnets.append(res_block)\n\n        self.resnets = resnets\n        self.attentions = attentions",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 323, "column": 4 },
          "end": { "row": 323, "column": 4 }
        }
      }
    }
  ],
  [
    "2683",
    {
      "pageContent": "def __call__(self, hidden_states, temb, encoder_hidden_states, deterministic=True):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            hidden_states = attn(hidden_states, encoder_hidden_states, deterministic=deterministic)\n            hidden_states = resnet(hidden_states, temb, deterministic=deterministic)\n\n        return hidden_states",
      "metadata": {
        "source": "src/diffusers/models/unet_2d_blocks_flax.py",
        "range": {
          "start": { "row": 358, "column": 4 },
          "end": { "row": 358, "column": 4 }
        }
      }
    }
  ],
  [
    "2684",
    {
      "pageContent": "class PriorTransformerOutput(BaseOutput):\n    \"\"\"\n    Args:\n        predicted_image_embedding (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`):\n            The predicted CLIP image embedding conditioned on the CLIP text embedding input.\n    \"\"\"\n\n    predicted_image_embedding: torch.FloatTensor",
      "metadata": {
        "source": "src/diffusers/models/prior_transformer.py",
        "range": {
          "start": { "row": 15, "column": 0 },
          "end": { "row": 15, "column": 0 }
        }
      }
    }
  ],
  [
    "2685",
    {
      "pageContent": "class PriorTransformer(ModelMixin, ConfigMixin):\n    \"\"\"\n    The prior transformer from unCLIP is used to predict CLIP image embeddings from CLIP text embeddings. Note that the\n    transformer predicts the image embeddings through a denoising diffusion process.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the models (such as downloading or saving, etc.)\n\n    For more details, see the original paper: https://arxiv.org/abs/2204.06125\n\n    Parameters:\n        num_attention_heads (`int`, *optional*, defaults to 32): The number of heads to use for multi-head attention.\n        attention_head_dim (`int`, *optional*, defaults to 64): The number of channels in each head.\n        num_layers (`int`, *optional*, defaults to 20): The number of layers of Transformer blocks to use.\n        embedding_dim (`int`, *optional*, defaults to 768): The dimension of the CLIP embeddings. Note that CLIP\n            image embeddings and text embeddings are both the same dimension.\n        num_embeddings (`int`, *optional*, defaults to 77): The max number of clip embeddings allowed. I.e. the\n            length of the prompt after it has been tokenized.\n        additional_embeddings (`int`, *optional*, defaults to 4): The number of additional tokens appended to the\n            projected hidden_states. The actual length of the used hidden_states is `num_embeddings +\n            additional_embeddings`.\n        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n\n    \"\"\"\n\n    @regist",
      "metadata": {
        "source": "src/diffusers/models/prior_transformer.py",
        "range": {
          "start": { "row": 25, "column": 0 },
          "end": { "row": 25, "column": 0 }
        }
      }
    }
  ],
  [
    "2686",
    {
      "pageContent": "def forward(\n        self,\n        hidden_states,\n        timestep: Union[torch.Tensor, float, int],\n        proj_embedding: torch.FloatTensor,\n        encoder_hidden_states: torch.FloatTensor,\n        attention_mask: Optional[torch.BoolTensor] = None,\n        return_dict: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`):\n                x_t, the currently predicted image embeddings.\n            timestep (`torch.long`):\n                Current denoising step.\n            proj_embedding (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`):\n                Projected embedding vector the denoising process is conditioned on.\n            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_embeddings, embedding_dim)`):\n                Hidden states of the text embeddings the denoising process is conditioned on.\n            attention_mask (`torch.BoolTensor` of shape `(batch_size, num_embeddings)`):\n                Text mask for the text embeddings.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`models.prior_transformer.PriorTransformerOutput`] instead of a plain\n                tuple.\n\n        Returns:\n            [`~models.prior_transformer.PriorTransformerOutput`] or `tuple`:\n            [`~models.prior_transformer.PriorTransformerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        batch_size = hidden_states.",
      "metadata": {
        "source": "src/diffusers/models/prior_transformer.py",
        "range": {
          "start": { "row": 106, "column": 4 },
          "end": { "row": 106, "column": 4 }
        }
      }
    }
  ],
  [
    "2687",
    {
      "pageContent": "class AttnProcsLayers(torch.nn.Module):\n    def __init__(self, state_dict: Dict[str, torch.Tensor]):\n        super().__init__()\n        self.layers = torch.nn.ModuleList(state_dict.values())\n        self.mapping = {k: v for k, v in enumerate(state_dict.keys())}\n        self.rev_mapping = {v: k for k, v in enumerate(state_dict.keys())}\n\n        # we add a hook to state_dict() and load_state_dict() so that the\n        # naming fits with `unet.attn_processors`\n        def map_to(module, state_dict, *args, **kwargs):\n            new_state_dict = {}\n            for key, value in state_dict.items():\n                num = int(key.split(\".\")[1])  # 0 is always \"layers\"\n                new_key = key.replace(f\"layers.{num}\", module.mapping[num])\n                new_state_dict[new_key] = value\n\n            return new_state_dict\n\n        def map_from(module, state_dict, *args, **kwargs):\n            all_keys = list(state_dict.keys())\n            for key in all_keys:\n                replace_key = key.split(\".processor\")[0] + \".processor\"\n                new_key = key.replace(replace_key, f\"layers.{module.rev_mapping[replace_key]}\")\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n\n        self._register_state_dict_hook(map_to)\n        self._register_load_state_dict_pre_hook(map_from, with_module=True)",
      "metadata": {
        "source": "src/diffusers/loaders.py",
        "range": {
          "start": { "row": 35, "column": 0 },
          "end": { "row": 35, "column": 0 }
        }
      }
    }
  ],
  [
    "2688",
    {
      "pageContent": "def __init__(self, state_dict: Dict[str, torch.Tensor]):\n        super().__init__()\n        self.layers = torch.nn.ModuleList(state_dict.values())\n        self.mapping = {k: v for k, v in enumerate(state_dict.keys())}\n        self.rev_mapping = {v: k for k, v in enumerate(state_dict.keys())}\n\n        # we add a hook to state_dict() and load_state_dict() so that the\n        # naming fits with `unet.attn_processors`\n        def map_to(module, state_dict, *args, **kwargs):\n            new_state_dict = {}\n            for key, value in state_dict.items():\n                num = int(key.split(\".\")[1])  # 0 is always \"layers\"\n                new_key = key.replace(f\"layers.{num}\", module.mapping[num])\n                new_state_dict[new_key] = value\n\n            return new_state_dict\n\n        def map_from(module, state_dict, *args, **kwargs):\n            all_keys = list(state_dict.keys())\n            for key in all_keys:\n                replace_key = key.split(\".processor\")[0] + \".processor\"\n                new_key = key.replace(replace_key, f\"layers.{module.rev_mapping[replace_key]}\")\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n\n        self._register_state_dict_hook(map_to)\n        self._register_load_state_dict_pre_hook(map_from, with_module=True)",
      "metadata": {
        "source": "src/diffusers/loaders.py",
        "range": {
          "start": { "row": 36, "column": 4 },
          "end": { "row": 36, "column": 4 }
        }
      }
    }
  ],
  [
    "2689",
    {
      "pageContent": "class UNet2DConditionLoadersMixin:\n    def load_attn_procs(self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs):\n        r\"\"\"\n        Load pretrained attention processor layers into `UNet2DConditionModel`. Attention processor layers have to be\n        defined in\n        [cross_attention.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py)\n        and be a `torch.nn.Module` class.\n\n        <Tip warning={true}>\n\n            This function is experimental and might change in the future.\n\n        </Tip>\n\n        Parameters:\n            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n                Can be either:\n\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n                      Valid model ids should have an organization name, like `google/ddpm-celebahq-256`.\n                    - A path to a *directory* containing model weights saved using [`~ModelMixin.save_config`], e.g.,\n                      `./my_model_directory/`.\n                    - A [torch state\n                      dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to forc",
      "metadata": {
        "source": "src/diffusers/loaders.py",
        "range": {
          "start": { "row": 65, "column": 0 },
          "end": { "row": 65, "column": 0 }
        }
      }
    }
  ],
  [
    "2690",
    {
      "pageContent": "def load_attn_procs(self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs):\n        r\"\"\"\n        Load pretrained attention processor layers into `UNet2DConditionModel`. Attention processor layers have to be\n        defined in\n        [cross_attention.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py)\n        and be a `torch.nn.Module` class.\n\n        <Tip warning={true}>\n\n            This function is experimental and might change in the future.\n\n        </Tip>\n\n        Parameters:\n            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n                Can be either:\n\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n                      Valid model ids should have an organization name, like `google/ddpm-celebahq-256`.\n                    - A path to a *directory* containing model weights saved using [`~ModelMixin.save_config`], e.g.,\n                      `./my_model_directory/`.\n                    - A [torch state\n                      dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weight",
      "metadata": {
        "source": "src/diffusers/loaders.py",
        "range": {
          "start": { "row": 66, "column": 4 },
          "end": { "row": 66, "column": 4 }
        }
      }
    }
  ],
  [
    "2691",
    {
      "pageContent": "def save_attn_procs(\n        self,\n        save_directory: Union[str, os.PathLike],\n        is_main_process: bool = True,\n        weights_name: str = None,\n        save_function: Callable = None,\n        safe_serialization: bool = False,\n    ):\n        r\"\"\"\n        Save an attention processor to a directory, so that it can be re-loaded using the\n        `[`~loaders.UNet2DConditionLoadersMixin.load_attn_procs`]` method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n            is_main_process (`bool`, *optional*, defaults to `True`):\n                Whether the process calling this is the main process or not. Useful when in distributed training like\n                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n                the main process to avoid race conditions.\n            save_function (`Callable`):\n                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n                need to replace `torch.save` by another method. Can be configured with the environment variable\n                `DIFFUSERS_SAVE_MODE`.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n            return\n\n        if save_function is None:\n            if safe_serialization:\n\n                def save_function(weights, filename):\n                    return safetensors.torch.save_file(",
      "metadata": {
        "source": "src/diffusers/loaders.py",
        "range": {
          "start": { "row": 223, "column": 4 },
          "end": { "row": 223, "column": 4 }
        }
      }
    }
  ]
]
